WEBVTT
Kind: captions
Language: en

00:00:04.760 --> 00:00:09.570我们开始吧。所以欢迎来到最后一堂课。00:00:09.570 --> 00:00:11.475我希望你们都能活下来00:00:11.475 --> 00:00:13.830呃，结束你的项目。00:00:13.830 --> 00:00:18.540所以今天我们将听到关于NLP和深度学习的未来。00:00:18.540 --> 00:00:22.440呃，所以克里斯还在旅行，今天我们要请凯文・克拉克，00:00:22.440 --> 00:00:24.825谁是实验室的博士生，呃，00:00:24.825 --> 00:00:26.580在NLP实验室，00:00:26.580 --> 00:00:29.610他也是去年班上的助教之一。00:00:29.610 --> 00:00:31.785所以他对整个班级都很熟悉。00:00:31.785 --> 00:00:33.765嗯，那么，把它拿走，凯文。00:00:33.765 --> 00:00:37.830可以。谢谢，艾比。嗯，是的，00:00:37.830 --> 00:00:40.440去年成为助教后能回来真是太好了。00:00:40.440 --> 00:00:45.350嗯，今天我真的很高兴能谈论深入学习和NLP的未来。00:00:45.350 --> 00:00:49.090嗯，很明显，试图预测未来，嗯，00:00:49.090 --> 00:00:51.800对于深入学习或在那个空间的任何事情00:00:51.800 --> 00:00:54.800很难，因为这个领域变化非常快。00:00:54.800 --> 00:00:57.080作为一个参考点，00:00:57.080 --> 00:01:00.050让我们看看NLP的深入学习，00:01:00.050 --> 00:01:02.290嗯，大概五年前吧。00:01:02.290 --> 00:01:08.300事实上，很多想法现在被认为是非常核心的技术，00:01:08.300 --> 00:01:10.445嗯，当我们想到深度学习和NLP时，00:01:10.445 --> 00:01:12.170嗯，那时候根本不存在。00:01:12.170 --> 00:01:14.870嗯，你在这节课上学到的东西，比如seq2seq，00:01:14.870 --> 00:01:17.180注意机制，嗯，大规模的，00:01:17.180 --> 00:01:20.105阅读理解，呃，甚至框架00:01:20.105 --> 00:01:23.300比如TensorFlow或者Pytorch，嗯，不存在。00:01:23.300 --> 00:01:27.145我想说的是，00:01:27.145 --> 00:01:31.205因此，很难展望未来并说，00:01:31.205 --> 00:01:33.665好吧，事情会是怎样的？00:01:33.665 --> 00:01:38.065嗯，我想我们能做的是看看，嗯，00:01:38.065 --> 00:01:41.870现在确实有点起色的地方，嗯，00:01:41.870 --> 00:01:43.640所以在这些区域，嗯，00:01:43.640 --> 00:01:46.370最近有很多，很多成功，还有一些，呃，00:01:46.370 --> 00:01:48.100从那个项目，那个，00:01:48.100 --> 00:01:50.900这些领域在未来可能会很重要。00:01:50.900 --> 00:01:55.820嗯，在这次谈话中，我将主要集中在00:01:55.820 --> 00:01:58.970wh-关键理念，即杠杆的理念00:01:58.970 --> 00:02:02.915培训我们的NLP系统时未标记的示例。00:02:02.915 --> 00:02:07.490所以我想谈一下机器翻译的事，嗯，00:02:07.490 --> 00:02:10.820提高翻译质量，甚至00:02:10.820 --> 00:02:14.315以无人监督的方式进行翻译。00:02:14.315 --> 00:02:16.170这意味着你没有，嗯，00:02:16.170 --> 00:02:19.230配对句子，呃，和，还有他们的翻译。00:02:19.230 --> 00:02:23.365嗯，你只能从单语语料库中学习翻译模型。00:02:23.365 --> 00:02:27.115嗯，我要说的第二件事是，呃，00:02:27.115 --> 00:02:29.330Openai的GPT-2，嗯，00:02:29.330 --> 00:02:32.435总的来说，这一现象实际上在扩大，00:02:32.435 --> 00:02:34.045嗯，深度学习模式。00:02:34.045 --> 00:02:38.330嗯，我知道你在关于上下文表示的讲座中看到了一点，00:02:38.330 --> 00:02:40.340但这一点，但这将更深入一点。00:02:40.340 --> 00:02:42.335嗯，我想，嗯，00:02:42.335 --> 00:02:46.655全国人民党的这些新进展，00:02:46.655 --> 00:02:48.600嗯，相当大，呃，00:02:48.600 --> 00:02:50.595影响方面：00:02:50.595 --> 00:02:53.755呃，更广泛地说，甚至超越了我们使用的技术，00:02:53.755 --> 00:02:55.070尤其是，我的意思是，00:02:55.070 --> 00:03:00.555开始越来越多地关注NLP的社会影响，嗯，00:03:00.555 --> 00:03:03.520两者都是，嗯，在我们的模型可以做什么和实物方面00:03:03.520 --> 00:03:06.590关于人们想要应用这些模型的计划，嗯，00:03:06.590 --> 00:03:09.755我认为这确实有一些风险，嗯，00:03:09.755 --> 00:03:13.160在安全方面，也在偏见等方面。00:03:13.160 --> 00:03:16.465嗯，我还要谈谈未来的研究领域，00:03:16.465 --> 00:03:19.140嗯，这些都是研究领域，现在，嗯，00:03:19.140 --> 00:03:22.060在过去的一年里00:03:22.060 --> 00:03:27.180有希望的领域，我希望它们在未来仍将是重要的。00:03:27.190 --> 00:03:29.700好吧，嗯，首先，00:03:29.700 --> 00:03:33.305我想问这个问题，为什么深度学习最近如此成功？00:03:33.305 --> 00:03:35.510嗯，我喜欢这个漫画，00:03:35.510 --> 00:03:38.045这里有一个统计学习者，00:03:38.045 --> 00:03:41.030嗯，他们有一些非常复杂的，00:03:41.030 --> 00:03:44.015嗯，动机很好，嗯，做的方法，嗯，00:03:44.015 --> 00:03:45.510他们关心的任务，00:03:45.510 --> 00:03:47.460然后神经网络的人说，00:03:47.460 --> 00:03:49.110呃，叠更多层。00:03:49.110 --> 00:03:52.020嗯，所以，我想说的是，00:03:52.020 --> 00:03:56.075深度学习最近没有成功，因为它更00:03:56.075 --> 00:04:01.530理论上是有动机的，或者比以前的技术更复杂。00:04:01.530 --> 00:04:04.245事实上，我会说，事实上，很多，嗯，00:04:04.245 --> 00:04:06.635旧的统计方法有更多的00:04:06.635 --> 00:04:10.175这是一个理论基础，而不是我们在深度学习中的一些技巧。00:04:10.175 --> 00:04:14.050嗯，真正让深度学习如此00:04:14.050 --> 00:04:17.660近年来成功的是它的扩展能力，对吧。00:04:17.660 --> 00:04:22.310所以神经网络，当我们增加数据的大小，00:04:22.310 --> 00:04:24.265当我们增加模型的尺寸时，00:04:24.265 --> 00:04:26.170他们的精准度大大提高，00:04:26.170 --> 00:04:28.565其他方法则不然。00:04:28.565 --> 00:04:32.210如果你看看80年代和90年代，00:04:32.210 --> 00:04:36.190实际上，在神经网络方面有很多研究正在进行，嗯。00:04:36.190 --> 00:04:39.130但它没有，也没有像它那样的大肆宣传00:04:39.130 --> 00:04:42.205现在看来，这可能是因为，00:04:42.205 --> 00:04:45.020嗯，过去没有，嗯，00:04:45.020 --> 00:04:47.180同样的资源在计算机方面，00:04:47.180 --> 00:04:49.235在数据方面，嗯，00:04:49.235 --> 00:04:53.120直到现在，在我们到达一个拐点之后00:04:53.120 --> 00:04:55.220真正利用00:04:55.220 --> 00:04:57.965我们的深度学习模式，我们开始看到它变成，00:04:57.965 --> 00:05:01.520嗯，一个非常成功的机器学习范例。00:05:01.520 --> 00:05:04.080嗯，如果我们看大，呃，00:05:04.080 --> 00:05:06.075深入学习成功故事，嗯，00:05:06.075 --> 00:05:10.200我想，呃，你能看到这种想法的效果，对吧？00:05:10.200 --> 00:05:16.490所以这里有三个可以说是最著名的深度学习的成功，对吧。00:05:16.490 --> 00:05:18.620所以有图像识别，以前，00:05:18.620 --> 00:05:20.870人们使用的是高度工程化的，嗯，00:05:20.870 --> 00:05:25.870分类图像的特征，现在的神经网络比那些方法优越得多。00:05:25.870 --> 00:05:29.785嗯，机器翻译确实缩小了，嗯，00:05:29.785 --> 00:05:33.020基于短语的系统和人的素质翻译，00:05:33.020 --> 00:05:35.730所以这被广泛应用于谷歌翻译等领域00:05:35.730 --> 00:05:39.115而且在过去的五年里，质量实际上变得更好了。00:05:39.115 --> 00:05:43.550嗯，另一个大肆宣传的例子是玩游戏，所以，嗯，00:05:43.550 --> 00:05:46.460有关于阿塔里游戏的工作，还有阿尔法戈，00:05:46.460 --> 00:05:50.395呃，最近有阿尔法斯塔和Openai五号。00:05:50.395 --> 00:05:53.599嗯，如果你看下面三个案例00:05:53.599 --> 00:05:57.200这些成功确实是大量的数据，对吧。00:05:57.200 --> 00:05:58.550对于ImageNet，嗯，00:05:58.550 --> 00:06:00.020对于图像识别，嗯，00:06:00.020 --> 00:06:03.035有IMAGENET数据集，它有1400万个图像，00:06:03.035 --> 00:06:06.320嗯，机器翻译数据集通常有数百万个例子。00:06:06.320 --> 00:06:09.275嗯，玩游戏你可以00:06:09.275 --> 00:06:12.470生成尽可能多的培训数据，00:06:12.470 --> 00:06:14.690嗯，只要运行你的代理，00:06:14.690 --> 00:06:16.040嗯，在比赛中，00:06:16.040 --> 00:06:18.660嗯，一遍又一遍。00:06:19.120 --> 00:06:21.360嗯，如果我们，00:06:21.360 --> 00:06:23.590如果我们看NLP，嗯，00:06:23.590 --> 00:06:27.740对于很多任务来说，这个故事有点不同，嗯，对吧。00:06:27.740 --> 00:06:32.030所以，如果你看一些非常核心的流行任务，00:06:32.030 --> 00:06:35.060比如说，英语阅读理解，嗯，00:06:35.060 --> 00:06:39.710像Squad这样的数据集大约有100000个例子00:06:39.710 --> 00:06:44.810这远远低于数百万或数千万个例子，00:06:44.810 --> 00:06:47.105嗯，这些以前的，00:06:47.105 --> 00:06:50.285嗯，成功已经从中受益。00:06:50.285 --> 00:06:54.210嗯，那当然只是英语，对吧。00:06:54.210 --> 00:06:55.770嗯，有，嗯，00:06:55.770 --> 00:06:59.570成千上万种其他语言，我想这就是00:06:59.570 --> 00:07:03.770当前存在的NLP数据问题。00:07:03.770 --> 00:07:06.455嗯，绝大多数数据都是英文的，嗯，00:07:06.455 --> 00:07:10.070当世界人口的实际比例低于10%时，00:07:10.070 --> 00:07:12.190嗯，把英语作为他们的第一语言。00:07:12.190 --> 00:07:17.565嗯，所以这些小数据集的问题只有当你看，00:07:17.565 --> 00:07:21.465嗯，各种各样的语言，嗯，它们都存在。00:07:21.465 --> 00:07:23.955嗯，那么，我们该怎么做，00:07:23.955 --> 00:07:25.800呃，当我们受到这些数据的限制时，00:07:25.800 --> 00:07:30.560但我们想利用深度学习的规模，训练我们能训练的最大的模型。00:07:30.560 --> 00:07:32.510嗯，流行的解决方案，嗯，00:07:32.510 --> 00:07:36.230尤其是最近的成功是使用未标记的数据，嗯，00:07:36.230 --> 00:07:37.805因为与标签数据不同，00:07:37.805 --> 00:07:40.840未标记的数据很容易为语言获取。00:07:40.840 --> 00:07:42.120嗯，你可以上网，00:07:42.120 --> 00:07:44.685你可以去看书，你可以得到很多文本，嗯，00:07:44.685 --> 00:07:49.370而标记数据通常至少需要众包示例。00:07:49.370 --> 00:07:54.730嗯，在某些情况下，你甚至需要一个像语言学这样的专家，00:07:54.730 --> 00:07:57.820嗯，到，来注释数据。00:07:59.510 --> 00:08:03.890好吧，那么，嗯，第一部分的谈话将要用到00:08:03.890 --> 00:08:08.195利用未标记数据改进NLP模型的想法，00:08:08.195 --> 00:08:11.070嗯，完成机器翻译的任务。00:08:11.990 --> 00:08:15.170嗯，我们来谈谈机器翻译数据。00:08:15.170 --> 00:08:20.520嗯，确实存在相当大的机器翻译数据集。00:08:20.520 --> 00:08:23.165嗯，这些数据集不存在是因为00:08:23.165 --> 00:08:26.870NLP的研究人员为了训练他们的模型，对文本进行了注释，对吧。00:08:26.870 --> 00:08:29.750它们的存在是因为，呃，在不同的环境中，00:08:29.750 --> 00:08:33.195翻译是因为它很有用，例如，00:08:33.195 --> 00:08:35.070欧洲议会议事录，00:08:35.070 --> 00:08:37.019嗯，联合国会议记录，00:08:37.019 --> 00:08:41.320嗯，一些新闻网站，他们把文章翻译成多种语言。00:08:41.320 --> 00:08:46.610嗯，实际上，我们用来训练模型的机器翻译数据通常是00:08:46.610 --> 00:08:52.745更多需要翻译的现有案例的副产品，而不是，00:08:52.745 --> 00:08:57.500嗯，这是我们在世界上看到的那种文本的完整样本。00:08:57.500 --> 00:08:58.910嗯，这意味着第一，00:08:58.910 --> 00:09:00.680它在领域上是相当有限的，对吧。00:09:00.680 --> 00:09:03.580所以很难找到翻译的tweets，00:09:03.580 --> 00:09:05.410嗯，除非你正好为Twitter工作。00:09:05.410 --> 00:09:08.145嗯，除此之外，嗯，00:09:08.145 --> 00:09:12.230在所涵盖的语言方面有局限性，对吧。00:09:12.230 --> 00:09:14.750一些语言，比如欧洲语言，00:09:14.750 --> 00:09:16.500有很多翻译数据，嗯，00:09:16.500 --> 00:09:19.180对于其他语言来说，这要少得多。00:09:19.180 --> 00:09:22.040嗯，所以在我们想处理的这些环境中00:09:22.040 --> 00:09:25.220一个不同的领域或者我们想使用低资源语言的地方，00:09:25.220 --> 00:09:27.995嗯，我们受到标记数据的限制，嗯，00:09:27.995 --> 00:09:30.985但是我们可以很容易地找到未标记的数据。00:09:30.985 --> 00:09:33.620嗯，这实际上是一个很好解决的问题，嗯，00:09:33.620 --> 00:09:37.010也许不是100%，但我们可以很准确地观察00:09:37.010 --> 00:09:41.195一些文本，决定它所使用的语言，并训练分类器来实现这一点。00:09:41.195 --> 00:09:43.610嗯，这意味着很容易找到00:09:43.610 --> 00:09:46.100你所关心的任何语言的数据，因为你可以继续00:09:46.100 --> 00:09:48.440在网络上搜索数据00:09:48.440 --> 00:09:52.650并获得大量的单语数据。00:09:55.240 --> 00:10:00.775好吧，嗯，我现在要进入第一种方法，00:10:00.775 --> 00:10:03.095嗯，我要谈谈使用00:10:03.095 --> 00:10:06.365未标记数据以改进机器翻译模型。00:10:06.365 --> 00:10:09.410这项技术叫做预训练00:10:09.410 --> 00:10:12.790真的让人想起一些想法，比如，嗯，埃尔莫。00:10:12.790 --> 00:10:16.580嗯，我们的想法是通过语言建模进行预先培训。00:10:16.580 --> 00:10:18.350如果我们有，嗯，00:10:18.350 --> 00:10:21.350我们有两种语言想翻译，00:10:21.350 --> 00:10:22.535嗯，从一端到另一端，00:10:22.535 --> 00:10:27.480我们将为这两种语言收集大型数据集，然后我们可以训练，00:10:27.480 --> 00:10:29.040嗯，两种语言模式，00:10:29.040 --> 00:10:33.365每个数据上都有一个，然后，嗯，00:10:33.365 --> 00:10:34.490我们可以用这些，呃，00:10:34.490 --> 00:10:38.450预先训练的语言模型作为机器翻译系统的初始化。00:10:38.450 --> 00:10:41.720嗯，所以编码器将被初始化为00:10:41.720 --> 00:10:45.485在源端语言上训练的语言模型的权重，嗯，00:10:45.485 --> 00:10:49.830解码器将被初始化为目标大小语言，呃，00:10:49.830 --> 00:10:51.225这会，嗯，00:10:51.225 --> 00:10:55.490提高你的模型的性能，因为在这个预培训期间，00:10:55.490 --> 00:10:59.750我们希望我们的语言模型能够学习有用的信息，比如，00:10:59.750 --> 00:11:02.460单词的意思，或者，呃，呃，00:11:02.460 --> 00:11:05.250语言的结构，嗯，00:11:05.250 --> 00:11:09.020他们正在处理，嗯，这个可以，呃，00:11:09.020 --> 00:11:12.410帮助机器翻译模型，00:11:12.410 --> 00:11:15.020嗯，当我们调整好的时候。00:11:15.020 --> 00:11:17.464嗯，让我停下来问一下有没有问题，00:11:17.464 --> 00:11:18.620总的来说，感觉，00:11:18.620 --> 00:11:24.870在整个演讲过程中，请随意提问。可以。00:11:25.920 --> 00:11:33.385所以，这里有一个情节展示了这种预训练技术的一些结果。00:11:33.385 --> 00:11:36.040嗯，这是英语到德语的翻译。00:11:36.040 --> 00:11:39.805呃，x轴是多少训练数据，00:11:39.805 --> 00:11:41.920在无监督的培训数据中，嗯，00:11:41.920 --> 00:11:43.075你提供这些模型，00:11:43.075 --> 00:11:45.355当然，它们也有大量的00:11:45.355 --> 00:11:48.940此培训前步骤的单语数据。00:11:48.940 --> 00:11:51.970你可以看到这个很好用，对吧？00:11:51.970 --> 00:11:54.445所以你有两个蓝点，嗯，00:11:54.445 --> 00:11:57.670提高性能，这就是蓝线以上的红线，00:11:57.670 --> 00:12:00.175嗯，做这种训练前技巧的时候。00:12:00.175 --> 00:12:01.689不足为奇，00:12:01.689 --> 00:12:06.740当标记的数据量很小时，此增益尤其大。00:12:10.350 --> 00:12:14.080嗯，有个问题，00:12:14.080 --> 00:12:17.260呃，我想谈的是培训前，也就是说，00:12:17.260 --> 00:12:18.850在预训中，你有00:12:18.850 --> 00:12:20.890这两种不同的语言模式00:12:20.890 --> 00:12:23.035两人之间的任何互动，00:12:23.035 --> 00:12:25.780当你在未标记的语料库上运行它们时。00:12:25.780 --> 00:12:28.435嗯，这是一个简单的技巧，嗯，00:12:28.435 --> 00:12:32.485试图解决这个问题，这叫做自我训练。00:12:32.485 --> 00:12:37.090嗯，这个想法来自我们的单语语料库，00:12:37.090 --> 00:12:40.210所以在这个例子中，“我去过比利时”，这是一个英语句子。00:12:40.210 --> 00:12:45.400嗯，这句话我们没有人工翻译，00:12:45.400 --> 00:12:48.925但我们能做的是运行我们的机器翻译模型，00:12:48.925 --> 00:12:52.750我们会得到目标语言的翻译。00:12:52.750 --> 00:12:56.320嗯，既然这是机器学习模型的结果，那就不完美了，00:12:56.320 --> 00:13:00.160但是我们可以希望我们的模型仍然可以从这种模式中学习。00:13:00.160 --> 00:13:03.580有噪音标签的例子，对吗？00:13:03.580 --> 00:13:05.275所以我们，我们治疗，嗯，00:13:05.275 --> 00:13:08.230我们原来的单语句子和它的机器00:13:08.230 --> 00:13:12.490翻译就像是人类提供的翻译，00:13:12.490 --> 00:13:17.030嗯，在这个例子中训练我们的机器学习模型。00:13:19.800 --> 00:13:24.190嗯，我觉得这看起来很奇怪，实际上00:13:24.190 --> 00:13:27.970当你第一次看到它是因为它看起来是圆形的，对吗？00:13:27.970 --> 00:13:31.315所以如果你看这个，呃，呃，00:13:31.315 --> 00:13:33.850模型训练的翻译00:13:33.850 --> 00:13:38.095生产实际上正是它已经开始生产的，00:13:38.095 --> 00:13:43.420对，因为，嗯，这个翻译首先来自我们的模型。00:13:43.420 --> 00:13:45.700嗯，实际上在实践中，00:13:45.700 --> 00:13:49.480由于这个问题，这不是一种广泛使用的技术，00:13:49.480 --> 00:13:53.365嗯，但它激发了另一种称为“反向翻译”的技术。00:13:53.365 --> 00:13:56.740这种技术非常流行，嗯，00:13:56.740 --> 00:13:59.950解决这个问题的方法，嗯，00:13:59.950 --> 00:14:04.240这在使用未标记的数据进行翻译方面取得了很大的成功。00:14:04.240 --> 00:14:06.940所以这是方法，而不仅仅是00:14:06.940 --> 00:14:10.855拥有从源语言到目标语言的翻译系统，00:14:10.855 --> 00:14:13.210嗯，我们还要训练一个00:14:13.210 --> 00:14:16.375从目标语言到源语言。00:14:16.375 --> 00:14:18.670在这种情况下，如果，00:14:18.670 --> 00:14:21.340如果一天结束的时候我们想要一个法式到英式的模特，嗯，00:14:21.340 --> 00:14:24.910我们首先要训练一个从英语到法语的模式。00:14:24.910 --> 00:14:27.880然后我们可以做一些类似于自我标记的事情。00:14:27.880 --> 00:14:30.205所以我们选一个英语句子。00:14:30.205 --> 00:14:33.370我们运行我们的英语到法语模式和翻译。00:14:33.370 --> 00:14:35.950与我们以前所做的不同之处在于00:14:35.950 --> 00:14:38.500我们实际上要切换源端和目标端。00:14:38.500 --> 00:14:42.640所以在这个例子中，法语句子是源序列。00:14:42.640 --> 00:14:45.985呃，目标序列是，嗯，00:14:45.985 --> 00:14:50.740我们最初的英语句子来自单语语料库。00:14:50.740 --> 00:14:52.165现在我们正在训练语言，00:14:52.165 --> 00:14:54.040机器翻译系统00:14:54.040 --> 00:14:57.265另一个方向是法语到英语。00:14:57.265 --> 00:15:00.445嗯，那么，为什么我们认为这会更好呢？00:15:00.445 --> 00:15:02.320嗯，第一，嗯，00:15:02.320 --> 00:15:05.230训练不再有这种循环00:15:05.230 --> 00:15:10.210因为正在训练的模型是完全不同模型的输出。00:15:10.210 --> 00:15:14.845嗯，另一件我认为非常重要的事情是，00:15:14.845 --> 00:15:18.970嗯，翻译，模型被训练来制作。00:15:18.970 --> 00:15:21.520所以解码器实际上正在学习的东西00:15:21.520 --> 00:15:24.430生成从不坏的翻译，对吗？00:15:24.430 --> 00:15:26.574所以如果你看这个例子，00:15:26.574 --> 00:15:29.545法语到英语模式的目标序列，00:15:29.545 --> 00:15:31.165我去过比利时，嗯，00:15:31.165 --> 00:15:34.645最初来自单语语料库。00:15:34.645 --> 00:15:37.420嗯，所以我认为凭直觉这是有道理的00:15:37.420 --> 00:15:40.435如果我们想培养一个好的翻译模式，00:15:40.435 --> 00:15:44.620嗯，把它暴露在嘈杂的输入信号中可能没关系。00:15:44.620 --> 00:15:47.515所以我们把它暴露在一个英语和法语系统的输出中，00:15:47.515 --> 00:15:48.730它可能并不完美。00:15:48.730 --> 00:15:52.330嗯，但我们不想做的是，暴露在00:15:52.330 --> 00:15:54.850目标序列差，因为00:15:54.850 --> 00:15:58.370不会有效地学习如何用这种语言生成。00:15:58.560 --> 00:16:04.300在我得到结果之前，有没有关于后译的问题？嗯，当然。00:16:04.300 --> 00:16:08.980[背景]00:16:08.980 --> 00:16:11.500所以这是假设我们有大量的00:16:11.500 --> 00:16:16.700未标记的数据，我们希望使用它来帮助我们的翻译模型。00:16:17.330 --> 00:16:19.875这有道理吗？00:16:19.875 --> 00:16:23.340嗯，也许你可以澄清这个问题。00:16:23.340 --> 00:16:29.160[背景]00:16:29.160 --> 00:16:32.830是的，没错。所以我们有大量的英语语料库，包括句子，00:16:32.830 --> 00:16:36.190“我去过比利时，”我们不知道翻译，但我们还是想去。00:16:36.190 --> 00:16:39.630使用此数据。是的，另一个问题。00:16:39.630 --> 00:16:45.280[背景]00:16:45.280 --> 00:16:47.110是的，所以这是一个很好的问题，你是如何做到的？00:16:47.110 --> 00:16:52.300避免这两种模式，比如说爆炸和产生垃圾？00:16:52.300 --> 00:16:54.400然后他们就在互相喂垃圾。00:16:54.400 --> 00:16:57.820答案是这里也有一定数量的标记数据。00:16:57.820 --> 00:17:00.820所以在未标记的数据上，您可以这样做，但在标记的数据上，00:17:00.820 --> 00:17:02.110你做标准训练，00:17:02.110 --> 00:17:04.795这样你就避免了，你，00:17:04.795 --> 00:17:07.900你要确保你的模型保持在轨道上，因为它们仍然需要适应00:17:07.900 --> 00:17:12.175标记的数据。是的，另一个问题。00:17:12.175 --> 00:17:15.475你如何安排这两种模式的培训？00:17:15.475 --> 00:17:17.500是的，这是个好问题。00:17:17.500 --> 00:17:21.580我认为这基本上就像一个超参数，你可以调整。00:17:21.580 --> 00:17:25.720所以我认为一个很常见的事情是首先，00:17:25.720 --> 00:17:28.270仅在标记数据上训练两个模型。00:17:28.270 --> 00:17:32.965然后贴标签，嗯，然后再做反向翻译00:17:32.965 --> 00:17:37.480在一个大的语料库上，一次又一次重复这个过程。00:17:37.480 --> 00:17:40.165所以每次迭代，你都要训练标签数据，00:17:40.165 --> 00:17:43.510标记一些未标记的数据，现在您可以使用更多的数据。00:17:43.510 --> 00:17:46.270但是我认为有很多种安排是有效的00:17:46.270 --> 00:17:50.380在这里。可以。另一个问题。00:17:50.380 --> 00:18:06.100我对评估很好奇，考虑到如果你有一个非常好的法语-英语模式，你可以尝试查找，或者竞争如果你有一个好的法语-英语模式，你可以尝试查找原始来源，看看它是否匹配。00:18:06.100 --> 00:18:07.435是的，我不是，我不太确定。00:18:07.435 --> 00:18:10.135你是建议像英语到法语再到英语，看看是否？00:18:10.135 --> 00:18:11.635我明白了，是的，是的，00:18:11.635 --> 00:18:12.775这真是个有趣的主意。00:18:12.775 --> 00:18:15.775实际上，我们要谈谈这类，00:18:15.775 --> 00:18:17.290这叫做循环一致性，00:18:17.290 --> 00:18:20.000这个想法在稍后的谈话中。00:18:20.970 --> 00:18:23.770好吧，我继续看结果。00:18:23.770 --> 00:18:28.120所以，这里是使用未标记数据来改进翻译的方法。00:18:28.120 --> 00:18:29.890效果如何？00:18:29.890 --> 00:18:33.220嗯，答案是这些改进至少对我来说是这样，他们00:18:33.220 --> 00:18:36.490出奇地非常好，对吧？00:18:36.490 --> 00:18:39.445所以，嗯，这是英语到德语的翻译。00:18:39.445 --> 00:18:44.515这是Facebook的一些研究成果，因此他们使用了500万个贴有标签的句子对。00:18:44.515 --> 00:18:52.040但他们也使用了230个单语句子，所以没有翻译的句子。00:18:52.290 --> 00:18:56.425你可以看到，与以前的技术相比，00:18:56.425 --> 00:18:59.755他们提高了6个布鲁分数，嗯，00:18:59.755 --> 00:19:03.010如果你把它与以前的研究和机器翻译相比较00:19:03.010 --> 00:19:04.180真的是个大收获，对吧？00:19:04.180 --> 00:19:08.020所以，即使是像变压器的发明，大多数人都会00:19:08.020 --> 00:19:13.165考虑到NLP是一项非常重要的研究进展，00:19:13.165 --> 00:19:16.825这比之前的工作提高了2.5个百分点。00:19:16.825 --> 00:19:22.330在这里，只要使用更多的数据，就不用做任何花哨的模型设计，00:19:22.330 --> 00:19:25.400嗯，实际上我们得到了更大的改进。00:19:29.130 --> 00:19:34.390可以。一个有趣的问题，00:19:34.390 --> 00:19:38.125嗯，假设我们只有单语语料库。00:19:38.125 --> 00:19:41.155所以我们没有任何人工翻译的句子。00:19:41.155 --> 00:19:43.390我们只有两种语言的句子。00:19:43.390 --> 00:19:47.080嗯，所以你可以想象的情况是，00:19:47.080 --> 00:19:48.985嗯，一个外星人下来了，00:19:48.985 --> 00:19:50.740嗯，开始和你说话，这是一个00:19:50.740 --> 00:19:53.965奇怪的外星语言，嗯，它能说很多，00:19:53.965 --> 00:19:58.120你最终能把它的意思翻译成英语吗？00:19:58.120 --> 00:20:01.670嗯，只要有大量的数据？00:20:03.300 --> 00:20:06.205嗯，所以我先从，嗯，00:20:06.205 --> 00:20:11.935当你只有未标记的句子时，翻译比完整的任务要简单。00:20:11.935 --> 00:20:15.220嗯，不是逐句翻译，00:20:15.220 --> 00:20:18.640让我们从只关心逐字翻译开始。00:20:18.640 --> 00:20:21.490所以这里的目标是用一种语言给出一个单词，00:20:21.490 --> 00:20:25.330找到它的翻译，但不使用任何标记的数据。00:20:25.330 --> 00:20:27.100嗯，还有方法，00:20:27.100 --> 00:20:29.440我们试图解决的方法00:20:29.440 --> 00:20:33.460这个任务叫做跨语言嵌入。00:20:33.460 --> 00:20:35.830嗯，所以目标是学习，呃，00:20:35.830 --> 00:20:39.265两种语言的单词矢量，00:20:39.265 --> 00:20:41.935我们希望向量这个词00:20:41.935 --> 00:20:45.550你已经学过的关于矢量词的所有好的性质，嗯，00:20:45.550 --> 00:20:49.149但我们也需要特定语言的词汇载体，00:20:49.149 --> 00:20:52.860嗯，接近它的翻译矢量。00:20:52.860 --> 00:20:57.090嗯，所以我不确定它是否在这个图中可见，但这个图显示了00:20:57.090 --> 00:21:02.475大量的英语和德语单词，你可以看到，00:21:02.475 --> 00:21:07.795每个英语单词都有对应的德语单词，00:21:07.795 --> 00:21:10.330嗯，在它的嵌入空间附近。00:21:10.330 --> 00:21:15.010所以，如果我们学习这样的嵌入，那么进行逐字翻译就相当容易了。00:21:15.010 --> 00:21:16.705嗯，我们只选一个英文单词，00:21:16.705 --> 00:21:18.550我们找到最近的，呃，00:21:18.550 --> 00:21:22.075这个联合嵌入空间中的德语单词00:21:22.075 --> 00:21:26.030这将给我们一个英语单词的翻译。00:21:28.470 --> 00:21:32.185嗯，我们的关键方法00:21:32.185 --> 00:21:35.500假设我们要用它来解决这个问题，00:21:35.500 --> 00:21:40.870嗯，这-即使你运行两次word2vec，你会得到非常不同的嵌入。00:21:40.870 --> 00:21:46.930嗯，嵌入空间的结构有很多规律性，00:21:46.930 --> 00:21:49.675我们可以利用这个规律，嗯，00:21:49.675 --> 00:21:51.700为了帮助找出时间，00:21:51.700 --> 00:21:54.370嗯，嵌入空间之间的对齐。00:21:54.370 --> 00:21:56.830所以在这里要更具体一些。00:21:56.830 --> 00:21:59.560这是两组单词嵌入的图片。00:21:59.560 --> 00:22:00.820所以红色的，我们有，嗯，00:22:00.820 --> 00:22:02.655英语单词，in，uh，00:22:02.655 --> 00:22:04.565蓝色我们有意大利语单词，00:22:04.565 --> 00:22:09.280虽然，嗯，向量空间现在看起来非常不同，00:22:09.280 --> 00:22:12.400嗯，你可以看到它们有一个非常相似的结构，对吧？00:22:12.400 --> 00:22:16.735所以你可以想象距离和00:22:16.735 --> 00:22:19.345呃，猫和猫在，嗯，00:22:19.345 --> 00:22:22.570英语嵌入空间应该与距离非常相似00:22:22.570 --> 00:22:27.880在意大利的加托和费利诺之间。00:22:27.880 --> 00:22:34.310嗯，这种方法激发了学习这些跨语言嵌入的算法。00:22:35.400 --> 00:22:38.440嗯，这就是我的想法。00:22:38.440 --> 00:22:40.960我们要做的是学习00:22:40.960 --> 00:22:44.080我们可以变换的旋转，00:22:44.080 --> 00:22:46.660嗯，我们的英文嵌入集，所以00:22:46.660 --> 00:22:50.515它们与我们的意大利刺绣相匹配。00:22:50.515 --> 00:22:52.780从数学上来说，这意味着我们要学习00:22:52.780 --> 00:22:55.660一个矩阵，如果我们取它的话，00:22:55.660 --> 00:23:00.355呃，英语中cat的矢量这个词，我们用w乘以。00:23:00.355 --> 00:23:06.205最后我们用西班牙语或意大利语表示加图，00:23:06.205 --> 00:23:09.550嗯，这里的一个细节是，00:23:09.550 --> 00:23:12.580我们将把w约束为正交的。00:23:12.580 --> 00:23:15.070从几何角度来说，这意味着00:23:15.070 --> 00:23:17.980只做一个旋转，00:23:17.980 --> 00:23:19.945向量，以x表示。00:23:19.945 --> 00:23:24.320它不会做其他更奇怪的转变。00:23:24.870 --> 00:23:29.305所以这是我们的目标是学习这个。00:23:29.305 --> 00:23:31.000接下来我要说的是，00:23:31.000 --> 00:23:36.985说到我们是怎么学会这个的，嗯，00:23:36.985 --> 00:23:41.665实际上有很多学习w矩阵的技巧，00:23:41.665 --> 00:23:44.740嗯，但是，嗯，这是00:23:44.740 --> 00:23:48.310我认为相当聪明的人被称为对抗性训练。00:23:48.310 --> 00:23:50.635嗯，它的工作原理如下：00:23:50.635 --> 00:23:53.770除了尝试学习这个w矩阵，00:23:53.770 --> 00:23:57.670我们还将尝试学习一个模型，呃，00:23:57.670 --> 00:23:58.915被称为鉴别器，00:23:58.915 --> 00:24:02.800它将要做的是，取一个向量，它将试图预测，00:24:02.800 --> 00:24:05.080是这个向量，嗯，00:24:05.080 --> 00:24:08.830一个英语单词嵌入还是一个意大利语单词嵌入？00:24:08.830 --> 00:24:11.425换句话说，如果你想一想，00:24:11.425 --> 00:24:14.920图表，我们要求鉴别器做的是，00:24:14.920 --> 00:24:17.680它给出了其中一个点，它试图预测00:24:17.680 --> 00:24:21.055基本上是一个红点，原来是一个英文单词，还是一个蓝点？00:24:21.055 --> 00:24:24.010如果我们没有w矩阵，这是00:24:24.010 --> 00:24:27.190对于鉴别器来说是一项非常简单的任务，因为，00:24:27.190 --> 00:24:32.425嗯，英语和意大利语的单词嵌入是分开的。00:24:32.425 --> 00:24:36.130但是，如果我们学习w矩阵00:24:36.130 --> 00:24:39.955它成功地将所有这些嵌入物排列在一起，00:24:39.955 --> 00:24:43.270那么我们的鉴别器就永远不会做得好，对吧。00:24:43.270 --> 00:24:46.210我们可以想象它永远不会比50%更好，00:24:46.210 --> 00:24:48.835因为给定了一个向量来表示猫，00:24:48.835 --> 00:24:51.190不知道是猫的向量00:24:51.190 --> 00:24:54.130由w转换，还是它实际上是gatto的向量？00:24:54.130 --> 00:24:58.885嗯，因为在这种情况下，这两个向量是对齐的，所以它们是在彼此之上的。00:24:58.885 --> 00:25:03.715嗯，所以，嗯，在训练期间，你首先，00:25:03.715 --> 00:25:06.790你交替训练鉴别器一点00:25:06.790 --> 00:25:09.640意味着确保尽可能的好00:25:09.640 --> 00:25:13.120区分英语和意大利语单词，然后你00:25:13.120 --> 00:25:16.930训练w，训练w的目标是，00:25:16.930 --> 00:25:20.050呃，基本上尽量混淆鉴别器。00:25:20.050 --> 00:25:23.215嗯，所以你想有一种情况，00:25:23.215 --> 00:25:26.170你不能用这种机器学习模式，00:25:26.170 --> 00:25:29.290弄清楚一个词是否真的嵌入了，嗯，00:25:29.290 --> 00:25:33.625是，嗯，最初是英语还是意大利语的矢量词。00:25:33.625 --> 00:25:36.085嗯，所以在一天结束的时候，00:25:36.085 --> 00:25:39.410你有一些向量是相互对齐的。00:25:39.420 --> 00:25:43.460嗯，关于这种方法有什么问题吗？00:25:47.220 --> 00:25:50.650可以。嗯，他-有一个链接指向一份更详细的文件。00:25:50.650 --> 00:25:53.275实际上还有很多其他的技巧你可以做，00:25:53.275 --> 00:25:55.850嗯，但这是一个关键的想法。00:25:58.440 --> 00:26:04.810嗯，好吧。所以这是逐字逐句的无监督翻译。00:26:04.810 --> 00:26:08.930嗯，我们如何进行全句对句翻译？00:26:09.150 --> 00:26:11.725嗯，所以我们要用，嗯，00:26:11.725 --> 00:26:13.750一种标准的seq2seq模型，00:26:13.750 --> 00:26:16.660嗯，甚至没有注意机制。00:26:16.660 --> 00:26:19.900嗯，标准的seq2seq有一个变化00:26:19.900 --> 00:26:23.050这里的模特就是这样，嗯，00:26:23.050 --> 00:26:25.780我们将使用相同的编码器和解码器，00:26:25.780 --> 00:26:30.160呃，不管输入和输出语言是什么。00:26:30.160 --> 00:26:31.930所以你可以看到，嗯，00:26:31.930 --> 00:26:33.340在这个例子中，嗯，00:26:33.340 --> 00:26:35.815我们可以给编码器一个英语句子，00:26:35.815 --> 00:26:40.360我们也可以给它一个法语句子，它会有这些跨语言的嵌入。00:26:40.360 --> 00:26:43.255所以它会有英文单词的向量表示00:26:43.255 --> 00:26:47.140和法语单词，这意味着它可以处理任何类型的输入。00:26:47.140 --> 00:26:49.375嗯，对于解码器，00:26:49.375 --> 00:26:52.930我们需要给它一些关于应该用什么语言生成的信息。00:26:52.930 --> 00:26:54.955它将生成法语还是英语？00:26:54.955 --> 00:26:58.660嗯，那么做的方式是，呃，00:26:58.660 --> 00:27:01.915喂入一个特殊的令牌，这里是fr00:27:01.915 --> 00:27:05.590在brack括号中表示法语，表示模型，00:27:05.590 --> 00:27:07.975好吧，你现在应该用法语生成。00:27:07.975 --> 00:27:11.380嗯，在这个数字里只有法语，00:27:11.380 --> 00:27:13.975但是你也可以想象喂养这个模型，呃，00:27:13.975 --> 00:27:17.635括号中的英语，然后告诉它生成英语。00:27:17.635 --> 00:27:21.670你能看到的一件事就是你可以用这种模型来生成，00:27:21.670 --> 00:27:23.155一定要从英语到法语。00:27:23.155 --> 00:27:25.450你也可以使用这个模型作为自动编码器，对吧。00:27:25.450 --> 00:27:27.295所以，在底部，嗯，00:27:27.295 --> 00:27:31.510它将一个法语句子作为输入，并将其生成为00:27:31.510 --> 00:27:37.580这里的输出意味着只是复制原始的输入序列。00:27:38.850 --> 00:27:43.104嗯，所以只需要对标准的seq2seq做一个小小的改动。00:27:43.104 --> 00:27:46.765下面是我们将如何训练seq2seq模型。00:27:46.765 --> 00:27:50.170嗯，有两个培训目标，嗯，00:27:50.170 --> 00:27:51.940我来解释一下他们为什么，呃，00:27:51.940 --> 00:27:55.060在这个模型中，只需几张幻灯片。00:27:55.060 --> 00:27:57.025现在，让我们说一下它们是什么。00:27:57.025 --> 00:27:59.110第一个是，嗯，00:27:59.110 --> 00:28:01.165称为去噪自动编码器。00:28:01.165 --> 00:28:06.430嗯，我们要训练我们的模型，在这种情况下，我们要做的就是用一个，呃，句子。00:28:06.430 --> 00:28:08.140所以，嗯，在这里00:28:08.140 --> 00:28:10.795一个英语句子，但也可以是一个法语句子。00:28:10.795 --> 00:28:14.170嗯，我们要把这些词拼凑一下，00:28:14.170 --> 00:28:16.885然后我们要让模特00:28:16.885 --> 00:28:20.560消除噪音，换言之，这句话的意思是00:28:20.560 --> 00:28:25.345重新生成句子在被置乱之前的实际内容。00:28:25.345 --> 00:28:31.740也许这是一个有用的培训目标的原因是，00:28:31.740 --> 00:28:35.505呃，因为我们有一个编码器解码器，没有引起注意，00:28:35.505 --> 00:28:41.780编码器正在将源语句的整体转换为单个向量，00:28:41.780 --> 00:28:47.110自动编码器所做的是确保向量包含有关00:28:47.110 --> 00:28:52.390使我们能够恢复原判的判决，00:28:52.390 --> 00:28:56.180嗯，来自编码器产生的矢量。00:28:57.960 --> 00:29:00.805嗯，这就是目标1。00:29:00.805 --> 00:29:05.005培训目标2是现在我们要做一个翻译，00:29:05.005 --> 00:29:07.479嗯，但是，嗯，和以前一样，00:29:07.479 --> 00:29:09.775我们将使用这个反向翻译的想法。00:29:09.775 --> 00:29:12.970所以记住，我们只有没有标记的句子，00:29:12.970 --> 00:29:16.015我们没有任何人工翻译，00:29:16.015 --> 00:29:19.750但是我们仍然可以做的是，00:29:19.750 --> 00:29:22.000嗯，我们说一个英语句子，或者说一个法语句子，00:29:22.000 --> 00:29:24.505如果是法语句子，我们可以把它翻译成英语，嗯，00:29:24.505 --> 00:29:28.120在当前状态下使用我们的模型，00:29:28.120 --> 00:29:32.605然后我们可以要求那个模型从英语中翻译或者翻译-是的，00:29:32.605 --> 00:29:34.690把英语译回法语。00:29:34.690 --> 00:29:37.105嗯，你能想象的是，在这种环境下，00:29:37.105 --> 00:29:39.640输入序列会有点混乱00:29:39.640 --> 00:29:42.820因为它是我们不完善的机器学习模型的输出。00:29:42.820 --> 00:29:47.050所以这里输入的顺序是“我是学生”，嗯，一个词被删除了，00:29:47.050 --> 00:29:51.820但是，嗯，我们现在要训练它，即使有这种糟糕的输入，00:29:51.820 --> 00:29:55.330复制原稿，嗯，00:29:55.330 --> 00:29:58.270法语句子，嗯，来自我们的，00:29:58.270 --> 00:30:01.270嗯，单语语料库，嗯，法语文本。00:30:01.270 --> 00:30:07.400[噪音]嗯，让我-让我在这里停下来问问题。00:30:08.910 --> 00:30:13.840当然。00:30:13.840 --> 00:30:16.000[噪音][听不见]如果，嗯，你为什么00:30:16.000 --> 00:30:20.305这个正交性约束让你的单词嵌入，00:30:20.305 --> 00:30:22.900是为了避免过度安装吗？00:30:22.900 --> 00:30:29.800你试过把它脱下来吗？你知道，看看[听不见]00:30:29.800 --> 00:30:31.060是啊。这是个好问题。00:30:31.060 --> 00:30:35.305嗯，这可以追溯到早期有单词翻译的时候。00:30:35.305 --> 00:30:39.325为什么我们要约束w矩阵是正交的？00:30:39.325 --> 00:30:43.030嗯，基本上是这样的。这是为了避免过拟合，尤其是，00:30:43.030 --> 00:30:46.060它假设我们的嵌入空间是00:30:46.060 --> 00:30:50.005类似的，实际上只有一个旋转可以区分，00:30:50.005 --> 00:30:53.500嗯，我们的英语单词向量和意大利语单词向量。00:30:53.500 --> 00:30:57.460嗯，我想有，嗯，00:30:57.460 --> 00:31:01.360有一些结果不包括正交性约束，00:31:01.360 --> 00:31:04.480我认为如果不把它放在里面的话，会对表演造成轻微的伤害。00:31:04.480 --> 00:31:09.130[噪音]好的。00:31:09.130 --> 00:31:11.155嗯，那么-继续说，00:31:11.155 --> 00:31:13.765嗯，无监督的机器翻译，00:31:13.765 --> 00:31:17.290嗯，我-我给了一个训练方法。00:31:17.290 --> 00:31:20.395我没有很好地解释为什么它会起作用，所以-所以，00:31:20.395 --> 00:31:24.790嗯，这是对这个想法的一些直觉。00:31:24.790 --> 00:31:27.730嗯，记住，嗯，00:31:27.730 --> 00:31:29.665我们要初始化00:31:29.665 --> 00:31:33.264我们的机器翻译模型有这些跨语言嵌入，00:31:33.264 --> 00:31:37.015这意味着英语和法语单词看起来应该差不多。00:31:37.015 --> 00:31:42.565嗯，我们也在使用共享的，嗯，编码器。00:31:42.565 --> 00:31:44.860嗯，这意味着如果你考虑一下，00:31:44.860 --> 00:31:46.645嗯，在顶部，我们只有，00:31:46.645 --> 00:31:51.760一个自动编码的目标，我们当然可以相信我们的模型可以学习到这一点。00:31:51.760 --> 00:31:54.250嗯，这是一项非常简单的任务。00:31:54.250 --> 00:31:59.395嗯，现在假设我们给模型一个法语句子作为输入。00:31:59.395 --> 00:32:01.555嗯，因为，呃，00:32:01.555 --> 00:32:03.850嵌入看起来很相似，00:32:03.850 --> 00:32:06.190既然编码器是一样的，嗯，00:32:06.190 --> 00:32:09.760很可能模型的表示00:32:09.760 --> 00:32:11.950这个法语句子应该是00:32:11.950 --> 00:32:15.520类似于英语句子的表达。00:32:15.520 --> 00:32:19.870当这个表示被传递到解码器时，00:32:19.870 --> 00:32:24.980我们希望能得到和以前一样的产量。00:32:25.200 --> 00:32:28.495嗯，嗯，所以这里有点像一个起点。00:32:28.495 --> 00:32:30.865我们-我们希望我们的模型，嗯，00:32:30.865 --> 00:32:33.430已经具备一定的翻译能力。00:32:33.430 --> 00:32:37.840[噪音]嗯，另一种思考方式是00:32:37.840 --> 00:32:42.355我们真正希望我们的模型能够编码一个句子，00:32:42.355 --> 00:32:44.335使代表，00:32:44.335 --> 00:32:47.410嗯，是一种通用的中间语。00:32:47.410 --> 00:32:49.885一个宇宙，嗯，呃，00:32:49.885 --> 00:32:53.680对那个句子的普遍表达00:32:53.680 --> 00:32:56.245呃，这不是语言的特殊性。00:32:56.245 --> 00:32:59.785所以-这里有一张图片，试图得到这个。00:32:59.785 --> 00:33:03.160所以我们的自动编码器，嗯，还有我们的，嗯，00:33:03.160 --> 00:33:05.290在我们后面的翻译示例中，00:33:05.290 --> 00:33:07.210嗯，这里，目标序列是一样的。00:33:07.210 --> 00:33:10.090[噪音]嗯，那本质上是指00:33:10.090 --> 00:33:14.200英语句子和法语句子的载体，00:33:14.200 --> 00:33:17.410嗯，要接受相同的训练吗，嗯，对吗？00:33:17.410 --> 00:33:19.645因为如果他们不同，我们的，呃，00:33:19.645 --> 00:33:21.520解码器将产生不同的，00:33:21.520 --> 00:33:25.040呃，这两个例子的输出。00:33:25.040 --> 00:33:29.640嗯，这里-这只是另一种直觉，就是我们的模型00:33:29.640 --> 00:33:31.290在这里学习是一种00:33:31.290 --> 00:33:33.870在矢量中编码一个句子的信息，00:33:33.870 --> 00:33:37.100嗯，但在某种程度上，这是语言不可知论。00:33:37.100 --> 00:33:39.460嗯，还有什么问题吗？00:33:39.460 --> 00:33:42.740嗯，无人监督的机器翻译？00:33:44.220 --> 00:33:50.350可以。嗯，继续这个方法的结果，嗯，00:33:50.350 --> 00:33:53.020这里，水平线是，00:33:53.020 --> 00:33:56.860嗯，一个无监督机器翻译模型的结果。00:33:56.860 --> 00:34:00.775嗯，上面的行是用于受监控机器翻译模型的，00:34:00.775 --> 00:34:03.900嗯，随着我们提供越来越多的数据。00:34:03.900 --> 00:34:06.300正确的？不出所料，嗯，00:34:06.300 --> 00:34:09.405考虑到大量的监控数据，嗯，00:34:09.405 --> 00:34:11.790监控机器翻译模型00:34:11.790 --> 00:34:15.725比无监督的机器翻译模型工作得更好。00:34:15.725 --> 00:34:19.285嗯，但是，嗯，无监督机器翻译模型，00:34:19.285 --> 00:34:21.310实际上还是很不错的。00:34:21.310 --> 00:34:26.995嗯，如果你看到大约10000到100000个训练例子，00:34:26.995 --> 00:34:30.565嗯，它实际上比有监督的翻译效果好或更好，00:34:30.565 --> 00:34:33.580我认为这是一个非常有希望的结果，00:34:33.580 --> 00:34:36.640呃，因为如果你想到，嗯，00:34:36.640 --> 00:34:39.550低资源设置，没有太多标记的示例，嗯，00:34:39.550 --> 00:34:42.280你能表现得这么好，突然变得非常好，00:34:42.280 --> 00:34:46.550嗯，甚至不需要使用训练设备。00:34:48.690 --> 00:34:51.745嗯，另一件有趣的事情是，00:34:51.745 --> 00:34:55.195一个无监督的机器翻译模型是属性转移。00:34:55.195 --> 00:34:58.855嗯，所以基本上，你可以，嗯，拿，呃，00:34:58.855 --> 00:35:00.520文本集合，00:35:00.520 --> 00:35:03.190呃，根据你想要的属性来划分。00:35:03.190 --> 00:35:04.900例如，你可以在Twitter上，00:35:04.900 --> 00:35:08.650查看hashtags来决定哪些tweet是烦人的，哪些tweet是放松的，00:35:08.650 --> 00:35:11.080然后你可以把这两个语料库当作00:35:11.080 --> 00:35:13.614文字就像是两种不同的语言，00:35:13.614 --> 00:35:16.510你可以训练一个无监督的机器翻译模型，00:35:16.510 --> 00:35:19.165呃，从一个转换到另一个。00:35:19.165 --> 00:35:22.495嗯，你可以看到这些例子，嗯，00:35:22.495 --> 00:35:26.650这个模型实际上很好地将句子的变化降到最低，00:35:26.650 --> 00:35:29.680保留了很多句子的原始语义，00:35:29.680 --> 00:35:33.620嗯，这样目标属性就改变了。00:35:36.540 --> 00:35:41.290嗯，我还想对这个想法泼点冷水。00:35:41.290 --> 00:35:44.410所以我认为这真的很令人兴奋而且几乎是00:35:44.410 --> 00:35:47.650令人吃惊的是，您可以在没有标记数据的情况下进行此翻译。00:35:47.650 --> 00:35:49.600嗯，当然，对。00:35:49.600 --> 00:35:54.520真的很难想象有人给我一大堆意大利语书然后说，“好吧。00:35:54.520 --> 00:35:56.410我们是意大利人，“嗯，没有，你知道，00:35:56.410 --> 00:35:59.755教你如何具体翻译。00:35:59.755 --> 00:36:03.955嗯，但是，嗯，即使这些方法显示出希望，00:36:03.955 --> 00:36:08.095嗯，他们大多对那些关系密切的语言表现出了希望。00:36:08.095 --> 00:36:09.775所以以前的结果，00:36:09.775 --> 00:36:11.065这些都是，嗯，00:36:11.065 --> 00:36:13.750英语到法语或英语到德语的组合，00:36:13.750 --> 00:36:16.270嗯，等等，这些语言非常相似。00:36:16.270 --> 00:36:18.280[噪音]嗯，如果你看，呃，00:36:18.280 --> 00:36:20.320一对不同的语言，比如说英语和土耳其语，00:36:20.320 --> 00:36:24.685这两种语言的语言学是完全不同的，呃，00:36:24.685 --> 00:36:27.610这些方法在某种程度上仍然有效，嗯，00:36:27.610 --> 00:36:30.910所以他们得到了五个布鲁分数，比如，00:36:30.910 --> 00:36:33.190但它们的效果不太好，00:36:33.190 --> 00:36:35.890嗯，就像他们在其他地方一样，对吧？00:36:35.890 --> 00:36:40.240因此，纯粹的监督学习还有很大的差距。嗯，对吧？00:36:40.240 --> 00:36:41.350所以我们可能不是，你知道，00:36:41.350 --> 00:36:45.040在外星人可能坠落的这个阶段，没问题，00:36:45.040 --> 00:36:48.220让我们使用我们的无监督机器翻译系统，嗯，00:36:48.220 --> 00:36:52.399但我仍然认为这是非常令人兴奋的进展。嗯，是的。有问题吗？00:36:52.399 --> 00:36:55.270嗯，你说的是00:36:55.270 --> 00:36:58.630一种语言可能需要它来叠加更糟的内容，对吗？00:36:58.630 --> 00:37:01.510因为我最初的想法是如果你举个例子，00:37:01.510 --> 00:37:04.810就像拉丁语，没有一个词来表示，00:37:04.810 --> 00:37:11.440现代的汽车分类，我认为会做得更差。但是如果-但是，呃，基本上，00:37:11.440 --> 00:37:15.280我要问的是，你认为英语地图更适合拉丁语吗？00:37:15.280 --> 00:37:20.380因为他们两个都有关系，更糟的是土耳其语，还是相反？00:37:20.380 --> 00:37:25.645嗯，我希望英语能更好地映射到拉丁语。00:37:25.645 --> 00:37:28.930我认为问题的一部分是，00:37:28.930 --> 00:37:33.460我认为翻译的困难并不在字面上。00:37:33.460 --> 00:37:35.410所以我的意思是这当然是一个词存在的问题00:37:35.410 --> 00:37:37.495一种语言不存在于另一种语言中，00:37:37.495 --> 00:37:38.740嗯，但我想事实上，00:37:38.740 --> 00:37:43.195语言之间更实质性的差异在于相似语法的层次，00:37:43.195 --> 00:37:45.820嗯，嗯，或者你知道，语义学，对吧？00:37:45.820 --> 00:37:47.410如何表达想法。00:37:47.410 --> 00:37:53.515嗯，所以-我想我-我希望意大利语-拉丁语有，你知道，00:37:53.515 --> 00:37:56.020与英语的语法相对相似，00:37:56.020 --> 00:37:57.580嗯，和土耳其语相比，00:37:57.580 --> 00:37:59.860我想那可能是更大的障碍00:37:59.860 --> 00:38:03.260对于无监督的机器翻译模型。00:38:07.190 --> 00:38:10.260嗯，我会很快进入00:38:10.260 --> 00:38:14.910这是最近的一篇研究论文，主要是以伯特为例，00:38:14.910 --> 00:38:17.265你知道的，嗯，对吗？00:38:17.265 --> 00:38:20.190对.可以。使之成为跨语言的。00:38:20.190 --> 00:38:23.730嗯，那么，嗯，这是伯特的常客，对吧？00:38:23.730 --> 00:38:26.295我们有一系列的英语句子。00:38:26.295 --> 00:38:28.215我们要屏蔽掉一些单词。00:38:28.215 --> 00:38:31.500我们要问伯特，哪一个是我们的变压器模型，嗯，00:38:31.500 --> 00:38:36.675基本上填补空白，预测掉的单词是什么。00:38:36.675 --> 00:38:42.990嗯，实际上谷歌已经做了一件事，那就是训练多语言伯特。00:38:42.990 --> 00:38:46.835所以他们所做的基本上是串联，嗯，00:38:46.835 --> 00:38:51.560一大堆不同语言的语料库，然后训练一个模型。00:38:51.560 --> 00:38:54.785使用这个蒙面的lm目标，嗯，00:38:54.785 --> 00:38:56.315在所有的文本上。00:38:56.315 --> 00:38:58.300这是一个公开发布的模型。00:38:58.300 --> 00:39:03.495嗯，最近这种新的扩展，呃，00:39:03.495 --> 00:39:06.300Facebook提出的建议是将00:39:06.300 --> 00:39:10.965这掩盖了我的培训目标，嗯，翻译。00:39:10.965 --> 00:39:17.130所以在这种情况下，他们有时会给这个模型A，00:39:17.130 --> 00:39:21.060一个英语序列和一个法语序列。00:39:21.060 --> 00:39:24.300嗯，把一些话说出来，就像以前一样，00:39:24.300 --> 00:39:26.130让模特来填。00:39:26.130 --> 00:39:28.635这里的动机是，嗯，00:39:28.635 --> 00:39:31.080这将更好地导致模型00:39:31.080 --> 00:39:33.525了解这两种语言之间的关系。00:39:33.525 --> 00:39:37.950因为如果你想找一个被删掉的英文单词，00:39:37.950 --> 00:39:40.500嗯，如果你有翻译，最好的办法是看00:39:40.500 --> 00:39:43.005在法国那边，试着找到那个词。00:39:43.005 --> 00:39:45.075希望那个也没被扔下。00:39:45.075 --> 00:39:48.525然后你可以UM，更容易填空。00:39:48.525 --> 00:39:52.575呃，这实际上导致了，00:39:52.575 --> 00:39:55.860无监督机器翻译的实质性改进。00:39:55.860 --> 00:39:59.670就像Bert在NLP中用于其他任务一样，00:39:59.670 --> 00:40:02.010他们基本上采用跨语言伯特。00:40:02.010 --> 00:40:03.930它们将其用作00:40:03.930 --> 00:40:07.410一个无监督的机器翻译系统，他们得到，你知道，00:40:07.410 --> 00:40:10.425在10个布鲁点上获得了巨大的收益嗯，00:40:10.425 --> 00:40:12.690使得00:40:12.690 --> 00:40:16.485无监督的机器翻译和当前监督的技术状态，00:40:16.485 --> 00:40:18.420嗯，要小得多。00:40:18.420 --> 00:40:23.190嗯，所以这是一个很新的想法，但我认为它也显示了承诺00:40:23.190 --> 00:40:28.320通过使用未标记的数据真正提高翻译质量。00:40:28.320 --> 00:40:30.945嗯，虽然我想是的，但我想在这件事上，伯特00:40:30.945 --> 00:40:33.930他们也在使用标记的翻译数据。00:40:33.930 --> 00:40:37.270有问题吗？00:40:37.820 --> 00:40:46.350可以。嗯，这就是我要说的关于使用未标记的数据进行翻译的全部内容。00:40:46.350 --> 00:40:48.750下一部分是关于，嗯，00:40:48.750 --> 00:40:54.720如果我们真的扩大这些无监督的语言模型会发生什么。00:40:54.720 --> 00:40:59.865嗯，特别是我要讲的GPT-2是OpenAI的一个新模型。00:40:59.865 --> 00:41:02.265这实际上是一个巨大的语言模型00:41:02.265 --> 00:41:05.685我认为它有一些有趣的含义。00:41:05.685 --> 00:41:15.060首先，这里是一些不同的NLP模型的大小，00:41:15.060 --> 00:41:18.165嗯，你知道，也许几年前，00:41:18.165 --> 00:41:19.230标准的那种00:41:19.230 --> 00:41:24.135LSTM中型模型的参数约为1000万。00:41:24.135 --> 00:41:30.660其中10-其中一个参数只是一个权重，比如在神经网络中，00:41:30.660 --> 00:41:33.090埃尔莫和呃，GPT。00:41:33.090 --> 00:41:35.520所以他们之前的原始Openai文件00:41:35.520 --> 00:41:38.820这个GPT-2比那个大10倍。00:41:38.820 --> 00:41:43.120嗯，GPT-2大约是另一个数量级。00:41:44.120 --> 00:41:48.825嗯，这里有一个有趣的比较点是，00:41:48.825 --> 00:41:51.735GPT-2是15亿个参数，00:41:51.735 --> 00:41:55.635事实上，它的参数比蜜蜂大脑中的突触还要多。00:41:55.635 --> 00:41:58.440嗯，听起来不错，对吧？00:41:58.440 --> 00:42:01.350你知道蜜蜂不是最聪明的00:42:01.350 --> 00:42:05.325但它们仍然可以四处飞，找到花蜜或其他什么。00:42:05.325 --> 00:42:08.760嗯，但是是的。当然，这不是一个苹果对苹果的比较，对吧？00:42:08.760 --> 00:42:11.970所以神经网络中的突触和重量是完全不同的。00:42:11.970 --> 00:42:14.490但我认为这是一个有趣的里程碑00:42:14.490 --> 00:42:16.815就车型尺寸来说，嗯，00:42:16.815 --> 00:42:18.150这已经超越了。00:42:18.150 --> 00:42:26.835[噪音]嗯，这里要指出的一点是，00:42:26.835 --> 00:42:32.130这种不断扩大的深度学习实际上是一种普遍趋势，呃，00:42:32.130 --> 00:42:34.845在所有的机器学习中，超越了NLP。00:42:34.845 --> 00:42:41.760所以这个图显示了x轴上的时间，y轴是对数刻度的。00:42:41.760 --> 00:42:45.255用于训练此模型的petaflops数量。00:42:45.255 --> 00:42:50.010嗯，这意味着至少目前的趋势是00:42:50.010 --> 00:42:53.130计算能力指数增长00:42:53.130 --> 00:42:55.735我们在扔我们的机器学习模型。00:42:55.735 --> 00:42:57.920我想有点不清楚，你知道，00:42:57.920 --> 00:43:00.695指数增长会继续吗，但当然，嗯，00:43:00.695 --> 00:43:03.560我们的模型规模迅速增长。00:43:03.560 --> 00:43:06.200它会带来一些非常惊人的结果，对吧？00:43:06.200 --> 00:43:09.445所以这不是语言的结果，而是视觉的结果。00:43:09.445 --> 00:43:13.155嗯，这是一个生成性的对抗性网络00:43:13.155 --> 00:43:16.920这是在大量数据的基础上进行的培训，而且是在非常大的范围内进行的培训。00:43:16.920 --> 00:43:22.710所以说它是一个介于埃尔莫和伯特之间的大模型。00:43:22.710 --> 00:43:27.510嗯，这些照片实际上是模特的作品。00:43:27.510 --> 00:43:28.740所以这些不是真照片。00:43:28.740 --> 00:43:31.515这些都是模型在稀薄空气中产生的幻觉。00:43:31.515 --> 00:43:34.770至少在我看来，这些照片基本上是真实的。00:43:34.770 --> 00:43:38.010还有一个网站，嗯，看它很有趣。00:43:38.010 --> 00:43:39.915如果你不感兴趣-如果你感兴趣，00:43:39.915 --> 00:43:42.195此人不在texist.com上。00:43:42.195 --> 00:43:43.950所以如果你去那里，你会看到00:43:43.950 --> 00:43:47.430一张很有说服力的照片，但不是真的照片。00:43:47.430 --> 00:43:51.400它又像是由甘产生的幻觉。00:43:51.440 --> 00:43:55.725我们也看到了用于图像识别的巨大模型。00:43:55.725 --> 00:43:58.110所以这是谷歌最近的工作，他们在那里训练00:43:58.110 --> 00:44:02.010一个有50亿参数的图像网络模型。00:44:02.010 --> 00:44:06.450所以这比伯特大，但不如GPT-2大。00:44:06.450 --> 00:44:09.420嗯，这张图显示00:44:09.420 --> 00:44:14.760在X轴上记录参数的比例数量，然后在ImageNet上记录精度00:44:14.760 --> 00:44:20.520在Y轴上，更大的模型表现得更好，这不足为奇。00:44:20.520 --> 00:44:24.000事实上这里有一个相当一致的趋势，呃，00:44:24.000 --> 00:44:28.240精度随着对数、模型尺寸的增大而增大。00:44:31.010 --> 00:44:35.100嗯，我想更详细一点，怎么样00:44:35.100 --> 00:44:39.060可能我们可以在如此大的程度上扩大模型和培训模型。00:44:39.060 --> 00:44:41.190一个答案就是更好的硬件。00:44:41.190 --> 00:44:42.675尤其是，嗯，00:44:42.675 --> 00:44:44.160越来越多的人，呃，00:44:44.160 --> 00:44:48.165专门为深度学习开发硬件的公司数量。00:44:48.165 --> 00:44:50.520所以这些更受约束00:44:50.520 --> 00:44:53.190他们可以比GPU做的操作，00:44:53.190 --> 00:44:55.950嗯，但他们做这些手术的速度更快。00:44:55.950 --> 00:44:59.610所以谷歌的张量处理单元就是一个例子。00:44:59.610 --> 00:45:03.180实际上还有很多其他公司在研究这个想法。00:45:03.180 --> 00:45:06.930嗯，扩大模型的另一种方法是利用00:45:06.930 --> 00:45:11.835并行性，有两种类型的并行性，我想简单地讨论一下。00:45:11.835 --> 00:45:13.980一个是数据并行性。00:45:13.980 --> 00:45:16.785在这种情况下，你们每个人，00:45:16.785 --> 00:45:19.380假设GPU，将有一个模型的副本。00:45:19.380 --> 00:45:21.480你基本上是分开的00:45:21.480 --> 00:45:25.350你在这些不同型号上训练的小批量。00:45:25.350 --> 00:45:27.165所以如果你有，我们假设，00:45:27.165 --> 00:45:30.94516 gpu，每批32个。00:45:30.945 --> 00:45:35.670你可以把这16个，呃，呃，00:45:35.670 --> 00:45:42.540如果你在这16个GPU上做一个后支撑，你最终得到的实际批量大小是512。00:45:42.540 --> 00:45:44.700所以这可以让你更快地训练模型。00:45:44.700 --> 00:45:50.340另一种日益重要的并行性是模型并行性。00:45:50.340 --> 00:45:54.510嗯，所以最终模特们变得这么大以至于他们00:45:54.510 --> 00:45:59.070甚至不能适应一个GPU，他们甚至不能做一个批量大小的一个。00:45:59.070 --> 00:46:00.660嗯，在这种情况下，00:46:00.660 --> 00:46:02.985你实际上需要把模型分割开来00:46:02.985 --> 00:46:06.075多台计算机-多个计算单元。00:46:06.075 --> 00:46:10.485嗯，这就是模特们所做的，00:46:10.485 --> 00:46:12.720比如说GPT-2。00:46:12.720 --> 00:46:15.540有一些新的框架，比如Mesh TensorFlow，嗯，00:46:15.540 --> 00:46:21.430这基本上是为了使这种模型并行性更容易。00:46:23.990 --> 00:46:27.390嗯，好吧。所以在GPT-2上，嗯，00:46:27.390 --> 00:46:31.560我知道你已经在语境中看到了这一点，呃，00:46:31.560 --> 00:46:36.540嗯，嵌入，嗯，演讲，但我会在这里更深入地讲。00:46:36.540 --> 00:46:41.265[噪音]所以本质上，它是一个非常大的变形金刚语言模型。00:46:41.265 --> 00:46:45.165嗯，所以这里没有什么真正意义上的小说00:46:45.165 --> 00:46:49.305新的训练算法，或者就um而言，00:46:49.305 --> 00:46:51.645损失函数之类的。00:46:51.645 --> 00:46:53.340嗯，让它与众不同的是00:46:53.340 --> 00:46:56.070之前的工作是它真的非常大。00:46:56.070 --> 00:46:59.970嗯，它的培训内容也相应地相当多。00:46:59.970 --> 00:47:04.800所以它的容量是40千兆字节，大约是以前的10倍。00:47:04.800 --> 00:47:07.215语言模型已经过训练。00:47:07.215 --> 00:47:11.070嗯，当你有这么大的数据集时，00:47:11.070 --> 00:47:14.325嗯，获得这么多文本的唯一方法就是上网。00:47:14.325 --> 00:47:18.840嗯，所以Openai在开发时投入了很多精力00:47:18.840 --> 00:47:23.570这个网络是为了确保文本的高质量。00:47:23.570 --> 00:47:26.180嗯，他们以一种有趣的方式做到了。00:47:26.180 --> 00:47:28.970他们在Reddit网站上看到人们00:47:28.970 --> 00:47:30.140可以在链接上投票。00:47:30.140 --> 00:47:31.640然后他们说如果00:47:31.640 --> 00:47:35.090一个链接有很多选票，那么它可能是一个不错的链接。00:47:35.090 --> 00:47:36.830可能有呃，你知道，00:47:36.830 --> 00:47:39.750有合理的文字供模特学习。00:47:40.610 --> 00:47:43.080嗯，好吧，如果有的话00:47:43.080 --> 00:47:45.600这种超大型的语言模式00:47:45.600 --> 00:47:49.515GPT-2关于你能用它做什么的问题，00:47:49.515 --> 00:47:53.415嗯，很明显，如果你有一个语言模型，你可以用它来做语言建模。00:47:53.415 --> 00:47:56.790但是有一件有趣的事是你00:47:56.790 --> 00:48:00.525可以在ER上运行此语言模型，00:48:00.525 --> 00:48:03.435现有基准，嗯，00:48:03.435 --> 00:48:05.250对于语言建模，嗯，00:48:05.250 --> 00:48:08.520即使在这些基准上，它也会产生最新的困惑。00:48:08.520 --> 00:48:11.700尽管它从未看到这些基准的培训数据，对吗？00:48:11.700 --> 00:48:16.770所以，通常情况下，如果你想说评估你的语言模型在宾州树库。00:48:16.770 --> 00:48:21.510你先是在宾夕法尼亚州的Treebank上训练，然后再评估一下这个保留设置。00:48:21.510 --> 00:48:23.790呃，在这种情况下，呃，00:48:23.790 --> 00:48:28.515一个GPT-2仅仅因为看到了如此多的文本和如此大的模型，00:48:28.515 --> 00:48:31.095比其他人都好，00:48:31.095 --> 00:48:34.540即使没有看到这些数据，先前的方法也能奏效。00:48:34.580 --> 00:48:39.430嗯，在一堆不同的语言建模基准上。00:48:40.800 --> 00:48:46.315嗯，但是还有很多其他有趣的实验00:48:46.315 --> 00:48:51.700用这种语言建模，这些都是基于零镜头学习。00:48:51.700 --> 00:48:57.250所以零镜头学习只是意味着尝试去做一个任务而不去训练它。00:48:57.250 --> 00:49:00.445而且，呃，你可以用语言模型来做这个00:49:00.445 --> 00:49:03.460是通过设计一个提示00:49:03.460 --> 00:49:06.880语言模型，然后让它从那里生成00:49:06.880 --> 00:49:11.065希望它能产生一些与您试图解决的任务相关的东西。00:49:11.065 --> 00:49:13.225例如，对于阅读理解，00:49:13.225 --> 00:49:16.090你能做的就是把上下文段落，00:49:16.090 --> 00:49:20.080把问题连起来然后加上00:49:20.080 --> 00:49:21.430结肠是一种方式，00:49:21.430 --> 00:49:22.705我想，告诉模特，00:49:22.705 --> 00:49:25.210“好吧，你应该回答这个问题。”00:49:25.210 --> 00:49:27.790然后让它生成文本，00:49:27.790 --> 00:49:30.940也许它会产生一些实际的答案，00:49:30.940 --> 00:49:32.365嗯，问题是，00:49:32.365 --> 00:49:34.060关注环境。00:49:34.060 --> 00:49:37.390[噪音]嗯，同样地，为了总结，00:49:37.390 --> 00:49:41.740然后您可以得到本文tl；dr，也许模型将生成摘要。00:49:41.740 --> 00:49:43.795嗯，你甚至可以做翻译，00:49:43.795 --> 00:49:45.655你给模特的地方，00:49:45.655 --> 00:49:49.720嗯，一些前-一份已知的英法翻译清单，所以你，有点，00:49:49.720 --> 00:49:53.770最好告诉它应该做翻译，然后你给00:49:53.770 --> 00:49:58.120它的源序列等于空白，只运行它，00:49:58.120 --> 00:49:59.920嗯，也许会产生，00:49:59.920 --> 00:50:02.690嗯，目标语言中的序列。00:50:03.300 --> 00:50:06.895嗯，好吧。所以结果是这样的。00:50:06.895 --> 00:50:09.100嗯，所有这些，00:50:09.100 --> 00:50:11.545呃，X轴是，00:50:11.545 --> 00:50:16.210是对数比例模型尺寸，Y轴是精确的，嗯，00:50:16.210 --> 00:50:18.715虚线基本上对应，00:50:18.715 --> 00:50:22.090嗯，这些任务的现有工作。00:50:22.090 --> 00:50:26.290嗯，所以在这些任务中，嗯，00:50:26.290 --> 00:50:31.765GPT-2远远低于现有系统，00:50:31.765 --> 00:50:33.625嗯，但当然有很大的区别，对吧？00:50:33.625 --> 00:50:37.195现有系统经过专门培训，00:50:37.195 --> 00:50:39.775嗯，不管他们在做什么工作，00:50:39.775 --> 00:50:42.520其中gpt-2是um，00:50:42.520 --> 00:50:46.540只有接受过语言建模培训，并且在学习语言建模时，00:50:46.540 --> 00:50:48.865这有点像是在处理其他任务。00:50:48.865 --> 00:50:50.785嗯，没错。例如，嗯，00:50:50.785 --> 00:50:54.385它有，呃，从英语到法语的机器翻译，嗯，00:50:54.385 --> 00:50:56.875不如，呃，00:50:56.875 --> 00:51:00.400标准的无监督机器翻译就是那些，00:51:00.400 --> 00:51:02.920虚线，嗯，但它仍然，00:51:02.920 --> 00:51:04.300它仍然很好。00:51:04.300 --> 00:51:06.370还有一点，00:51:06.370 --> 00:51:07.810有趣的是趋势线，对吧，00:51:07.810 --> 00:51:09.520几乎所有这些任务。00:51:09.520 --> 00:51:11.530嗯，表演开始了，00:51:11.530 --> 00:51:13.600随着模型尺寸的增加，效果会更好。00:51:13.600 --> 00:51:18.535[噪音]嗯，我觉得特别有趣，00:51:18.535 --> 00:51:21.580呃，其中一个任务是机器翻译，对吧？00:51:21.580 --> 00:51:23.290所以问题是，它是如何做到的00:51:23.290 --> 00:51:26.440机器翻译当我们把它当作一堆00:51:26.440 --> 00:51:28.540网页和那些网页几乎都在00:51:28.540 --> 00:51:31.810英语，但不知怎的，它神奇地变了起来，00:51:31.810 --> 00:51:33.340一点机器翻译，对吧。00:51:33.340 --> 00:51:35.395所以这不是一个很好的模型，但它仍然可以，00:51:35.395 --> 00:51:38.260嗯，你知道，在某些情况下做一件体面的工作。00:51:38.260 --> 00:51:40.510嗯，答案是，00:51:40.510 --> 00:51:43.810如果你看看这个庞大的英语语料库，00:51:43.810 --> 00:51:47.050偶尔，呃，在语料库内，00:51:47.050 --> 00:51:48.880你看到翻译的例子了吧？00:51:48.880 --> 00:51:50.290你看，嗯，00:51:50.290 --> 00:51:52.810法语成语及其翻译或00:51:52.810 --> 00:51:56.035引自法语的人，然后用英语翻译。00:51:56.035 --> 00:51:57.400而且，嗯，有点，00:51:57.400 --> 00:52:00.700令人惊讶的是，我认为这个大模型，嗯，00:52:00.700 --> 00:52:05.380看到这些例子，它实际上开始学习如何生成法语，00:52:05.380 --> 00:52:07.030嗯，尽管那不是真的，00:52:07.030 --> 00:52:09.980某种程度上，是培训的一部分。00:52:11.970 --> 00:52:14.560嗯，另一个有趣的，嗯，00:52:14.560 --> 00:52:18.700更深入一点的研究是它回答问题的能力。00:52:18.700 --> 00:52:24.040所以，一个简单的回答问题的基线可以得到1%的准确率，00:52:24.040 --> 00:52:27.295GPT-2的精确度仅为4%。00:52:27.295 --> 00:52:28.840所以这不是，就像，你知道，00:52:28.840 --> 00:52:32.440超级神奇的解决了问题回答，嗯，但是，嗯，00:52:32.440 --> 00:52:34.420这仍然很有趣，00:52:34.420 --> 00:52:37.435如果你看答案模型最有信心，00:52:37.435 --> 00:52:39.010你可以看到它有点00:52:39.010 --> 00:52:41.320已经了解了一些关于世界的事实，对吧。00:52:41.320 --> 00:52:45.550据了解，查尔斯・达尔文写了物种起源。00:52:45.550 --> 00:52:50.740嗯，通常在NLP的历史上，如果你想得到，有点，00:52:50.740 --> 00:52:52.765将世界知识融入到一个NLP系统中，00:52:52.765 --> 00:52:55.435你需要一个大的事实数据库。00:52:55.435 --> 00:52:57.340即使这仍然是，00:52:57.340 --> 00:52:59.500有点，很早就开始了，嗯，00:52:59.500 --> 00:53:04.0004%的准确率和，呃，你知道，00:53:04.000 --> 00:53:05.87570%左右，呃，00:53:05.875 --> 00:53:09.550最先进的开放领域问答系统可以做到，00:53:09.550 --> 00:53:12.010嗯，它，它，嗯，00:53:12.010 --> 00:53:14.200它仍然可以，呃，00:53:14.200 --> 00:53:17.740仅仅通过阅读大量的文本来获取一些世界知识，嗯，没有，00:53:17.740 --> 00:53:21.895有点，明确地将这些知识放入模型中。00:53:21.895 --> 00:53:26.810嗯，到目前为止GPT-2还有什么问题吗？00:53:28.050 --> 00:53:33.865可以。所以一个有趣的问题是，00:53:33.865 --> 00:53:36.505如果我们的模型变得更大会发生什么？00:53:36.505 --> 00:53:38.305嗯，我已经完成了，嗯，00:53:38.305 --> 00:53:42.565在PowerPoint中画一些线条，看看它们在哪里相遇是非常科学的事情。00:53:42.565 --> 00:53:44.655嗯，你可以看到，嗯，00:53:44.655 --> 00:53:48.429如果趋势保持在1万亿个参数左右，00:53:48.429 --> 00:53:52.390嗯，我们要达到人的阅读理解水平。00:53:52.390 --> 00:53:55.480嗯，如果这是真的，那真的会让人吃惊。00:53:55.480 --> 00:54:00.505我真的希望1万亿参数模型能在00:54:00.505 --> 00:54:02.155我不知道，十年左右，00:54:02.155 --> 00:54:04.240嗯，但当然，00:54:04.240 --> 00:54:05.665是的，趋势还不清楚。00:54:05.665 --> 00:54:07.630例如，如果你看总结，00:54:07.630 --> 00:54:09.040好像表演已经开始了，00:54:09.040 --> 00:54:11.005呃，呃，结束了。00:54:11.005 --> 00:54:15.760嗯，所以我认为这将是一件非常有趣的事情00:54:15.760 --> 00:54:17.980展望NLP的未来，嗯，00:54:17.980 --> 00:54:20.710是比例的变化，00:54:20.710 --> 00:54:23.570嗯，接近NLP的方式。00:54:24.120 --> 00:54:29.755关于gpt-2的另一个有趣的事情是它的反应00:54:29.755 --> 00:54:32.125媒体和其他研究人员。00:54:32.125 --> 00:54:35.455嗯，真正的原因是00:54:35.455 --> 00:54:39.295关于这一点，很多争议都来自于Openai的声明。00:54:39.295 --> 00:54:43.000他们说，“我们不会发布我们的全语言模型，00:54:43.000 --> 00:54:44.590嗯，因为太危险了，00:54:44.590 --> 00:54:46.015你知道，我们的语言模式太好了。00:54:46.015 --> 00:54:51.115嗯，所以媒体真的很喜欢这个，00:54:51.115 --> 00:54:52.330你知道，这么说，00:54:52.330 --> 00:54:55.135嗯，机器学习会破坏互联网。00:54:55.135 --> 00:55:00.580嗯，我们的研究人员也有一些非常有趣的反应，对吧。00:55:00.580 --> 00:55:02.020嗯，有一些，00:55:02.020 --> 00:55:04.195有点，舌尖对这里的反应，对吧。00:55:04.195 --> 00:55:05.755你知道，我是在mnist上训练这个模特的。00:55:05.755 --> 00:55:07.915释放它对我来说太危险了吗？00:55:07.915 --> 00:55:11.530嗯，同样的，我们也做了非常好的工作00:55:11.530 --> 00:55:15.715但我们不能释放它太危险了，所以你只能相信我们。00:55:15.715 --> 00:55:18.970看看更多，有点，有道理，嗯，00:55:18.970 --> 00:55:20.665关于这个问题的辩论，00:55:20.665 --> 00:55:22.885你仍然可以看到文章，00:55:22.885 --> 00:55:24.610嗯，争论双方。00:55:24.610 --> 00:55:26.469这是两篇AR文章，00:55:26.469 --> 00:55:29.545从梯度来看，00:55:29.545 --> 00:55:31.690机器学习时事通讯，嗯，00:55:31.690 --> 00:55:35.875他们在争论这个问题的完全相反的方面，00:55:35.875 --> 00:55:38.510嗯，是不是应该放出来。00:55:40.770 --> 00:55:47.120所以我想我可以简单地回顾一下一些赞成或反对的论点。00:55:47.130 --> 00:55:50.185关于这件事有很多争论，我不想00:55:50.185 --> 00:55:53.480深入讨论一个有争议的问题，00:55:54.150 --> 00:55:56.710嗯，但这里有一长串，00:55:56.710 --> 00:55:58.570有点，人们说过的，对吧。00:55:58.570 --> 00:56:01.450所以，嗯，这就是为什么你应该释放。00:56:01.450 --> 00:56:03.280一个抱怨是，00:56:03.280 --> 00:56:05.065这个模型真的很特别吗？00:56:05.065 --> 00:56:06.595这里没有什么新鲜事。00:56:06.595 --> 00:56:09.640比以前的型号大10倍，嗯，00:56:09.640 --> 00:56:11.860还有一些观点认为，00:56:11.860 --> 00:56:14.500嗯，即使这个还没有发布，你知道，00:56:14.500 --> 00:56:17.185五年后，每个人都能训练出这样好的模特，嗯，00:56:17.185 --> 00:56:22.270实际上，如果你看图像识别或者看图像和语音数据，00:56:22.270 --> 00:56:25.780已经有可能合成高度令人信服的，00:56:25.780 --> 00:56:28.405嗯，假象和假话。00:56:28.405 --> 00:56:34.750所以有点，是什么让这个东西不同于其他的，嗯，系统。00:56:34.750 --> 00:56:36.310说到其他系统，是吗？00:56:36.310 --> 00:56:38.335photoshop已经存在很长时间了，00:56:38.335 --> 00:56:41.950所以我们已经可以令人信服地伪造图像了，嗯，00:56:41.950 --> 00:56:44.140人们刚刚学会了调整和学习00:56:44.140 --> 00:56:46.645你不应该总是相信图像中的东西，00:56:46.645 --> 00:56:47.995嗯，因为可能是，00:56:47.995 --> 00:56:50.065嗯，在某种程度上改变了。00:56:50.065 --> 00:56:52.450嗯，另一方面，你可以说，00:56:52.450 --> 00:56:55.780“好吧，呃，photoshop存在，但是，嗯，你不能，有点，00:56:55.780 --> 00:57:00.130扩大photoshop的规模，开始以这种方式大量生产假内容00:57:00.130 --> 00:57:04.660“关于模特，”他们指出，嗯，假新闻的危险，嗯，00:57:04.660 --> 00:57:08.950虚假评论，嗯，一般来说，只是铺天盖地，这基本上意味着，00:57:08.950 --> 00:57:15.370嗯，创建假用户内容来支持你希望其他人持有的视图。00:57:15.370 --> 00:57:18.870嗯，这实际上是已经做过的事情，00:57:18.870 --> 00:57:21.660嗯，相当广泛的国家公司和政府。00:57:21.660 --> 00:57:23.475有很多证据证明这一点，嗯，00:57:23.475 --> 00:57:25.500但他们当然会雇佣员工00:57:25.500 --> 00:57:27.795把这些评论都写在新闻文章上，比如说00:57:27.795 --> 00:57:30.390我们不想让他们的工作更轻松00:57:30.390 --> 00:57:33.620通过生产一台可能做到这一点的机器。00:57:33.620 --> 00:57:37.330所以，嗯，我不想站在这里，00:57:37.330 --> 00:57:39.565嗯，关于这个还有很多争论。00:57:39.565 --> 00:57:41.110我想，你知道，00:57:41.110 --> 00:57:43.300主要的，主要的外卖是，00:57:43.300 --> 00:57:46.959作为一个关于机器学习和NLP的人的社区，00:57:46.959 --> 00:57:48.910对这件事不太了解，对吧？00:57:48.910 --> 00:57:51.355我们有点惊讶，嗯，00:57:51.355 --> 00:57:56.095Openai的，嗯，这里的决定，嗯，呃，00:57:56.095 --> 00:57:57.760也就是说，你知道，00:57:57.760 --> 00:58:01.120确实有一些人想知道需要做什么00:58:01.120 --> 00:58:05.515完全有责任公开发布。00:58:05.515 --> 00:58:09.430我们应该研究什么样的研究问题等等。00:58:09.430 --> 00:58:11.530[噪音]是的。00:58:11.530 --> 00:58:13.795关于这个有什么问题吗？00:58:13.795 --> 00:58:16.450这种反应还是一般的辩论？00:58:16.450 --> 00:58:20.930[噪音]好的。00:58:22.140 --> 00:58:27.610嗯，我认为这场辩论的结果是，嗯，00:58:27.610 --> 00:58:29.310问题是，嗯，00:58:29.310 --> 00:58:32.580真的，ML人应该是制造这些东西的人，00:58:32.580 --> 00:58:38.085决策还是需要更多跨学科的科学，我们看，嗯，00:58:38.085 --> 00:58:40.425计算机安全专家，00:58:40.425 --> 00:58:42.705嗯，来自社会科学的人，00:58:42.705 --> 00:58:46.185嗯，你知道，那些道德专家，00:58:46.185 --> 00:58:48.365嗯，看看这些决定。00:58:48.365 --> 00:58:54.595嗯，对。所以GPT-2绝对是一个例子，突然间它看起来像，00:58:54.595 --> 00:58:58.420嗯，我们的NLP技术有很多缺陷，对吧。00:58:58.420 --> 00:59:02.005它们可能会被恶意使用或造成损害。00:59:02.005 --> 00:59:05.725我认为这种趋势只会增加，嗯，00:59:05.725 --> 00:59:07.165如果你看，有点，00:59:07.165 --> 00:59:10.540人们正在研究的NLP领域，00:59:10.540 --> 00:59:16.510越来越多的人致力于NLP的高风险应用，00:59:16.510 --> 00:59:19.570嗯，那些经常非常大，嗯，00:59:19.570 --> 00:59:25.280后果，尤其是从偏见和公平的角度考虑。00:59:25.980 --> 00:59:32.420嗯，那么，让我们来看几个例子，嗯-00:59:32.690 --> 00:59:35.955嗯，一个-一些，一些地区，00:59:35.955 --> 00:59:37.875发生这种情况的地方是人们所看到的，00:59:37.875 --> 00:59:40.050呃，全国人民党来看看司法裁决。00:59:40.050 --> 00:59:41.895例如，如果这个人，00:59:41.895 --> 00:59:43.305呃，保释还是不保释？00:59:43.305 --> 00:59:45.210嗯，对于招聘决定，是吗？00:59:45.210 --> 00:59:46.680所以你看别人的简历，00:59:46.680 --> 00:59:48.000你在上面运行NLP，00:59:48.000 --> 00:59:50.775然后你会自动做出决定，00:59:50.775 --> 00:59:53.130嗯，嘘-我们要不要把这份简历扔掉？00:59:53.130 --> 00:59:56.850所以做一些，类似的，筛选，嗯，分级测试。00:59:56.850 --> 00:59:58.650嗯，如果你接受GRE，嗯，00:59:58.650 --> 01:00:00.825你的，你的考试将由一台机器评分。01:00:00.825 --> 01:00:03.090嗯，一个人也会看，嗯，01:00:03.090 --> 01:00:05.295但是，嗯，你知道的，01:00:05.295 --> 01:00:09.090有时是你生命中非常重要的一部分，嗯，当它，01:00:09.090 --> 01:00:11.085当测试结果是，嗯，inf-你知道，01:00:11.085 --> 01:00:14.490影响你，嗯，进入学校，比如说。01:00:14.490 --> 01:00:17.265嗯，所以我认为有-有一些，01:00:17.265 --> 01:00:20.790在这种情况下使用机器学习的一些好方面。01:00:20.790 --> 01:00:24.120一个是我们可以很快地评估，01:00:24.120 --> 01:00:26.985一个机器学习系统和搜索。01:00:26.985 --> 01:00:28.680有什么偏见吗？01:00:28.680 --> 01:00:31.350只需在一堆数据上运行它并查看它的功能，01:00:31.350 --> 01:00:34.350更重要的是，01:00:34.350 --> 01:00:35.640嗯，我们可以解决这个问题，01:00:35.640 --> 01:00:37.080如果出现问题，对吗？01:00:37.080 --> 01:00:42.240所以，嗯，修复一个屏幕恢复的机器学习系统可能更容易，01:00:42.240 --> 01:00:44.730你知道，要解决的问题比解决的问题更重要，01:00:44.730 --> 01:00:48.3005000个有点性别歧视的高管，对吧？01:00:48.300 --> 01:00:49.725所以，这样，01:00:49.725 --> 01:00:51.180嗯，有一种，01:00:51.180 --> 01:00:57.840在这些高风险的决策中使用机器学习的积极角度。01:00:57.840 --> 01:01:00.015嗯，另一方面，嗯，01:01:00.015 --> 01:01:02.220大家都知道，01:01:02.220 --> 01:01:04.770我知道你有一个关于偏见和公平的讲座，01:01:04.770 --> 01:01:07.770机器学习经常反映出数据集中的偏差，01:01:07.770 --> 01:01:11.025嗯，它甚至可以放大数据集中的偏差。01:01:11.025 --> 01:01:12.660嗯，有点担心，01:01:12.660 --> 01:01:15.315有偏算法的反馈回路01:01:15.315 --> 01:01:18.360实际上会导致产生更多有偏见的数据，01:01:18.360 --> 01:01:23.050嗯，在这种情况下，这些问题只会加剧并恶化。01:01:23.150 --> 01:01:28.950嗯，所以对于所有的，呃，高影响力的决定，01:01:28.950 --> 01:01:30.990嗯，我，我在幻灯片上列出了，01:01:30.990 --> 01:01:34.320有一些例子说明事情出了差错，对吧？01:01:34.320 --> 01:01:36.690所以亚马逊有一些人工智能，01:01:36.690 --> 01:01:39.975嗯，作为一个招聘工具，结果是性别歧视。01:01:39.975 --> 01:01:42.255嗯，嗯，有一些，有点，01:01:42.255 --> 01:01:44.550早期使用人工智能的飞行员，嗯，01:01:44.550 --> 01:01:46.680在司法系统中，他们也有，01:01:46.680 --> 01:01:49.710嗯，在某些情况下，结果很糟糕。01:01:49.710 --> 01:01:52.920嗯，如果你看自动的，01:01:52.920 --> 01:01:54.855自动论文评分，嗯，01:01:54.855 --> 01:01:56.430不是很好，01:01:56.430 --> 01:01:57.720你知道，NLP系统，对吧？01:01:57.720 --> 01:01:59.730这是一个例子，嗯，01:01:59.730 --> 01:02:02.355一篇文章的摘录，嗯，01:02:02.355 --> 01:02:06.240GRE测试使用的自动评分系统给出，01:02:06.240 --> 01:02:08.040得分很高，嗯，01:02:08.040 --> 01:02:10.230但实际上，它只是，某种程度上，一个固体的，01:02:10.230 --> 01:02:12.420嗯，花言巧语，那是01:02:12.420 --> 01:02:16.060足以让模型相信这是一篇伟大的论文。01:02:17.240 --> 01:02:19.410最后一个，嗯，01:02:19.410 --> 01:02:21.555我想谈谈的地方，哪里，嗯，01:02:21.555 --> 01:02:23.550你可以看到确实有一些风险和01:02:23.550 --> 01:02:26.655使用NLP技术的一些陷阱是聊天机器人。01:02:26.655 --> 01:02:31.560嗯，所以我认为聊天机器人确实有一个非常有益的一面。01:02:31.560 --> 01:02:33.930嗯，Woebot就是一个例子，01:02:33.930 --> 01:02:37.545这家公司有这个聊天机器人，如果你没有，你可以和它聊天，01:02:37.545 --> 01:02:39.480嗯，感觉太好了，会努力的，01:02:39.480 --> 01:02:41.565嗯，我不知道，让你振作起来。01:02:41.565 --> 01:02:43.830嗯，所以，所以，你知道，01:02:43.830 --> 01:02:46.770可能是一种非常好的技术，可以帮助人们，01:02:46.770 --> 01:02:49.380嗯，但另一方面，有很大的风险。01:02:49.380 --> 01:02:53.520所以，微软研究院的一个例子就是在tweets上训练了聊天机器人。01:02:53.520 --> 01:02:56.850它很快就开始说种族主义的话，不得不被拉出来。01:02:56.850 --> 01:02:59.625嗯，所以我认为所有这些都突显了，嗯，01:02:59.625 --> 01:03:02.505随着NLP越来越有效，01:03:02.505 --> 01:03:05.835人们看到了在，嗯，01:03:05.835 --> 01:03:09.300越来越高的风险决策，尽管，01:03:09.300 --> 01:03:11.775你知道，有一些不错的-有一些吸引力，01:03:11.775 --> 01:03:14.310嗯，还有很多风险。01:03:14.310 --> 01:03:17.310嗯，还有什么问题吗，呃，01:03:17.310 --> 01:03:20.620NLP的这种社会影响？01:03:21.650 --> 01:03:29.250可以。嗯，这节课的最后一部分是看未来的研究，对吗？01:03:29.250 --> 01:03:30.465尤其是，嗯，01:03:30.465 --> 01:03:33.510我认为目前很多研究趋势是，01:03:33.510 --> 01:03:35.760对伯特的反应，嗯，对吧？01:03:35.760 --> 01:03:40.080所以，问题是伯特解决了什么，我们下一步要做什么？01:03:40.080 --> 01:03:44.295嗯，这是胶水基准测试的结果。01:03:44.295 --> 01:03:47.070嗯，那是，嗯，一份，01:03:47.070 --> 01:03:50.28010项自然语言理解任务。01:03:50.280 --> 01:03:54.420嗯，这10项任务的平均分是多少。01:03:54.420 --> 01:03:57.810嗯，左边，嗯，两个-两个是，01:03:57.810 --> 01:04:00.720对不起，对了-两个最合适的型号是，01:04:00.720 --> 01:04:03.330呃，呃，S-不，呃，01:04:03.330 --> 01:04:06.480只是监督培训过的机器学习系统，对吗？01:04:06.480 --> 01:04:08.355我们有一袋向量，嗯，01:04:08.355 --> 01:04:10.920相反，我们使用我们奇特的神经网络架构01:04:10.920 --> 01:04:13.650
of BiLSTM + Attention and we get about five points.

01:04:13.650 --> 01:04:15.600嗯，但是伯特的收获，01:04:15.600 --> 01:04:17.520呃，真的比这差小，对吧？01:04:17.520 --> 01:04:19.890所以，伯特提高了，呃，01:04:19.890 --> 01:04:24.12017分，最后我们的比分非常接近，01:04:24.120 --> 01:04:26.925嗯，这些任务的人的表现。01:04:26.925 --> 01:04:29.820嗯，那么一个，有点，01:04:29.820 --> 01:04:32.220人们想知道的是，01:04:32.220 --> 01:04:35.115这就是建筑工程的死亡吗？01:04:35.115 --> 01:04:39.225嗯，所以我敢肯定你们所有参与过默认最终项目的人，嗯，01:04:39.225 --> 01:04:42.570看到了一大堆不同的花哨图片，01:04:42.570 --> 01:04:44.490呃，解决小组的架构。01:04:44.490 --> 01:04:46.710嗯，有很多文件。01:04:46.710 --> 01:04:48.390他们都提出了一些，01:04:48.390 --> 01:04:50.895呃，注意力机制之类的。01:04:50.895 --> 01:04:53.880嗯，还有，嗯，是的。01:04:53.880 --> 01:04:55.170和伯特在一起，有点，01:04:55.170 --> 01:04:56.970嗯，你不需要这么做，对吧？01:04:56.970 --> 01:04:59.190你只要训练一个变压器，给它足够的数据，01:04:59.190 --> 01:05:01.020事实上你在球队里表现出色，01:05:01.020 --> 01:05:03.885你知道，也许，嗯，这些，呃，01:05:03.885 --> 01:05:07.800架构增强不一定是，嗯，01:05:07.800 --> 01:05:10.590推动进步的关键因素，01:05:10.590 --> 01:05:13.720嗯，改进这些任务的结果。01:05:14.150 --> 01:05:16.740嗯，对。所以，呃，01:05:16.740 --> 01:05:18.630如果你以研究者的视角来看待这个问题，01:05:18.630 --> 01:05:20.610你可以认为研究人员会说，“好吧，01:05:20.610 --> 01:05:23.520我可以花六个月的时间为01:05:23.520 --> 01:05:27.930如果我做得好，也许我会把成绩提高1分，嗯，F1分。”01:05:27.930 --> 01:05:30.030嗯，但是对于伯特来说，01:05:30.030 --> 01:05:32.160把他们的模型增加3倍，01:05:32.160 --> 01:05:33.240这就是两者的区别，01:05:33.240 --> 01:05:36.090它们就像一个基本尺寸的模型和一个大模型，01:05:36.090 --> 01:05:39.585嗯，把结果提高了5个F1点。01:05:39.585 --> 01:05:42.150嗯，所以它确实表明我们需要，某种程度上，01:05:42.150 --> 01:05:46.635重新排定优先次序，嗯，我们会追求哪种研究途径，01:05:46.635 --> 01:05:49.500因为这个建筑工程没有提供，01:05:49.500 --> 01:05:52.605时间投资收益的方式，01:05:52.605 --> 01:05:54.765嗯，利用未标记的数据是。01:05:54.765 --> 01:05:57.735嗯，那么现在，如果你看看球队排行榜，嗯，01:05:57.735 --> 01:06:02.350我认为至少前20名都是伯特加上一些东西。01:06:04.190 --> 01:06:07.725嗯，还有一个问题，呃，01:06:07.725 --> 01:06:09.540我想伯特已经长大了，01:06:09.540 --> 01:06:11.400嗯，我们需要更艰巨的任务，对吧？01:06:11.400 --> 01:06:13.560伯特几乎解决了球队的问题，01:06:13.560 --> 01:06:15.060如果你用，呃，01:06:15.060 --> 01:06:16.860接近人类的表现。01:06:16.860 --> 01:06:19.230嗯，所以有，嗯，01:06:19.230 --> 01:06:22.635新数据集的增长，呃，01:06:22.635 --> 01:06:25.020更具挑战性，有几种方法，01:06:25.020 --> 01:06:26.370嗯，它们可能更具挑战性。01:06:26.370 --> 01:06:28.140一个是，嗯，01:06:28.140 --> 01:06:30.240对较长的文件进行阅读理解，01:06:30.240 --> 01:06:32.625或者跨多个文档执行。01:06:32.625 --> 01:06:35.280嗯，有一个区域在看C-Uh，01:06:35.280 --> 01:06:38.850提出更难的问题，需要多跳推理。01:06:38.850 --> 01:06:41.550嗯，所以这基本上是测量-意味着你必须串01:06:41.550 --> 01:06:45.180来自不同地方的多个支持性事实，01:06:45.180 --> 01:06:47.670嗯，为了得到正确的答案。01:06:47.670 --> 01:06:49.350嗯，还有另一个地区，01:06:49.350 --> 01:06:51.870将问题回答置于对话中。01:06:51.870 --> 01:06:54.330嗯，还有一种，01:06:54.330 --> 01:06:58.260阅读理解数据集的构造细节，01:06:58.260 --> 01:07:00.600这确实影响了，01:07:00.600 --> 01:07:02.835嗯，任务的难度。01:07:02.835 --> 01:07:04.110这就是，嗯，01:07:04.110 --> 01:07:06.495当你创建这些数据集时，01:07:06.495 --> 01:07:09.420是写文章问题的人，01:07:09.420 --> 01:07:11.535他们能看到那条通道吗？01:07:11.535 --> 01:07:14.070嗯，所以，当然，更容易出现01:07:14.070 --> 01:07:16.110当你看到这段话的时候，01:07:16.110 --> 01:07:18.870如果你在看不到文章的情况下提出一个问题，01:07:18.870 --> 01:07:21.810你甚至可能没有一个可以回答的问题。01:07:21.810 --> 01:07:23.730嗯，但是看问题01:07:23.730 --> 01:07:26.460这段话首先是不现实的，对吧？01:07:26.460 --> 01:07:28.845所以，呃，如果我问一个问题，你知道，01:07:28.845 --> 01:07:30.585我通常不会01:07:30.585 --> 01:07:33.870坐在我面前回答那个问题的段落。01:07:33.870 --> 01:07:35.670嗯，除此之外，01:07:35.670 --> 01:07:37.560它确实鼓励简单的问题，对吗？01:07:37.560 --> 01:07:39.840所以，如果你是一个机械土耳其人，01:07:39.840 --> 01:07:42.869你可以写尽可能多的问题，01:07:42.869 --> 01:07:44.790然后你看到一篇文章说，01:07:44.790 --> 01:07:46.350嗯，我不知道，你知道，01:07:46.350 --> 01:07:50.040亚伯拉罕・林肯是美国第16任总统，01:07:50.040 --> 01:07:51.600嗯，你要写什么？01:07:51.600 --> 01:07:53.100作为你的问题，你要写，01:07:53.100 --> 01:07:55.365他是美国第16任总统。01:07:55.365 --> 01:07:58.035你不会写一些更有趣、更难回答的东西。01:07:58.035 --> 01:08:01.890嗯，所以这是众包数据集改变的一种方式，嗯，01:08:01.890 --> 01:08:04.170人们现在确定问题是，01:08:04.170 --> 01:08:07.410有点独立于上下文。01:08:07.410 --> 01:08:09.375嗯，所以我要简单地，呃，01:08:09.375 --> 01:08:11.610检查这一行中的几个新数据集。01:08:11.610 --> 01:08:15.150所以有一个叫做quac，它代表上下文中的问答。01:08:15.150 --> 01:08:16.815嗯，在这个数据集中，01:08:16.815 --> 01:08:18.690有一个老师和一个学生，01:08:18.690 --> 01:08:21.390嗯，老师看到一篇维基百科文章。01:08:21.390 --> 01:08:24.195这个学生想了解维基百科的这篇文章，01:08:24.195 --> 01:08:28.005目标是训练一个充当教师的机器学习模型。01:08:28.005 --> 01:08:30.000嗯，所以你可以想象未来，这个，01:08:30.000 --> 01:08:32.190某种程度上，技术对，01:08:32.190 --> 01:08:34.320呃，嗯，教育，有点，01:08:34.320 --> 01:08:37.035增加了一些自动化。01:08:37.035 --> 01:08:42.495嗯，呃，有一件事让这项任务变得困难，01:08:42.495 --> 01:08:46.545嗯，问题取决于整个谈话的历史。01:08:46.545 --> 01:08:48.225嗯，例如，呃，01:08:48.225 --> 01:08:50.790如果你看左边，呃，01:08:50.790 --> 01:08:54.810例如，嗯，对话，01:08:54.810 --> 01:08:57.315第三个问题是他是明星吗？01:08:57.315 --> 01:09:02.070嗯，很明显你不能回答这个问题，除非你在对话中回顾一下，01:09:02.070 --> 01:09:04.095认识到这个问题，01:09:04.095 --> 01:09:06.180嗯，谈话是愚蠢的鸭子。01:09:06.180 --> 01:09:09.060嗯，还有，有点，01:09:09.060 --> 01:09:11.040因为这个数据集更具挑战性，01:09:11.040 --> 01:09:14.340你可以看到，人类的表现有一个更大的差距，对吧？01:09:14.340 --> 01:09:17.610所以如果你用一些扩展训练伯特，你会…01:09:17.610 --> 01:09:22.185结果仍然比人类的表现差15个F1点。01:09:22.185 --> 01:09:28.935这是另一个数据集，叫做Hotpotqa。01:09:28.935 --> 01:09:30.510嗯，是，呃，01:09:30.510 --> 01:09:32.760设计用于多跳推理。01:09:32.760 --> 01:09:35.610嗯，本质上，为了回答一个问题，01:09:35.610 --> 01:09:37.875你必须查看多个文档，01:09:37.875 --> 01:09:40.350你必须从这些文件中找出不同的事实，01:09:40.350 --> 01:09:41.925做一些推断，01:09:41.925 --> 01:09:44.655嗯，为了得到正确的答案。01:09:44.655 --> 01:09:48.645嗯，所以我想，你知道，这是一项非常艰巨的任务。01:09:48.645 --> 01:09:54.040再说一次，嗯，人的表现之间有很大的差距。01:09:54.590 --> 01:09:57.390嗯，有什么问题吗，呃，01:09:57.390 --> 01:10:01.720新的数据集，嗯，对NLP来说更困难的chi-任务？01:10:01.900 --> 01:10:07.035可以。嗯，我会的，01:10:07.035 --> 01:10:09.360一种快速的火焰，穿过，嗯，01:10:09.360 --> 01:10:12.210在本次谈话的最后几分钟还有几个方面。01:10:12.210 --> 01:10:16.335嗯，所以我认为多任务学习真的越来越重要了。01:10:16.335 --> 01:10:18.390嗯，当然，嗯，01:10:18.390 --> 01:10:20.190你已经讲了一整堂课了，对吧？01:10:20.190 --> 01:10:21.750所以我不会花太多时间在上面。01:10:21.750 --> 01:10:24.330嗯，但也许有一个，呃，01:10:24.330 --> 01:10:28.920有趣的是，如果你看看这个胶水基准测试的性能，01:10:28.920 --> 01:10:31.320所以这是自然语言理解的基准，01:10:31.320 --> 01:10:34.920嗯，所有的顶级组合结果，嗯，01:10:34.920 --> 01:10:37.980是-那现在实际上超过了伯特在01:10:37.980 --> 01:10:42.390绩效是――以多任务的方式训练伯特。01:10:42.390 --> 01:10:47.370嗯，我觉得另一个有趣的，呃，01:10:47.370 --> 01:10:52.020多任务学习的动机是，如果你在训练伯特，你会01:10:52.020 --> 01:10:54.480非常大的型号和一种制造方法01:10:54.480 --> 01:10:58.690更有效地使用这个模型是训练它同时做许多事情。01:11:00.950 --> 01:11:04.920另一个绝对重要的领域，嗯，01:11:04.920 --> 01:11:09.090我认为未来处理低资源环境很重要。01:11:09.090 --> 01:11:10.890嗯，这里我用的是非常广泛的，01:11:10.890 --> 01:11:13.020呃，资源的定义，对吧。01:11:13.020 --> 01:11:15.435这可能意味着计算能力，嗯，你知道，01:11:15.435 --> 01:11:18.990伯特很棒，但运行它也需要大量的计算。01:11:18.990 --> 01:11:20.310所以说这是不现实的，01:11:20.310 --> 01:11:22.545嗯，如果你在建，我们假设一部手机，呃，01:11:22.545 --> 01:11:27.510一个移动设备的应用程序，你可以运行一个伯特大小的模型。01:11:27.510 --> 01:11:31.845嗯，正如我之前在谈话中提到的，嗯，你知道的，01:11:31.845 --> 01:11:36.225低资源语言是一个我认为很漂亮的领域，嗯，01:11:36.225 --> 01:11:39.120目前在NLP研究中的代表不足，01:11:39.120 --> 01:11:41.460因为大多数数据集都是英文的，嗯，01:11:41.460 --> 01:11:42.570但我认为，是的，01:11:42.570 --> 01:11:44.130真的，你知道，01:11:44.130 --> 01:11:49.245很多人为了从NLP技术中获益，01:11:49.245 --> 01:11:52.200我们需要有能在很多01:11:52.200 --> 01:11:56.055不同的语言，尤其是那些没有太多训练数据的语言。01:11:56.055 --> 01:12:00.870而且，嗯，说到低-低数量的培训数据，我认为一般来说，01:12:00.870 --> 01:12:04.065呃，A-一个有趣的研究领域，01:12:04.065 --> 01:12:05.550嗯，在机器学习中。01:12:05.550 --> 01:12:07.305事实上，人们是，嗯，01:12:07.305 --> 01:12:09.315在这方面也做了很多工作。01:12:09.315 --> 01:12:11.460嗯，所以一个术语通常是，呃，01:12:11.460 --> 01:12:14.025常用的一个术语是“少量学习”。01:12:14.025 --> 01:12:16.410嗯，这基本上意味着能够01:12:16.410 --> 01:12:18.720训练一个只看到01:12:18.720 --> 01:12:20.730我们来举五到十个例子。01:12:20.730 --> 01:12:23.370嗯，有一个动机，嗯，01:12:23.370 --> 01:12:29.445我认为我们现有的机器学习系统是如何学习的，01:12:29.445 --> 01:12:31.875人类是如何学习的，嗯，01:12:31.875 --> 01:12:35.550人类可以很快地从五个左右的例子中归纳出来。01:12:35.550 --> 01:12:37.185如果你在训练神经网络，01:12:37.185 --> 01:12:38.580你通常需要，你知道，01:12:38.580 --> 01:12:41.610成千上万的例子，甚至数万个，01:12:41.610 --> 01:12:45.060数以万计的例子来获得有用的东西。01:12:45.060 --> 01:12:49.650嗯，所以我也认为这是未来一个非常重要的领域。01:12:49.650 --> 01:12:53.730嗯，最后一个我想进去的地方，嗯，01:12:53.730 --> 01:12:57.600更深入的一点是解释和理解模型。01:12:57.600 --> 01:13:00.570嗯，所以，这真的有两个方面。01:13:00.570 --> 01:13:04.095一个是如果我有一个机器学习模型，它会做出预测，01:13:04.095 --> 01:13:06.450我想，呃，01:13:06.450 --> 01:13:08.790知道它为什么做出这样的预测吗？01:13:08.790 --> 01:13:11.385所以有了一些理由，有了一些解释，01:13:11.385 --> 01:13:15.180嗯，这在像医疗保健这样的领域尤其重要，对吗？01:13:15.180 --> 01:13:17.910所以如果你是个医生，你在做决定，嗯，01:13:17.910 --> 01:13:21.090你的机器学习模式可能还不够好，01:13:21.090 --> 01:13:22.470“患者患有X病。”01:13:22.470 --> 01:13:23.805你真的想说，01:13:23.805 --> 01:13:26.070“由于这些原因，患者患有X病。”01:13:26.070 --> 01:13:28.589嗯，因为你作为一名医生可以复查，01:13:28.589 --> 01:13:30.540并尝试验证，01:13:30.540 --> 01:13:33.165呃，机器，嗯，我想，01:13:33.165 --> 01:13:35.610嗯，想出来那个诊断。01:13:35.610 --> 01:13:38.640嗯，另一个解释领域01:13:38.640 --> 01:13:41.370理解模型更像是一个科学问题，对吗？01:13:41.370 --> 01:13:43.860我们知道伯特这样的人工作得很好吗？01:13:43.860 --> 01:13:45.960嗯，我们想知道他们为什么工作得很好？01:13:45.960 --> 01:13:48.195他们模拟语言的哪些方面？01:13:48.195 --> 01:13:49.995嗯，他们没有做什么模型？01:13:49.995 --> 01:13:52.020嗯，这可能会导致，嗯，01:13:52.020 --> 01:13:55.695改进的想法，嗯，那些-那些模型。01:13:55.695 --> 01:13:59.580嗯，那么，嗯，这是一个，呃，01:13:59.580 --> 01:14:04.935几张关于评估主要方法的幻灯片-回答科学问题。01:14:04.935 --> 01:14:06.975机器学习模型学习什么？01:14:06.975 --> 01:14:10.530嗯，你要做的是你有一个模型，所以我们假设它是伯特。01:14:10.530 --> 01:14:13.440它需要输入一系列单词，嗯，01:14:13.440 --> 01:14:16.470它产生一系列向量作为输出，嗯，01:14:16.470 --> 01:14:18.570我们想问一下，它知道吗，例如，01:14:18.570 --> 01:14:19.680言语的一部分？01:14:19.680 --> 01:14:22.455所以，它在向量表示中是这样的，01:14:22.455 --> 01:14:24.630它捕获了一些关于语法的信息吗？01:14:24.630 --> 01:14:29.850嗯，问这个问题的一个简单方法是在Bert上训练另一个分类器，01:14:29.850 --> 01:14:31.965嗯，这是训练的，01:14:31.965 --> 01:14:34.395嗯，我们来说说语音标记的一部分。01:14:34.395 --> 01:14:36.825嗯，但我们只有，嗯，01:14:36.825 --> 01:14:39.945回到诊断分类器本身。01:14:39.945 --> 01:14:43.680换句话说，我们是在处理伯特的输出，嗯，01:14:43.680 --> 01:14:46.185作为固定输入的向量序列，01:14:46.185 --> 01:14:48.600我们正在探索这些向量，01:14:48.600 --> 01:14:50.505嗯，它们含有吗，嗯，01:14:50.505 --> 01:14:52.440关于演讲的一部分的信息01:14:52.440 --> 01:14:56.445上面的第二个诊断分类器可以解码，01:14:56.445 --> 01:14:59.050嗯，要得到正确的标签吗？01:14:59.120 --> 01:15:03.690嗯，所以，嗯，这是相当多的关注点。01:15:03.690 --> 01:15:06.540嗯，一个问题是，01:15:06.540 --> 01:15:09.915如果你的诊断分类器太复杂，01:15:09.915 --> 01:15:13.200它只需解决分类――任务本身，01:15:13.200 --> 01:15:15.210基本上可以忽略，01:15:15.210 --> 01:15:17.565伯特所作的任何陈述。01:15:17.565 --> 01:15:20.040嗯，所以-所以现在的标准是使用01:15:20.040 --> 01:15:23.205在Bert上有一个SoftMax层，01:15:23.205 --> 01:15:25.185嗯，做这些决定。01:15:25.185 --> 01:15:29.100嗯，有一大堆工作要做01:15:29.100 --> 01:15:32.895从本质上评价这些模型的语言知识。01:15:32.895 --> 01:15:34.785嗯，你可以做部分语音标记，01:15:34.785 --> 01:15:37.080你可以做更多的语义任务，比如，01:15:37.080 --> 01:15:39.285嗯，关系提取，嗯，01:15:39.285 --> 01:15:41.265或者-或者类似共同参考的东西。01:15:41.265 --> 01:15:44.280嗯，这是一个相当活跃的工作领域。01:15:44.280 --> 01:15:47.055嗯，这是，呃，只有一个，呃，01:15:47.055 --> 01:15:51.195图中显示了这种方法的一些结果。01:15:51.195 --> 01:15:53.865所以我们要做的是增加01:15:53.865 --> 01:15:56.955诊断分类器到不同层的bert，01:15:56.955 --> 01:16:02.620我们看到哪些层的bert对于特定的任务更有用。01:16:02.620 --> 01:16:07.025嗯，而且，嗯，某种有趣的事情从中产生，那就是，嗯，01:16:07.025 --> 01:16:10.310伯特的不同层次似乎是对应的，嗯，01:16:10.310 --> 01:16:12.890很好的理解，01:16:12.890 --> 01:16:15.395嗯，不同层次的语言学。01:16:15.395 --> 01:16:19.110嗯，所以，呃，依赖性分析，这是一项语法任务，01:16:19.110 --> 01:16:20.940嗯，呃，你知道，这算是一种，01:16:20.940 --> 01:16:23.430理解句子的中级任务。01:16:23.430 --> 01:16:28.125嗯，伯特的中层，所以从6层到8层之类的，01:16:28.125 --> 01:16:30.480是最擅长依赖分析的。01:16:30.480 --> 01:16:34.095嗯，如果你有一个非常语义化的任务，比如情感分析，01:16:34.095 --> 01:16:35.880嗯，你想学点什么，呃，01:16:35.880 --> 01:16:38.325整个句子的语义属性，嗯，01:16:38.325 --> 01:16:41.490那么伯特的最后一层似乎就是01:16:41.490 --> 01:16:45.700编码关于这个，呃，现象的大部分信息。01:16:46.310 --> 01:16:48.690嗯，好吧。01:16:48.690 --> 01:16:50.835这几乎就是谈话的内容了，嗯，01:16:50.835 --> 01:16:54.600我只有一张幻灯片，呃，嗯，01:16:54.600 --> 01:16:57.870NLP不是在学术研究的背景下，01:16:57.870 --> 01:17:00.735我已经说过很多了，但是在工业界，NLP，01:17:00.735 --> 01:17:03.075实际上，那里的进展很快。01:17:03.075 --> 01:17:06.315我想指出两个我认为01:17:06.315 --> 01:17:10.650特别是对使用NLP技术的极大兴趣。01:17:10.650 --> 01:17:12.240嗯，一个是对话，01:17:12.240 --> 01:17:14.010嗯，像聊天机器人之类的东西，对吧？01:17:14.010 --> 01:17:17.580亚历克萨奖，他们在那里投资了很多钱，01:17:17.580 --> 01:17:21.105嗯，让小组找出如何改进聊天对话。01:17:21.105 --> 01:17:25.230嗯，我也认为客户服务的潜力很大，对吧？01:17:25.230 --> 01:17:28.170所以改进基本上自动化的系统，嗯，01:17:28.170 --> 01:17:29.580你知道，给你订个航班，01:17:29.580 --> 01:17:32.385或者帮助您取消订阅或类似的操作。01:17:32.385 --> 01:17:35.460嗯，同样，在医疗保健方面也有很多潜力。01:17:35.460 --> 01:17:39.180嗯，一种是了解某人的记录，01:17:39.180 --> 01:17:42.060嗯，生病了，帮助他们-帮助诊断。01:17:42.060 --> 01:17:43.935嗯，我想还有一个，嗯，01:17:43.935 --> 01:17:46.215同样重要的是，01:17:46.215 --> 01:17:49.020分析生物医学论文。01:17:49.020 --> 01:17:54.285嗯，那么，嗯，正在写的生物医学论文的数量真是太疯狂了，01:17:54.285 --> 01:17:56.100嗯，它比数字大得多01:17:56.100 --> 01:17:57.960正在撰写的计算机科学论文。01:17:57.960 --> 01:18:01.530[噪音]嗯，如果你是医生，01:18:01.530 --> 01:18:03.150或者如果你是研究人员，嗯，01:18:03.150 --> 01:18:06.360在医学上，你可能想查找一些非常具体的东西，对吗？01:18:06.360 --> 01:18:07.620你可能想知道什么是01:18:07.620 --> 01:18:11.370这种特殊药物对这种特殊基因的影响，01:18:11.370 --> 01:18:13.140或者一个有这种特殊基因的细胞。01:18:13.140 --> 01:18:16.710嗯，现在找不到好方法，嗯，01:18:16.710 --> 01:18:20.175数十万份文件来寻找是否有人有，呃，01:18:20.175 --> 01:18:23.085做了这个实验并取得了结果，01:18:23.085 --> 01:18:25.095嗯，事情的特殊组合。01:18:25.095 --> 01:18:28.590嗯，自动阅读所有的生物医学文献，01:18:28.590 --> 01:18:30.850嗯，可能有很多价值。01:18:31.100 --> 01:18:33.960好吧，嗯，最后，嗯，01:18:33.960 --> 01:18:38.280在过去的五年里，由于在全国人民计划中的深入学习，进步很快。01:18:38.280 --> 01:18:42.780去年，我们看到了另一种01:18:42.780 --> 01:18:45.300我们的系统能力大幅提高，01:18:45.300 --> 01:18:47.610感谢，呃，使用未标记的数据。01:18:47.610 --> 01:18:49.095这就是伯特这样的方法。01:18:49.095 --> 01:18:54.210嗯，还有，嗯，我认为另一件重要的事情是，01:18:54.210 --> 01:18:58.170NLP系统开始处于一个可以产生巨大社会影响的地方。01:18:58.170 --> 01:19:04.845嗯，这使得一些诸如偏见和安全性的问题非常重要。嗯，谢谢。01:19:04.845 --> 01:19:06.690呃，祝你完成所有项目好运。01:19:06.690 --> 01:19:14.800
[APPLAUSE].

