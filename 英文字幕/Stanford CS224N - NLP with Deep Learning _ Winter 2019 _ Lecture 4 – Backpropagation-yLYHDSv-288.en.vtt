WEBVTT
Kind: captions
Language: en

00:00:04.790 --> 00:00:11.940
Okay. So great to see everyone back for lecture four of the class.

00:00:11.940 --> 00:00:14.160
Um, so, for lec,

00:00:14.160 --> 00:00:16.500
for today's lecture, um,

00:00:16.500 --> 00:00:20.040
what I want to do for most of the time is actually

00:00:20.040 --> 00:00:23.730
get into the heart of these ideas of having

00:00:23.730 --> 00:00:28.290
the backpropagation algorithm for neural nets and how we can construct

00:00:28.290 --> 00:00:33.690
computation graphs that allow sufficiently to do backpropagation,

00:00:33.690 --> 00:00:36.195
neural nets to train the neural nets.

00:00:36.195 --> 00:00:41.330
So, overall, um, this is sort of what I plan to do it today.

00:00:41.330 --> 00:00:43.160
So, at the end of last lecture,

00:00:43.160 --> 00:00:47.870
I slightly ran out of time and I started mumbling and waving my hands about the,

00:00:47.870 --> 00:00:51.620
um, doing the derivatives with respect to the weight gradients.

00:00:51.620 --> 00:00:54.265
So, I kinda of wanted to do that but again.

00:00:54.265 --> 00:00:57.665
So hopefully it actually communicates slightly better.

00:00:57.665 --> 00:01:04.070
So, we'll do that and talk a bit more about sort of just tips for doing matrix gradients,

00:01:04.070 --> 00:01:07.835
um, and a particular issue that comes up with word vectors.

00:01:07.835 --> 00:01:10.010
And so then the main part of the class,

00:01:10.010 --> 00:01:12.619
we'll be talking about the backpropagation algorithm

00:01:12.619 --> 00:01:15.770
and how it runs over computation graphs.

00:01:15.770 --> 00:01:18.870
Um, and then for the last part of the class,

00:01:18.870 --> 00:01:22.380
um, is I'm not going to hide that, um,

00:01:22.380 --> 00:01:26.750
this is sort of just a grab bag of miscellaneous stuff you should

00:01:26.750 --> 00:01:32.005
know about neural networks and training neural networks.

00:01:32.005 --> 00:01:33.940
Um, like, I think,

00:01:33.940 --> 00:01:38.720
you know we dream of a future of artificial intelligence where our machines are

00:01:38.720 --> 00:01:43.670
really intelligent and you can just say to them this is the data and this is my problem,

00:01:43.670 --> 00:01:46.850
go and train me a model and it might work.

00:01:46.850 --> 00:01:48.560
Um, and in some future world,

00:01:48.560 --> 00:01:50.720
that may be  [NOISE] that comes along.

00:01:50.720 --> 00:01:53.150
It's something that's certainly being actively

00:01:53.150 --> 00:01:56.840
researched at the moment under the topic of Auto ML.

00:01:56.840 --> 00:02:02.240
I guess the question is whether it turns out that Auto ML was a scalable solution or

00:02:02.240 --> 00:02:06.110
the climate change consequences of Auto ML techniques are

00:02:06.110 --> 00:02:11.000
sufficiently bad that someone actually decides that these much lower power,

00:02:11.000 --> 00:02:17.135
um, neural systems might actually be better still for doing some parts of the problem.

00:02:17.135 --> 00:02:20.240
But anyway, either way we're not really there yet.

00:02:20.240 --> 00:02:22.235
And the fact of the matter is,

00:02:22.235 --> 00:02:24.320
when you're training neural networks,

00:02:24.320 --> 00:02:27.860
there's just a whole bunch of stuff you have to know about

00:02:27.860 --> 00:02:31.565
initialization and nonlinearities and learning rates and so on.

00:02:31.565 --> 00:02:34.440
And, you know, when I taught this class

00:02:34.440 --> 00:02:41.410
last time I somehow thought that people would pick this up by osmosis.

00:02:41.410 --> 00:02:43.689
That if we gave starter,

00:02:43.689 --> 00:02:47.440
cut code to people and now start

00:02:47.440 --> 00:02:52.760
a code we initialized how matrices and we set our learning rates,

00:02:52.760 --> 00:02:58.445
that by osmosis people would understand that's what you have to do and do it.

00:02:58.445 --> 00:03:03.859
Um, it didn't really sort of teach in class the practical tips and tricks enough,

00:03:03.859 --> 00:03:05.840
but it was perfectly obvious that when we got to

00:03:05.840 --> 00:03:12.035
final project time that at least for quite a few people, osmosis hadn't worked.

00:03:12.035 --> 00:03:13.789
Um, so this time,

00:03:13.789 --> 00:03:16.625
[LAUGHTER] I'm at least wanting to spend a few minutes on that

00:03:16.625 --> 00:03:20.510
and at least point out some other things that are important.

00:03:20.510 --> 00:03:22.729
And, I mean just in general,

00:03:22.729 --> 00:03:27.330
you know the reality of 2018, deep learning, no,

00:03:27.330 --> 00:03:30.510
wait, it's 2019 now, 2019, um,

00:03:30.510 --> 00:03:34.430
deep learning, is deep learning is still kind of a craft.

00:03:34.430 --> 00:03:39.620
There's quite a bit you have to know of techniques of doing things that lead

00:03:39.620 --> 00:03:42.440
neural net training to work successfully as

00:03:42.440 --> 00:03:46.050
opposed to your models failing to work successfully.

00:03:46.050 --> 00:03:50.390
Okay. One final announcement and I go in to it.

00:03:50.390 --> 00:03:56.345
Um, so, we've sort of been doing some further working on Office,

00:03:56.345 --> 00:03:59.840
our placement and I guess there are sort of multiple issues which

00:03:59.840 --> 00:04:04.460
include the opportunities for local ICPD students without Stanford IDs.

00:04:04.460 --> 00:04:05.795
We have to, um,

00:04:05.795 --> 00:04:07.880
get, um, to office hours.

00:04:07.880 --> 00:04:10.670
So for the Thursday night office hour,

00:04:10.670 --> 00:04:12.830
um, that's after this class,

00:04:12.830 --> 00:04:14.655
if you'd like to go and talk about,

00:04:14.655 --> 00:04:16.705
um, the second homework, um,

00:04:16.705 --> 00:04:21.335
the Thursday night office hour is going to be in Thorton- Thornton 110.

00:04:21.335 --> 00:04:24.260
Um, now I didn't know where Thornton was.

00:04:24.260 --> 00:04:29.390
It made more sense to me when I translated that as that's the old terman annex,

00:04:29.390 --> 00:04:32.450
but that's probably just showing my age since probably none

00:04:32.450 --> 00:04:35.620
of you remember when they used to be a building called Terman.

00:04:35.620 --> 00:04:37.695
So that probably doesn't help you either.

00:04:37.695 --> 00:04:38.980
Um, but you know,

00:04:38.980 --> 00:04:40.460
if you're heading, right,

00:04:40.460 --> 00:04:42.230
I don't know which direction we're facing.

00:04:42.230 --> 00:04:44.855
If you're heading that way I guess

00:04:44.855 --> 00:04:49.520
and if you know where the Papua New Guinea Sculpture Garden is, um, the,

00:04:49.520 --> 00:04:54.650
the sort of open grassy area before you get to the Papua New Guinea Sculpture Garden,

00:04:54.650 --> 00:04:59.695
that's where Terman used to be and the building that still stands in there is Thornton.

00:04:59.695 --> 00:05:02.700
Um, Thornton 110 um tonight.

00:05:02.700 --> 00:05:04.349
I think it starts at 6:30,

00:05:04.349 --> 00:05:06.210
right? 6:30 to nine.

00:05:06.210 --> 00:05:11.120
Okay. Right. So, let me just finish off where we were last time.

00:05:11.120 --> 00:05:15.195
So remember we had this window of five words and then we're

00:05:15.195 --> 00:05:19.700
putting it through a neural net layer of C equals WX plus B,

00:05:19.700 --> 00:05:22.160
non-linearity of H equals F of X,

00:05:22.160 --> 00:05:23.480
and then we're, um,

00:05:23.480 --> 00:05:29.150
going to just get a score as to whether this has in its center [NOISE]

00:05:29.150 --> 00:05:31.955
named entity like Paris which is sort of

00:05:31.955 --> 00:05:35.090
taking this dot product of a vector times the hidden layer.

00:05:35.090 --> 00:05:36.774
So this was our model,

00:05:36.774 --> 00:05:41.120
and then we are wanting to work out partial derivatives of S with

00:05:41.120 --> 00:05:45.680
respect to all of our variables and we did various of the cases,

00:05:45.680 --> 00:05:48.110
but one we hadn't yet done is the weights,

00:05:48.110 --> 00:05:50.645
and the weight through all of this neural net layer here.

00:05:50.645 --> 00:05:53.285
Okay. So, chain rule, um,

00:05:53.285 --> 00:05:58.005
the partial of ds dw is DS times HD,

00:05:58.005 --> 00:06:01.695
um, dHDZ times DZ, DW.

00:06:01.695 --> 00:06:05.055
And well, if you remember last time,

00:06:05.055 --> 00:06:09.945
we had sort of done some computation of what those first two,

00:06:09.945 --> 00:06:12.125
um, partial derivatives were.

00:06:12.125 --> 00:06:14.780
And we could say that we could just call

00:06:14.780 --> 00:06:19.145
those delta which is our error signal coming from above.

00:06:19.145 --> 00:06:23.060
And that concept of having an error signal coming from above is

00:06:23.060 --> 00:06:24.920
something I'll get back to in the main part of

00:06:24.920 --> 00:06:27.020
the lecture and a sort of a central notion.

00:06:27.020 --> 00:06:29.630
But the bit we hadn't dealt with is this dz,

00:06:29.630 --> 00:06:35.540
dw and we started to look at that and I made the argument, um,

00:06:35.540 --> 00:06:39.050
based on our shape convention that the shape of

00:06:39.050 --> 00:06:42.320
that should be the same shape as our W matrix.

00:06:42.320 --> 00:06:43.835
So it should be, um,

00:06:43.835 --> 00:06:47.360
same in times M shape as this W matrix.

00:06:47.360 --> 00:06:55.590
So we want to work out the partial of Z by W which is the same as this,

00:06:55.590 --> 00:06:59.335
um, [NOISE] dwx plus b, dw.

00:06:59.335 --> 00:07:03.065
And so we want to work out what that derivative is.

00:07:03.065 --> 00:07:04.940
Um, and if that's not obvious,

00:07:04.940 --> 00:07:09.740
one way to think about it is to go back to this elements of the matrix

00:07:09.740 --> 00:07:14.830
and actually first off work it out element-wise and think out what it should be,

00:07:14.830 --> 00:07:17.795
and then once you've thought out what it should be, um,

00:07:17.795 --> 00:07:22.090
to rewrite it back in matrix form to give the compact answer.

00:07:22.090 --> 00:07:26.750
So what we have is we have the inputs here and a biased term

00:07:26.750 --> 00:07:31.730
and we're going to do the matrix multiply it this vector to produce these.

00:07:31.730 --> 00:07:34.400
And if you think about what's happening there,

00:07:34.400 --> 00:07:39.200
so we've got this matrix of weights and for a particular weight,

00:07:39.200 --> 00:07:44.885
a weight is first index is going to correspond to a position in

00:07:44.885 --> 00:07:48.950
the hidden layer and its second index is going to

00:07:48.950 --> 00:07:53.240
correspond to a position in the input vector.

00:07:53.240 --> 00:07:56.870
And one weight in the matrix ends up being

00:07:56.870 --> 00:08:01.100
part of what's used to compute one element of the hidden layer.

00:08:01.100 --> 00:08:04.939
So, the one element of the hidden layer you're taking, um,

00:08:04.939 --> 00:08:08.270
a row of the matrix and you're multiplying it by

00:08:08.270 --> 00:08:11.600
the components of this vector so they sum together when the bias

00:08:11.600 --> 00:08:15.050
is added on but one element of the matrix is sort of only being

00:08:15.050 --> 00:08:19.020
used in the computation between one element of the,

00:08:19.020 --> 00:08:22.150
um, important one element of the hidden vector.

00:08:22.150 --> 00:08:25.170
Okay. So, well, that means, um,

00:08:25.170 --> 00:08:31.040
if we're thinking about what's the partial derivative with respect to WIJ, well,

00:08:31.040 --> 00:08:38.535
it's only contributing to ZI and it's only,

00:08:38.535 --> 00:08:42.750
it's only doing anything with XJ.

00:08:42.750 --> 00:08:44.850
So, that we end up with,

00:08:44.850 --> 00:08:47.870
we're getting the partial with respect to WIJ,

00:08:47.870 --> 00:08:50.570
we can work that out with respect to,

00:08:50.570 --> 00:08:53.690
just to respect to ZI.

00:08:53.690 --> 00:08:57.080
And when we're going to look at this multiplication here,

00:08:57.080 --> 00:09:01.370
what we're ending up is this sort of sum of terms WIK times

00:09:01.370 --> 00:09:04.490
Xk where there's sort of weights in that row

00:09:04.490 --> 00:09:07.685
of the matrix going across the positions of the vector.

00:09:07.685 --> 00:09:17.105
So the only position in which WIJ is used is multiplying, um, by XJ.

00:09:17.105 --> 00:09:18.485
And at that point,

00:09:18.485 --> 00:09:20.570
what we have in terms of sort of,

00:09:20.570 --> 00:09:24.440
in our basic one variable doing a differentiation,

00:09:24.440 --> 00:09:27.035
this is just like we have 3x,

00:09:27.035 --> 00:09:30.260
um, and we say what's the derivative of 3x?

00:09:30.260 --> 00:09:31.700
Actually X is confusing,

00:09:31.700 --> 00:09:32.795
so I shouldn't say that.

00:09:32.795 --> 00:09:38.450
Is like we have three W and what's the derivative of three W with respect to W?

00:09:38.450 --> 00:09:39.890
It's three, right?

00:09:39.890 --> 00:09:44.660
So, that we've have a term here which is what would have been W,

00:09:44.660 --> 00:09:47.805
will be WIJ times XJ,

00:09:47.805 --> 00:09:51.725
and its derivative with respect to WIJ is just XJ.

00:09:51.725 --> 00:09:53.825
Does that makes sense?

00:09:53.825 --> 00:09:55.310
Everyone believe it?

00:09:55.310 --> 00:09:57.270
[NOISE] Fingers crossed.

00:09:57.270 --> 00:10:01.920
Okay. Um, so, so for one element of this matrix,

00:10:01.920 --> 00:10:04.290
we're just getting out XJ.

00:10:04.290 --> 00:10:05.715
And at that point,

00:10:05.715 --> 00:10:07.910
um, we say, um,

00:10:07.910 --> 00:10:14.060
well of course we want to know what the Jacobian is for the full matrix W. Well,

00:10:14.060 --> 00:10:15.410
if you start thinking about it,

00:10:15.410 --> 00:10:18.545
this argument applies to every cell.

00:10:18.545 --> 00:10:20.210
So, that for every,

00:10:20.210 --> 00:10:22.175
um, cell of, um,

00:10:22.175 --> 00:10:24.260
the Jacobian for W,

00:10:24.260 --> 00:10:26.900
um, it's going to be XJ.

00:10:26.900 --> 00:10:30.530
So, that means, um,

00:10:30.530 --> 00:10:34.850
we're just going to be able to make use of that in calculating our Jacobian.

00:10:34.850 --> 00:10:42.650
So, the derivative for a single WIJ is delta IXJ and that's true for all cells.

00:10:42.650 --> 00:10:48.430
So we wanted to have a matrix for our Jacobian which has delta I,

00:10:48.430 --> 00:10:51.560
um, XJ in every cell of it.

00:10:51.560 --> 00:10:55.850
And the way we can create that is by using an outer products.

00:10:55.850 --> 00:11:00.049
So, if we have a row vector of the deltas,

00:11:00.049 --> 00:11:03.325
the error signals from above and a column,

00:11:03.325 --> 00:11:05.795
right, I said that wrong, sorry.

00:11:05.795 --> 00:11:11.420
If we have a column of the delta error signals

00:11:11.420 --> 00:11:17.300
from above and we have a row of X transfers vectors,

00:11:17.300 --> 00:11:21.995
um, when we multiply those together we get the outer product

00:11:21.995 --> 00:11:26.840
and we get delta IXJ in each cell and that is our Jacobian answer,

00:11:26.840 --> 00:11:29.615
um, for working out,

00:11:29.615 --> 00:11:34.070
um, the delta S delta W that we started off with at the beginning.

00:11:34.070 --> 00:11:36.710
Okay. And this, um,

00:11:36.710 --> 00:11:40.100
and we get this form where it's a multiplication of

00:11:40.100 --> 00:11:44.855
an error signal from above and our computed local gradient signal.

00:11:44.855 --> 00:11:47.720
And that's the pattern that we're going to see over and over

00:11:47.720 --> 00:11:51.260
again and that will exploit and our computation graphs.

00:11:51.260 --> 00:11:53.315
Okay, all good?

00:11:53.315 --> 00:11:56.810
Okay. Um, so, here's just,

00:11:56.810 --> 00:12:01.070
um, here's homework two.

00:12:01.070 --> 00:12:03.110
You're meant to do some of this stuff.

00:12:03.110 --> 00:12:06.845
Um, here are just over a couple of collected tips,

00:12:06.845 --> 00:12:10.010
um, which I hope will help.

00:12:10.010 --> 00:12:13.480
I mean keeping here track of your variables and

00:12:13.480 --> 00:12:16.150
their dimensionality is really useful because we

00:12:16.150 --> 00:12:19.255
just can work out what the dimensionality of things should be.

00:12:19.255 --> 00:12:21.385
You're often kind of halfway there.

00:12:21.385 --> 00:12:24.190
I mean basically what you're doing is sort of

00:12:24.190 --> 00:12:27.100
applying the chain rule over and over again.

00:12:27.100 --> 00:12:28.800
It always looks like this.

00:12:28.800 --> 00:12:33.920
Um, but doing it in this sort of matrix calculus sense of the chain rule.

00:12:33.920 --> 00:12:37.610
Um, in the homework you have to do a softmax,

00:12:37.610 --> 00:12:39.410
which we haven't done in class.

00:12:39.410 --> 00:12:42.305
Um, something that I think you'll find useful,

00:12:42.305 --> 00:12:46.985
if you want to break apart the softmax is to consider two cases.

00:12:46.985 --> 00:12:51.650
One, the case is to when you're working it out for the correct class.

00:12:51.650 --> 00:12:56.795
And then, the other case is for all the other incorrect classes.

00:12:56.795 --> 00:12:58.505
Um, yeah.

00:12:58.505 --> 00:13:00.860
Um, in the the little derivation,

00:13:00.860 --> 00:13:03.035
I did before, I said well,

00:13:03.035 --> 00:13:05.360
let's work out an element-wise partial

00:13:05.360 --> 00:13:08.660
derivative because that should give me some sense of what's going on,

00:13:08.660 --> 00:13:09.860
what the answer is.

00:13:09.860 --> 00:13:12.230
I think that can be a really good thing to do

00:13:12.230 --> 00:13:14.780
if you're getting confused by matrix calculus.

00:13:14.780 --> 00:13:16.565
And I sort of,

00:13:16.565 --> 00:13:20.660
um, slightly skipped past another slide.

00:13:20.660 --> 00:13:22.295
Last time that was talking about

00:13:22.295 --> 00:13:25.400
the shape convention that I talked about it for a moment that

00:13:25.400 --> 00:13:31.550
for the homeworks you can work out your answer however you want,

00:13:31.550 --> 00:13:33.380
you can work it out in terms of;

00:13:33.380 --> 00:13:35.630
you know numerator ordered Jacobians,

00:13:35.630 --> 00:13:37.280
if that seems best to you.

00:13:37.280 --> 00:13:40.130
But we'd like you to give the final answer to

00:13:40.130 --> 00:13:43.730
your assignment questions following the shape convention.

00:13:43.730 --> 00:13:46.760
So, that the derivative should be shaped in

00:13:46.760 --> 00:13:50.825
a vector matrix in the same way as the variable,

00:13:50.825 --> 00:13:54.920
with respect to which you're working out your derivatives.

00:13:54.920 --> 00:14:00.320
Okay. Um, the last little bit for finishing up this example from last time,

00:14:00.320 --> 00:14:01.700
I want to say a little bit about,

00:14:01.700 --> 00:14:05.150
is what happens with words.

00:14:05.150 --> 00:14:08.300
And one answer is nothing different.

00:14:08.300 --> 00:14:12.785
But another answer is they are a little bit of a special case here because,

00:14:12.785 --> 00:14:16.610
you know, really we have a matrix of word vectors, right?

00:14:16.610 --> 00:14:19.535
We have a vector for each word.

00:14:19.535 --> 00:14:23.540
And so then you can think of that as sort of this matrix of word vectors,

00:14:23.540 --> 00:14:25.310
which row has a different word.

00:14:25.310 --> 00:14:28.760
But we're not actually kind of connecting up

00:14:28.760 --> 00:14:33.170
that matrix directly to our classifier system.

00:14:33.170 --> 00:14:37.190
Instead of that, what we're connect connecting up to the classifier system is

00:14:37.190 --> 00:14:41.630
this window and the window will have it in at five words.

00:14:41.630 --> 00:14:43.490
Most commonly they're different words.

00:14:43.490 --> 00:14:46.145
But you know occasionally the same word might appear,

00:14:46.145 --> 00:14:49.160
um, in two positions in that window.

00:14:49.160 --> 00:14:52.190
And so, we can nevertheless do

00:14:52.190 --> 00:14:57.349
exactly the same thing and continue our gradients down and say okay,

00:14:57.349 --> 00:14:59.435
um, let's work out, um,

00:14:59.435 --> 00:15:03.335
the gradients of this word window vector.

00:15:03.335 --> 00:15:09.755
And if, um, these are of dimension D we'll have this sort of 5-D, um, vector.

00:15:09.755 --> 00:15:13.025
But, you know then what do we do about it,

00:15:13.025 --> 00:15:15.140
and the answer of what we do about it.

00:15:15.140 --> 00:15:21.980
Is we can just sort of split this window vector into five pieces and say aha,

00:15:21.980 --> 00:15:25.190
we have five updates to word vectors.

00:15:25.190 --> 00:15:30.335
We're just going to go off and apply them to the word Vector Matrix.

00:15:30.335 --> 00:15:34.565
Um, and you know if we if the same word occurs twice,

00:15:34.565 --> 00:15:38.990
um, in that window we literally apply both of the updates.

00:15:38.990 --> 00:15:41.810
So, it gets updated twice or maybe

00:15:41.810 --> 00:15:44.810
actually you want to sum them first and then do the update once but yeah,

00:15:44.810 --> 00:15:46.955
that's a technical issue.

00:15:46.955 --> 00:15:53.345
Um, so what that actually means is that we're extremely sparsely

00:15:53.345 --> 00:15:57.180
updating the word Vector Matrix because most of

00:15:57.180 --> 00:16:01.655
the word Vector Matrix will be unchanged and just a few rows of that,

00:16:01.655 --> 00:16:03.440
um, will be being updated.

00:16:03.440 --> 00:16:07.880
And if- um, soon we're going to be here doing stuff with PyTorch

00:16:07.880 --> 00:16:11.990
Um, and if you poke around Pytorch it even has some special stuff.

00:16:11.990 --> 00:16:15.440
Um, look for things like Sparse SGD for meaning

00:16:15.440 --> 00:16:19.100
that you're sort of doing a very sparse updating like that.

00:16:19.100 --> 00:16:24.590
Um, but there's one other sort of interesting thing that you should know about.

00:16:24.590 --> 00:16:26.285
For a lot of um,

00:16:26.285 --> 00:16:29.600
things that you do is just what actually happens if we push

00:16:29.600 --> 00:16:33.380
down these gradients into our word vectors.

00:16:33.380 --> 00:16:35.645
Well, the idea is no,

00:16:35.645 --> 00:16:39.094
if we do that would be just like all other neural net learning,

00:16:39.094 --> 00:16:46.655
that we will sort of in principle say move the word vectors around in such a way

00:16:46.655 --> 00:16:50.134
as they're more useful in helping determine

00:16:50.134 --> 00:16:54.665
named entity classification in this case because that was our motivating example.

00:16:54.665 --> 00:16:59.150
Um, so you know it might for example learn that the word in is

00:16:59.150 --> 00:17:04.970
a very good indicator of a named entity fall or sorry the place name following.

00:17:04.970 --> 00:17:08.090
So, after n you often get London, Paris et cetera.

00:17:08.090 --> 00:17:11.030
Right, so it's sort of got a special behavior that

00:17:11.030 --> 00:17:14.360
other prepositions don't as being a good location indicator.

00:17:14.360 --> 00:17:16.040
And so, it could sort of um,

00:17:16.040 --> 00:17:19.940
move it's location around and say here are words that are

00:17:19.940 --> 00:17:26.135
good location indicators and therefore help our classifier work even better.

00:17:26.135 --> 00:17:30.020
So, in principle that's good and it's a good thing to do,

00:17:30.020 --> 00:17:34.100
to update word vectors to help you perform better on

00:17:34.100 --> 00:17:39.710
a supervised task such as this Named Entity Recognition classification.

00:17:39.710 --> 00:17:45.050
But, there's a catch which is that it doesn't always work actually.

00:17:45.050 --> 00:17:47.120
And so, why doesn't it always work?

00:17:47.120 --> 00:17:50.750
Well, suppose that we're training a classifier.

00:17:50.750 --> 00:17:56.360
Um, you know it could be the one I just did or a softmax or logistic regression.

00:17:56.360 --> 00:17:59.525
And we wanting to classify um,

00:17:59.525 --> 00:18:02.855
movie reviews sentiment for positive or negative.

00:18:02.855 --> 00:18:07.730
Well, you know if we have trained our word vectors,

00:18:07.730 --> 00:18:13.310
we've got some word vector space and maybe in the word vector space, um, TV,

00:18:13.310 --> 00:18:15.860
telly and television are all very close

00:18:15.860 --> 00:18:19.520
together because they mean basically the same thing.

00:18:19.520 --> 00:18:22.400
So, that's great, our word vectors are good.

00:18:22.400 --> 00:18:25.250
But, well suppose it was the case,

00:18:25.250 --> 00:18:28.820
that in our training data for our classifier.

00:18:28.820 --> 00:18:32.630
So, this is our training data for movie sentiment review.

00:18:32.630 --> 00:18:38.465
We had the word TV and telly but we didn't have the word television.

00:18:38.465 --> 00:18:40.400
Well, then what's going to happen,

00:18:40.400 --> 00:18:45.350
is well while we try and train our sentiment classifier,

00:18:45.350 --> 00:18:51.200
if we push gradient back down into the word vectors what's likely to happen

00:18:51.200 --> 00:18:58.040
is that it will move around the word vectors of the words we saw in the training data.

00:18:58.040 --> 00:19:01.070
But, necessarily television's not moving, right?

00:19:01.070 --> 00:19:05.090
Because we're only pushing gradient down to words that are in our training data.

00:19:05.090 --> 00:19:06.890
So, this word goes nowhere,

00:19:06.890 --> 00:19:09.740
so it just stays where it was all along.

00:19:09.740 --> 00:19:13.760
So, if the result of our training is words get moved around.

00:19:13.760 --> 00:19:17.525
So, here a good words for indicating negative sentiment, um,

00:19:17.525 --> 00:19:20.839
will actually if at test time,

00:19:20.839 --> 00:19:22.400
when we're running our model,

00:19:22.400 --> 00:19:25.385
if we evaluate on a sentence with television in it,

00:19:25.385 --> 00:19:27.620
it's actually going to give the wrong answer.

00:19:27.620 --> 00:19:32.510
Whereas if we haven't changed the word vectors at all and had just left

00:19:32.510 --> 00:19:37.610
them where our word embedding learning system put them.

00:19:37.610 --> 00:19:39.500
Then it would have said television,

00:19:39.500 --> 00:19:42.620
that's a word that means about the same as TV or telly.

00:19:42.620 --> 00:19:43.850
I should treat it the same and

00:19:43.850 --> 00:19:47.840
my sentiment classifier and it would actually do a better job.

00:19:47.840 --> 00:19:54.740
So, it's sort of two-sided whether you gain by training word vectors.

00:19:54.740 --> 00:19:58.025
And so, this is a summary um, that says;

00:19:58.025 --> 00:20:01.715
that it's two sided and practically what you should do.

00:20:01.715 --> 00:20:09.080
So, the first choice is G is a good idea to use pre-trained word vectors like

00:20:09.080 --> 00:20:12.575
the word2vec vectors that you used in assignment one or

00:20:12.575 --> 00:20:17.110
using the training methods that you're doing right now for homework two.

00:20:17.110 --> 00:20:20.830
And the answer that is almost always yes.

00:20:20.830 --> 00:20:24.970
And the reason for that is this word vector training methods are

00:20:24.970 --> 00:20:29.500
extremely easy to run on billions of words of texts.

00:20:29.500 --> 00:20:35.750
So, we you know train these models like [inaudible] on billions or tens of billions of words.

00:20:35.750 --> 00:20:38.315
And it's easy to do that for two reasons.

00:20:38.315 --> 00:20:41.960
Firstly, because the training algorithms are very simple, right?

00:20:41.960 --> 00:20:46.925
That um, the word2vec training algorithms skip grams very simple algorithm.

00:20:46.925 --> 00:20:50.900
Secondly; because we don't need any expensive resources,

00:20:50.900 --> 00:20:54.470
all or we need as a big pile of text documents and we can run it on them.

00:20:54.470 --> 00:20:56.870
So, really easy to run it on,

00:20:56.870 --> 00:20:59.360
you know five or 50 billion words.

00:20:59.360 --> 00:21:03.350
Whereas, you know, we can't do that for most of the classifiers that we

00:21:03.350 --> 00:21:04.760
want to build because if it's something

00:21:04.760 --> 00:21:07.670
I sentiment classifier or a named entity recognizer,

00:21:07.670 --> 00:21:10.280
we need labeled training data to train

00:21:10.280 --> 00:21:15.605
our classifier and then we ask someone how many words have labeled training data,

00:21:15.605 --> 00:21:18.800
do you have for named entity recognition and they give this back

00:21:18.800 --> 00:21:22.130
a number like 300,000 words or one million words, right.

00:21:22.130 --> 00:21:24.655
It's orders a magnitude smaller.

00:21:24.655 --> 00:21:27.554
Okay. Um. So, therefore,

00:21:27.554 --> 00:21:30.340
we can gain using pre-trained word vectors,

00:21:30.340 --> 00:21:32.795
because they know about all the words that aren't

00:21:32.795 --> 00:21:35.705
now supervised, classifies training data.

00:21:35.705 --> 00:21:38.180
And they also know much more about the words that actually

00:21:38.180 --> 00:21:40.960
are in the training data, but only rarely.

00:21:40.960 --> 00:21:42.750
So, the exception to that is,

00:21:42.750 --> 00:21:45.490
if you have hundreds of millions of words of data,

00:21:45.490 --> 00:21:50.115
then you can start off with random word vectors and go from there.

00:21:50.115 --> 00:21:52.775
And so, a case where this is actually commonly done,

00:21:52.775 --> 00:21:54.450
is for machine translation,

00:21:54.450 --> 00:21:56.115
which we do later in the class.

00:21:56.115 --> 00:21:58.415
It's relatively easy for

00:21:58.415 --> 00:22:03.335
large languages to get hundreds of millions of words of translated text.

00:22:03.335 --> 00:22:04.635
If you wanted to build something,

00:22:04.635 --> 00:22:09.510
like a German- English or Chinese-English machine translation system.

00:22:09.510 --> 00:22:14.320
Not hard to get 150 million words of translated texts.

00:22:14.320 --> 00:22:16.760
And so, that's sort of sufficiently much data,

00:22:16.760 --> 00:22:21.015
that people commonly just start with word vectors, um,

00:22:21.015 --> 00:22:24.300
being randomly initialized and start training,

00:22:24.300 --> 00:22:27.000
um, their translation system.

00:22:27.000 --> 00:22:29.855
Okay. So then the second question is, okay.

00:22:29.855 --> 00:22:32.150
I'm using pre-trained word vectors.

00:22:32.150 --> 00:22:35.780
Um, when I train my supervised classifier,

00:22:35.780 --> 00:22:40.285
should I push gradients down into the word vectors and up, and update them?

00:22:40.285 --> 00:22:44.325
Which is often referred to as fine tuning the word vectors, um,

00:22:44.325 --> 00:22:45.900
or should I not,

00:22:45.900 --> 00:22:47.670
should I just sort of throw away

00:22:47.670 --> 00:22:51.090
those gradients and not push them down into the word vectors?

00:22:51.090 --> 00:22:53.710
And you know, the answer to that is it depends,

00:22:53.710 --> 00:22:55.470
and it just depends on the size.

00:22:55.470 --> 00:23:01.345
So, if you only have a small training data set, um, typically,

00:23:01.345 --> 00:23:06.065
it's best to just treat the pre-trained word vectors as fixed,

00:23:06.065 --> 00:23:08.805
um, and not do any updating of them at all.

00:23:08.805 --> 00:23:11.270
If you have a large data set,

00:23:11.270 --> 00:23:16.620
then you can normally gain by doing fine tuning of the word vectors.

00:23:16.620 --> 00:23:17.910
And of course, the answer here,

00:23:17.910 --> 00:23:19.955
is what counts as large.

00:23:19.955 --> 00:23:21.850
Um, you know, if certainly,

00:23:21.850 --> 00:23:24.370
if you're down in the regime of 100 thousand words,

00:23:24.370 --> 00:23:27.105
a couple of hundred thousand words, you're small.

00:23:27.105 --> 00:23:29.860
If you're starting to be over a million words,

00:23:29.860 --> 00:23:31.020
then maybe you're large.

00:23:31.020 --> 00:23:34.265
But you know, on practice, people do it both ways and see which number is higher,

00:23:34.265 --> 00:23:36.290
and that's what they stick with.

00:23:36.290 --> 00:23:39.955
Um. Yes. Um, then, the sort of,

00:23:39.955 --> 00:23:44.755
there's the sort of point here that is just worth underlying is " Yes",

00:23:44.755 --> 00:23:51.980
so on principle, we can back-propagate this gradient to every variable in our model.

00:23:51.980 --> 00:23:56.650
Um, it's actually a theorem that we can arbitrarily

00:23:56.650 --> 00:24:02.360
decide to throw any subset of those gradients away,

00:24:02.360 --> 00:24:07.960
and we are still improving the log-likelihood of our model, all right?

00:24:07.960 --> 00:24:09.815
It kind of can't be inconsistent.

00:24:09.815 --> 00:24:12.240
You can just sort of pick some subset and say only

00:24:12.240 --> 00:24:14.980
train those 37 and throw away all the rest.

00:24:14.980 --> 00:24:17.389
And the algorithm will still improve,

00:24:17.389 --> 00:24:19.185
um, the log-likelihood of the model.

00:24:19.185 --> 00:24:22.355
Perhaps not by as much as if you trained the rest of the variables,

00:24:22.355 --> 00:24:24.280
as well, um, but yes,

00:24:24.280 --> 00:24:27.145
it can't actually do any harm not to train anything.

00:24:27.145 --> 00:24:32.315
Um, that's one of the reasons why often people don't notice bugs in their code, as well.

00:24:32.315 --> 00:24:34.800
It is because if your code is kind of broken

00:24:34.800 --> 00:24:37.550
and only half of the variables are being updated,

00:24:37.550 --> 00:24:40.650
it will still seem to be training something and improving.

00:24:40.650 --> 00:24:43.415
Um. It's just not doing as well as it could be doing,

00:24:43.415 --> 00:24:45.455
if you've coded correctly.

00:24:45.455 --> 00:24:49.185
Okay. Um, so, at this point, um,

00:24:49.185 --> 00:24:51.035
that's sort of, um,

00:24:51.035 --> 00:24:53.730
almost shown you back propagation, right?

00:24:53.730 --> 00:24:59.030
So, back-propagation is really taking derivatives with a generalized chain rule,

00:24:59.030 --> 00:25:03.420
with the one further trick which we sort of represented with that delta,

00:25:03.420 --> 00:25:06.860
which is G. You want to be, um,

00:25:06.860 --> 00:25:08.560
clever in doing this, so,

00:25:08.560 --> 00:25:12.995
you minimize computation by reusing shared stuff.

00:25:12.995 --> 00:25:16.620
Um, but now what I want to move on is to sort of look at how we can do

00:25:16.620 --> 00:25:19.890
that much more systematically, which is this idea.

00:25:19.890 --> 00:25:22.540
We have a computation graph and we're going to run

00:25:22.540 --> 00:25:26.380
a back-propagation algorithm through the computation graph.

00:25:27.170 --> 00:25:33.730
So, this is kind of like an abstracts syntax tree,

00:25:33.730 --> 00:25:37.085
expression tree that you might see in a compiler's class,

00:25:37.085 --> 00:25:38.595
or something like that, right?

00:25:38.595 --> 00:25:44.150
So, when we have an arithmetic expression of the kind that we're going to compute,

00:25:44.150 --> 00:25:48.565
we can make this tipped over on its side tree representation.

00:25:48.565 --> 00:25:50.990
So, we've got the X and W variables,

00:25:50.990 --> 00:25:52.740
we're going to multiply them.

00:25:52.740 --> 00:25:53.990
There's the B variable,

00:25:53.990 --> 00:25:56.470
we're going to add it to the previous partial result.

00:25:56.470 --> 00:25:59.185
We're going to stick it through our non-linearity F

00:25:59.185 --> 00:26:01.315
and then we're going to multiply it by U.

00:26:01.315 --> 00:26:03.005
And that was the computation,

00:26:03.005 --> 00:26:05.455
that we're doing in our neural network.

00:26:05.455 --> 00:26:08.790
So, um the source nodes or inputs,

00:26:08.790 --> 00:26:12.530
the interior nodes of this tree are operations.

00:26:12.530 --> 00:26:17.255
And then we've got these edges that pass along the results of our computation.

00:26:17.255 --> 00:26:20.955
And so, this is the computation graph for precisely the example

00:26:20.955 --> 00:26:25.330
I've been doing for the last lecture [NOISE].

00:26:25.330 --> 00:26:28.660
Okay, so there are two things that we want to be able to do.

00:26:28.660 --> 00:26:30.090
The first one is,

00:26:30.090 --> 00:26:34.315
we want to be able to start with these variables and do this computation,

00:26:34.315 --> 00:26:36.130
and calculate what S is.

00:26:36.130 --> 00:26:38.485
That's the part that's dead simple,

00:26:38.485 --> 00:26:41.815
that's referred to as forward propagation.

00:26:41.815 --> 00:26:45.800
So, forward propagation is just expression evaluation,

00:26:45.800 --> 00:26:48.870
as you do in any any programming in language interpreter.

00:26:48.870 --> 00:26:51.220
Um, that's not hard at all.

00:26:51.220 --> 00:26:54.390
Um, but the difference here is, "Hey,

00:26:54.390 --> 00:26:59.705
we want to do a learning algorithm" so we're going to do the opposite of that, as well.

00:26:59.705 --> 00:27:04.070
What we want to be able to do is also backward propagation,

00:27:04.070 --> 00:27:07.805
or back-propagation or just back-prop, it's commonly called,

00:27:07.805 --> 00:27:10.170
which is we want to be able to go,

00:27:10.170 --> 00:27:12.210
um, from the final part.

00:27:12.210 --> 00:27:14.190
The final part here.

00:27:14.190 --> 00:27:16.395
And then at each step,

00:27:16.395 --> 00:27:18.315
we want to be calculating

00:27:18.315 --> 00:27:22.680
these partial derivatives and passing them back through the graph.

00:27:22.680 --> 00:27:27.210
And so, this was sort of the notion before that we had an error signal, right?

00:27:27.210 --> 00:27:28.860
So, we're starting from up here,

00:27:28.860 --> 00:27:32.190
we've calculated a partial of S by Z,

00:27:32.190 --> 00:27:34.920
which is this with respect to that.

00:27:34.920 --> 00:27:38.735
And so, that's sort of our calculated error signal, up to here,

00:27:38.735 --> 00:27:41.940
and then we want to pass that further back, to start, um,

00:27:41.940 --> 00:27:46.010
computing, um, um, gradients further back.

00:27:46.010 --> 00:27:49.570
Right? And we started off, um, right here,

00:27:49.570 --> 00:27:54.560
with the partial of S by S. What's the partial of S by S going to be?

00:27:54.560 --> 00:27:57.040
One. Okay, yes.

00:27:57.040 --> 00:28:00.240
So, the rate at which S changes is the rate at which S changes.

00:28:00.240 --> 00:28:02.130
So, we just start off with one,

00:28:02.130 --> 00:28:07.565
and then we want to work out how this gradient changes as we go along.

00:28:07.565 --> 00:28:14.515
Um, so what we're doing here is when we're working out things for one node,

00:28:14.515 --> 00:28:18.815
that a node is going to have passed in towards it upstream gradient,

00:28:18.815 --> 00:28:20.465
which is its error signal.

00:28:20.465 --> 00:28:26.045
So, that's the partial of our final, f- final result,

00:28:26.045 --> 00:28:29.320
which was our loss, um, by um,

00:28:29.320 --> 00:28:32.970
the va- variable was the output of these computation nodes.

00:28:32.970 --> 00:28:35.315
So, that's the partial of S I H, here.

00:28:35.315 --> 00:28:39.340
And then, we did some operation here.

00:28:39.340 --> 00:28:42.800
Here's the non-linearity, but it might be something else.

00:28:42.800 --> 00:28:47.100
And so what we want to then work out is a downstream gradient,

00:28:47.100 --> 00:28:49.700
which is the partial of S by Z,

00:28:49.700 --> 00:28:51.825
which was the input to this function.

00:28:51.825 --> 00:28:53.320
And well then the question is,

00:28:53.320 --> 00:28:54.845
how do we do that?

00:28:54.845 --> 00:28:56.895
And the answer to that is,

00:28:56.895 --> 00:28:59.045
we use the chain rule, of course, right?

00:28:59.045 --> 00:29:02.760
So, at, we have a concept of a local gradients.

00:29:02.760 --> 00:29:06.425
So, here's H as the output,

00:29:06.425 --> 00:29:08.505
um, Z is the input.

00:29:08.505 --> 00:29:10.175
So, this function here,

00:29:10.175 --> 00:29:11.980
this is our non-linearity, right?

00:29:11.980 --> 00:29:14.825
So, this is whatever we're using as our non-linearity,

00:29:14.825 --> 00:29:19.095
like a logistic or T and H. We calculate H in terms of Z,

00:29:19.095 --> 00:29:21.690
and we can work out the partial of H by Z.

00:29:21.690 --> 00:29:23.440
So, that's our local gradient.

00:29:23.440 --> 00:29:28.370
And so then, if we have both the upstream gradient and the local gradient.

00:29:28.370 --> 00:29:32.825
We can then work out the downstream gradient because we know the

00:29:32.825 --> 00:29:38.880
partial of S by Z is going to be DSDH times, um, DHDZ.

00:29:38.880 --> 00:29:44.995
And so, then we'll be able to pass down the downstream gradient to the next node.

00:29:44.995 --> 00:29:47.275
Okay. So our basic rule,

00:29:47.275 --> 00:29:52.320
which is just the chain rule written in different terms

00:29:52.320 --> 00:29:58.010
is downstream gradient equals upstream gradient times local gradient.

00:29:58.010 --> 00:30:01.480
Um, easy as that,um, okay.

00:30:01.480 --> 00:30:03.435
So, this was um,

00:30:03.435 --> 00:30:09.510
the very simplest case where we have a node with one input and one output.

00:30:09.510 --> 00:30:11.230
So, that's a function um,

00:30:11.230 --> 00:30:13.040
like our logistic function.

00:30:13.040 --> 00:30:16.780
But, we also want to have things work out for general computation graphs.

00:30:16.780 --> 00:30:18.390
So, how are we going to do that?

00:30:18.390 --> 00:30:20.780
Well, the next case is,

00:30:20.780 --> 00:30:24.250
um, what about if we have multiple inputs?

00:30:24.250 --> 00:30:29.760
So, if we're calculating something like Z equals W times X.

00:30:29.760 --> 00:30:36.965
Um, where actually yes Z and X are themselves vectors and W um,

00:30:36.965 --> 00:30:42.305
is a matrix, but we're treating X as an input and W as an input,

00:30:42.305 --> 00:30:44.735
and Z as our output, right?

00:30:44.735 --> 00:30:47.405
We kind of group vectors and matrices together.

00:30:47.405 --> 00:30:51.350
Well, if you have multiple inputs,

00:30:51.350 --> 00:30:54.030
you then end up with multiple local gradients.

00:30:54.030 --> 00:30:55.640
So, you can work out um,

00:30:55.640 --> 00:30:57.860
the partial of Z with respect to X,

00:30:57.860 --> 00:31:01.645
or the partial of Z with respect to W. And so,

00:31:01.645 --> 00:31:05.405
you essentially you take the upstream gradient,

00:31:05.405 --> 00:31:09.155
you multiply it by each of the local gradients,

00:31:09.155 --> 00:31:12.340
and you pass it down the respective path,

00:31:12.340 --> 00:31:17.530
and we calculate these different downstream gradients to pass along.

00:31:17.530 --> 00:31:20.310
Is that making sense?

00:31:22.260 --> 00:31:25.930
Yeah. Okay. How chug.

00:31:25.930 --> 00:31:31.930
Okay. So, let's sort of look in an example of this and then we'll see one other case.

00:31:31.930 --> 00:31:34.420
So here's the little baby example.

00:31:34.420 --> 00:31:37.150
This isn't kind of really looking like a neural net,

00:31:37.150 --> 00:31:41.260
but we've got three inputs x, y, and z.

00:31:41.260 --> 00:31:45.895
And x and y get added together and y and z you get maxed.

00:31:45.895 --> 00:31:50.830
And then we take the results of those two operations and we multiply them together.

00:31:50.830 --> 00:31:57.340
So overall what we're calculating is x plus y times the max of y plus z.

00:31:57.340 --> 00:32:05.350
But, you know, we have here a general technique and we can apply it in any cases.

00:32:05.350 --> 00:32:09.895
Okay, so if we wanted to have this graph and we want to run it forward,

00:32:09.895 --> 00:32:13.270
well, we need to know the values of x, y, and z.

00:32:13.270 --> 00:32:19.180
So, for my example x equals one y equals two z equals zero.

00:32:19.180 --> 00:32:23.650
Um, so, we take the values of those variables and

00:32:23.650 --> 00:32:28.600
push them onto the calculations for the forward arrows.

00:32:28.600 --> 00:32:32.650
And then well the first thing we do is add and the result of that is three.

00:32:32.650 --> 00:32:34.300
And so we can put that onto the arrow.

00:32:34.300 --> 00:32:35.575
That's the output of add.

00:32:35.575 --> 00:32:39.730
Max it's two as the output of the value of add times is six.

00:32:39.730 --> 00:32:42.910
And so the forward pass we have evaluated the expression.

00:32:42.910 --> 00:32:44.710
Its value is six.

00:32:44.710 --> 00:32:48.700
That wasn't hard. Okay. So then the next step is we

00:32:48.700 --> 00:32:54.100
then want to run back-propagation to work out gradients.

00:32:54.100 --> 00:33:00.205
Um, and so we sort of want to know how to sort of,

00:33:00.205 --> 00:33:03.190
um work out these local gradients.

00:33:03.190 --> 00:33:10.570
So a is our right a is the result of sum.

00:33:10.570 --> 00:33:12.460
So here's a as the result of sum.

00:33:12.460 --> 00:33:15.040
So a equals x plus y.

00:33:15.040 --> 00:33:23.560
So if you're taking da dx that's just one and d a d y is also one that makes sense.

00:33:23.560 --> 00:33:28.030
Um, the max is slightly trickier because where

00:33:28.030 --> 00:33:33.610
there's some slopes and gradient for the max depends on which one's bigger.

00:33:33.610 --> 00:33:37.930
So, if y is bigger than z d- delta,

00:33:37.930 --> 00:33:40.299
the partial of b by z,

00:33:40.299 --> 00:33:49.030
plus partial b by y is one otherwise it's 0 and conversely for the partial of b by z.

00:33:49.030 --> 00:33:52.315
So that one's a little bit dependent.

00:33:52.315 --> 00:33:56.410
And then we do the multiplication, um,

00:33:56.410 --> 00:33:58.900
case at the end, um,

00:33:58.900 --> 00:34:04.495
and work out its partials with respect to a and b.

00:34:04.495 --> 00:34:09.520
And, um, since that's a and b which has the values two and three.

00:34:09.520 --> 00:34:14.725
If you're taking the partial of f by a it equals b which is two and vice versa.

00:34:14.725 --> 00:34:19.645
Okay. So that means we can work out the local gradients at each node.

00:34:19.645 --> 00:34:22.690
And so then we want to use those to

00:34:22.690 --> 00:34:26.590
calculate our gradients backwards and the back-propagation paths.

00:34:26.590 --> 00:34:28.165
So we start at the top.

00:34:28.165 --> 00:34:31.030
The partial of f with respect to F is one.

00:34:31.030 --> 00:34:37.375
Because if you move if you know by a tenth then you've moved the f by a tenth.

00:34:37.375 --> 00:34:39.535
So that's a cancels out as one.

00:34:39.535 --> 00:34:42.460
Okay. So then we want to pass backwards.

00:34:42.460 --> 00:34:47.230
So, the first thing that we have is this sort of multiply node.

00:34:47.230 --> 00:34:53.095
And so we worked- we know its local gradients that partial of f by a is two,

00:34:53.095 --> 00:34:57.085
and the partial of f by b is three.

00:34:57.085 --> 00:34:59.350
And so we get those values.

00:34:59.350 --> 00:35:03.280
So formally we're taking the local gradients

00:35:03.280 --> 00:35:07.795
multiplying them by the upstream gradients and getting our three and two.

00:35:07.795 --> 00:35:13.045
And notice the fact that so effectively what happens is the values on the two arcs swaps.

00:35:13.045 --> 00:35:15.460
Um, and then we sort of continue back.

00:35:15.460 --> 00:35:17.500
Okay. There's a max node.

00:35:17.500 --> 00:35:23.650
So our upstream gradient is now three and then we want to multiply by the local gradient.

00:35:23.650 --> 00:35:29.920
And since the max of these two as two has a slope of one on this side.

00:35:29.920 --> 00:35:31.300
So you get three,

00:35:31.300 --> 00:35:34.855
there's no gradient on this side and we get zero.

00:35:34.855 --> 00:35:37.720
And then we do the similar calculation on

00:35:37.720 --> 00:35:41.295
the other side where we have local gradients of one.

00:35:41.295 --> 00:35:47.744
And so both of them come out of two And then the one other thing to do is we notice,

00:35:47.744 --> 00:35:48.975
well, wait a minute.

00:35:48.975 --> 00:35:52.295
There are two arcs that started from the y

00:35:52.295 --> 00:35:56.290
both of which we've backed complicated some gradient on.

00:35:56.290 --> 00:35:58.870
And so what do we do about that.

00:35:58.870 --> 00:36:02.125
Um, what we do about that is we sum.

00:36:02.125 --> 00:36:07.740
So, the partial of f by x is to the partial of f by z is 0 that the

00:36:07.740 --> 00:36:13.450
partial of f by y is the sum of the two and five, right?

00:36:13.450 --> 00:36:15.670
And so this isn't complete voodoo.

00:36:15.670 --> 00:36:21.730
This is something that should make sense in terms of what gradients are, right?

00:36:21.730 --> 00:36:23.845
So, that what we're saying,

00:36:23.845 --> 00:36:25.615
is what we're calculating,

00:36:25.615 --> 00:36:27.860
is if you wiggle x a little bit

00:36:27.860 --> 00:36:32.175
how big an effect does that have on the outcome of the whole thing?

00:36:32.175 --> 00:36:34.650
And so, you know, we should be able to work this out.

00:36:34.650 --> 00:36:40.185
So, our x started offers one but let's suppose we wiggle it up a bit

00:36:40.185 --> 00:36:47.685
and make it 1.1 well according to this output should change by about 0.2,

00:36:47.685 --> 00:36:49.485
it should be magnified by two.

00:36:49.485 --> 00:36:51.330
And we should be able to work that out, right?

00:36:51.330 --> 00:36:55.510
So it's then 1.1 plus two,

00:36:55.510 --> 00:36:58.495
so that's then 3.1.

00:36:58.495 --> 00:37:03.610
And then we've got the two here that multiplies by it and it's 6.2.

00:37:03.610 --> 00:37:05.890
And lo and behold it went up by 0.2, right?

00:37:05.890 --> 00:37:07.510
So that seems correct.

00:37:07.510 --> 00:37:09.940
And if we try and do the same for,

00:37:09.940 --> 00:37:11.935
well, let's do the z. It's easy.

00:37:11.935 --> 00:37:16.510
So if we wiggle the z which had a value of zero by 0.1.

00:37:16.510 --> 00:37:18.400
This is 0.1.

00:37:18.400 --> 00:37:21.370
When we max if this is still two and

00:37:21.370 --> 00:37:24.655
so a calculated value doesn't change, it's still six.

00:37:24.655 --> 00:37:26.830
So the gradient here is zero.

00:37:26.830 --> 00:37:28.690
Wiggling this does nothing.

00:37:28.690 --> 00:37:32.230
And then the final one is y.

00:37:32.230 --> 00:37:35.350
So, it's starting off value as two.

00:37:35.350 --> 00:37:38.545
So, if we wiggle it a little and make it 2.1,

00:37:38.545 --> 00:37:44.350
our claim is that the results are change by about 0.5.

00:37:44.350 --> 00:37:46.735
It should be multiplied by five times.

00:37:46.735 --> 00:37:54.430
So, if we make this 2.1 we then have 2.1 plus one and b 3.1.

00:37:54.430 --> 00:37:59.245
When we get the max here would also be 2.1.

00:37:59.245 --> 00:38:03.100
And so we'd have 2.1 times 3.1.

00:38:03.100 --> 00:38:06.820
And that's too hard arithmetic for me to do in my head.

00:38:06.820 --> 00:38:16.360
But if we take 2.1 times 3.1 it comes out to 6.51.

00:38:16.360 --> 00:38:19.210
So, basically it's gone up by half.

00:38:19.210 --> 00:38:22.390
We don't expect the answers to be exact of course, right?

00:38:22.390 --> 00:38:24.730
Because you know that's not the way calculus works, right?

00:38:24.730 --> 00:38:29.545
[NOISE]. Where that it's showing that we're getting the gradients right.

00:38:29.545 --> 00:38:32.620
Okay. So this actually works.

00:38:32.620 --> 00:38:35.800
So, what are the techniques that we need to know?

00:38:35.800 --> 00:38:39.310
Um, so we've sort of already seen them all.

00:38:39.310 --> 00:38:44.050
So, you know, we discussed when there are multiple incoming arcs,

00:38:44.050 --> 00:38:47.905
how he saw workout the different local derivatives.

00:38:47.905 --> 00:38:52.000
The main other case that we need to know is if, um,

00:38:52.000 --> 00:38:54.805
in the function computation there's a branch

00:38:54.805 --> 00:38:59.050
outward the resultant something is used in multiple places.

00:38:59.050 --> 00:39:01.060
And so this was like the case here.

00:39:01.060 --> 00:39:03.490
I mean, here this was an initial variable,

00:39:03.490 --> 00:39:06.400
but you know, it could have been computed by something further back.

00:39:06.400 --> 00:39:10.090
So, if this thing is used in multiple places and

00:39:10.090 --> 00:39:14.020
you have the computation going out in different ways.

00:39:14.020 --> 00:39:17.680
It's just this simple rule that when you do backpropagation

00:39:17.680 --> 00:39:23.320
backwards you sum the gradients that you get from the different output branches.

00:39:23.320 --> 00:39:28.270
Okay. So, if a equals X plus Y and while that's the one we showed you

00:39:28.270 --> 00:39:34.555
before that were doing this some operation to work out the total partial of f by y.

00:39:34.555 --> 00:39:40.570
Okay. And if you sort of think about it just a little bit more,

00:39:40.570 --> 00:39:43.480
there's sort of these obvious patterns,

00:39:43.480 --> 00:39:46.855
um, which we saw in this very simple example.

00:39:46.855 --> 00:39:54.310
So, if you've got a plus that really the upstream gradient is going to

00:39:54.310 --> 00:39:56.920
be sort of heading down every one of

00:39:56.920 --> 00:40:02.080
these grant branches when you have multiple branches are things being summed.

00:40:02.080 --> 00:40:03.950
Now, in this case,

00:40:03.950 --> 00:40:10.080
it just as copied unchanged but that's because our computation was x plus y.

00:40:10.080 --> 00:40:11.700
You know, it could be more complicated,

00:40:11.700 --> 00:40:14.970
but we're passing it down down each of those branches.

00:40:14.970 --> 00:40:19.310
So plus distributes upstream gradient.

00:40:19.310 --> 00:40:23.950
When you have a max that's kind of like a routing operation,

00:40:23.950 --> 00:40:29.419
because max is going to be sending the gradient to in the direction that's the max,

00:40:29.419 --> 00:40:33.140
and other things are going to get no gradient being passed down to them.

00:40:33.140 --> 00:40:36.280
Um, and then when you have, um,

00:40:36.280 --> 00:40:39.130
a multiplication this has this kind of

00:40:39.130 --> 00:40:42.610
fun effect that what you do is switch the gradient, right?

00:40:42.610 --> 00:40:46.355
And so this reflects the fact that when you have u times

00:40:46.355 --> 00:40:50.865
v regardless of whether u and v are vectors or just,

00:40:50.865 --> 00:40:55.120
um, scalars that the derivative of the result with respect to

00:40:55.120 --> 00:41:00.050
u is v and the derivative of those spot- result with respect to v is u.

00:41:00.050 --> 00:41:01.550
And so, the, um,

00:41:01.550 --> 00:41:03.715
gradient signal is the flip,

00:41:03.715 --> 00:41:07.890
um, of the tw- two numbers on the different sides.

00:41:07.890 --> 00:41:14.070
Okay. Um, so this is sort of most of how we have

00:41:14.070 --> 00:41:19.730
these computation graphs and we can work out backpropagation backwards in them.

00:41:19.730 --> 00:41:23.765
There's sort of one more part of this to do,

00:41:23.765 --> 00:41:25.780
um, which is to say g,

00:41:25.780 --> 00:41:28.070
we want to do this eff- efficiently.

00:41:28.070 --> 00:41:31.830
So, there's a bad way to do this which is to say, "Oh well,

00:41:31.830 --> 00:41:37.535
we wanted to calculate the partial of this by b and so we can calculate that partial."

00:41:37.535 --> 00:41:41.045
Which was essentially what I was doing on last time slides.

00:41:41.045 --> 00:41:48.545
We say, "Um, partial of s by b equals the partial of s by h,

00:41:48.545 --> 00:41:51.040
times the partial of h by z,

00:41:51.040 --> 00:41:53.590
times the partial of z by b,

00:41:53.590 --> 00:41:55.120
and we have all of those partials.

00:41:55.120 --> 00:41:59.670
We work them all out and multiply them together and then someone says,

00:41:59.670 --> 00:42:02.590
um, what's the partial of s by w?

00:42:02.590 --> 00:42:05.105
And we say, huh, that's the chain rule again, I'll do it all again.

00:42:05.105 --> 00:42:08.449
It's the partial of s by,

00:42:08.449 --> 00:42:11.530
um, h times the partial of h by z,

00:42:11.530 --> 00:42:17.435
times the partial of and z by x,

00:42:17.435 --> 00:42:19.750
no, no, right, ah, lost it.

00:42:19.750 --> 00:42:23.385
But you do big long list of them and you calculate all again.

00:42:23.385 --> 00:42:25.275
That's not what we want to do.

00:42:25.275 --> 00:42:26.885
Instead we want to say, "Oh,

00:42:26.885 --> 00:42:29.010
look there's this shared stuff.

00:42:29.010 --> 00:42:31.985
There's this error signal coming from above."

00:42:31.985 --> 00:42:37.285
And we can work out the error signal the upstream gradient for this node.

00:42:37.285 --> 00:42:41.000
We can use it to calculate the upstream gradient for this node.

00:42:41.000 --> 00:42:45.860
We can use this to calculate the upstream gradient for this node and then,

00:42:45.860 --> 00:42:49.360
using the local gradients of which there are two calculated

00:42:49.360 --> 00:42:53.880
this node we can then calculate this one and that one.

00:42:53.880 --> 00:42:59.885
Um, and then, from here having knowing this upstream gradient,

00:42:59.885 --> 00:43:05.035
we can use the local gradients at this node to compute this one and that one.

00:43:05.035 --> 00:43:10.380
And so, we're sort of doing this efficient computer science like computation,

00:43:10.380 --> 00:43:14.300
um, where we don't do any repeated work. That makes sense?

00:43:14.300 --> 00:43:18.880
Yeah. Okay. Um, and so if that is,

00:43:18.880 --> 00:43:20.945
um, the whole of backprop.

00:43:20.945 --> 00:43:26.220
So, um, here's sort of a slightly sketchy um graph

00:43:26.220 --> 00:43:29.350
which is sort of just re-capitulating this thing.

00:43:29.350 --> 00:43:37.175
So, if you have any computation that you want to perform, um, well,

00:43:37.175 --> 00:43:42.800
the hope is that you can sort your nodes into

00:43:42.800 --> 00:43:48.215
what's called a topological sort which means that things that are arguments,

00:43:48.215 --> 00:43:50.970
variables that are arguments are sorted before

00:43:50.970 --> 00:43:54.485
variables that are results that depend on that argument.

00:43:54.485 --> 00:43:57.980
You know, providing you have something there's an a cyclic graph,

00:43:57.980 --> 00:43:59.465
you'll be able to do that.

00:43:59.465 --> 00:44:02.435
If you have a cyclic graph, you're in trouble.

00:44:02.435 --> 00:44:05.045
Um, well, I'd be there actually techniques people

00:44:05.045 --> 00:44:07.890
use to roll out those graphs but I'm not gonna go into that now.

00:44:07.890 --> 00:44:12.160
So, we've sorted the nodes which is kind of loosely represented here from

00:44:12.160 --> 00:44:16.615
bottom to top in a topological sort area, sort.

00:44:16.615 --> 00:44:21.660
Okay. So then, for the forward prop we sort of go through the nodes in

00:44:21.660 --> 00:44:25.445
the topological sort order and we

00:44:25.445 --> 00:44:30.640
if it's a variable we just set its value to what it's favorite val- variable value is.

00:44:30.640 --> 00:44:34.355
If it's computed from other variables their values must have been

00:44:34.355 --> 00:44:38.330
set already because there earlier in the topological sort, um,

00:44:38.330 --> 00:44:43.815
and then we compute the value of those nodes according to their predecessors,

00:44:43.815 --> 00:44:47.380
and we pass it up and work out the final output,

00:44:47.380 --> 00:44:51.845
the loss function of our neural network and that is our forward pass.

00:44:51.845 --> 00:44:55.050
Okay. So then, after that we do our backward pass and so for

00:44:55.050 --> 00:45:00.305
the backward pass we initialize the output gradient with one.

00:45:00.305 --> 00:45:01.930
The top thing is always one,

00:45:01.930 --> 00:45:04.310
the partial of z with respect to z.

00:45:04.310 --> 00:45:09.590
And then, we now sort of go through the nodes in reverse topological sort.

00:45:09.590 --> 00:45:14.645
And so therefore, each of them will all ready- anything that's,

00:45:14.645 --> 00:45:18.025
ah, anything that's, uh, language is complex.

00:45:18.025 --> 00:45:19.235
Anything that's above that.

00:45:19.235 --> 00:45:22.680
Anything that we calculated based on it in terms of, ah,

00:45:22.680 --> 00:45:28.085
forward pass will already have had calculated it's, um,

00:45:28.085 --> 00:45:32.050
it's gradient as a product of upstream gradient

00:45:32.050 --> 00:45:35.855
times local gradient and then we can use that,

00:45:35.855 --> 00:45:38.575
um, to compute the next thing down.

00:45:38.575 --> 00:45:43.299
Um, and so basically the ov- the overall role

00:45:43.299 --> 00:45:47.945
is for any node you work out its set of successors,

00:45:47.945 --> 00:45:49.770
the things that are above it that it,

00:45:49.770 --> 00:45:52.690
that depend on it and then you say, "Okay,

00:45:52.690 --> 00:45:59.080
the partial of z with respect to x is simply the sum over the set of

00:45:59.080 --> 00:46:03.040
successors of the local gradient that you

00:46:03.040 --> 00:46:08.105
calculated the node times the upstream gradient of that node."

00:46:08.105 --> 00:46:12.565
Um, and in the examples that I gave before there was never,

00:46:12.565 --> 00:46:14.950
never multiple upstream gradients.

00:46:14.950 --> 00:46:18.460
But if you imagine a, a general big graph there could actually be

00:46:18.460 --> 00:46:23.885
so different upstream gradients that are being used in- for the various successors.

00:46:23.885 --> 00:46:31.480
So, we apply that backwards and then we've worked out in backpropagation, um,

00:46:31.480 --> 00:46:33.710
the gradient of every,

00:46:33.710 --> 00:46:39.110
the gradient of the final result z with respect to every node in our graph.

00:46:39.110 --> 00:46:42.410
Um, and the thing to notice about this is,

00:46:42.410 --> 00:46:45.665
if you're doing it right and efficiently,

00:46:45.665 --> 00:46:50.550
the bigger o order of complexity of doing backpropagation is exactly the

00:46:50.550 --> 00:46:55.390
same as doing forward propagation i.e expression evaluation.

00:46:55.390 --> 00:46:59.620
So, it's not some super expensive complex procedure

00:46:59.620 --> 00:47:02.505
that you can imagine doing and scaling up.

00:47:02.505 --> 00:47:07.440
Um, you're actually in exactly the same complexity order.

00:47:07.440 --> 00:47:11.950
Okay. Um, so as [inaudible] entered it here this procedure,

00:47:11.950 --> 00:47:15.635
you could just think of something that you're running on

00:47:15.635 --> 00:47:21.875
an arbitrary graph and calculating this forward pass and the backwards pass.

00:47:21.875 --> 00:47:24.990
I mean, almost without exception that the kind of

00:47:24.990 --> 00:47:28.660
neural nets that we actually use have a regular layer

00:47:28.660 --> 00:47:31.715
like structure and that's then precisely why it makes

00:47:31.715 --> 00:47:35.935
to- sense to work out these gradients in terms of,

00:47:35.935 --> 00:47:40.595
um, vectors matrices and Jacobian's as the kind we were before.

00:47:40.595 --> 00:47:47.120
Okay. Um, so since we have this sort of really nice algorithm now, um,

00:47:47.120 --> 00:47:49.160
this sort of means that, um,

00:47:49.160 --> 00:47:55.205
we can do this just computationally and so we don't have to think or know how to do math.

00:47:55.205 --> 00:47:59.150
Um, and we can just have our computers do all of this with this.

00:47:59.150 --> 00:48:03.020
Um, so that using this graph structure, um,

00:48:03.020 --> 00:48:09.470
we can just automatically work out how to apply, um, backprop.

00:48:09.470 --> 00:48:12.485
And there are sort of two cases of this, right?

00:48:12.485 --> 00:48:16.455
So, if what was calculated at each node,

00:48:16.455 --> 00:48:20.165
um, is given as a symbolic expression,

00:48:20.165 --> 00:48:24.190
we could actually have our computer work out for

00:48:24.190 --> 00:48:28.530
us what the derivative of that symbolic expression is.

00:48:28.530 --> 00:48:30.760
So, it could actually calculate, um,

00:48:30.760 --> 00:48:36.605
the gradient of that node and that's referred to as often as automatic differentiation.

00:48:36.605 --> 00:48:39.810
So, this is kind of like Mathematica Wolfram Alpha.

00:48:39.810 --> 00:48:41.950
You know how you can do your math homework on it?

00:48:41.950 --> 00:48:43.235
You just type in your expression,

00:48:43.235 --> 00:48:45.755
say what's a derivative and it gives it back to you right?

00:48:45.755 --> 00:48:51.625
Um, it's working doing symbolic computation and working out the derivative for you.

00:48:51.625 --> 00:48:54.660
Um, so that- so that method could be used to

00:48:54.660 --> 00:48:57.970
work out the local gradients and then we can use

00:48:57.970 --> 00:49:00.925
the graph structure and now rule

00:49:00.925 --> 00:49:04.844
upstream gradient times local gradient gives downstream gradient,

00:49:04.844 --> 00:49:06.500
i.e the chain rule, um,

00:49:06.500 --> 00:49:09.070
to then propagate it through the graph and do

00:49:09.070 --> 00:49:13.340
the whole backward pass completely automatically.

00:49:13.340 --> 00:49:17.515
And so that sounds, um, great.

00:49:17.515 --> 00:49:20.380
Um, slight disappointment, um,

00:49:20.380 --> 00:49:23.530
current deep learning frameworks don't quite give you that.

00:49:23.530 --> 00:49:27.070
Um, there was actually a famous framework that attempted to give you that.

00:49:27.070 --> 00:49:32.020
So the Theano Framework that was developed at the University of Montreal, um,

00:49:32.020 --> 00:49:34.825
those they've now abandoned in the modern era

00:49:34.825 --> 00:49:38.065
of large technology corporation, deep learning frameworks.

00:49:38.065 --> 00:49:40.060
Theano did precisely that.

00:49:40.060 --> 00:49:43.720
It did the full thing of automatic differentiation, um,

00:49:43.720 --> 00:49:47.545
for reasons that we could either think of good or bad,

00:49:47.545 --> 00:49:50.260
current deep learning frameworks like TensorFlow or

00:49:50.260 --> 00:49:53.500
PyTorch actually do a little bit less than that.

00:49:53.500 --> 00:49:55.600
So what they do is, say,

00:49:55.600 --> 00:49:59.890
well for an indiv- for the computations at an individual node,

00:49:59.890 --> 00:50:03.145
you have to do the calculus for yourself.

00:50:03.145 --> 00:50:05.155
Um, for this individual node,

00:50:05.155 --> 00:50:08.860
you have to write the forward propagation, say, you know,

00:50:08.860 --> 00:50:13.870
return X plus Y and you have to write the backward propagation,

00:50:13.870 --> 00:50:16.300
saying the local gradients, uh,

00:50:16.300 --> 00:50:20.290
one and one to the two inputs X and Y, um,

00:50:20.290 --> 00:50:23.380
but providing you or someone else has

00:50:23.380 --> 00:50:28.105
written out the forward and backward local step at this node,

00:50:28.105 --> 00:50:31.060
then TensorFlow or PyTorch does all the rest

00:50:31.060 --> 00:50:34.030
of it for you and runs the backpropagation algorithm.

00:50:34.030 --> 00:50:37.420
[NOISE] Um, and then, you know, effectively,

00:50:37.420 --> 00:50:42.444
that sort of saves you having to have a big symbolic computation engine,

00:50:42.444 --> 00:50:45.970
because somewhat, the person coding

00:50:45.970 --> 00:50:49.030
the node computations is writing

00:50:49.030 --> 00:50:52.420
a bit of code as you might normally imagine doing it whether in,

00:50:52.420 --> 00:50:54.355
you know, C or Pascal,

00:50:54.355 --> 00:50:57.294
of saying returning X plus Y,

00:50:57.294 --> 00:51:00.325
and, you know, local Gradient return one.

00:51:00.325 --> 00:51:05.680
Right? And- and you don't actually have to have a whole symbolic computation engine.

00:51:05.680 --> 00:51:09.580
Okay. So that means the overall picture looks like this.

00:51:09.580 --> 00:51:12.040
Right? So um, schematically,

00:51:12.040 --> 00:51:14.905
we have a computation graph, um,

00:51:14.905 --> 00:51:19.480
and to calculate the forward computation, um,

00:51:19.480 --> 00:51:22.810
we, um, so- sort of put inputs into

00:51:22.810 --> 00:51:26.575
our computation graph where there's sort of X and Y variables,

00:51:26.575 --> 00:51:32.005
and then we run through the nodes in topologically sorted order,

00:51:32.005 --> 00:51:36.910
and for each node we calculate its forward and

00:51:36.910 --> 00:51:40.000
necessarily the things that depends on and have already been

00:51:40.000 --> 00:51:43.465
computed and we just do expression evaluation forward.

00:51:43.465 --> 00:51:46.345
And then we return, um,

00:51:46.345 --> 00:51:48.010
the final gate in the graph,

00:51:48.010 --> 00:51:51.100
which is our loss function, or objective function.

00:51:51.100 --> 00:51:54.430
But then, also we have the backward pass,

00:51:54.430 --> 00:51:55.750
and for the backward pass,

00:51:55.750 --> 00:52:00.520
we go in the nodes in reversed topological, um, resorted order,

00:52:00.520 --> 00:52:02.394
and for each of those nodes,

00:52:02.394 --> 00:52:04.990
we've return their backward value,

00:52:04.990 --> 00:52:06.580
and for their top node,

00:52:06.580 --> 00:52:08.560
we return backward value of one,

00:52:08.560 --> 00:52:11.200
and that will then give us our gradients.

00:52:11.200 --> 00:52:14.200
And so that means, um,

00:52:14.200 --> 00:52:19.195
for any node, any piece of computation that we perform,

00:52:19.195 --> 00:52:23.170
we need to write a little bit of code that um

00:52:23.170 --> 00:52:27.550
says what it's doing on the forward pass and what it's doing on the backward pass.

00:52:27.550 --> 00:52:30.745
So on the forward pass, um,

00:52:30.745 --> 00:52:32.440
this is our multiplication,

00:52:32.440 --> 00:52:35.935
so we're just saying return X times Y.

00:52:35.935 --> 00:52:37.030
So that's pretty easy.

00:52:37.030 --> 00:52:38.500
That's what you're used to doing.

00:52:38.500 --> 00:52:42.280
But while we also need to do the backward passes,

00:52:42.280 --> 00:52:45.430
local gradients of return what is the

00:52:45.430 --> 00:52:50.395
partial of L with respect to Z and with respect to X.

00:52:50.395 --> 00:52:51.730
And well, to do that,

00:52:51.730 --> 00:52:54.085
we have to do a little bit more work.

00:52:54.085 --> 00:52:56.425
So we have to do a little bit more work,

00:52:56.425 --> 00:52:58.555
first of all, in the forward pass.

00:52:58.555 --> 00:53:00.655
So, in the forward pass,

00:53:00.655 --> 00:53:04.870
we have to remember to sort of stuff away in some variables

00:53:04.870 --> 00:53:07.090
what values we computed in the for-

00:53:07.090 --> 00:53:10.420
what- what values were given to us in the forward pass,

00:53:10.420 --> 00:53:13.480
or else we won't be able to calculate the backward pass.

00:53:13.480 --> 00:53:17.620
So we store away the values of X and Y,

00:53:17.620 --> 00:53:19.030
um, and so then,

00:53:19.030 --> 00:53:21.250
when we're doing the backward pass,

00:53:21.250 --> 00:53:24.550
we are passed into us the upstream Gradient,

00:53:24.550 --> 00:53:29.845
the error signal, and now we just do calculate, um,

00:53:29.845 --> 00:53:35.064
upstream Gradient times local Gradient- upstream Gradient times local Gradient,

00:53:35.064 --> 00:53:37.510
and we return backwards,

00:53:37.510 --> 00:53:40.900
um, those um downstream Gradients.

00:53:40.900 --> 00:53:45.865
And so providing we do that for all the nodes of our graph,

00:53:45.865 --> 00:53:48.365
um, we then have something that, um,

00:53:48.365 --> 00:53:51.940
the system can learn for us as a deep learning system.

00:53:51.940 --> 00:53:54.580
And so what that means in practice,

00:53:54.580 --> 00:53:56.545
um, is that, you know,

00:53:56.545 --> 00:54:01.180
any of these deep learning frameworks come with a whole box of tools that says,

00:54:01.180 --> 00:54:04.090
um, here is a fully connected forward layer,

00:54:04.090 --> 00:54:05.770
here is a sigmoid unit,

00:54:05.770 --> 00:54:08.560
here is other more complicated things we'll do later,

00:54:08.560 --> 00:54:10.795
like convolutions and recurrent layers.

00:54:10.795 --> 00:54:13.570
And to the extent that you are using one of those,

00:54:13.570 --> 00:54:15.955
somebody else has done this work for you.

00:54:15.955 --> 00:54:19.795
Right? That they've um defined, um,

00:54:19.795 --> 00:54:25.855
nodes or a layer of nodes that have forward and backward already written for- for them.

00:54:25.855 --> 00:54:28.980
And to the extent that that's true, um,

00:54:28.980 --> 00:54:32.850
that means that making neural nets is heaps of fun. It's just like lego.

00:54:32.850 --> 00:54:35.340
Right? You just stick these layers together and say,

00:54:35.340 --> 00:54:37.005
"God, I have to learn on some data and train it."

00:54:37.005 --> 00:54:40.560
You know, it's so easy that my high school student is building these things.

00:54:40.560 --> 00:54:43.020
Right? Um, you don't have to understand much really,

00:54:43.020 --> 00:54:44.460
um, but, you know,

00:54:44.460 --> 00:54:47.715
to the extent that you actually want to do some original research and think,

00:54:47.715 --> 00:54:50.635
"I've got this really cool idea of how to do things differently.

00:54:50.635 --> 00:54:53.920
I'm going to define my own kind of different computation."

00:54:53.920 --> 00:54:57.580
Well, then you have to do this and define your class,

00:54:57.580 --> 00:54:59.050
and as well as, sort of saying,

00:54:59.050 --> 00:55:00.760
how to compute the forward value,

00:55:00.760 --> 00:55:02.710
you will have to pull out your copy of

00:55:02.710 --> 00:55:05.665
Wolfram Alpha and work out what the derivatives are,

00:55:05.665 --> 00:55:08.005
um, and put that into the backward pass.

00:55:08.005 --> 00:55:09.955
Um, yeah.

00:55:09.955 --> 00:55:13.600
Okay. So here's just one little more note on that.

00:55:13.600 --> 00:55:17.890
Um, you know, in the early days of deep learning,

00:55:17.890 --> 00:55:20.995
say prior to 2014,

00:55:20.995 --> 00:55:24.610
what we always used to state to everybody very sternly is,

00:55:24.610 --> 00:55:26.590
"You should check all your Gradients,

00:55:26.590 --> 00:55:28.660
by doing numeric Gradient checks.

00:55:28.660 --> 00:55:30.459
It's really really important."

00:55:30.459 --> 00:55:34.420
Um, and so what that meant was, well, you know,

00:55:34.420 --> 00:55:38.470
if you want to know whether you have coded your backward pass right,

00:55:38.470 --> 00:55:40.735
an easy way to check, um,

00:55:40.735 --> 00:55:43.015
whether you've coded it right,

00:55:43.015 --> 00:55:46.450
is to do this numeric Gradient

00:55:46.450 --> 00:55:50.785
where you're sort of estimating the slope by wiggling it a bit,

00:55:50.785 --> 00:55:52.915
and wiggling the input a bit,

00:55:52.915 --> 00:55:54.640
and seeing what effect it has.

00:55:54.640 --> 00:55:59.080
So I'm working out the value of the function the F of X plus H,

00:55:59.080 --> 00:56:01.840
for H very small like E to the minus four,

00:56:01.840 --> 00:56:04.150
and then F of X minus H, um,

00:56:04.150 --> 00:56:05.590
and then dividing by 2H,

00:56:05.590 --> 00:56:07.945
and I'm saying well, what is the slope at this point,

00:56:07.945 --> 00:56:11.920
and I'm getting a numeric estimate of the Gradient with respect,

00:56:11.920 --> 00:56:15.265
um, to my variable X here.

00:56:15.265 --> 00:56:18.310
Um, so this is what you will have seen in

00:56:18.310 --> 00:56:22.929
high school when you did the sort of first um estimates of Gradients,

00:56:22.929 --> 00:56:26.710
where you sort of worked out F of X plus H divided by H

00:56:26.710 --> 00:56:30.970
and you're doing rise over run and got a point estimate of the Gradient.

00:56:30.970 --> 00:56:32.770
Um, exactly the same thing,

00:56:32.770 --> 00:56:34.225
except for the fact,

00:56:34.225 --> 00:56:38.410
in this case, rather than doing it one sided like that,

00:56:38.410 --> 00:56:40.045
we are doing it two-sided.

00:56:40.045 --> 00:56:42.385
It turns out that if you actually wanna do this,

00:56:42.385 --> 00:56:47.035
two-sided is asymptotically hugely [NOISE] better,

00:56:47.035 --> 00:56:48.610
and so you're always better off doing

00:56:48.610 --> 00:56:52.900
two-sided Gradient checks rather than one-sided Gradient checks.

00:56:52.900 --> 00:56:56.920
Um, so since you saw that- since it's hard to implement this wrong,

00:56:56.920 --> 00:56:59.470
this is a good way to check that your Gradients are

00:56:59.470 --> 00:57:02.395
correct if you've defined them yourselves.

00:57:02.395 --> 00:57:06.625
Um, as a technique to use it [NOISE] for anything,

00:57:06.625 --> 00:57:08.950
it's completely, completely hopeless,

00:57:08.950 --> 00:57:12.040
because we're thinking of doing this over

00:57:12.040 --> 00:57:15.520
our deep learning model for a fully connected layer.

00:57:15.520 --> 00:57:17.080
What this means [NOISE] is that,

00:57:17.080 --> 00:57:22.555
if you've got this sort of like a W matrix of N by M and you want to, um,

00:57:22.555 --> 00:57:27.474
calculate um your partial derivatives to check if they're correct,

00:57:27.474 --> 00:57:31.360
it means that you have to do this for every element of the matrix.

00:57:31.360 --> 00:57:34.090
So you have to calculate the eventual loss,

00:57:34.090 --> 00:57:37.390
first jiggling W11, then jiggling W12,

00:57:37.390 --> 00:57:40.675
then jiggling one- W13, 14 et cetera.

00:57:40.675 --> 00:57:42.880
So you have- in the complex network,

00:57:42.880 --> 00:57:46.030
you'll end up literally doing millions of function evaluations

00:57:46.030 --> 00:57:49.565
to check the Gradients at one point in time.

00:57:49.565 --> 00:57:51.520
So, you know, it's,

00:57:51.520 --> 00:57:54.010
it's not like what I advertised for

00:57:54.010 --> 00:57:57.220
backprop when I said it's just as efficient as calculating,

00:57:57.220 --> 00:57:59.875
um, the forward value.

00:57:59.875 --> 00:58:01.840
Doing this is forward

00:58:01.840 --> 00:58:06.190
value computation time multiplied by number of parameters in our model,

00:58:06.190 --> 00:58:08.680
which is often huge for deep learning networks.

00:58:08.680 --> 00:58:10.450
So this is something that you only want to

00:58:10.450 --> 00:58:14.170
have inside- if statements that you could turn off.

00:58:14.170 --> 00:58:18.805
So you could just sort of run it to check that your code isn't bre- um, debuggy.

00:58:18.805 --> 00:58:21.640
Um, you know, in honesty,

00:58:21.640 --> 00:58:24.220
this is just much less needed now because, you know,

00:58:24.220 --> 00:58:28.120
by and large you can plug together your components and layers and PyTorch,

00:58:28.120 --> 00:58:32.665
um, and other people wrote the code right and it will work.

00:58:32.665 --> 00:58:35.515
Um, so you probably don't need to do this all the time.

00:58:35.515 --> 00:58:38.050
But it is still a useful thing to look at and to know

00:58:38.050 --> 00:58:42.190
about if things um, are going wrong.

00:58:42.190 --> 00:58:46.840
Yeah. Okay, so we- we've now mastered the core technology of neural nets.

00:58:46.840 --> 00:58:51.070
We saw now well, basically everything we need to know about neural nets,

00:58:51.070 --> 00:58:54.280
and I sort of just, um, summarized it there.

00:58:54.280 --> 00:58:59.140
Um, just to sort of emphasize um once more.

00:58:59.140 --> 00:59:03.760
Um, you know, I think some people think,

00:59:03.760 --> 00:59:07.840
why do we even lear- need to learn all this stuff about gradients?'

00:59:07.840 --> 00:59:09.790
And there's a sense in which it's [inaudible] really,

00:59:09.790 --> 00:59:14.770
because these modern deep learning frameworks will compute all of the gradients for you.

00:59:14.770 --> 00:59:17.080
You know, we make you suffer on homework two,

00:59:17.080 --> 00:59:18.640
but in homework three,

00:59:18.640 --> 00:59:21.625
you can have your gradients computed for you.

00:59:21.625 --> 00:59:24.655
But, you know, I- so you know it's sort of just, like, well,

00:59:24.655 --> 00:59:27.940
why should you take a c- a class on compilers, right?

00:59:27.940 --> 00:59:33.415
That there's actually something useful in understanding what goes on under the hood,

00:59:33.415 --> 00:59:35.080
even though most of the time,

00:59:35.080 --> 00:59:38.815
we're just perfectly happy to let the C compiler do its thing,

00:59:38.815 --> 00:59:44.080
without being experts on X86 assembler every day of the wa- week.

00:59:44.080 --> 00:59:46.525
But, you know, there is more to it than that.

00:59:46.525 --> 00:59:49.840
Um, you know, because even though backpropagation is great,

00:59:49.840 --> 00:59:51.850
once you're building complex models,

00:59:51.850 --> 00:59:56.320
backpropagation doesn't always work as you would expect it to.

00:59:56.320 --> 00:59:58.180
Perfectly is maybe the wrong word,

00:59:58.180 --> 01:00:00.565
because you know mathematically it's perfect.

01:00:00.565 --> 01:00:03.595
Um, but it might not be achieving what you're wanting it to.

01:00:03.595 --> 01:00:06.850
And well, if you want to sort of then debug an improved models,

01:00:06.850 --> 01:00:09.775
it's kind of crucial to understand what's going on.

01:00:09.775 --> 01:00:12.880
So, there's a nice medium piece by Andre Karpathy,

01:00:12.880 --> 01:00:17.890
of yes you should understand backprop um that's on the syllabus page, um,

01:00:17.890 --> 01:00:21.520
that talks about this and indeed um, um,

01:00:21.520 --> 01:00:26.410
week after next, Abby is actually going to lecture about recurrent neural networks,

01:00:26.410 --> 01:00:28.300
and you know one of the places, um,

01:00:28.300 --> 01:00:30.790
where you can easily fail um,

01:00:30.790 --> 01:00:33.580
and doing backpropagation turns up there,

01:00:33.580 --> 01:00:35.485
um, is a good example.

01:00:35.485 --> 01:00:43.250
Okay. So anyone have any questions about backpropagation and computation graphs?

01:00:45.660 --> 01:00:51.205
Okay. If not the remainder of the time is, um,

01:00:51.205 --> 01:00:55.165
the grab bag of things that you really should know about,

01:00:55.165 --> 01:00:57.310
if you're going to be doing deep learning.

01:00:57.310 --> 01:01:00.445
And so, yeah, this is just itsy-bitsy and,

01:01:00.445 --> 01:01:01.920
but let me say them.

01:01:01.920 --> 01:01:04.335
Um, so up until now,

01:01:04.335 --> 01:01:07.080
when we've had um loss functions,

01:01:07.080 --> 01:01:10.560
and we've been maximizing the likelihood of our data,

01:01:10.560 --> 01:01:11.760
and stuff like that,

01:01:11.760 --> 01:01:17.235
we've sort of just had this part here which is the likelihood of our data,

01:01:17.235 --> 01:01:19.500
and we've worked to maximize it.

01:01:19.500 --> 01:01:27.445
Um, however, um, in practice that works badly usually,

01:01:27.445 --> 01:01:31.510
and we need to do something else which is regularize our models.

01:01:31.510 --> 01:01:34.645
And if you've done the Machine Learning class,

01:01:34.645 --> 01:01:38.065
or something like that you will have seen regularization.

01:01:38.065 --> 01:01:42.445
And there are various techniques to do regularization, but, um,

01:01:42.445 --> 01:01:43.930
compared to anything else,

01:01:43.930 --> 01:01:46.975
regularization is even more important,

01:01:46.975 --> 01:01:48.820
um, for deep learning models, right?

01:01:48.820 --> 01:01:54.610
So, um, the general idea is if you have a lot of parameters in your model,

01:01:54.610 --> 01:02:00.850
those parameters can just essentially memorize what's in the data that you trained at.

01:02:00.850 --> 01:02:04.030
And so they're very good at predicting the answers.

01:02:04.030 --> 01:02:09.205
The model becomes very good at predicting the answers to the data you trained it on,

01:02:09.205 --> 01:02:15.040
but the model may become poor at working in the real world, and different examples.

01:02:15.040 --> 01:02:18.250
And somehow we want to stop that.

01:02:18.250 --> 01:02:22.000
And this problem is especially bad for deep learning models,

01:02:22.000 --> 01:02:24.760
because typically deep learning models have vast,

01:02:24.760 --> 01:02:26.485
vast numbers of parameters.

01:02:26.485 --> 01:02:29.800
So in the good old days when statisticians ruled the show,

01:02:29.800 --> 01:02:34.270
they told people that it was completely ridiculous to

01:02:34.270 --> 01:02:38.650
have a number of parameters that approached your number of training examples.

01:02:38.650 --> 01:02:41.260
You know, you should never have more parameters in your model,

01:02:41.260 --> 01:02:44.710
than one-tenth of the number of your training examples.

01:02:44.710 --> 01:02:47.545
So it's the kind of um rules of thumb you are told,

01:02:47.545 --> 01:02:51.865
so that you had lots of examples with which to estimate every parameter.

01:02:51.865 --> 01:02:54.970
Um, that's just not true with deep learning models,

01:02:54.970 --> 01:02:57.010
is just really common that we trained

01:02:57.010 --> 01:03:00.550
deep learning models that have 10 times as many parameters,

01:03:00.550 --> 01:03:02.980
as we have training examples.

01:03:02.980 --> 01:03:05.485
Um, but miraculously it works.

01:03:05.485 --> 01:03:06.955
In fact it works brilliantly.

01:03:06.955 --> 01:03:10.120
Those highly over parameterized models,

01:03:10.120 --> 01:03:14.935
and this one of the big secret sources of why deep learning has been so brilliant,

01:03:14.935 --> 01:03:18.085
but it only works if we regularize the model.

01:03:18.085 --> 01:03:22.630
So, if you train a model without sufficient regularization,

01:03:22.630 --> 01:03:29.050
what you find is that you're training it and working out your loss on the training data,

01:03:29.050 --> 01:03:30.910
and the model keeps on getting better,

01:03:30.910 --> 01:03:32.515
and better, and better, and better.

01:03:32.515 --> 01:03:38.170
Um, necessarily, alg- algorithm has to improve loss on the training data.

01:03:38.170 --> 01:03:39.805
So the worst thing that could happen,

01:03:39.805 --> 01:03:43.375
is that the graph could become absolutely fa- flat.

01:03:43.375 --> 01:03:47.199
What you'll find is with most models that we train,

01:03:47.199 --> 01:03:51.565
they have so many parameters that this will just keep on going down,

01:03:51.565 --> 01:03:56.124
until the loss is sort of approaching the numerical precision of zero,

01:03:56.124 --> 01:03:57.940
if you leave it training for long enough.

01:03:57.940 --> 01:04:01.450
It just learns the correct answer for every example,

01:04:01.450 --> 01:04:04.405
beca- because effectively can memorize the examples.

01:04:04.405 --> 01:04:06.085
Okay, but if you then say,

01:04:06.085 --> 01:04:09.640
''Let me test out this model on some different data.''

01:04:09.640 --> 01:04:11.890
What you find is this red curve,

01:04:11.890 --> 01:04:15.610
that up until a certain point, um,

01:04:15.610 --> 01:04:20.110
that you are also building a model that's better at predicting on different data,

01:04:20.110 --> 01:04:23.845
but after some point this curve starts to curve up again.

01:04:23.845 --> 01:04:25.930
And ignore that bit where it seems to curve down again,

01:04:25.930 --> 01:04:27.340
that was a mistake in the drawing.

01:04:27.340 --> 01:04:31.075
Um, and so this is then referred to as over-fitting,

01:04:31.075 --> 01:04:35.290
that the- from here on the training model is

01:04:35.290 --> 01:04:39.535
just learning to memorize whatever was in the training data,

01:04:39.535 --> 01:04:44.590
but not in a way that later generalized to other examples.

01:04:44.590 --> 01:04:46.765
And so this is not what we want.

01:04:46.765 --> 01:04:50.575
We want to try and avoid over-fitting as much as possible,

01:04:50.575 --> 01:04:54.160
and there are various regularization techniques that we use for that.

01:04:54.160 --> 01:05:01.060
And simple starting one is this one here where we penalize the log-likelihood by saying,

01:05:01.060 --> 01:05:07.389
''You're going to be penalized to the extent that you move parameters away from zero.''

01:05:07.389 --> 01:05:11.500
So the default state of nature is all parameters are zeros,

01:05:11.500 --> 01:05:13.675
so they're ignored on computations.

01:05:13.675 --> 01:05:16.345
You can have parameters that have big values,

01:05:16.345 --> 01:05:18.370
but you'll pee penalized a bit four,

01:05:18.370 --> 01:05:21.490
and this is referred to as L-2 regularization.

01:05:21.490 --> 01:05:23.980
And, you know, that's sort of a starting point of

01:05:23.980 --> 01:05:26.530
something sensible you could do with regularization,

01:05:26.530 --> 01:05:28.600
but there's more to say later.

01:05:28.600 --> 01:05:32.320
And we'll talk in this sort of lecture before we discuss

01:05:32.320 --> 01:05:37.480
final projects of other clever regularization techniques at neural networks.

01:05:37.480 --> 01:05:40.840
Okay. Um, grab bag number two,

01:05:40.840 --> 01:05:44.290
vectorization is the term that you have here,

01:05:44.290 --> 01:05:46.330
um, but it's not only vectors.

01:05:46.330 --> 01:05:48.820
This is also matrixization,

01:05:48.820 --> 01:05:52.870
and higher dimensional matrices what are called tensors,

01:05:52.870 --> 01:05:55.135
in this field tensorization.

01:05:55.135 --> 01:05:58.690
Um, getting deep learning systems to run fast and

01:05:58.690 --> 01:06:05.215
efficiently is only possible if we vectorize things.

01:06:05.215 --> 01:06:07.210
Um, and what does that mean?

01:06:07.210 --> 01:06:09.685
What that means is, you know,

01:06:09.685 --> 01:06:13.300
the straightforward way to write a lot of code um,

01:06:13.300 --> 01:06:15.730
that you saw in your first CS class,

01:06:15.730 --> 01:06:22.120
is you say for I in range in um calculate random randi-1.

01:06:22.120 --> 01:06:25.990
Um, but when we want to be clever,

01:06:25.990 --> 01:06:32.305
um, people, um, that are doing things fast,

01:06:32.305 --> 01:06:38.620
um, we say rather than work out this W dot one word vector at a time,

01:06:38.620 --> 01:06:40.285
and do it in a four loop,

01:06:40.285 --> 01:06:44.950
we could instead put all of our word vectors into one matrix,

01:06:44.950 --> 01:06:52.675
and then do simply one matrix-matrix multiply of W by our word vector matrix.

01:06:52.675 --> 01:06:58.735
And even if you run your code on your laptop on a CPU,

01:06:58.735 --> 01:07:02.560
you will find out that if you do it the vectorized way,

01:07:02.560 --> 01:07:04.900
things will become hugely faster.

01:07:04.900 --> 01:07:05.950
So in this example,

01:07:05.950 --> 01:07:08.484
it became over an order of magnitude faster,

01:07:08.484 --> 01:07:11.785
when doing it with a vector- vectorized rather than,

01:07:11.785 --> 01:07:13.735
um, with a full loop.

01:07:13.735 --> 01:07:19.000
Um, and those gains are only compounded when we run code on a GPU,

01:07:19.000 --> 01:07:22.600
that you'll get no gains and speed of tall on a GPU,

01:07:22.600 --> 01:07:24.190
unless your code is vectorized.

01:07:24.190 --> 01:07:25.705
But if it is vectorized,

01:07:25.705 --> 01:07:27.580
then you can hope to have results, of oh,

01:07:27.580 --> 01:07:29.650
yeah, this runs 40 times faster,

01:07:29.650 --> 01:07:31.735
than it did on the CPU.

01:07:31.735 --> 01:07:39.415
Okay, um, yeah, so always try to use vectors and matrices not for loops.

01:07:39.415 --> 01:07:42.550
Um, of course it's useful when developing stuff to time your code,

01:07:42.550 --> 01:07:44.065
and find out what's slow.

01:07:44.065 --> 01:07:45.955
Um, okay.

01:07:45.955 --> 01:07:47.515
Point three.

01:07:47.515 --> 01:07:53.845
Um, okay, so we discussed this idea, um, last time,

01:07:53.845 --> 01:07:59.575
and the time before that after- after having the sort of affine layer,

01:07:59.575 --> 01:08:01.270
where we took, you know,

01:08:01.270 --> 01:08:03.880
go from X to WX, plus B.

01:08:03.880 --> 01:08:05.500
That's referred to as an affine layer,

01:08:05.500 --> 01:08:06.880
so we're doing this, um,

01:08:06.880 --> 01:08:09.925
multiplying a vector by a matrice- matrix,

01:08:09.925 --> 01:08:12.505
and adding um biases.

01:08:12.505 --> 01:08:15.925
We necessarily to have power and a deep network, um,

01:08:15.925 --> 01:08:19.915
have to have some form of non-linearity.

01:08:19.915 --> 01:08:22.495
And so, I just wanted to go through a bit of background

01:08:22.495 --> 01:08:25.644
on non-linearity is in what people use,

01:08:25.644 --> 01:08:27.085
and what to use.

01:08:27.085 --> 01:08:33.340
So, if you're sort of starting from the idea of what we know is logistic regression, um,

01:08:33.340 --> 01:08:36.549
what's commonly referred to as the sigmoid curve,

01:08:36.549 --> 01:08:39.670
or maybe more precisely is the logistic,

01:08:39.670 --> 01:08:42.955
um, function is this picture here.

01:08:42.955 --> 01:08:46.060
So something that's squashes any real

01:08:46.060 --> 01:08:49.660
number positive or negative into the range zero to one.

01:08:49.660 --> 01:08:51.730
It gives you a probability output.

01:08:51.730 --> 01:08:55.600
Um, these- this use of this, um,

01:08:55.600 --> 01:09:00.400
logistic function was really really common in early neural nets.

01:09:00.400 --> 01:09:02.785
If you go back to '80s, '90s neural nets,

01:09:02.785 --> 01:09:07.135
there were, um, sigmoid functions absolutely everywhere.

01:09:07.135 --> 01:09:10.150
Um, in more recent times,

01:09:10.150 --> 01:09:13.150
90 percent of the time nobody uses

01:09:13.150 --> 01:09:16.435
this and they've been found to sort of actually work quite poorly.

01:09:16.435 --> 01:09:19.870
The only place these are used is when you

01:09:19.870 --> 01:09:24.730
actually want a value between zero and one is your output.

01:09:24.730 --> 01:09:28.270
So we'll talk later about how you have gating in networks,

01:09:28.270 --> 01:09:32.800
and so gating as a place where you want to have a probability between two things.

01:09:32.800 --> 01:09:34.795
And then you will use one of those,

01:09:34.795 --> 01:09:37.240
but you use some absolutely nowhere else.

01:09:37.240 --> 01:09:40.240
Um, here is the tanh curve.

01:09:40.240 --> 01:09:42.880
Um, so the formula for tanh, um,

01:09:42.880 --> 01:09:46.300
looks like a scary thing with thoughts of exponentials in it,

01:09:46.300 --> 01:09:51.145
and it doesn't really look much like a logistic curve whatsoever.

01:09:51.145 --> 01:09:56.740
Um, but if you um dig up your math textbook you can convince yourself that

01:09:56.740 --> 01:09:59.920
a tanh curve is actually exactly the same as

01:09:59.920 --> 01:10:04.090
the logistic curve apart from you multiply it by two,

01:10:04.090 --> 01:10:06.505
so it has a range of two rather than one,

01:10:06.505 --> 01:10:08.095
and you shift it down line.

01:10:08.095 --> 01:10:10.735
So, this is sort of just a re-scaled logistic.

01:10:10.735 --> 01:10:13.690
There's now symmetric between one and minus one,

01:10:13.690 --> 01:10:16.900
and the fact that some metric in the output actually helps

01:10:16.900 --> 01:10:20.545
a lot for putting into neural networks. Um.

01:10:20.545 --> 01:10:25.070
So, tanh's, are still reasonably widely used

01:10:25.070 --> 01:10:29.270
in quite a number of places um in um your networks.

01:10:29.270 --> 01:10:32.755
So, tanh should be a friend of yours and you should know about that.

01:10:32.755 --> 01:10:36.545
But you know, one of the bad things about using

01:10:36.545 --> 01:10:41.320
um transcendental functions like the sigmoid or tanh is,

01:10:41.320 --> 01:10:48.300
you know, they involve this expensive math operations um that slow you down.

01:10:48.300 --> 01:10:51.050
Like, it's sort of a nuisance to be kind

01:10:51.050 --> 01:10:53.830
of computing exponentials and tanh's in your computer,

01:10:53.830 --> 01:10:55.070
things are kind of slow.

01:10:55.070 --> 01:10:58.870
So people started um playing around with ways

01:10:58.870 --> 01:11:02.940
to make things faster and so someone came up with this idea like,

01:11:02.940 --> 01:11:05.360
maybe we could come up with a hard tanh,

01:11:05.360 --> 01:11:08.560
um where it's just sort of flat out here

01:11:08.560 --> 01:11:12.000
and then it has a linear slope and then it's flat at the top.

01:11:12.000 --> 01:11:16.035
You know, it sort of looks like a tanh but we just squared it off.

01:11:16.035 --> 01:11:19.580
Um, and while this is really cheap to compute right, you say,

01:11:19.580 --> 01:11:21.965
x less than minus one,

01:11:21.965 --> 01:11:26.630
return minus one, return plus one or just return the number.

01:11:26.630 --> 01:11:29.135
No complex transcendentals.

01:11:29.135 --> 01:11:30.700
The funny thing is,

01:11:30.700 --> 01:11:33.475
it turns out that this actually works pretty well.

01:11:33.475 --> 01:11:36.580
You might be scared and you might justifiably be

01:11:36.580 --> 01:11:40.340
scared because if you start thinking about gradients,

01:11:40.340 --> 01:11:41.645
once you're over here,

01:11:41.645 --> 01:11:43.190
there's no gradient, right?

01:11:43.190 --> 01:11:46.495
It's completely flat at zero.

01:11:46.495 --> 01:11:51.035
So, things go dead as soon as they're at one of the ends.

01:11:51.035 --> 01:11:54.350
So, it's sort of important to stay in this middle section at least for

01:11:54.350 --> 01:11:58.120
a while and then its just got a slope of one, right?

01:11:58.120 --> 01:11:59.800
It's a constant slope of one.

01:11:59.800 --> 01:12:03.920
But this is enough of a linearity that actually it

01:12:03.920 --> 01:12:08.770
works well in neural networks and you can train neural networks.

01:12:08.770 --> 01:12:13.995
So, that's sent the whole field in the opposite direction and people thought,

01:12:13.995 --> 01:12:15.880
oh, if that works,

01:12:15.880 --> 01:12:18.855
maybe we can make things even simpler.

01:12:18.855 --> 01:12:24.100
And that led to the now famous what's referred to [inaudible] as ReLU.

01:12:24.100 --> 01:12:27.090
So there is a mistake in my editing there,

01:12:27.090 --> 01:12:28.500
delete off hard tanh.

01:12:28.500 --> 01:12:30.830
That was in slides by mistake.

01:12:30.830 --> 01:12:32.370
[LAUGHTER] The ReLU unit,

01:12:32.370 --> 01:12:36.795
everyone calls it ReLU which stands for rectified linear unit.

01:12:36.795 --> 01:12:42.250
So, the Re-, the ReLU is essentially the simplest non-linearity you can have.

01:12:42.250 --> 01:12:45.950
So the ReLU is zero,

01:12:45.950 --> 01:12:52.015
slope zero as soon as you're in the negative regime and it's just a line slope one,

01:12:52.015 --> 01:12:53.800
when you're in the positive regime.

01:12:53.800 --> 01:12:56.600
I mean, when I first saw this,

01:12:56.600 --> 01:12:59.835
I mean, it's sort of blew my mind it could possibly work.

01:12:59.835 --> 01:13:01.770
Because it sort of, I guess,

01:13:01.770 --> 01:13:07.220
I was brought up on these sort of tanh's and sigmoids and the sorts of these arguments

01:13:07.220 --> 01:13:13.250
about the slope and you get these gradients and you can move around with the gradient.

01:13:13.250 --> 01:13:17.240
And how is it meant to work if half of this function just says

01:13:17.240 --> 01:13:21.720
output zero and no gradient and the other half is just this straight line.

01:13:21.720 --> 01:13:25.365
And in particular, when you're in the positive regime,

01:13:25.365 --> 01:13:27.905
this is just an identity function.

01:13:27.905 --> 01:13:35.140
And, you know, I sort of argued before that if you just compose linear transforms,

01:13:35.140 --> 01:13:40.560
you don't get any power but provided when this is the right-hand part of the regime.

01:13:40.560 --> 01:13:42.190
Since this is an identity function,

01:13:42.190 --> 01:13:43.400
that's exactly what we're doing.

01:13:43.400 --> 01:13:45.770
We're just composing linear transforms.

01:13:45.770 --> 01:13:48.280
So you- you sort of believe it just can't possibly

01:13:48.280 --> 01:13:51.755
work but it turns out that this works brilliantly.

01:13:51.755 --> 01:13:54.860
And this is now by far

01:13:54.860 --> 01:13:59.640
the default choice when people are building feed for deep networks.

01:13:59.640 --> 01:14:05.190
That people use ReLU non-linearities and they are very fast,

01:14:05.190 --> 01:14:09.260
they train very quickly and they perform very well.

01:14:09.260 --> 01:14:10.845
And so, effectively, you know,

01:14:10.845 --> 01:14:13.630
it is, it is simply just each u-,

01:14:13.630 --> 01:14:15.350
depending on the inputs,

01:14:15.350 --> 01:14:20.495
each unit is just either dead or it's passing things on as an identity function.

01:14:20.495 --> 01:14:22.590
But that's enough of lini-,

01:14:22.590 --> 01:14:24.460
non-linearity that you can do

01:14:24.460 --> 01:14:28.400
arbitrary function approximation still with a deep learning network.

01:14:28.400 --> 01:14:32.255
And people now make precisely the opposite argument which is,

01:14:32.255 --> 01:14:41.775
because this unit just has a slope of one over it's non-zero range, that means,

01:14:41.775 --> 01:14:45.280
the gradient is past spec very efficiently to

01:14:45.280 --> 01:14:50.860
the inputs and therefore the models train very efficiently whereas,

01:14:50.860 --> 01:14:53.655
when you are with these kind of curves,

01:14:53.655 --> 01:14:58.850
when you're over here, there's very little slope so your models might train very slowly.

01:14:58.850 --> 01:15:01.760
Okay. So, you know,

01:15:01.760 --> 01:15:05.325
for feed-forward network, try this before you try anything else.

01:15:05.325 --> 01:15:08.855
But there's sort of then been a sub literature that says,

01:15:08.855 --> 01:15:12.620
well, maybe that's too simple and we could do a bit better.

01:15:12.620 --> 01:15:15.610
And so that led to the leaky ReLU which said,

01:15:15.610 --> 01:15:19.775
"Maybe we should put a tiny bit of slope over here so it's not completely dead."

01:15:19.775 --> 01:15:22.085
So you can make it something like one,

01:15:22.085 --> 01:15:25.405
one 100th as the slope of this part.

01:15:25.405 --> 01:15:26.690
And then people had, well,

01:15:26.690 --> 01:15:27.880
let's build off that,

01:15:27.880 --> 01:15:31.360
maybe we could actually put another parameter into

01:15:31.360 --> 01:15:34.960
our neural network and we could have a parametric ReLU.

01:15:34.960 --> 01:15:38.980
So, there's some slope over here but we're also going to

01:15:38.980 --> 01:15:45.280
backpropagate into our non-linearity which has this extra alpha parameter,

01:15:45.280 --> 01:15:47.645
which is how ma- much slope it has.

01:15:47.645 --> 01:15:50.835
And so, variously people have used these,

01:15:50.835 --> 01:15:55.150
you can sort of find 10 papers on archive where people say,

01:15:55.150 --> 01:15:57.950
you can get better results from using one or other of these.

01:15:57.950 --> 01:16:00.770
You can also find papers where people said it made

01:16:00.770 --> 01:16:03.955
no difference for them versus just using a ReLU.

01:16:03.955 --> 01:16:05.334
So, I think basically,

01:16:05.334 --> 01:16:08.925
you can start off with a ReLU and work from there.

01:16:08.925 --> 01:16:13.495
Yes. So, parameter initialization,

01:16:13.495 --> 01:16:18.225
it's when, so, when we have these matrices and parameters in our model,

01:16:18.225 --> 01:16:20.910
it's vital, vital, vital,

01:16:20.910 --> 01:16:27.900
that you have to initialize those parameter weights with small random values.

01:16:27.900 --> 01:16:30.140
This was precisely the lesson that

01:16:30.140 --> 01:16:33.520
some people hadn't discovered when it came to final project time.

01:16:33.520 --> 01:16:36.255
So I'll emphasize it is vital, vital.

01:16:36.255 --> 01:16:40.025
So, if you just start off with the weights being zero,

01:16:40.025 --> 01:16:42.699
you kind of have these complete symmetries,

01:16:42.699 --> 01:16:45.320
right, that everything will be calculated the same,

01:16:45.320 --> 01:16:49.350
everything will move the same and you're not actually training

01:16:49.350 --> 01:16:54.600
this complex network with a lot of units that are specializing to learn different things.

01:16:54.600 --> 01:16:57.550
So, somehow, you have to break the symmetry and we

01:16:57.550 --> 01:17:00.510
do that by giving small random weights.

01:17:00.510 --> 01:17:02.980
So, you know, there's sort of some fine points.

01:17:02.980 --> 01:17:04.660
When you have biases,

01:17:04.660 --> 01:17:06.400
you may as well just start them at Zero,

01:17:06.400 --> 01:17:11.640
as neutral and see how the system learn the bias that you want et cetera.

01:17:11.640 --> 01:17:18.965
But in general, the weights you want to initialize to small random values.

01:17:18.965 --> 01:17:24.905
You'll find in PyTorch or other deep learning practi- packages,

01:17:24.905 --> 01:17:30.540
a common initialization that's used and often recommended is this Xavier Initialization.

01:17:30.540 --> 01:17:34.110
And so, the trick of this is that,

01:17:34.110 --> 01:17:37.350
for a lot of models and a lot of places,

01:17:37.350 --> 01:17:40.720
think of some of these things like these ones and these,

01:17:40.720 --> 01:17:46.205
you'd like the values in the network to sort of stay small,

01:17:46.205 --> 01:17:48.880
in this sort of middle range here.

01:17:48.880 --> 01:17:53.145
And well, if you kind of have a matrix with big values in it

01:17:53.145 --> 01:17:57.470
and you multiply a vector by this matrix,

01:17:57.470 --> 01:17:58.910
you know, things might get bigger.

01:17:58.910 --> 01:18:00.550
And then if you put in through another layer,

01:18:00.550 --> 01:18:03.180
it'll get bigger again and then sort of everything

01:18:03.180 --> 01:18:05.980
will be too big and you will have problems.

01:18:05.980 --> 01:18:10.285
So, really, Xavier Initialization is seeking to avoid that by saying,

01:18:10.285 --> 01:18:14.590
how many inputs are there to this node?

01:18:14.590 --> 01:18:16.405
How many outputs are there?

01:18:16.405 --> 01:18:20.920
We want to sort of temp it down the initialization based on the inputs

01:18:20.920 --> 01:18:26.395
and the outputs because effectively we'll be using this number that many times.

01:18:26.395 --> 01:18:30.185
It's a good thing to use, you can use that.

01:18:30.185 --> 01:18:34.955
Optimizers. Up till now,

01:18:34.955 --> 01:18:37.770
we saw, just talked about plain SGD.

01:18:37.770 --> 01:18:43.785
You know, normally plain SGD actually works just fine.

01:18:43.785 --> 01:18:46.500
But often if you want to use just plain SGD,

01:18:46.500 --> 01:18:49.860
you have to spend time tuning the learning rate,

01:18:49.860 --> 01:18:54.440
that alpha that we multiplied the gradient by.

01:18:54.440 --> 01:18:58.535
For complex nets and situations or to avoid worry,

01:18:58.535 --> 01:19:03.710
there's sort of now this big family and more sophisticated adaptive optimizers.

01:19:03.710 --> 01:19:10.055
And so, effectively they're scaling the parameter adjustment by accumulated gradients,

01:19:10.055 --> 01:19:14.370
which have the effect that they learn per parameter learning rates.

01:19:14.370 --> 01:19:18.120
So that they can see which parameters would be useful to move

01:19:18.120 --> 01:19:22.580
more and which one is less depending on the sensitivity of those parameters.

01:19:22.580 --> 01:19:24.040
So, where things are flat,

01:19:24.040 --> 01:19:25.960
you can be trying to move quickly.

01:19:25.960 --> 01:19:27.450
Where things are bouncing around a lot,

01:19:27.450 --> 01:19:30.550
you are going to be trying to move just a little so as not to overshoot.

01:19:30.550 --> 01:19:32.850
And so, there's a whole family of these; Adagrad,

01:19:32.850 --> 01:19:35.200
RMSprop, Adam, there are actually other ones.

01:19:35.200 --> 01:19:37.675
There's Adam Max and whole lot of them.

01:19:37.675 --> 01:19:43.395
I mean, Adam is one fairly reliable one that many people use and that's not bad.

01:19:43.395 --> 01:19:45.685
And then one more slide and I'm done.

01:19:45.685 --> 01:19:47.555
Yes, so learning rates.

01:19:47.555 --> 01:19:51.420
So, normally you have to choose a learning rate.

01:19:51.420 --> 01:19:54.515
So, one choice is just have a constant learning rate.

01:19:54.515 --> 01:19:59.500
You pick a number, may be 10 to the minus three and say that's my learning rate.

01:19:59.500 --> 01:20:04.165
You want your learning rate to be order of magnitude, right.

01:20:04.165 --> 01:20:07.670
If your learning rate is too big,

01:20:07.670 --> 01:20:12.990
your model might diverge or not converge because it just sort of leaps you around by

01:20:12.990 --> 01:20:19.915
huge cram movements and you completely miss the good parts of your function space.

01:20:19.915 --> 01:20:22.925
If your model, if your learning rate is too small,

01:20:22.925 --> 01:20:28.600
your model may not train by the assignment deadline and then you'll be unhappy.

01:20:28.600 --> 01:20:30.675
So, you saw that, you know,

01:20:30.675 --> 01:20:35.730
commonly people sort of try powers of 10 and sees how it looks, right.

01:20:35.730 --> 01:20:39.445
They might try, you know, 0.01, 0.001,

01:20:39.445 --> 01:20:45.680
0.0001 and see, look at how the loss is declining and see what seems to work.

01:20:45.680 --> 01:20:46.690
In general, you want to use

01:20:46.690 --> 01:20:50.960
the fastest learning rate that isn't making things become unstable.

01:20:50.960 --> 01:20:58.000
Commonly, you could get better results by decreasing the learning rate as you train.

01:20:58.000 --> 01:21:00.660
So, sometimes people just do that by hand.

01:21:00.660 --> 01:21:03.190
So, we use the term epoch for a full pass

01:21:03.190 --> 01:21:05.895
through your training data and people might say,

01:21:05.895 --> 01:21:08.520
half the learning rate after every three epochs

01:21:08.520 --> 01:21:11.215
as you train and that can work pretty well.

01:21:11.215 --> 01:21:16.385
You can use formulas to get per epoch tra- learning rates.

01:21:16.385 --> 01:21:18.550
There are even fancier methods.

01:21:18.550 --> 01:21:22.075
You can look up cyclic learning rates online if you want,

01:21:22.075 --> 01:21:24.350
which sort of actually makes the learning rates

01:21:24.350 --> 01:21:26.770
sometimes bigger and then sometimes smaller,

01:21:26.770 --> 01:21:29.460
and people have found that that can be useful for getting you out

01:21:29.460 --> 01:21:32.440
of bad regions in interesting ways.

01:21:32.440 --> 01:21:35.820
The one other thing to know is,

01:21:35.820 --> 01:21:38.775
if you're using one of the fancier optimizers,

01:21:38.775 --> 01:21:43.160
they still ask you for a learning rate but that learning rate is

01:21:43.160 --> 01:21:49.790
the initial learning rate which typically the optimizer will shrink as you train.

01:21:49.790 --> 01:21:53.690
So, commonly if you're using something like Adam,

01:21:53.690 --> 01:21:58.740
you might be starting off by saying the learning rate is 0.1,

01:21:58.740 --> 01:22:03.945
so of a bigger number and it will be shrinking it later as the training goes along.

01:22:03.945 --> 01:22:08.480
Okay, all done. See you next week.

