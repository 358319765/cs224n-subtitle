WEBVTT
Kind: captions
Language: en

00:00:05.210 --> 00:00:07.950
Hi, everyone. I'm Abby,

00:00:07.950 --> 00:00:09.544
I'm the head TA for this class

00:00:09.544 --> 00:00:12.510
and I'm also a PhD student in the Stanford NLP group.

00:00:12.510 --> 00:00:14.670
And today I'm gonna be telling you about

00:00:14.670 --> 00:00:17.040
language models and recurrent neural networks.

00:00:17.040 --> 00:00:19.980
So, here's an overview of what we're gonna do today.

00:00:19.980 --> 00:00:24.885
Today, first, we're going to introduce a new NLP task, that's language modelling,

00:00:24.885 --> 00:00:29.504
and that's going to motivate us to learn about a new family of neural networks,

00:00:29.504 --> 00:00:32.730
that is recurrent neural networks or RNNs.

00:00:32.730 --> 00:00:34.440
So, I'd say that these are two of

00:00:34.440 --> 00:00:37.415
the most important ideas you're going to learn for the rest of the course.

00:00:37.415 --> 00:00:41.070
So, we're going to be covering some fairly cool material today.

00:00:41.070 --> 00:00:44.305
So, let's start off with language modeling.

00:00:44.305 --> 00:00:48.775
Language modeling is the task of predicting what word comes next.

00:00:48.775 --> 00:00:52.230
So, given this piece of text the students opens their blank,

00:00:52.230 --> 00:00:56.200
could anyone shout out a word which you think might be coming next?

00:00:58.370 --> 00:00:59.550
Purpose. [NOISE].

00:00:59.550 --> 00:01:03.840
[OVERLAPPING] Mind, what else? I didn't quite hear them,

00:01:03.840 --> 00:01:06.720
but, uh, yeah, these are all likely things, right?

00:01:06.720 --> 00:01:08.150
So, these are some things which I thought,

00:01:08.150 --> 00:01:09.490
students might be opening, uh,

00:01:09.490 --> 00:01:11.200
students open their books, seems likely.

00:01:11.200 --> 00:01:13.280
Uh, students open their laptops,

00:01:13.280 --> 00:01:15.040
students open their exams,

00:01:15.040 --> 00:01:16.580
Students open their minds, incredibly,

00:01:16.580 --> 00:01:18.700
someone came up with one, that one just now,

00:01:18.700 --> 00:01:20.330
uh, it's kind of a metaphorical meaning of opening.

00:01:20.330 --> 00:01:23.675
So, you are all performing language modeling right now.

00:01:23.675 --> 00:01:25.475
And thinking about what word comes next,

00:01:25.475 --> 00:01:27.250
you are being a language model.

00:01:27.250 --> 00:01:31.385
So, here's a more formal definition of what a language model is.

00:01:31.385 --> 00:01:34.430
Given a sequence of words X1 up to Xt,

00:01:34.430 --> 00:01:37.340
a language model, is something that computes

00:01:37.340 --> 00:01:41.200
the probability distribution of the next word, Xt plus 1.

00:01:41.200 --> 00:01:44.194
So, a language model comes up with the probability distribution,

00:01:44.194 --> 00:01:49.067
the conditional probability, of what X t plus 1 is given the words it found.

00:01:49.067 --> 00:01:50.810
And here we're assuming that, Xt plus 1

00:01:50.810 --> 00:01:53.960
can be any word w from a fixed vocabulary V.

00:01:53.960 --> 00:01:55.520
So we are assuming that there is

00:01:55.520 --> 00:01:58.205
a pre-defined list of words that we're considering.

00:01:58.205 --> 00:02:00.140
In this way, you can view language modeling

00:02:00.140 --> 00:02:01.850
as a type of classification task,

00:02:01.850 --> 00:02:04.580
because there's a predefined number of possibilities.

00:02:04.580 --> 00:02:09.360
Um, we call a system that does this a language model.

00:02:09.850 --> 00:02:12.050
There's an alternative way of thinking

00:02:12.050 --> 00:02:13.715
about a language model as well.

00:02:13.715 --> 00:02:15.200
You can think of a language model

00:02:15.200 --> 00:02:19.060
as a system which assigns probability to a piece of text.

00:02:19.060 --> 00:02:21.470
So, for example, if we have some piece of text,

00:02:21.470 --> 00:02:23.180
X up to X capital T,

00:02:23.180 --> 00:02:25.040
then, the probability of this text

00:02:25.040 --> 00:02:27.830
according to the language model can be broken down.

00:02:27.830 --> 00:02:29.250
So, just by definition,

00:02:29.250 --> 00:02:31.250
you can say that the probability is equal to,

00:02:31.250 --> 00:02:34.530
the product of all of these conditional probabilities.

00:02:34.530 --> 00:02:37.375
And, uh, the form inside,

00:02:37.375 --> 00:02:40.480
the products is exactly what a language model provides.

00:02:40.480 --> 00:02:42.465
So, you can think of these things as somewhat equivalent.

00:02:42.465 --> 00:02:44.720
Predicting next words, gives you a system,

00:02:44.720 --> 00:02:49.110
that can give the probability of a given piece of text.

00:02:49.270 --> 00:02:52.745
So, in fact, you, use language models every day.

00:02:52.745 --> 00:02:56.120
For example, when you're texting on your phone and you're writing a message,

00:02:56.120 --> 00:02:57.610
then most likely if you have a smartphone,

00:02:57.610 --> 00:03:00.080
it will be predicting what word you might be about to say.

00:03:00.080 --> 00:03:01.790
So, if you say, um, I'll meet you at the-

00:03:01.790 --> 00:03:04.250
your phone might suggest perhaps you mean airport or cafe,

00:03:04.250 --> 00:03:06.025
or office, for example.

00:03:06.025 --> 00:03:08.905
Another situation which you use language models every day

00:03:08.905 --> 00:03:11.495
is when you search for something on the internet, for example, Google,

00:03:11.495 --> 00:03:12.830
and you start typing your query,

00:03:12.830 --> 00:03:15.952
then Google tries to complete your query for you, and that's language modeling.

00:03:15.952 --> 00:03:19.020
It's predicting what word or words might come next.

00:03:20.480 --> 00:03:23.715
So, that's what a language model is,

00:03:23.715 --> 00:03:26.675
and the question is, how would you learn a language model?

00:03:26.675 --> 00:03:29.917
So, if I was to ask that question in the pre- deep learning era,

00:03:29.917 --> 00:03:31.730
which was really only a few years ago,

00:03:31.730 --> 00:03:35.005
the answer would be, you would learn a n-gram language model.

00:03:35.005 --> 00:03:38.570
So, today first we're going to learn about n-gram language models.

00:03:38.570 --> 00:03:41.330
So, before I can tell you what a n-gram language model is,

00:03:41.330 --> 00:03:43.160
you need to know what an n-gram is.

00:03:43.160 --> 00:03:47.905
So, by definition an n-gram is a chunk of n  consecutive words.

00:03:47.905 --> 00:03:50.400
So, for example, a one gram or unigram,

00:03:50.400 --> 00:03:52.050
is just all of the individual words

00:03:52.050 --> 00:03:55.020
in the sequence that would be "the students open the-"

00:03:55.020 --> 00:03:58.810
A two gram or bigram would be all of the consecutive chunks of pairs of words,

00:03:58.810 --> 00:04:00.980
"the students", "students opened", "opened their"

00:04:00.980 --> 00:04:04.560
and so on for trigrams and four-grams, etc.

00:04:05.050 --> 00:04:08.575
So, the core idea of an n-gram language model

00:04:08.575 --> 00:04:11.035
is that in order to predict what word comes next,

00:04:11.035 --> 00:04:12.815
you're going to collect a bunch of statistics,

00:04:12.815 --> 00:04:14.930
about how frequent different n-grams are,

00:04:14.930 --> 00:04:16.490
from some kind of training data,

00:04:16.490 --> 00:04:18.110
and then you can use those statistics

00:04:18.110 --> 00:04:21.120
to predict what next words might be likely.

00:04:21.830 --> 00:04:23.640
Here is some more detail.

00:04:23.640 --> 00:04:26.325
So, to make an n-gram language model,

00:04:26.325 --> 00:04:28.490
first you need to make a simplifying assumption,

00:04:28.490 --> 00:04:30.305
and this your assumption.

00:04:30.305 --> 00:04:33.350
You say that the next word Xt plus 1

00:04:33.350 --> 00:04:37.535
depends only on the preceding N-1 words.

00:04:37.535 --> 00:04:39.900
So, what we're assuming,

00:04:39.900 --> 00:04:41.650
is that the probability distribution,

00:04:41.650 --> 00:04:45.020
the conditional probability of Xt plus 1 given all of the words they follow,

00:04:45.020 --> 00:04:46.160
we're just going to simplify that,

00:04:46.160 --> 00:04:50.485
and say it only depends on the last N-1 words,  and that's our assumption.

00:04:50.485 --> 00:04:53.950
So, by the definition of conditional probability,

00:04:53.950 --> 00:04:55.600
we can say that this probability,

00:04:55.600 --> 00:04:58.385
is just the ratio of two different probabilities.

00:04:58.385 --> 00:05:01.180
So, on the top, you've got the probability of

00:05:01.180 --> 00:05:03.220
a particular n-gram and on the bottom we've

00:05:03.220 --> 00:05:06.192
got the probability of a particular N-1 gram

00:05:06.192 --> 00:05:08.020
This is a little hard to read because of all the superscripts

00:05:08.020 --> 00:05:11.015
but I'm gonna give an example with words on the next slide.

00:05:11.015 --> 00:05:15.055
Okay. So, that's the definition of the probability of the next word,

00:05:15.055 --> 00:05:17.140
but the question remains, how do we get all of

00:05:17.140 --> 00:05:19.980
these n-gram and N-1 gram probabilities?

00:05:19.980 --> 00:05:22.300
So, the answer is, we're going to get them by

00:05:22.300 --> 00:05:25.050
counting them in some large corpus of text.

00:05:25.050 --> 00:05:26.510
So, we're going to approximate,

00:05:26.510 --> 00:05:29.560
these probabilities just by the count of the number of times that

00:05:29.560 --> 00:05:34.190
these particular n-grams and N-1 grams appeared in our training corpus.

00:05:34.410 --> 00:05:37.370
Okay. So, here's an example with some words.

00:05:37.370 --> 00:05:40.565
Suppose we are trying to learn a 4-gram language model,

00:05:40.565 --> 00:05:42.830
and suppose that we have a piece of text, that says,

00:05:42.830 --> 00:05:44.540
"As the proctor started the clock,

00:05:44.540 --> 00:05:46.100
the students opened their blank",

00:05:46.100 --> 00:05:48.895
and we're trying to predict what word is coming next.

00:05:48.895 --> 00:05:51.740
So, because we're learning a 4-gram language model,

00:05:51.740 --> 00:05:55.910
a simplifying assumption is that the next word depends only on the last three words,

00:05:55.910 --> 00:05:57.605
last N-1 words.

00:05:57.605 --> 00:06:01.520
So, we're going to discard all of the context so far except for the last few words,

00:06:01.520 --> 00:06:03.780
which is, "Students opened their."

00:06:03.800 --> 00:06:07.620
So, as a reminder, n-gram language model says that,

00:06:07.620 --> 00:06:08.940
the probability of the next word being,

00:06:08.940 --> 00:06:13.230
some particular word W in the vocabulary is equal to the number of times we saw

00:06:13.230 --> 00:06:15.510
students opened their W divided by the number of

00:06:15.510 --> 00:06:18.655
times we saw students opened their,  in the training corpus.

00:06:18.655 --> 00:06:21.440
So, let's suppose that in our training corpus,

00:06:21.440 --> 00:06:24.215
we saw the phrase "students open their" 1,000 times.

00:06:24.215 --> 00:06:28.340
And suppose that, we saw "students opened their books" 400 times.

00:06:28.340 --> 00:06:32.220
This means that the probability of the next word being books is 0.4.

00:06:32.220 --> 00:06:36.810
And uh, similarly, let's suppose that we saw students open their exams 100 times,

00:06:36.810 --> 00:06:39.260
this means that the probability of exams given students

00:06:39.260 --> 00:06:41.930
open their is 0.1. Is there a question?

00:06:41.930 --> 00:06:44.900
[inaudible].

00:06:44.900 --> 00:06:47.010
The question is, does the order of the words matter?

00:06:47.010 --> 00:06:50.340
And the answer is yes, the order of students open there does matter.

00:06:50.340 --> 00:06:53.190
It's different to "the students opened."

00:06:53.190 --> 00:06:56.985
So, the question I want to raise now is,

00:06:56.985 --> 00:07:00.805
was it a good idea for us to discard the proctor context?

00:07:00.805 --> 00:07:03.115
If you look at the actual example that we had,

00:07:03.115 --> 00:07:06.070
the example was as the proctor started the clock,

00:07:06.070 --> 00:07:07.850
the students opened their blank.

00:07:07.850 --> 00:07:12.360
So, do we think that books or exams is more likely given the actual context,

00:07:12.360 --> 00:07:14.550
the full context? Yep.

00:07:14.550 --> 00:07:15.450
Exams.

00:07:15.450 --> 00:07:17.795
Right. Exams is more likely because the proctor and

00:07:17.795 --> 00:07:20.260
the clock heavily implies that it's an exam scenario, so

00:07:20.260 --> 00:07:22.625
they're more likely to be opening the exams than the books,

00:07:22.625 --> 00:07:24.400
unless it's an open book exam.

00:07:24.400 --> 00:07:26.830
Uh, but I think, overall, it should be exams.

00:07:26.830 --> 00:07:29.890
So, the problem that we're seeing here is that in the training corpus,

00:07:29.890 --> 00:07:31.240
the fact that students were opening

00:07:31.240 --> 00:07:33.990
something means that it's more likely to be books than exams

00:07:33.990 --> 00:07:36.305
because overall, books are more common than exams.

00:07:36.305 --> 00:07:38.565
But if we know that the context is,

00:07:38.565 --> 00:07:41.078
the proctor and the clock, then it should be exams.

00:07:41.078 --> 00:07:44.240
So, what I'm highlighting here is a problem with our simplifying assumption.

00:07:44.240 --> 00:07:45.860
If we throw away too much context,

00:07:45.860 --> 00:07:50.455
then we are not as good as predicting the words as we would be if we kept the context.

00:07:50.455 --> 00:07:54.690
Okay. So, that's one problem with n-gram, uh, language models.

00:07:54.690 --> 00:07:56.810
Uh, there are some other problems as well.

00:07:56.810 --> 00:08:00.470
So, uh, here again is the equation that you saw before.

00:08:00.470 --> 00:08:01.880
One problem which we're gonna call

00:08:01.880 --> 00:08:05.465
the sparsity problem is what happens if the number on top,

00:08:05.465 --> 00:08:08.380
the numerator, what if that count is equal to zero.

00:08:08.380 --> 00:08:11.210
So, what if for some particular word W,

00:08:11.210 --> 00:08:14.450
the phrase students opened their W never occurred in the data.

00:08:14.450 --> 00:08:17.240
So, for example, let's suppose students opened their petri dishes,

00:08:17.240 --> 00:08:19.880
is fairly uncommon and it never appears in the data,

00:08:19.880 --> 00:08:24.355
then that means our probability of the next word being petri dishes will be zero.

00:08:24.355 --> 00:08:27.390
And this is bad, because it might be uncommon but it is,

00:08:27.390 --> 00:08:29.385
a valid scenario, right?

00:08:29.385 --> 00:08:31.090
If you're a biology student for example.

00:08:31.090 --> 00:08:34.085
So, this is a problem and we call it the sparsity problem,

00:08:34.085 --> 00:08:37.790
because the problem is that if we'd never seen an event happen in the training data,

00:08:37.790 --> 00:08:41.485
then our model assigns zero probability to that event.

00:08:41.485 --> 00:08:46.415
So, one partial solution to this problem is that maybe we should add a small delta,

00:08:46.415 --> 00:08:48.290
small number delta to the count,

00:08:48.290 --> 00:08:50.420
for every word in the vocabulary.

00:08:50.420 --> 00:08:53.920
And then this way, every possible word that come next,

00:08:53.920 --> 00:08:56.250
has at least some small probability.

00:08:56.250 --> 00:08:59.089
So, petri dishes will have some small probability,

00:08:59.089 --> 00:09:02.410
but then so, will all of the other words which are possibly bad choices.

00:09:02.410 --> 00:09:05.580
So, this, uh, technique is called smoothing, because the idea is,

00:09:05.580 --> 00:09:06.945
you're going from a very, uh,

00:09:06.945 --> 00:09:10.050
sparse probability distribution, which is zero, almost everywhere,

00:09:10.050 --> 00:09:11.550
with a few spikes where there's,

00:09:11.550 --> 00:09:13.445
uh, being n-grams that we've seen,

00:09:13.445 --> 00:09:16.100
it goes from that to being a more smooth probability distribution

00:09:16.100 --> 00:09:19.615
where everything has at least a small probability on it.

00:09:19.615 --> 00:09:24.270
So, the second sparsity problem which is possibly worse than the first one is,

00:09:24.270 --> 00:09:28.130
what happens if the number in the denominator is zero?

00:09:28.130 --> 00:09:30.200
So, in our example, that would mean,

00:09:30.200 --> 00:09:34.655
what if we never even saw the trigram "students opened their" in the training data.

00:09:34.655 --> 00:09:38.480
If that happens, then we can't even calculate this probability distribution at

00:09:38.480 --> 00:09:42.820
all for any word W because we never even saw this context before.

00:09:42.820 --> 00:09:45.825
So, a possible solution to this is that

00:09:45.825 --> 00:09:48.450
if you can't find "students open their" in the corpus,

00:09:48.450 --> 00:09:51.940
then you should back off to just conditioning on the last two words,

00:09:51.940 --> 00:09:53.545
rather than the last three words.

00:09:53.545 --> 00:09:55.900
So, now you'd be looking at times when you'd seen,

00:09:55.900 --> 00:09:58.460
uh, "open their" and seeing what what's come next.

00:09:58.460 --> 00:10:01.350
So, this is called back-off because in this failure case,

00:10:01.350 --> 00:10:04.025
for when you have no data for your 4-gram language model,

00:10:04.025 --> 00:10:06.020
you're backing off to a trigram language model.

00:10:06.020 --> 00:10:09.510
Are there any questions at this point?

00:10:12.310 --> 00:10:17.570
Okay. So, um, another thing to note is that these sparsity problems

00:10:17.570 --> 00:10:22.100
get worse if you increase N. If you make N larger in your n-gram language model,

00:10:22.100 --> 00:10:23.870
and you might want to do this, for example,

00:10:23.870 --> 00:10:26.390
you might think, uh, I want to have a larger context,

00:10:26.390 --> 00:10:28.565
so I can pay attention to words that

00:10:28.565 --> 00:10:30.890
happened longer ago and that's gonna make it a better predictor.

00:10:30.890 --> 00:10:33.275
So, you might think making N bigger is a good idea.

00:10:33.275 --> 00:10:36.410
But the problem is if you do that then the sparsity problems get worse.

00:10:36.410 --> 00:10:37.700
Because, let's suppose you say,

00:10:37.700 --> 00:10:39.215
I want a 10-gram language model.

00:10:39.215 --> 00:10:40.910
Then the problem is that you're going to be counting,

00:10:40.910 --> 00:10:43.480
how often you seen process in 9-grams and 10-grams.

00:10:43.480 --> 00:10:45.485
But 9-grams and 10-grams, there's so many of them,

00:10:45.485 --> 00:10:47.615
that the one you are interested in probably never occurred,

00:10:47.615 --> 00:10:51.155
in your training data which means that the whole thing becomes dysfunctional.

00:10:51.155 --> 00:10:55.680
So, in practice, we usually can't have N much bigger than five.

00:10:56.170 --> 00:10:58.490
Okay. So, that was, uh,

00:10:58.490 --> 00:11:00.875
two sparsity problems with n-gram language models.

00:11:00.875 --> 00:11:02.770
Here is a problem with storage.

00:11:02.770 --> 00:11:04.710
So, if we look at this equation, uh,

00:11:04.710 --> 00:11:06.780
you have to think about what do you need to

00:11:06.780 --> 00:11:09.365
store in order to use your n-gram language model.

00:11:09.365 --> 00:11:12.020
You need to store this count number,

00:11:12.020 --> 00:11:14.090
for all of the n-grams that you observed in

00:11:14.090 --> 00:11:17.215
the corpus when you were going through the training corpus counting them.

00:11:17.215 --> 00:11:19.440
And the problem is, that as you increase N,

00:11:19.440 --> 00:11:23.480
then this number of n-grams that you have to store and count increases.

00:11:23.480 --> 00:11:27.515
So, another problem with increasing N is that the size of your model,

00:11:27.515 --> 00:11:30.750
or your n-gram model, uh, gets bigger.

00:11:31.490 --> 00:11:37.215
Okay, so n-gram Language Models in practice. Let's look at an example.

00:11:37.215 --> 00:11:42.540
You can actually build a simple trigram Language Model over a 1.7 million word corpus,

00:11:42.540 --> 00:11:44.325
uh, in a few seconds on your laptop.

00:11:44.325 --> 00:11:46.140
And in fact, the corpus that I used to do this

00:11:46.140 --> 00:11:47.970
was the same one that you met in assignment one.

00:11:47.970 --> 00:11:49.605
It's Reuters' corpus which is,

00:11:49.605 --> 00:11:51.180
uh, business and financial news.

00:11:51.180 --> 00:11:52.380
So, if you want to do this yourself,

00:11:52.380 --> 00:11:55.005
you can follow that link at the bottom of the slide later.

00:11:55.005 --> 00:11:57.000
So, uh, this is, uh,

00:11:57.000 --> 00:11:59.280
something which I ran on my laptop in a few second.

00:11:59.280 --> 00:12:02.790
So I gave it the context of the bigram today the,

00:12:02.790 --> 00:12:06.480
and then I asked the trigram Language Model what word is likely to come next.

00:12:06.480 --> 00:12:09.855
So, the Language Model said that the top next most likely words are

00:12:09.855 --> 00:12:13.455
company, bank, price, Italian, emirate, et cetera.

00:12:13.455 --> 00:12:17.640
So already just looking at these probabilities that are assigned to these different words,

00:12:17.640 --> 00:12:19.590
uh, you can see that there is a sparsity problem.

00:12:19.590 --> 00:12:21.840
For example, the top two most likely words have

00:12:21.840 --> 00:12:24.720
the exact same probability and the reason for that is,

00:12:24.720 --> 00:12:26.760
that this number is 4 over 26.

00:12:26.760 --> 00:12:28.800
So these are quite small integers, uh,

00:12:28.800 --> 00:12:30.270
meaning that we only saw, uh,

00:12:30.270 --> 00:12:33.000
today the company and today the bank four times each.

00:12:33.000 --> 00:12:34.560
So, uh, this is an example of

00:12:34.560 --> 00:12:37.290
the sparsity problem because overall these are quite low counts,

00:12:37.290 --> 00:12:39.165
we haven't seen that many different, uh,

00:12:39.165 --> 00:12:40.500
versions of this event,

00:12:40.500 --> 00:12:43.885
so we don't have a very granular probability distribution.

00:12:43.885 --> 00:12:46.385
But in any case ignoring the sparsity problem,

00:12:46.385 --> 00:12:47.765
I would say that overall,

00:12:47.765 --> 00:12:50.640
these, uh, top suggestions look pretty reasonable.

00:12:52.600 --> 00:12:55.670
So you can actually use a Language Model to

00:12:55.670 --> 00:12:58.305
generate text and this is how you would do it.

00:12:58.305 --> 00:13:00.735
So let's suppose you have your first two words already, uh,

00:13:00.735 --> 00:13:04.560
you condition on this and you ask your Language Model what's likely to come next.

00:13:04.560 --> 00:13:07.305
So then given this probability distribution over the words,

00:13:07.305 --> 00:13:08.850
you can sample from it, that is,

00:13:08.850 --> 00:13:11.865
select some words with, you know, the associated probability.

00:13:11.865 --> 00:13:14.235
So let's suppose that gives us the word price.

00:13:14.235 --> 00:13:17.730
So then price is your next word, and then you just condition on the last two words,

00:13:17.730 --> 00:13:20.385
which in this ex- example is now the price.

00:13:20.385 --> 00:13:23.790
So now you get a new probability distribution and you can continue this process,

00:13:23.790 --> 00:13:27.960
uh, sampling and then conditioning again and sampling.

00:13:27.960 --> 00:13:30.150
So if you do this long enough,

00:13:30.150 --> 00:13:31.350
you will get a piece of text,

00:13:31.350 --> 00:13:33.690
so this is the actual text that I got when

00:13:33.690 --> 00:13:37.005
I run this generation process with this trigram Language Model.

00:13:37.005 --> 00:13:40.260
So it says, "Today the price of gold per ton,

00:13:40.260 --> 00:13:43.260
while production of shoe lasts and shoe industry,

00:13:43.260 --> 00:13:46.230
the bank intervened just after it considered and rejected

00:13:46.230 --> 00:13:49.365
an IMF demand to rebuild depleted European stocks,

00:13:49.365 --> 00:13:52.810
September, 30th end primary 76 counts a share.''

00:13:52.810 --> 00:13:55.250
Okay. So, uh, what do we think about this text?

00:13:55.250 --> 00:13:59.195
We think it's good? We, uh, surprised?

00:13:59.195 --> 00:14:02.370
Um, I would say that in some ways it is good,

00:14:02.370 --> 00:14:04.620
it's kind of surprisingly grammatical, you know,

00:14:04.620 --> 00:14:07.860
it mostly, uh, kind of pauses,

00:14:07.860 --> 00:14:09.150
uh, but you would definitely say that it,

00:14:09.150 --> 00:14:10.500
it doesn't really make any sense.

00:14:10.500 --> 00:14:12.180
It's pretty incoherent.

00:14:12.180 --> 00:14:14.580
And we shouldn't be surprised that it's incoherent I

00:14:14.580 --> 00:14:17.715
think because if you remember this is a trigram Language Model,

00:14:17.715 --> 00:14:20.265
it has a memory of just the last well,

00:14:20.265 --> 00:14:22.635
three or two words depending on how you look at it.

00:14:22.635 --> 00:14:24.510
So clearly we need to consider

00:14:24.510 --> 00:14:27.990
more than three words at a time if we want to model language well.

00:14:27.990 --> 00:14:32.265
But as we already know, increasing n makes the sparsity problem worse,

00:14:32.265 --> 00:14:38.370
n-gram Language Models, and it also increases model size. Is that a question?

00:14:38.370 --> 00:14:40.320
How does it [inaudible] [NOISE]

00:14:40.320 --> 00:14:43.380
So the question is, how does the n-gram Language Model know when to put commas.

00:14:43.380 --> 00:14:45.150
Uh, so you can,

00:14:45.150 --> 00:14:50.400
[NOISE] decide that commas and other punctuation are just another kind of word,

00:14:50.400 --> 00:14:51.705
is that well or token,

00:14:51.705 --> 00:14:54.510
and then, to the Language Model it doesn't really make much difference.

00:14:54.510 --> 00:14:57.705
It's just used that as another possible world that can be, um, predicted,

00:14:57.705 --> 00:14:59.445
that's why we've got the weird spacing around the,

00:14:59.445 --> 00:15:01.770
the commas is because it was essentially viewed as a separate word.

00:15:01.770 --> 00:15:06.135
[NOISE] Okay.

00:15:06.135 --> 00:15:09.195
So this course is called NLP with Deep Learning.

00:15:09.195 --> 00:15:12.765
So you probably thinking how do we build a neural Language Model?

00:15:12.765 --> 00:15:15.450
So let's just recap, uh, in case you forgot.

00:15:15.450 --> 00:15:17.940
Remember that a Language Model is something that takes

00:15:17.940 --> 00:15:20.760
inputs which is a sequence of words X1 up to Xt,

00:15:20.760 --> 00:15:26.290
and then it outputs a probability distribution of what the next word might be Xt plus 1.

00:15:27.470 --> 00:15:32.070
Okay, so when we think about what kind of neural models we've met in this course so far.

00:15:32.070 --> 00:15:34.545
Uh, we've already met window-based neural models.

00:15:34.545 --> 00:15:36.780
And in lecture three, we saw how you could apply

00:15:36.780 --> 00:15:40.035
a window-based neural model to a named entity recognition.

00:15:40.035 --> 00:15:43.050
So in that scenario you take some kind of window around the word that you

00:15:43.050 --> 00:15:46.125
care about which in this example is Paris, and then, uh,

00:15:46.125 --> 00:15:48.780
you get the word embeddings for those, concatenate them put them through

00:15:48.780 --> 00:15:52.890
some layers, and then you get your decision which is that Paris is a location not,

00:15:52.890 --> 00:15:55.425
you know, a person or organization.

00:15:55.425 --> 00:15:57.900
So that's a recap of what we saw in lecture three.

00:15:57.900 --> 00:16:03.795
How would we apply a model like this to language modeling? So here's how you would do it.

00:16:03.795 --> 00:16:06.930
Here's an example of a fixed-window neural language model.

00:16:06.930 --> 00:16:09.420
So, again, we have some kind of context

00:16:09.420 --> 00:16:12.060
which is, as the proctor started the clock the students opened their,

00:16:12.060 --> 00:16:15.225
um, we're trying to guess what word might come next.

00:16:15.225 --> 00:16:18.450
So we have to make a similar simplifying assumption to before.

00:16:18.450 --> 00:16:21.255
Uh, because it's a fixed size window, uh,

00:16:21.255 --> 00:16:25.500
we have to discard the context except for the window that we're conditioning on.

00:16:25.500 --> 00:16:29.070
So let's suppose that our fixed window is of size four.

00:16:29.070 --> 00:16:34.390
So what we'll do is similarly to the, ah, NER model.

00:16:34.390 --> 00:16:38.400
We're going to represent these words with one-hot vectors,

00:16:38.400 --> 00:16:42.745
and then we'll use those to look up the word embeddings for these words using the,

00:16:42.745 --> 00:16:44.895
uh, embedding lookup matrix.

00:16:44.895 --> 00:16:48.075
So then we get all of our word embeddings E,1, 2, 3, 4,

00:16:48.075 --> 00:16:51.270
and then we concatenate them together to get e. We put this through

00:16:51.270 --> 00:16:55.215
a linear layer and a nonlinearity function f to get some kind of hidden layer,

00:16:55.215 --> 00:16:57.720
and then we put it through another linear layer and

00:16:57.720 --> 00:17:01.860
the softmax function and now we have an output probability distribution y hat.

00:17:01.860 --> 00:17:05.925
And in our case because we're trying to predict what word comes next, ah, ah,

00:17:05.925 --> 00:17:08.430
vector y hat will be of length v where v is

00:17:08.430 --> 00:17:10.020
the vocabulary and it will contain

00:17:10.020 --> 00:17:12.555
the probabilities of all the different words in the vocabulary.

00:17:12.555 --> 00:17:15.600
So here I've represented that as a bar charts where if you suppose

00:17:15.600 --> 00:17:18.690
you've got all of the words listed alphabetically from a to z,

00:17:18.690 --> 00:17:21.300
and then there's the different probabilities of the words.

00:17:21.300 --> 00:17:22.845
So if everything goes well,

00:17:22.845 --> 00:17:24.480
then this language model should tell us that

00:17:24.480 --> 00:17:27.930
some likely next words are books and laptops, for example.

00:17:27.930 --> 00:17:29.940
So none of this should be, um,

00:17:29.940 --> 00:17:31.770
unfamiliar to you because you saw it all last week.

00:17:31.770 --> 00:17:36.100
We're just applying a Window-based model to a different task,  such as language modeling.

00:17:36.470 --> 00:17:38.940
Okay, so what are,

00:17:38.940 --> 00:17:42.240
some good things about this model compared to n-gram language models?

00:17:42.240 --> 00:17:46.305
So one, ah, advantage I'd say is that there's no sparsity problem.

00:17:46.305 --> 00:17:49.695
If you remember an n-gram language model has a sparsity problem

00:17:49.695 --> 00:17:53.205
which is that if you've never seen a particular n-gram in training then,

00:17:53.205 --> 00:17:55.005
you can't assign any probability to it.

00:17:55.005 --> 00:17:56.445
You don't have any data on it.

00:17:56.445 --> 00:17:59.340
Whereas at least here you can take any, you know, for example,

00:17:59.340 --> 00:18:02.115
4-gram you want and you can feed it into the, ah,

00:18:02.115 --> 00:18:03.795
the neural nets and it will give you

00:18:03.795 --> 00:18:06.150
an output distribution of what it thinks the next word would be.

00:18:06.150 --> 00:18:10.245
It might not be a good prediction but at least it will, it will run.

00:18:10.245 --> 00:18:12.930
Another advantage is you don't need to store

00:18:12.930 --> 00:18:15.090
all of the observed n-grams that you ever saw.

00:18:15.090 --> 00:18:17.280
So, uh, this an advantage by, uh,

00:18:17.280 --> 00:18:19.230
comparison you just have to store

00:18:19.230 --> 00:18:22.155
all of the word vectors for all the words in your vocabulary.

00:18:22.155 --> 00:18:26.085
Uh, but there are quite a lot of problems with this fixed-window language model.

00:18:26.085 --> 00:18:29.160
So here are some remaining problems: Uh,

00:18:29.160 --> 00:18:31.470
one is that your fixed window is probably too small.

00:18:31.470 --> 00:18:33.885
No matter how big you make your fixed window, uh,

00:18:33.885 --> 00:18:35.640
you're probably going to be losing some kind of

00:18:35.640 --> 00:18:38.490
useful context that you would want to use sometimes.

00:18:38.490 --> 00:18:41.745
And in fact, if you try to enlarge the window size,

00:18:41.745 --> 00:18:44.175
then you also have to enlarge the size of your,

00:18:44.175 --> 00:18:45.480
uh, weight factor, sorry,

00:18:45.480 --> 00:18:47.580
your weight matrix W. Uh,

00:18:47.580 --> 00:18:49.590
so the width of W because you're multiplying it

00:18:49.590 --> 00:18:52.110
by e which is the concatenation of your word embeddings.

00:18:52.110 --> 00:18:56.230
The width of W grows as you increase the size of your window.

00:18:56.390 --> 00:19:01.210
So in inclusion really your window can never be large enough.

00:19:01.280 --> 00:19:05.460
Another problem with this model which is more of a subtle point is that

00:19:05.460 --> 00:19:08.820
X1 and X2 and really all of the words in the window they're,

00:19:08.820 --> 00:19:11.100
uh, multiplied by completely diffe rent weights in

00:19:11.100 --> 00:19:14.565
W. So to demonstrate this you could draw a picture.

00:19:14.565 --> 00:19:17.610
So the problem is that if you have

00:19:17.610 --> 00:19:21.720
your weight matrix W and then you have

00:19:21.720 --> 00:19:26.910
your concatenation of embeddings e and we have, uh, four embeddings.

00:19:26.910 --> 00:19:30.390
So we have e_1, e_2, e_3,

00:19:30.390 --> 00:19:33.135
e_4, and you multiply, uh,

00:19:33.135 --> 00:19:36.615
the concatenated embeddings by the weight matrix.

00:19:36.615 --> 00:19:39.120
So really you can see that there are essentially

00:19:39.120 --> 00:19:42.449
kind of four sections of the weight matrix,

00:19:42.449 --> 00:19:45.570
and the first word embedding e_1 is only

00:19:45.570 --> 00:19:48.825
ever multiplied by the weights for it in this section,

00:19:48.825 --> 00:19:53.025
and that's completely separate to the weights that multiply by e_2 and so forth.

00:19:53.025 --> 00:19:56.700
So the problem with this is that what you

00:19:56.700 --> 00:20:00.060
learn in the weight matrix in one section is not shared with the others.

00:20:00.060 --> 00:20:03.985
You're kind of learning a lot of similar functions four times.

00:20:03.985 --> 00:20:07.910
So the reason why we think this is a problem is because there should be a lot of

00:20:07.910 --> 00:20:12.130
commonalities in how you process the incoming word embeddings.

00:20:12.130 --> 00:20:14.880
So what you learn about how to process, you know,

00:20:14.880 --> 00:20:18.375
the third embedding, some of it at least should be shared with all of the embeddings.

00:20:18.375 --> 00:20:21.960
So what I'm saying is it's kind of inefficient that we're learning, uh,

00:20:21.960 --> 00:20:24.300
all of these separate weights for these different words

00:20:24.300 --> 00:20:27.970
when there's a lot of commonalities between them. Is there a question?

00:20:29.840 --> 00:20:31.180
So that's why [inaudible] [NOISE].

00:20:31.180 --> 00:20:31.965
Okay-

00:20:31.965 --> 00:20:36.560
Yeah, hopefully- hopefully the verbal description is on.

00:20:38.280 --> 00:20:42.310
So, in conclusion, I'd say that the biggest problem that we've got with

00:20:42.310 --> 00:20:45.280
this fixed-size neural model is that clearly we

00:20:45.280 --> 00:20:48.355
need some kind of neural architecture that can process any length input,

00:20:48.355 --> 00:20:51.070
because most of the problems here come from the fact that we had to make

00:20:51.070 --> 00:20:54.920
this simplifying assumption that there was a fixed window.

00:20:56.670 --> 00:21:00.040
Okay. So this motivates, uh,

00:21:00.040 --> 00:21:02.590
us to introduce this new family of neural architecture,

00:21:02.590 --> 00:21:05.515
it's called recurrent neural networks or RNNs.

00:21:05.515 --> 00:21:09.100
So, this is a simplified diagram that shows you the most important,

00:21:09.100 --> 00:21:11.320
um, features of an RNN.

00:21:11.320 --> 00:21:15.070
So we have again an input sequence of X1, X2,

00:21:15.070 --> 00:21:20.245
et cetera, but you can assume that this sequence is of any arbitrary length you like.

00:21:20.245 --> 00:21:24.460
The idea is that you have a sequence of hidden states instead of just having,

00:21:24.460 --> 00:21:27.175
for example, one hidden state as we did in the previous model.

00:21:27.175 --> 00:21:30.940
We have a sequence of hidden states and we have as many of them as we have inputs.

00:21:30.940 --> 00:21:35.440
And the important thing is that each hidden state ht is computed based

00:21:35.440 --> 00:21:40.315
on the previous hidden state and also the input on that step.

00:21:40.315 --> 00:21:44.050
So the reason why they're called hidden states is because you could think of

00:21:44.050 --> 00:21:47.425
this as a single state that's mutating over time.

00:21:47.425 --> 00:21:50.260
It's kind of like several versions of the same thing.

00:21:50.260 --> 00:21:53.830
And for this reason, we often call these time-steps, right?

00:21:53.830 --> 00:21:55.540
So these steps that go left to right,

00:21:55.540 --> 00:21:57.860
we often call them time-steps.

00:21:58.950 --> 00:22:01.870
So the really important thing is that

00:22:01.870 --> 00:22:07.210
the same weight matrix W is applied on every time-step of this RNN.

00:22:07.210 --> 00:22:11.365
That's what makes us able to process any length input we want.

00:22:11.365 --> 00:22:13.930
Is because we don't have to have different weights on every step,

00:22:13.930 --> 00:22:17.990
because we just apply the exact same transformation on every step.

00:22:18.870 --> 00:22:22.690
So additionally, you can also have some outputs from the RNN.

00:22:22.690 --> 00:22:23.995
So these y hats,

00:22:23.995 --> 00:22:26.155
these are the outputs on each step.

00:22:26.155 --> 00:22:28.735
And they're optional because you don't have to compute them

00:22:28.735 --> 00:22:31.210
or you can compute them on just some steps and not others.

00:22:31.210 --> 00:22:34.160
It depends on where you want to use your RNN to do.

00:22:34.920 --> 00:22:38.260
Okay. So that's a simple diagram of an RNN.

00:22:38.260 --> 00:22:39.850
Uh, here I'm going to give you a bit more detail.

00:22:39.850 --> 00:22:43.630
So here's how you would apply an RNN to do language modeling.

00:22:43.630 --> 00:22:48.175
So, uh, again, let's suppose that we have some kind of text so far.

00:22:48.175 --> 00:22:50.860
My text is only four words long,

00:22:50.860 --> 00:22:53.320
but you can assume that it could be any length, right?

00:22:53.320 --> 00:22:55.420
It's just short because we can't fit more on the slide.

00:22:55.420 --> 00:22:58.390
So you have some sequence of tags, which could be kind of long.

00:22:58.390 --> 00:23:02.020
And again, we're going to represent these by some kind of one-hot vectors and

00:23:02.020 --> 00:23:06.460
use those to look up the word embeddings from our embedding matrix.

00:23:06.460 --> 00:23:10.370
So then to compute the first hidden state H1,

00:23:10.370 --> 00:23:14.300
we need to compute it based on the previous hidden state and the current input.

00:23:14.300 --> 00:23:16.615
We already have the current input, that's E1,

00:23:16.615 --> 00:23:19.570
but the question is where do we get this first hidden state from?

00:23:19.570 --> 00:23:21.160
All right, what comes before H1?

00:23:21.160 --> 00:23:24.670
So we often call the initial hidden state H0, uh, yes,

00:23:24.670 --> 00:23:28.015
we call the initial hidden state and it can either be something that you learn,

00:23:28.015 --> 00:23:32.065
like it's a parameter of the network and you learn how to initialize it,

00:23:32.065 --> 00:23:35.395
or you can assume something like maybe it's the zero vector.

00:23:35.395 --> 00:23:40.495
So the formula we use to compute the new hidden state based on the previous one,

00:23:40.495 --> 00:23:43.195
and also the current inputs is written on the left.

00:23:43.195 --> 00:23:46.690
So you do a linear transformation on the previous hidden state and on

00:23:46.690 --> 00:23:48.640
the current input and then you add some kind of

00:23:48.640 --> 00:23:50.919
bias and then put it through a non-linearity,

00:23:50.919 --> 00:23:52.990
like for example, the sigmoid function.

00:23:52.990 --> 00:23:55.700
And that gives you a new hidden state.

00:23:56.670 --> 00:23:59.470
Okay. So, once you've done that,

00:23:59.470 --> 00:24:01.480
then you can compute the next hidden state and you

00:24:01.480 --> 00:24:03.850
can keep unrolling the network like this.

00:24:03.850 --> 00:24:06.025
And that's, uh, yeah,

00:24:06.025 --> 00:24:07.450
that's called unrolling because you're kind of

00:24:07.450 --> 00:24:10.270
computing each step given the previous one.

00:24:10.270 --> 00:24:12.160
All right. So finally, if you remember,

00:24:12.160 --> 00:24:13.330
we're trying to do language modeling.

00:24:13.330 --> 00:24:17.530
So we're trying to predict which words should come next after the students opened their.

00:24:17.530 --> 00:24:19.870
So on this fourth step over here,

00:24:19.870 --> 00:24:21.205
we can use, uh,

00:24:21.205 --> 00:24:22.825
the current hidden state, H4,

00:24:22.825 --> 00:24:27.430
and put it through a linear layer and put it through a softmax function and then we get

00:24:27.430 --> 00:24:32.800
our output distribution Y-hat 4 which is a distribution over the vocabulary.

00:24:32.800 --> 00:24:34.720
And again, hopefully, we'll get some kind of

00:24:34.720 --> 00:24:38.080
sensible estimates for what the next word might be.

00:24:38.080 --> 00:24:43.210
Any questions at this point. Yep?

00:24:43.210 --> 00:24:47.650
Is the- the number of hidden state or is it gonna be the number of words in your input?

00:24:47.650 --> 00:24:50.845
The question is, is the number of hidden states the number of words in your input?

00:24:50.845 --> 00:24:53.485
Yeah, in this setting here, uh, yes,

00:24:53.485 --> 00:24:58.405
or you could say more generally the number of hidden states is the number of inputs. Yep.

00:24:58.405 --> 00:24:59.950
And just as with the n-gram model,

00:24:59.950 --> 00:25:05.590
we could use the output as the input from the tasks mutation in transformational model?

00:25:05.590 --> 00:25:07.000
Yeah, so the question is,

00:25:07.000 --> 00:25:08.650
as with the n-gram language model,

00:25:08.650 --> 00:25:10.570
could we use the output as the input on the next step?

00:25:10.570 --> 00:25:12.715
And the answer is yes, and I'll show you that in a minute.

00:25:12.715 --> 00:25:15.700
Any other questions? Yeah.

00:25:15.700 --> 00:25:17.995
Are you learning the embedding?

00:25:17.995 --> 00:25:20.560
The question is, are you learning the embeddings?

00:25:20.560 --> 00:25:21.925
Um, that's a choice.

00:25:21.925 --> 00:25:23.770
You could have the embeddings be for example,

00:25:23.770 --> 00:25:27.370
pre-generated embeddings that you download and you use those and they're frozen,

00:25:27.370 --> 00:25:28.750
or maybe you could download them,

00:25:28.750 --> 00:25:30.190
but then you could fine-tune them.

00:25:30.190 --> 00:25:32.200
That is,  allow them to be changed as parameters of

00:25:32.200 --> 00:25:35.170
the network or you could initialize them to,

00:25:35.170 --> 00:25:38.560
you know, small, uh, random values and learn them from scratch.

00:25:38.560 --> 00:25:40.570
Any other questions? Yeah.

00:25:40.570 --> 00:25:43.690
So you said you use the same delta matrix,

00:25:43.690 --> 00:25:45.490
like you do back propagation,

00:25:45.490 --> 00:25:48.030
does that you only update like WE,

00:25:48.030 --> 00:25:51.080
or do you update both WH and WE?

00:25:51.080 --> 00:25:56.085
So the question is, you say we reuse the matrix, do we update WE and WH, or just one?

00:25:56.085 --> 00:25:58.980
So you suddenly learn both WE and WH.

00:25:58.980 --> 00:26:01.410
I suppose I was emphasizing WH more, but yeah,

00:26:01.410 --> 00:26:04.090
they're both matrices that are applied repeatedly.

00:26:04.090 --> 00:26:05.500
There was also a question about back-prop,

00:26:05.500 --> 00:26:07.675
but we're going to cover that later in this lecture.

00:26:07.675 --> 00:26:12.250
Okay, moving on for now. Um, so,

00:26:12.250 --> 00:26:17.530
what are some advantages and disadvantages of this RNN language model?

00:26:17.530 --> 00:26:23.005
So here are some advantages that we can see in comparison to the fixed window one.

00:26:23.005 --> 00:26:28.210
So an obvious advantage is that this RNN can process any length of input.

00:26:28.210 --> 00:26:31.180
Another advantage is that the computation for

00:26:31.180 --> 00:26:35.050
step t can in theory use information from many steps back.

00:26:35.050 --> 00:26:36.730
So in our motivation example,

00:26:36.730 --> 00:26:38.650
which was as the proctor started the clock,

00:26:38.650 --> 00:26:39.970
the students opened their.

00:26:39.970 --> 00:26:42.250
We think that proctor and maybe clock are

00:26:42.250 --> 00:26:45.340
both pretty important hints for what might be coming up next.

00:26:45.340 --> 00:26:47.275
So, at least in theory,

00:26:47.275 --> 00:26:49.390
the hidden state at the end

00:26:49.390 --> 00:26:54.950
can have access to the information from the input from many steps ago.

00:26:55.350 --> 00:26:59.785
Another advantage is that the model size doesn't increase for longer inputs.

00:26:59.785 --> 00:27:02.485
So, uh, the size of the model is actually fixed.

00:27:02.485 --> 00:27:05.005
It's just WH and WE,s

00:27:05.005 --> 00:27:09.400
and then also the biases and also the embedding matrix, if you're counting that.

00:27:09.400 --> 00:27:13.000
None of those get bigger if you want to apply it to more,

00:27:13.000 --> 00:27:17.300
uh, longer inputs because you just apply the same weights repeatedly.

00:27:18.030 --> 00:27:23.995
And another advantage is that you have the same weights applied on every time-step.

00:27:23.995 --> 00:27:29.425
So I said this thing before about how the fixed-sized window neural model,

00:27:29.425 --> 00:27:31.720
it was less efficient because it was applying

00:27:31.720 --> 00:27:34.270
different weights of the weight matrix to the different,

00:27:34.270 --> 00:27:35.905
uh, words in the window.

00:27:35.905 --> 00:27:38.470
And the advantage about this RNN is that it's

00:27:38.470 --> 00:27:41.650
applying the exact same transformation to each of the inputs.

00:27:41.650 --> 00:27:45.835
So this means that if it learns a good way to process one input,

00:27:45.835 --> 00:27:48.010
that is applied to every input in the sequence.

00:27:48.010 --> 00:27:50.630
So you can see it as more efficient in that way.

00:27:51.480 --> 00:27:54.805
Okay, so what are the disadvantages of this model?

00:27:54.805 --> 00:27:58.270
One is that recurrent computation is pretty slow.

00:27:58.270 --> 00:27:59.995
Uh, as you saw before,

00:27:59.995 --> 00:28:03.865
you have to compute the hidden state based on the previous hidden state.

00:28:03.865 --> 00:28:06.925
So this means that you can't compute all of the hidden states in parallel.

00:28:06.925 --> 00:28:08.665
You have to compute them in sequence.

00:28:08.665 --> 00:28:13.120
So, especially if you're trying to compute an RNN over a pretty long sequence of inputs,

00:28:13.120 --> 00:28:16.660
this means that the RNN can be pretty slow to compute.

00:28:16.660 --> 00:28:20.425
Another disadvantage of RNNs is that it tuns out,

00:28:20.425 --> 00:28:24.175
in practice, it's quite difficult to access information from many steps back.

00:28:24.175 --> 00:28:26.290
So even though I said we should be able to remember about

00:28:26.290 --> 00:28:28.930
the proctor and the clock and use that to predict exams and our books,

00:28:28.930 --> 00:28:30.430
it turns out that RNNs,

00:28:30.430 --> 00:28:32.470
at least the ones that I've presented in this lecture,

00:28:32.470 --> 00:28:35.305
are not as good as that as you would think.

00:28:35.305 --> 00:28:39.295
Um, we're gonna learn more about both of these disadvantages later in the course,

00:28:39.295 --> 00:28:42.610
and we're going to learn something about how you can try to fix them.

00:28:42.610 --> 00:28:46.900
Have we gotten any questions at this point? Yep.

00:28:46.900 --> 00:28:48.010
Why do we assume that WH are the same?

00:28:48.010 --> 00:28:51.265
Sorry, can you speak up?

00:28:51.265 --> 00:28:55.900
Why do we assume that the WH should be the same?

00:28:55.900 --> 00:28:59.635
So the question is, why should you assume that the WH are the same?

00:28:59.635 --> 00:29:01.450
I suppose, it's not exactly an assumption,

00:29:01.450 --> 00:29:04.390
it's more a deliberate decision in the design of an RNN.

00:29:04.390 --> 00:29:06.460
So, an RNN is by definition,

00:29:06.460 --> 00:29:10.450
a network where you apply the exact same weights on every step.

00:29:10.450 --> 00:29:13.800
So, I suppose the question why do you assume maybe should be,

00:29:13.800 --> 00:29:15.225
why is that a good idea?

00:29:15.225 --> 00:29:17.520
Um, so I spoke a little bit about why it's a good idea,

00:29:17.520 --> 00:29:18.690
and this list of advantages,

00:29:18.690 --> 00:29:23.950
I suppose, are the reasons why you'd want to do that. Does that answer your question?

00:29:24.560 --> 00:29:29.020
Open their books, right? If you assume that WH are the same,

00:29:29.020 --> 00:29:31.420
you mean that like, uh,

00:29:31.420 --> 00:29:34.660
Markov chain, it's like a Markov chain.

00:29:34.660 --> 00:29:37.780
Uh, the trans- transmit, uh,

00:29:37.780 --> 00:29:42.955
trans- transfer probability for the human moods open,

00:29:42.955 --> 00:29:44.890
they are the same,

00:29:44.890 --> 00:29:50.940
but actually the Markov chain.

00:29:50.940 --> 00:29:56.535
The model, [inaudible] the transfer probability for that is the same,

00:29:56.535 --> 00:30:00.895
so [inaudible] probability,

00:30:00.895 --> 00:30:07.105
it- it's just an approximation but it's another test.

00:30:07.105 --> 00:30:08.240
Okay. So I think that [OVERLAPPING]

00:30:08.240 --> 00:30:10.810
If you assume WH could be the same,

00:30:10.810 --> 00:30:14.725
it's good because you used a number of parameters,

00:30:14.725 --> 00:30:20.560
but this is just an, this is just an approximation.

00:30:20.560 --> 00:30:23.410
The underlying transfer, uh,

00:30:23.410 --> 00:30:25.660
probability, it shouldn't be the same. Especially [OVERLAPPING]

00:30:25.660 --> 00:30:28.835
Okay. Um, so I think the question is saying that given the- these

00:30:28.835 --> 00:30:30.540
words the students opened their

00:30:30.540 --> 00:30:32.490
are all different and they're happening in different context,

00:30:32.490 --> 00:30:35.850
then why should we be applying the same transformation each time?

00:30:35.850 --> 00:30:37.440
So that's a- that's a good question.

00:30:37.440 --> 00:30:41.670
I think, uh, the idea is that you are learning a general function, not just, you know,

00:30:41.670 --> 00:30:43.535
how to deal with students,

00:30:43.535 --> 00:30:46.090
the one-word students in this one context.

00:30:46.090 --> 00:30:48.520
We're trying to learn a general function of how you

00:30:48.520 --> 00:30:51.070
should deal with a word given the word so far.

00:30:51.070 --> 00:30:55.090
You're trying to learn a general representation of language and context so far,

00:30:55.090 --> 00:30:57.055
which is indeed a very difficult problem.

00:30:57.055 --> 00:31:00.175
Um, I think you also mentioned that something about an approximation.

00:31:00.175 --> 00:31:01.780
Uh, another thing to note is that all of

00:31:01.780 --> 00:31:04.570
the hidden states are vectors, they're not just single numbers, right?

00:31:04.570 --> 00:31:06.670
They are vectors of lengths, I don't know, 500 or something?

00:31:06.670 --> 00:31:09.610
So they have quite a large capacity to hold lots of information about

00:31:09.610 --> 00:31:13.530
different things in all of their different, um, positions.

00:31:13.530 --> 00:31:15.630
So, I think the idea is that you can

00:31:15.630 --> 00:31:18.255
store a lot of different information in different contexts,

00:31:18.255 --> 00:31:19.830
in different parts of the hidden state,

00:31:19.830 --> 00:31:21.960
but it is indeed an approximation and there is

00:31:21.960 --> 00:31:24.575
some kind of limit to how much information you can store.

00:31:24.575 --> 00:31:26.845
Okay, any other questions? Yes.

00:31:26.845 --> 00:31:29.410
Since you kinda process any single length frame,

00:31:29.410 --> 00:31:31.135
what length do you use during your training?

00:31:31.135 --> 00:31:35.035
And does the length you use for training affect WH?

00:31:35.035 --> 00:31:39.355
Okay, so, the question is, given that you can have any length input,

00:31:39.355 --> 00:31:41.950
what length is the input during training?

00:31:41.950 --> 00:31:44.185
So, I suppose in practice,

00:31:44.185 --> 00:31:46.510
you choose how long the inputs are in

00:31:46.510 --> 00:31:49.630
training either based on what your data is or maybe based on,

00:31:49.630 --> 00:31:52.615
uh, your efficiency concerns so maybe you make it artificially

00:31:52.615 --> 00:31:55.900
shorter by chopping it up. Um, what was the other question?

00:31:55.900 --> 00:31:58.360
Uh, does WH depend on that?

00:31:58.360 --> 00:32:01.255
Okay. So the question was, does WH depend on the length you used?

00:32:01.255 --> 00:32:04.075
So, no, and that's one of the good things in the advantages list.

00:32:04.075 --> 00:32:07.165
Is that the model size doesn't increase for longer input,

00:32:07.165 --> 00:32:09.040
because we just unroll the RNN

00:32:09.040 --> 00:32:11.245
applying the same weights again and again for as long as we'd like.

00:32:11.245 --> 00:32:13.930
There's no need to have more weights just because you have a longer input.

00:32:13.930 --> 00:32:16.795
[NOISE] Yeah.

00:32:16.795 --> 00:32:24.235
So how the ratios that you mentioned are [inaudible] the number of words.

00:32:24.235 --> 00:32:28.405
[NOISE] Are you asking about capital E or the lowercase E?

00:32:28.405 --> 00:32:29.485
Uh, lowercase E.

00:32:29.485 --> 00:32:30.790
Okay. So, the question is,

00:32:30.790 --> 00:32:32.890
how do we choose the dimension of the lowercase Es?

00:32:32.890 --> 00:32:34.300
Uh, so, you could, for example,

00:32:34.300 --> 00:32:37.120
assume that those are just pre-trained word vectors like the ones that you,

00:32:37.120 --> 00:32:38.815
uh, used in assignment one.

00:32:38.815 --> 00:32:39.715
More like word2vec.

00:32:39.715 --> 00:32:41.140
Yeah. For example, word2vec,

00:32:41.140 --> 00:32:42.610
and you just download them and use them,

00:32:42.610 --> 00:32:44.380
or maybe you learn them from scratch, in which case,

00:32:44.380 --> 00:32:46.930
you decide at the beginning of training how big you want those vectors to be.

00:32:46.930 --> 00:32:49.210
[NOISE] Okay. I'm gonna move on for now.

00:32:49.210 --> 00:32:54.895
[NOISE] So, we've learned what an RNN language model is and we've learned how you would,

00:32:54.895 --> 00:32:56.845
uh, run one forward, but the question remains,

00:32:56.845 --> 00:32:59.080
how would you train an RNN language model?

00:32:59.080 --> 00:33:02.230
How would you learn it? [NOISE]

00:33:02.230 --> 00:33:03.850
So, as always, in machine learning,

00:33:03.850 --> 00:33:06.670
our answer starts with, you're going to get a big corpus of text,

00:33:06.670 --> 00:33:11.230
and we're gonna call that just a sequence of words X1 up to X capital T. So,

00:33:11.230 --> 00:33:15.115
you feed the sequence of words into the RNN language model, and then,

00:33:15.115 --> 00:33:19.615
the idea is that you compute the output distribution Y-hat T for every step T. So,

00:33:19.615 --> 00:33:21.700
I know that the picture I showed on the previous, uh,

00:33:21.700 --> 00:33:23.560
slide [NOISE] only showed us doing on the last step,

00:33:23.560 --> 00:33:26.140
but the idea is, you would actually compute this on every step.

00:33:26.140 --> 00:33:28.420
So, this means that you're actually predicting

00:33:28.420 --> 00:33:31.000
the probability of the next word on every step.

00:33:31.000 --> 00:33:33.130
[NOISE] Okay.

00:33:33.130 --> 00:33:35.515
So, once you've done that, then you can define the loss function,

00:33:35.515 --> 00:33:37.120
and this should be familiar to you by now.

00:33:37.120 --> 00:33:39.190
Uh, this is the cross-entropy between [NOISE]

00:33:39.190 --> 00:33:43.915
our predicted probability distribution Y-hat T and the true, uh,

00:33:43.915 --> 00:33:47.260
distribution, which is Y-hat- sorry, just YT,

00:33:47.260 --> 00:33:49.570
which is a one-hot vector, uh,

00:33:49.570 --> 00:33:51.055
representing the true next [NOISE] words,

00:33:51.055 --> 00:33:52.495
which is XT plus one.

00:33:52.495 --> 00:33:54.490
So, as you've seen before, this, uh,

00:33:54.490 --> 00:33:57.100
cross-entropy [NOISE] between those two vectors can be written

00:33:57.100 --> 00:34:00.640
also as a negative log probability.

00:34:00.640 --> 00:34:05.635
And then, lastly, if you average this cross-entropy loss across every step, uh,

00:34:05.635 --> 00:34:08.740
every T in the corpus time step T, then,

00:34:08.740 --> 00:34:11.800
uh, this gives you your overall loss for the entire training set.

00:34:11.800 --> 00:34:16.360
[NOISE] Okay.

00:34:16.360 --> 00:34:18.475
So, just to make that even more clear with a picture,

00:34:18.475 --> 00:34:20.080
uh, suppose that our corpus is,

00:34:20.080 --> 00:34:21.370
the students open their exams,

00:34:21.370 --> 00:34:23.020
et cetera, and it goes on for a long time.

00:34:23.020 --> 00:34:24.550
Then, what we'd be doing is,

00:34:24.550 --> 00:34:26.980
we'd be running our RNN over this text, and then,

00:34:26.980 --> 00:34:30.535
on every step, we would be predicting the probability [NOISE] distribution Y-hats,

00:34:30.535 --> 00:34:31.780
and then, from each of those,

00:34:31.780 --> 00:34:33.310
you can calculate what your loss is,

00:34:33.310 --> 00:34:36.400
which is the JT, and then, uh, on the first step,

00:34:36.400 --> 00:34:38.965
the loss would be the negative log probability of the next word,

00:34:38.965 --> 00:34:40.060
which is, in this example,

00:34:40.060 --> 00:34:42.040
students, [NOISE] and so on.

00:34:42.040 --> 00:34:45.070
Each of those is the negative log probability of the next word.

00:34:45.070 --> 00:34:47.515
[NOISE] And then, once you've computed all of those,

00:34:47.515 --> 00:34:49.585
you can add them [NOISE] all up and average them,

00:34:49.585 --> 00:34:51.160
and then, this gives you your final loss.

00:34:51.160 --> 00:34:56.260
[NOISE] Okay. So, there's a caveat here.

00:34:56.260 --> 00:34:59.935
Um, computing the loss and gradients across the entire corpus,

00:34:59.935 --> 00:35:02.350
all of those words X1 up to X capital T is too

00:35:02.350 --> 00:35:04.840
expensive [NOISE] because your corpus is probably really big.

00:35:04.840 --> 00:35:07.810
[NOISE] So, um, as a student asked earlier,

00:35:07.810 --> 00:35:10.555
uh, in practice, what do you actually regard as your sequence?

00:35:10.555 --> 00:35:12.580
So, in practice, you might regard your sequence as, uh,

00:35:12.580 --> 00:35:14.590
something like a sentence or a document,

00:35:14.590 --> 00:35:17.270
some shorter unit of text.

00:35:17.430 --> 00:35:20.890
So, uh, another thing you'll do [NOISE] is, if you remember,

00:35:20.890 --> 00:35:23.785
stochastic gradient descent allows you to compute gradients

00:35:23.785 --> 00:35:26.980
for small chunks of data rather than the whole corpus at a time.

00:35:26.980 --> 00:35:29.275
So, in practice, if you're training a language model,

00:35:29.275 --> 00:35:32.830
what you're actually likely to be doing is computing the loss for a sentence,

00:35:32.830 --> 00:35:35.290
but that's actually a batch of sentences, and then,

00:35:35.290 --> 00:35:37.945
you compute the gradients with respect to that batch of sentences,

00:35:37.945 --> 00:35:39.760
update your weights, and repeat.

00:35:39.760 --> 00:35:46.405
Any questions at this point? [NOISE] Okay.

00:35:46.405 --> 00:35:48.040
So, uh, moving onto backprop.

00:35:48.040 --> 00:35:51.055
Don't worry, there won't be as much backprop as there was last week,

00:35:51.055 --> 00:35:53.230
but, uh, there's an interesting question here, right?

00:35:53.230 --> 00:35:55.899
So, the, uh, characteristic thing about RNNs

00:35:55.899 --> 00:35:58.975
is that they apply the same weight matrix repeatedly.

00:35:58.975 --> 00:36:00.280
So, the question is,

00:36:00.280 --> 00:36:02.215
[NOISE] what's the derivative of our loss function,

00:36:02.215 --> 00:36:03.610
let's say, on step T?

00:36:03.610 --> 00:36:08.635
What's the derivative of that loss with respect to the repeated weight matrix WH?

00:36:08.635 --> 00:36:13.570
So, the answer is that the derivative of the loss, uh,

00:36:13.570 --> 00:36:16.390
the gradient with respect to the repeated weight is

00:36:16.390 --> 00:36:19.780
the sum of the gradient with respect to each time it appears,

00:36:19.780 --> 00:36:21.355
and that's what that equation says.

00:36:21.355 --> 00:36:25.615
So, on the right, the notation with the vertical line and the I is saying, uh,

00:36:25.615 --> 00:36:30.670
the derivative of the loss with respect to WH when it appears on the Ith step.

00:36:30.670 --> 00:36:32.770
Okay. So, so, why is that true?

00:36:32.770 --> 00:36:35.260
[NOISE] Uh, to sketch why this is true,

00:36:35.260 --> 00:36:37.840
uh, [NOISE] I'm gonna remind you of the multivariable chain rule.

00:36:37.840 --> 00:36:42.535
So, uh, this is a screenshot from a Khan Academy article on the multivariable chain rule,

00:36:42.535 --> 00:36:44.440
and, uh, I advise you check it out if you

00:36:44.440 --> 00:36:46.630
want to learn more because it's very easy to understand.

00:36:46.630 --> 00:36:48.220
Uh, and what it says is,

00:36:48.220 --> 00:36:52.045
given a function F [NOISE] which depends on X and Y,

00:36:52.045 --> 00:36:56.140
which are both themselves functions of some variable T, then,

00:36:56.140 --> 00:36:59.430
if you want to get the derivative of F with respect to T,

00:36:59.430 --> 00:37:04.380
then you need to do the chain ru- rule across X and Y separately and then add them up.

00:37:04.380 --> 00:37:07.020
[NOISE] So, that's the multivariable chain rule,

00:37:07.020 --> 00:37:10.510
[NOISE] and if we apply this to our scenario with trying to take

00:37:10.510 --> 00:37:14.889
the derivative of the loss JT with respect to our weight matrix WH,

00:37:14.889 --> 00:37:19.300
then you could view it as this kind of diagram [NOISE] where WH has, uh,

00:37:19.300 --> 00:37:22.810
a relationship with all of these individual appearances of WH,

00:37:22.810 --> 00:37:23.860
but it's a [NOISE] simple relationship,

00:37:23.860 --> 00:37:25.495
it's just equality, and then,

00:37:25.495 --> 00:37:29.690
each of those appearances of WH affect the loss in different ways.

00:37:29.690 --> 00:37:34.080
So, then, if we apply the multivariable chain rule,

00:37:34.080 --> 00:37:37.470
then it says that the derivative of the loss with respect to

00:37:37.470 --> 00:37:41.190
WH is the sum of those chain rule things,

00:37:41.190 --> 00:37:45.600
but the expression on the right is just one because it's an equality relation,

00:37:45.600 --> 00:37:50.480
[NOISE] and then, that gives us the equation that I wrote on the previous slide.

00:37:50.480 --> 00:37:55.240
So, this is a proof sketch for why the derivative of the loss with

00:37:55.240 --> 00:38:00.565
respect to our recurrent matrix is the sum of the derivatives each time it appears.

00:38:00.565 --> 00:38:03.190
Okay. So, suppose you believe me on that, that is,

00:38:03.190 --> 00:38:04.555
how you compute the, uh,

00:38:04.555 --> 00:38:06.475
gradient with respect to the recurrent weight.

00:38:06.475 --> 00:38:08.440
So, a remaining question is, well,

00:38:08.440 --> 00:38:10.720
how [NOISE] do we actually calculate this in practice?

00:38:10.720 --> 00:38:16.660
[NOISE] So, the answer is that you're going to calculate this sum by doing backprop,

00:38:16.660 --> 00:38:19.390
uh, backwards, kind of right to left, um,

00:38:19.390 --> 00:38:23.590
through the RNN, and you're going to accumulate this sum as you go.

00:38:23.590 --> 00:38:24.940
So, the important thing is,

00:38:24.940 --> 00:38:28.435
you shouldn't compute each of those things separately, uh,

00:38:28.435 --> 00:38:30.880
you should compute them by accumulating, like,

00:38:30.880 --> 00:38:34.360
each one can be computed in form- in terms of the previous one.

00:38:34.360 --> 00:38:39.130
[NOISE] So, this algorithm of computing each of these,

00:38:39.130 --> 00:38:41.320
uh, each of these gradients with respect to

00:38:41.320 --> 00:38:44.305
the previous one is called backpropagation through time.

00:38:44.305 --> 00:38:47.650
And, um, I always think that this sounds way more sci-fi than it is.

00:38:47.650 --> 00:38:49.030
It sounds like it's time travel or something,

00:38:49.030 --> 00:38:50.560
but it's actually pretty simple.

00:38:50.560 --> 00:38:53.290
Uh, it's just the name you give to

00:38:53.290 --> 00:38:57.290
applying the backprop algorithm to a recurrent neural network.

00:38:57.960 --> 00:39:02.350
Any questions at this point? Yep. [NOISE]

00:39:02.350 --> 00:39:07.240
So, it seems that how you break up the batches matter your end result.

00:39:07.240 --> 00:39:15.700
[inaudible].

00:39:15.700 --> 00:39:21.460
So, if you break it into much more [inaudible].

00:39:21.460 --> 00:39:23.605
Okay. So the question is, um, surely,

00:39:23.605 --> 00:39:27.865
how you decide to break up your batches affects how you learn, right?

00:39:27.865 --> 00:39:29.560
Because if you choose, uh,

00:39:29.560 --> 00:39:31.660
one set of data to be your batch, right, then,

00:39:31.660 --> 00:39:33.880
you will make your update based on that, and then,

00:39:33.880 --> 00:39:36.760
you only update the next one based on [NOISE] where you go from there.

00:39:36.760 --> 00:39:38.950
So, if you decided to put different data in the batch,

00:39:38.950 --> 00:39:40.495
then you would have made a different step.

00:39:40.495 --> 00:39:42.910
So, that's true, [NOISE] and that is why

00:39:42.910 --> 00:39:45.910
stochastic gradient descent is only an approximation of

00:39:45.910 --> 00:39:49.660
true gradient descent because the gradient that you compute with

00:39:49.660 --> 00:39:53.950
respect to one batch is just an approximation of the true gradient with respect to the,

00:39:53.950 --> 00:39:56.095
uh, the loss over the whole corpus.

00:39:56.095 --> 00:39:58.165
So, yes, it's true that it's an approximation

00:39:58.165 --> 00:40:00.580
and how [NOISE] you choose to batch up your data can matter,

00:40:00.580 --> 00:40:03.040
and that's why, for example, shuffling your data is a good idea,

00:40:03.040 --> 00:40:05.575
and shuffling it differently, each epoch, is a good idea.

00:40:05.575 --> 00:40:09.130
Uh, but the, the core idea of SGD is [NOISE] that, um,

00:40:09.130 --> 00:40:12.085
it should be a good enough approximation that over many steps,

00:40:12.085 --> 00:40:14.740
you will, uh, minimize your loss.

00:40:14.740 --> 00:40:33.010
[NOISE] Any other questions? [NOISE] Yeah.

00:40:33.010 --> 00:40:35.410
[NOISE] So, is, uh, is the question,

00:40:35.410 --> 00:40:37.180
as you compute forward prop,

00:40:37.180 --> 00:40:40.345
do you start computing backprop before you've even, like, got to the loss?

00:40:40.345 --> 00:40:41.620
Is that the question? [NOISE]

00:40:41.620 --> 00:40:42.325
Yes.

00:40:42.325 --> 00:40:45.640
I didn't think so, right? Because you need to know what the loss is in

00:40:45.640 --> 00:40:49.030
order to compute the derivative of the loss with respect to something.

00:40:49.030 --> 00:40:50.560
So, I think you need to get to the end.

00:40:50.560 --> 00:40:51.760
So, if we assume simplicity,

00:40:51.760 --> 00:40:54.490
that there is only one loss which you get at the end of several steps,

00:40:54.490 --> 00:40:55.585
then you need to get to the end,

00:40:55.585 --> 00:40:59.365
compute the loss before you can compute the derivatives.

00:40:59.365 --> 00:41:02.200
But I suppose you, you, you could compute the derivative of two,

00:41:02.200 --> 00:41:04.240
kind of, adjacent things of one with respect to the other.

00:41:04.240 --> 00:41:05.470
[OVERLAPPING] But, yeah. [NOISE]

00:41:05.470 --> 00:41:07.780
As you're going forward, do- you need to sort of keep a track of what,

00:41:07.780 --> 00:41:13.720
what you would have [inaudible] the one you eventually get the loss. [inaudible]

00:41:13.720 --> 00:41:15.865
Yes. So, when you forward prop,

00:41:15.865 --> 00:41:19.660
you certainly have to hang on to all of the intervening factors.

00:41:19.660 --> 00:41:20.680
[NOISE] Okay. I'm gonna move on for now.

00:41:20.680 --> 00:41:24.790
Uh, so, that was a maths-heavy bit but,

00:41:24.790 --> 00:41:27.130
um, now, we're getting on to text generation,

00:41:27.130 --> 00:41:28.675
which someone asked about earlier.

00:41:28.675 --> 00:41:32.965
So, um, just as we use the n-gram language model to generate text,

00:41:32.965 --> 00:41:36.115
you can also use an RNN language model to generate text,

00:41:36.115 --> 00:41:38.650
uh, via the same repeated sampling technique.

00:41:38.650 --> 00:41:41.050
Um, so, here's a picture of how that would work.

00:41:41.050 --> 00:41:43.990
How you start off with your initial hidden state H0, uh,

00:41:43.990 --> 00:41:46.330
which, uh, we have either as a parameter of

00:41:46.330 --> 00:41:49.060
the model or we initialize it to zero, or something like that.

00:41:49.060 --> 00:41:51.340
So, let's suppose that we have the first word my,

00:41:51.340 --> 00:41:54.235
and Iet's suppose I, um, supply that to the model.

00:41:54.235 --> 00:41:57.235
So, then, using the inputs and the initial hidden state,

00:41:57.235 --> 00:41:59.200
you can get our first hidden state H1.

00:41:59.200 --> 00:42:01.555
And then from there, we can compute the, er,

00:42:01.555 --> 00:42:04.765
probability distribution Y hat one of what's coming next,

00:42:04.765 --> 00:42:07.435
and then we can use that distribution to sample some word.

00:42:07.435 --> 00:42:09.385
So let's suppose that we sampled the word favorite.

00:42:09.385 --> 00:42:14.200
So, the idea is that we use the outputted word as the input on the next step.

00:42:14.200 --> 00:42:16.960
So, we feed favorite into the second step of the RNN,

00:42:16.960 --> 00:42:18.220
we get a new hidden state,

00:42:18.220 --> 00:42:20.784
and again we get a new probability distribution,

00:42:20.784 --> 00:42:22.885
and from that we can sample a new word.

00:42:22.885 --> 00:42:25.675
So, we can just continue doing this process again and again,

00:42:25.675 --> 00:42:27.685
and in this way we can generate some text.

00:42:27.685 --> 00:42:29.500
So, uh, here we've generated the text,

00:42:29.500 --> 00:42:30.760
My favorite season is Spring,

00:42:30.760 --> 00:42:34.070
and we can keep going for as long as we'd like.

00:42:36.060 --> 00:42:39.130
Okay, so, uh, let's have some fun with this.

00:42:39.130 --> 00:42:41.395
Uh, you can generate,

00:42:41.395 --> 00:42:43.885
uh, text using an RNN language model.

00:42:43.885 --> 00:42:48.070
If you train the RNN language model on any kind of text,

00:42:48.070 --> 00:42:51.340
then you can use it to generate text in that style.

00:42:51.340 --> 00:42:53.380
And in fact, this has become a whole kind of

00:42:53.380 --> 00:42:55.780
genre of internet humor that you might've seen.

00:42:55.780 --> 00:42:57.595
So, uh, for example,

00:42:57.595 --> 00:43:00.925
here is an RNN language model trained on Obama speeches,

00:43:00.925 --> 00:43:03.100
and I found this in a blog post online.

00:43:03.100 --> 00:43:07.120
So, here's the text that the RNN language model generated.

00:43:07.120 --> 00:43:11.350
"The United States will step up to the cost of a new challenges of

00:43:11.350 --> 00:43:15.520
the American people that will share the fact that we created the problem.

00:43:15.520 --> 00:43:19.630
They were attacked and so that they have to say that

00:43:19.630 --> 00:43:24.190
all the task of the final days of war that I will not be able to get this done."

00:43:24.190 --> 00:43:27.130
[LAUGHTER] Okay.

00:43:27.130 --> 00:43:30.205
So, if we look at this and

00:43:30.205 --> 00:43:32.230
especially think about what did

00:43:32.230 --> 00:43:34.570
that text look like that we got from the n-gram language model,

00:43:34.570 --> 00:43:36.160
the one about the, the price of gold.

00:43:36.160 --> 00:43:39.715
Um, I'd say that this is kind of recognizably better than that.

00:43:39.715 --> 00:43:41.620
It seems more fluent overall.

00:43:41.620 --> 00:43:43.690
Uh, I'd say it has a more of

00:43:43.690 --> 00:43:48.535
a sustained context in that it kind of makes sense for longer stretches at a time,

00:43:48.535 --> 00:43:51.666
and I'd say it does sound totally like Obama as well.

00:43:51.666 --> 00:43:53.035
So, all of that's pretty good,

00:43:53.035 --> 00:43:55.735
but you can see that it's still pretty incoherent overall,

00:43:55.735 --> 00:43:58.930
like i- it was quite difficult to read it because it didn't really make sense, right?

00:43:58.930 --> 00:44:00.130
So I had to read the words carefully.

00:44:00.130 --> 00:44:02.890
Um, so, yeah, I think this shows

00:44:02.890 --> 00:44:06.310
some of the progress you can get from using RNNs to generate text but still,

00:44:06.310 --> 00:44:09.610
um, very far from human level. Here are some more examples.

00:44:09.610 --> 00:44:13.285
Uh, here's an RNN language model that was trained on the Harry Potter books.

00:44:13.285 --> 00:44:17.095
And here's what it said. "Sorry." Harry shouted, panicking.

00:44:17.095 --> 00:44:19.600
"I'll leave those brooms in London." Are they?

00:44:19.600 --> 00:44:21.880
"No idea." said Nearly Headless Nick,

00:44:21.880 --> 00:44:23.740
casting low close by Cedric,

00:44:23.740 --> 00:44:26.980
carrying the last bit of treacle Charms from Harry's shoulder.

00:44:26.980 --> 00:44:29.290
And to answer him the common room perched upon it,

00:44:29.290 --> 00:44:33.025
four arms held a shining knob from when the Spider hadn't felt it seemed.

00:44:33.025 --> 00:44:34.855
He reached the teams too."

00:44:34.855 --> 00:44:38.065
So, again, I'd say that this is fairly fluent.

00:44:38.065 --> 00:44:40.000
It sounds totally like the Harry Potter books.

00:44:40.000 --> 00:44:41.710
In fact, I'm pretty impressed by how much it does

00:44:41.710 --> 00:44:44.170
sound like in the voice of the Harry Potter books.

00:44:44.170 --> 00:44:46.510
You even got some character attributes,

00:44:46.510 --> 00:44:50.395
I'd say that Harry the character does often panic in the book so that seems right.

00:44:50.395 --> 00:44:54.520
Um, [LAUGHTER] but some bad things are that we have,

00:44:54.520 --> 00:44:58.660
for example, a pretty long run-on sentence in the second paragraph that's hard to read.

00:44:58.660 --> 00:45:01.490
Uh, you have some nonsensical things that really make no sense.

00:45:01.490 --> 00:45:03.195
Like, I don't know what a treacle charm is.

00:45:03.195 --> 00:45:04.890
It sounds delicious but I don't think it's real,

00:45:04.890 --> 00:45:07.790
uh, and overall it's just pretty nonsensical.

00:45:07.790 --> 00:45:12.865
Here's another example. Here is an RNN language model that was trained on recipes.

00:45:12.865 --> 00:45:16.000
So, uh, [LAUGHTER] this one's pretty bizarre,

00:45:16.000 --> 00:45:18.565
the title is 'chocolate ranch barbecue',

00:45:18.565 --> 00:45:20.950
It contains Parmesan cheese,

00:45:20.950 --> 00:45:25.555
coconut milk, eggs, and the recipe says place each pasta over layers of lumps,

00:45:25.555 --> 00:45:29.500
shape mixture into the moderate oven and simmer until firm.

00:45:29.500 --> 00:45:31.210
Serve hot in bodied fresh,

00:45:31.210 --> 00:45:32.575
mustard orange and cheese.

00:45:32.575 --> 00:45:35.815
Combine the cheese and salt together the dough in a large skillet;

00:45:35.815 --> 00:45:38.140
add the ingredients and stir in the chocolate and pepper.

00:45:38.140 --> 00:45:41.635
[LAUGHTER] Um, so, one thing that I think is

00:45:41.635 --> 00:45:45.340
even more clear here in the recipes example than the prose example,

00:45:45.340 --> 00:45:49.405
is the inability to remember what's [NOISE] what's happening overall, right?

00:45:49.405 --> 00:45:53.020
Cuz a recipe you could say is pretty challenging because you need to remember

00:45:53.020 --> 00:45:57.100
the title of what you're trying to make which in this case is chocolate ranch barbecue,

00:45:57.100 --> 00:45:59.470
and you need to actually, you know, make that thing by the end.

00:45:59.470 --> 00:46:01.060
Uh, you also need to remember what were the ingredients

00:46:01.060 --> 00:46:02.500
in the beginning and did you use them.

00:46:02.500 --> 00:46:05.230
And in a recipe, if you make something and put it in the oven,

00:46:05.230 --> 00:46:07.720
you need to take it out later, a- and stuff like that, right?

00:46:07.720 --> 00:46:09.400
So, clearly it's not really

00:46:09.400 --> 00:46:11.890
remembering what's happening overall or what it's trying to do,

00:46:11.890 --> 00:46:13.915
it seems to be just generating kind of

00:46:13.915 --> 00:46:17.785
generic recipe sentences and putting them in a random order.

00:46:17.785 --> 00:46:20.635
Uh, but again, I mean, we can see that it's fairly fluent,

00:46:20.635 --> 00:46:23.350
it's grammatically right, it kind of sounds like a recipe.

00:46:23.350 --> 00:46:25.855
Uh, but the problem is it's just nonsensical.

00:46:25.855 --> 00:46:28.300
Like for example, shape mixture into

00:46:28.300 --> 00:46:31.345
the moderate oven is grammatical but it doesn't make any sense.

00:46:31.345 --> 00:46:33.295
Okay, last example.

00:46:33.295 --> 00:46:37.510
So, here's an RNN language model that's trained on paint-color names.

00:46:37.510 --> 00:46:41.200
And this is an example of a character-level language model because

00:46:41.200 --> 00:46:44.845
it's predicting what character comes next not what word comes next.

00:46:44.845 --> 00:46:47.650
And this is why it's able to come up with new words.

00:46:47.650 --> 00:46:49.840
Another thing to note is that this language model was

00:46:49.840 --> 00:46:52.090
trained to be conditioned on some kind of input.

00:46:52.090 --> 00:46:55.780
So here, the input is the color itself I think represented by the three numbers,

00:46:55.780 --> 00:46:57.145
that's probably RGB numbers.

00:46:57.145 --> 00:47:00.925
And it generated some names for the colors.

00:47:00.925 --> 00:47:02.140
And I think these are pretty funny.

00:47:02.140 --> 00:47:04.060
My favorite one is Stanky Bean,

00:47:04.060 --> 00:47:05.140
which is in the bottom right.

00:47:05.140 --> 00:47:07.930
[LAUGHTER] Um, so, it's pretty creative,

00:47:07.930 --> 00:47:10.210
[LAUGHTER] and I think these do sound kind of

00:47:10.210 --> 00:47:13.360
like paint colors but often they're quite bizarre.

00:47:13.360 --> 00:47:20.570
[LAUGHTER] Light of Blast is pretty good too.

00:47:20.910 --> 00:47:23.500
So, uh, you're gonna learn more about

00:47:23.500 --> 00:47:25.765
character-level language models in a future lecture,

00:47:25.765 --> 00:47:28.870
and you're also going to learn more about how to condition a language model

00:47:28.870 --> 00:47:32.440
based on some kind of input such as the color, um, code.

00:47:32.440 --> 00:47:34.330
So, these are pretty funny,

00:47:34.330 --> 00:47:35.890
uh, but I do want to say a warning.

00:47:35.890 --> 00:47:38.920
Um, you'll find a lot of these kinds of articles online,

00:47:38.920 --> 00:47:40.585
uh, often with headlines like,

00:47:40.585 --> 00:47:43.000
"We forced a bot to watch, you know,

00:47:43.000 --> 00:47:46.705
1000 hours of sci-fi movies and it wrote a script," something like that.

00:47:46.705 --> 00:47:50.800
Um, so, my advice is you have to take these with a big pinch of salt, because often,

00:47:50.800 --> 00:47:53.080
uh, the examples that people put online were

00:47:53.080 --> 00:47:55.375
hand selected by humans to be the funniest examples.

00:47:55.375 --> 00:47:58.660
Like I think all of the examples I've shown today were definitely hand selected

00:47:58.660 --> 00:48:02.200
by humans as the funniest examples that the RNN came up with.

00:48:02.200 --> 00:48:05.455
And in some cases they might even have been edited by a human.

00:48:05.455 --> 00:48:08.560
So, uh, yeah, you do need to be a little bit skeptical when you look at these examples.

00:48:08.560 --> 00:48:10.195
[OVERLAPPING] Yep.

00:48:10.195 --> 00:48:12.925
So, uh, in the Harry Potter one,

00:48:12.925 --> 00:48:16.630
there was a opening quote and then there was a closing quote.

00:48:16.630 --> 00:48:18.745
So, like do you expect the RNN,

00:48:18.745 --> 00:48:22.000
like when it puts that opening quote and keeps putting more words,

00:48:22.000 --> 00:48:28.825
do you expect the probability of a closing quote to like increase as you're going or decrease?

00:48:28.825 --> 00:48:31.150
That's a great question. So, uh,

00:48:31.150 --> 00:48:32.515
the question was, uh,

00:48:32.515 --> 00:48:34.450
we noticed that in the Harry Potter example,

00:48:34.450 --> 00:48:36.295
there was some open quotes and some closed quotes.

00:48:36.295 --> 00:48:38.410
And it looks like the model didn't screw up, right?

00:48:38.410 --> 00:48:40.075
All of these open quotes and closed quotes,

00:48:40.075 --> 00:48:41.815
uh, are in the correct places.

00:48:41.815 --> 00:48:44.455
So, the question is, do we expect the model to put

00:48:44.455 --> 00:48:48.775
a higher probability on closing the quote given that is inside a quo- quote passage?

00:48:48.775 --> 00:48:51.115
So, I should say definitely yes and

00:48:51.115 --> 00:48:54.220
that's most- mostly the explanation for why this works.

00:48:54.220 --> 00:48:56.500
Um, there's been some really interesting work in trying

00:48:56.500 --> 00:48:58.540
to look inside the hidden states of, uh,

00:48:58.540 --> 00:49:01.345
language models to see whether it's tracking things like,

00:49:01.345 --> 00:49:03.610
are we inside an open quote or a close quote?

00:49:03.610 --> 00:49:06.430
And there has been some limited evidence to show that

00:49:06.430 --> 00:49:09.370
maybe there are certain neuron or neurons inside the hidden state,

00:49:09.370 --> 00:49:10.900
which are tracking things like,

00:49:10.900 --> 00:49:12.550
are we currently inside a quote or not?

00:49:12.550 --> 00:49:13.855
[NOISE]. Yeah.

00:49:13.855 --> 00:49:18.370
So, so, like do you think the probability would increase  as you go more to the right [OVERLAPPING]?

00:49:18.370 --> 00:49:22.270
So, the question is as the quote passage goes on for longer,

00:49:22.270 --> 00:49:23.740
do you think the priority or

00:49:23.740 --> 00:49:26.770
the probability of outputting a closed quote should increase?

00:49:26.770 --> 00:49:28.045
Um, I don't know.

00:49:28.045 --> 00:49:31.420
Maybe. Um, that would be good, I suppose,

00:49:31.420 --> 00:49:32.980
because you don't want an infinite quote,

00:49:32.980 --> 00:49:35.650
uh, but I wouldn't be surprised if that didn't happen.

00:49:35.650 --> 00:49:39.400
Like I wouldn't be surprised if maybe some other worse-trained language models,

00:49:39.400 --> 00:49:41.395
just opened quotes and never closed them.

00:49:41.395 --> 00:49:44.815
Uh, any other questions? Yeah.

00:49:44.815 --> 00:49:47.605
What are the dimensions of the W metric?

00:49:47.605 --> 00:49:50.710
Okay. So, the question is what are the dimensions of the W metric?

00:49:50.710 --> 00:49:52.480
So we're going back to the online stuff.

00:49:52.480 --> 00:49:55.900
Uh, okay. You're asking me about W_h or W_e or something else?

00:49:55.900 --> 00:49:56.610
Yeah.

00:49:56.610 --> 00:49:58.960
So, W_h will be,

00:49:58.960 --> 00:50:01.435
uh, if we say that the hidden size has size n,

00:50:01.435 --> 00:50:07.240
then W_h will be n by n. And if we suppose that the embeddings have size d,

00:50:07.240 --> 00:50:08.635
then W_e will be, uh,

00:50:08.635 --> 00:50:12.550
d by n, n by d, maybe.

00:50:12.550 --> 00:50:19.990
Does that answer your question? [NOISE] Uh,

00:50:19.990 --> 00:50:23.380
any other questions about generating or anything? Yep.

00:50:23.380 --> 00:50:28.030
So, you said that there was a long sentence in the Harry Potter-related text?

00:50:28.030 --> 00:50:28.425
Yeah.

00:50:28.425 --> 00:50:33.640
Is it ever sort of practical to combine RNNs with like in this hand written rules?

00:50:33.640 --> 00:50:35.395
Sorry. Is it ever practical to combine-

00:50:35.395 --> 00:50:37.810
RNNs with a written list of hand-written rules.

00:50:37.810 --> 00:50:38.830
[OVERLAPPING]

00:50:38.830 --> 00:50:39.880
Okay. Yeah. That's a great question.

00:50:39.880 --> 00:50:42.220
So the question was, is it ever practical to

00:50:42.220 --> 00:50:44.980
combine RNNs with a list of hand-written rules?

00:50:44.980 --> 00:50:49.285
For example, don't let your sentence be longer than this many words.

00:50:49.285 --> 00:50:50.530
Um, so yeah.

00:50:50.530 --> 00:50:54.070
I'd say it probably is practical maybe especially if you're interested in, uh,

00:50:54.070 --> 00:50:56.260
making sure that certain bad things don't happen,

00:50:56.260 --> 00:51:01.900
you might apply some hacky rules like yeah forcing it to end, uh, early.

00:51:01.900 --> 00:51:03.580
I mean, okay. So there's this thing called Beam Search

00:51:03.580 --> 00:51:05.335
which we're going to learn about in a later lecture,

00:51:05.335 --> 00:51:06.640
which essentially doesn't just,

00:51:06.640 --> 00:51:09.340
um, choose one word in each step and continue.

00:51:09.340 --> 00:51:12.325
It explores many different options for words you could generate.

00:51:12.325 --> 00:51:14.410
And you can apply some kinds of rules on that

00:51:14.410 --> 00:51:16.540
where if you have lots of different things to choose from,

00:51:16.540 --> 00:51:18.250
then you can maybe get rid of

00:51:18.250 --> 00:51:21.265
some options if you don't like them because they break some of your rules.

00:51:21.265 --> 00:51:28.340
But, um, it can be difficult to do. Any other questions?

00:51:29.490 --> 00:51:38.380
Okay. Um, so we've talked about generating from language models.

00:51:38.380 --> 00:51:40.630
Uh, so unfortunately, you can't just use

00:51:40.630 --> 00:51:44.140
generation as your evaluation metric for the language models.

00:51:44.140 --> 00:51:47.245
You do need some kind of, um, measurable metric.

00:51:47.245 --> 00:51:52.015
So, the standard evaluation metric for language models is called perplexity.

00:51:52.015 --> 00:51:54.250
And, uh, perplexity is defined as

00:51:54.250 --> 00:51:58.480
the inverse probability of the corpus according to the language model.

00:51:58.480 --> 00:52:02.200
So, if you look at it you can see that that's what this formula is saying.

00:52:02.200 --> 00:52:04.075
It's saying that for every, uh,

00:52:04.075 --> 00:52:07.555
word xt, lowercase t, in the corpus, uh,

00:52:07.555 --> 00:52:10.420
we're computing the probability of that word given

00:52:10.420 --> 00:52:13.630
everything that came so far but its inverse is one over that.

00:52:13.630 --> 00:52:16.600
And then lastly, when normalizing this big,

00:52:16.600 --> 00:52:19.960
uh, product by the number of words,

00:52:19.960 --> 00:52:23.995
which is capital T. And the reason why we're doing that is because if we didn't do that,

00:52:23.995 --> 00:52:28.195
then perplexity would just get smaller and smaller as your corpus got bigger.

00:52:28.195 --> 00:52:31.070
So we need to normalize by that factor.

00:52:31.140 --> 00:52:33.910
So, you can actually show you that this, uh,

00:52:33.910 --> 00:52:38.470
perplexity is equal to the exponential of the cross-entropy loss J Theta.

00:52:38.470 --> 00:52:41.470
So if you remember, cross-entropy loss J Theta is, uh,

00:52:41.470 --> 00:52:44.305
the training objective that we're using to train the language model.

00:52:44.305 --> 00:52:46.555
And, uh, by rearranging things a little bit,

00:52:46.555 --> 00:52:50.890
you can see that perplexity is actually the exponential of the cross-entropy.

00:52:50.890 --> 00:52:52.750
And this is a good thing, uh,

00:52:52.750 --> 00:52:55.750
because if we're training the language model to, uh,

00:52:55.750 --> 00:52:58.900
minimize the cross-entropy loss,

00:52:58.900 --> 00:53:04.070
then you are training it to optimize the perplexity as well.

00:53:04.800 --> 00:53:08.860
So you should remember that the lower perplexity is better,

00:53:08.860 --> 00:53:12.640
uh, because perplexity is the inverse probability of the corpus.

00:53:12.640 --> 00:53:17.965
So, uh, if you want your language model to assign high probability to the corpus, right?

00:53:17.965 --> 00:53:21.470
Then that means you want to get low perplexity.

00:53:21.600 --> 00:53:28.480
Uh, any questions? [NOISE] Okay.

00:53:28.480 --> 00:53:36.220
Uh, so RNNs have been pretty successful in recent years in improving perplexity.

00:53:36.220 --> 00:53:39.880
So, uh, this is a results table from a recent,

00:53:39.880 --> 00:53:43.630
uh, Facebook research paper about RNN language models.

00:53:43.630 --> 00:53:46.600
And, uh, you don't have to understand all of the details of this table,

00:53:46.600 --> 00:53:48.055
but what it's telling you is that,

00:53:48.055 --> 00:53:50.785
on the, uh, top where we have n gram language model.

00:53:50.785 --> 00:53:52.240
And thessssssssssssn in the subsequent various,

00:53:52.240 --> 00:53:55.735
we have some increasingly complex and large RNNs.

00:53:55.735 --> 00:53:58.945
And you can see that the perplexity numbers are decreasing,

00:53:58.945 --> 00:54:00.475
because lower is better.

00:54:00.475 --> 00:54:02.770
So RNNs have been really great for

00:54:02.770 --> 00:54:06.320
making more effective language models in the last few years.

00:54:08.910 --> 00:54:11.695
Okay. So to zoom out a little bit,

00:54:11.695 --> 00:54:13.120
you might be thinking, uh,

00:54:13.120 --> 00:54:15.460
why should I care about Language Modelling?

00:54:15.460 --> 00:54:17.350
Why is it important? I'd say there are

00:54:17.350 --> 00:54:19.735
two main reasons why Language Modelling is important.

00:54:19.735 --> 00:54:21.160
Uh, so the first one is,

00:54:21.160 --> 00:54:23.620
that language modelling is a benchmark task that

00:54:23.620 --> 00:54:26.770
helps us measure our progress on understanding language.

00:54:26.770 --> 00:54:28.540
So, you could view language modeling as

00:54:28.540 --> 00:54:31.990
a pretty general language understanding task, right?

00:54:31.990 --> 00:54:35.425
Because predicting what word comes next to given any,

00:54:35.425 --> 00:54:37.795
any kind of, uh, generic text.

00:54:37.795 --> 00:54:40.975
Um, that's quite a difficult and general problem.

00:54:40.975 --> 00:54:43.330
And in order to be good at language modelling,

00:54:43.330 --> 00:54:45.340
you have to understand a lot of things, right?

00:54:45.340 --> 00:54:46.780
You have to understand grammar,

00:54:46.780 --> 00:54:48.115
you have to understand syntax,

00:54:48.115 --> 00:54:49.615
and you have to understand,

00:54:49.615 --> 00:54:51.115
uh, logic and reasoning.

00:54:51.115 --> 00:54:52.570
And you have to understand something about,

00:54:52.570 --> 00:54:53.845
you know, real-world knowledge.

00:54:53.845 --> 00:54:55.720
You have to understand a lot of things in order to be

00:54:55.720 --> 00:54:57.970
able to do language modelling properly.

00:54:57.970 --> 00:54:59.530
So, the reason why we care about it as

00:54:59.530 --> 00:55:02.350
a benchmark task is because if you're able to build a model,

00:55:02.350 --> 00:55:05.050
which is a better language model than the ones that came before it,

00:55:05.050 --> 00:55:07.930
then you must have made some kind of progress on at

00:55:07.930 --> 00:55:11.620
least some of those sub-components of natural language understanding.

00:55:11.620 --> 00:55:14.470
So, another more tangible reason why you might

00:55:14.470 --> 00:55:16.930
care about language modelling is that it's a sub-component of

00:55:16.930 --> 00:55:19.990
many many NLP tasks especially those which involve

00:55:19.990 --> 00:55:23.560
generating text or estimating the probability of text.

00:55:23.560 --> 00:55:25.675
So, here's a bunch of examples.

00:55:25.675 --> 00:55:27.220
Uh, one is predictive typing.

00:55:27.220 --> 00:55:29.170
That's the example that we showed at the beginning of the lecture

00:55:29.170 --> 00:55:31.450
with typing on your phone or searching on Google.

00:55:31.450 --> 00:55:35.185
Uh, this is also very useful for people who have movement disabilities, uh,

00:55:35.185 --> 00:55:39.595
because they are these systems that help people communicate using fewer movements.

00:55:39.595 --> 00:55:41.920
Uh, another example is speech recognition.

00:55:41.920 --> 00:55:43.600
So, in speech recognition you have

00:55:43.600 --> 00:55:45.820
some kind of audio recording of a person saying something

00:55:45.820 --> 00:55:49.975
and often it's kind of noisy and hard to make out what they're saying and you need to,

00:55:49.975 --> 00:55:51.700
uh, figure out what words did they say.

00:55:51.700 --> 00:55:55.300
So this an example where you have to estimate the probability of different,

00:55:55.300 --> 00:55:58.210
uh, different options of what, what it is they could have said.

00:55:58.210 --> 00:56:00.445
And in the same way, handwriting recognition,

00:56:00.445 --> 00:56:02.410
is an example where there's a lot of noise

00:56:02.410 --> 00:56:05.470
and you have to figure out what the person intended to say.

00:56:05.470 --> 00:56:07.810
Uh, spelling and grammar correction is yet

00:56:07.810 --> 00:56:10.705
another example where it's all about trying to figure out what someone meant.

00:56:10.705 --> 00:56:12.340
And that means you actually understand how

00:56:12.340 --> 00:56:14.695
likely it is that they were saying different things.

00:56:14.695 --> 00:56:19.555
Uh, an interesting, an interesting application is authorship identification.

00:56:19.555 --> 00:56:22.480
So suppose that you have a piece of text and you're trying to

00:56:22.480 --> 00:56:25.495
figure out who likely wrote it and maybe you have,

00:56:25.495 --> 00:56:29.830
uh, several different authors and you have text written by those different authors.

00:56:29.830 --> 00:56:31.285
So you could, for example,

00:56:31.285 --> 00:56:34.720
train a separate language model on each of the different authors' texts.

00:56:34.720 --> 00:56:36.160
And then, because, remember,

00:56:36.160 --> 00:56:39.805
a language model can tell you the probability of a given piece of text.

00:56:39.805 --> 00:56:42.430
Then you could ask all the different language models,

00:56:42.430 --> 00:56:45.790
um, how likely the texts and the question is,

00:56:45.790 --> 00:56:49.720
and then if a certain author's language model says that it's likely then that

00:56:49.720 --> 00:56:55.000
means that text the texts and the question is more likely to be written by that author.

00:56:55.000 --> 00:56:57.820
Um, other examples include machine translation.

00:56:57.820 --> 00:56:59.200
This is a huge, uh,

00:56:59.200 --> 00:57:01.390
application of language models,

00:57:01.390 --> 00:57:03.565
uh, because it's all about generating text.

00:57:03.565 --> 00:57:05.740
Uh, similarly, summarization is

00:57:05.740 --> 00:57:09.280
a task where we need to generate some text given some input text.

00:57:09.280 --> 00:57:11.185
Uh, dialogue as well,

00:57:11.185 --> 00:57:14.980
not all dialogue agents necessarily are RNN language models but you can

00:57:14.980 --> 00:57:19.285
build a dialogue agent that generates the text using an RNN language model.

00:57:19.285 --> 00:57:21.560
And there are more examples as well.

00:57:21.560 --> 00:57:25.360
Any questions on this? [LAUGHTER] Yep.

00:57:25.360 --> 00:57:47.875
So, I know that [inaudible]

00:57:47.875 --> 00:57:49.945
Great question. So, the question was,

00:57:49.945 --> 00:57:51.475
uh, for some of these examples, uh,

00:57:51.475 --> 00:57:55.315
such as speech recognition or maybe [NOISE] image captioning,

00:57:55.315 --> 00:57:59.290
the input is audio or image or something that is not text, right?

00:57:59.290 --> 00:58:01.780
So, you can't represent it in the way that we've talked about so far.

00:58:01.780 --> 00:58:04.180
Um, so, [NOISE] in those examples,

00:58:04.180 --> 00:58:06.460
you will have some way of representing the input,

00:58:06.460 --> 00:58:08.725
some way of encoding the audio or the image or whatever.

00:58:08.725 --> 00:58:13.315
Uh, the reason I brought it up now in terms of language models is that that's the input,

00:58:13.315 --> 00:58:15.685
but you use the language model to get the output, right?

00:58:15.685 --> 00:58:17.170
So, the language model, [NOISE] uh, generates

00:58:17.170 --> 00:58:19.345
the output in the way that we saw earlier, uh,

00:58:19.345 --> 00:58:22.120
but we're gonna learn more about those conditional language [NOISE] models later.

00:58:22.120 --> 00:58:25.090
[NOISE] Anyone else?

00:58:25.090 --> 00:58:29.020
[NOISE] Okay.

00:58:29.020 --> 00:58:32.965
[NOISE] So, uh, here's a recap.

00:58:32.965 --> 00:58:36.730
If I've lost you somewhere in this lecture, uh, or you got tired,

00:58:36.730 --> 00:58:38.770
um, now's a great time to jump back in

00:58:38.770 --> 00:58:41.050
because things are gonna get a little bit more accessible.

00:58:41.050 --> 00:58:43.045
Okay. So, here's a recap of what we've done today.

00:58:43.045 --> 00:58:46.210
Uh, a language model is a system that predicts the next word,

00:58:46.210 --> 00:58:48.460
[NOISE] and a recurrent neural network,

00:58:48.460 --> 00:58:50.590
is a new family, oh, new to us,

00:58:50.590 --> 00:58:53.710
a family of neural networks that takes sequential input

00:58:53.710 --> 00:58:57.175
of any length and it applies the same weights on every step,

00:58:57.175 --> 00:58:59.620
and it can optionally produce some kind of output on

00:58:59.620 --> 00:59:02.020
each step or some of the steps or none of the steps.

00:59:02.020 --> 00:59:04.945
[NOISE] So, don't be confused.

00:59:04.945 --> 00:59:08.305
A recurrent neural network is not [NOISE] the same thing as a language model.

00:59:08.305 --> 00:59:12.970
Uh, we've seen today that an RNN is a great way to build a language model, but actually,

00:59:12.970 --> 00:59:15.010
it turns out that you can use RNNs for,

00:59:15.010 --> 00:59:17.710
uh, a lot of other different things that are not language modeling.

00:59:17.710 --> 00:59:19.840
[NOISE] So, here's a few examples of that.

00:59:19.840 --> 00:59:24.085
[NOISE] Uh, you can use an RNN to do a tagging task.

00:59:24.085 --> 00:59:26.320
So, some examples of tagging tasks are

00:59:26.320 --> 00:59:29.260
part-of-speech tagging and named entity recognition.

00:59:29.260 --> 00:59:32.590
So, pictured here is part-of-speech tagging, and this is the task.

00:59:32.590 --> 00:59:35.245
We have some kind of input text such as, uh,

00:59:35.245 --> 00:59:37.645
the startled cat knocked over the vase,

00:59:37.645 --> 00:59:39.385
and your job is to, uh,

00:59:39.385 --> 00:59:42.085
label or tag each word with its part of speech.

00:59:42.085 --> 00:59:45.160
So, for example, cat is a noun and knocked is a verb.

00:59:45.160 --> 00:59:48.205
So, you can use an RNN to do this task in,

00:59:48.205 --> 00:59:50.350
in the way that we've pictured, which is that you, uh,

00:59:50.350 --> 00:59:52.720
feed the text into the RNN, [NOISE] and then,

00:59:52.720 --> 00:59:53.905
on each step of the RNN,

00:59:53.905 --> 00:59:55.705
you, uh, have an output,

00:59:55.705 --> 00:59:57.790
probably a distribution over what, uh,

00:59:57.790 --> 01:00:01.775
tag you think it is, and then, uh, you can tag it in that way.

01:00:01.775 --> 01:00:04.050
And then, also for named entity recognition,

01:00:04.050 --> 01:00:05.190
that's all about, um,

01:00:05.190 --> 01:00:08.085
tagging each of the words with what named entity type they are.

01:00:08.085 --> 01:00:11.820
So, you do it in the same way. [NOISE] Okay.

01:00:11.820 --> 01:00:13.470
Here's another thing you can use RNNs for,

01:00:13.470 --> 01:00:16.200
uh, you can use them for sentence classification.

01:00:16.200 --> 01:00:19.080
So, sentence classification is just a general term to mean

01:00:19.080 --> 01:00:22.170
any kind of task where you want to take sentence or other piece of text,

01:00:22.170 --> 01:00:24.945
and then, you want to classify it into one of several classes.

01:00:24.945 --> 01:00:28.120
So, an example of that is sentiment classification.

01:00:28.120 --> 01:00:30.400
Uh, sentiment classification is when you have some kind

01:00:30.400 --> 01:00:32.680
of input text such as, let's say, overall,

01:00:32.680 --> 01:00:34.510
I enjoyed the movie a lot, and then,

01:00:34.510 --> 01:00:35.770
you're trying to classify that as being

01:00:35.770 --> 01:00:38.095
positive or negative or [NOISE] neutral sentiment.

01:00:38.095 --> 01:00:40.090
So, in this example, this is positive sentiment.

01:00:40.090 --> 01:00:45.400
[NOISE] So, one way you might use an RNN to tackle this task is, uh,

01:00:45.400 --> 01:00:49.450
you might encode the text using the RNN, and then,

01:00:49.450 --> 01:00:53.350
really what you want is some kind of sentence encoding so that you

01:00:53.350 --> 01:00:57.265
can output your label for the sentence, right?

01:00:57.265 --> 01:00:59.680
And it'll be useful if you would have a single vector to

01:00:59.680 --> 01:01:02.965
represent the sentence rather than all of these separate vectors.

01:01:02.965 --> 01:01:04.870
So, how would you do this?

01:01:04.870 --> 01:01:07.000
How would you get the sentence encoding from the RNN?

01:01:07.000 --> 01:01:10.540
[NOISE] Uh, one thing you could do [NOISE] is,

01:01:10.540 --> 01:01:14.290
you could use the final hidden state as your sentence encoding.

01:01:14.290 --> 01:01:18.460
So, um, the reason why you might think this is a good idea is because,

01:01:18.460 --> 01:01:19.810
for example, in the RNN,

01:01:19.810 --> 01:01:22.675
we regard the, the final hidden state as,

01:01:22.675 --> 01:01:25.735
um, this is the thing you use to predict what's coming next, right?

01:01:25.735 --> 01:01:28.300
So, we're assuming that the final hidden state contains

01:01:28.300 --> 01:01:31.465
information about all of the text that has come so far, right?

01:01:31.465 --> 01:01:34.990
So, for that reason, you might suppose that this is a good sentence encoding,

01:01:34.990 --> 01:01:36.460
and we could use that [NOISE] to predict, you know,

01:01:36.460 --> 01:01:39.040
what, uh, what sentiment is this sentence.

01:01:39.040 --> 01:01:41.350
And it turns out that usually, a better way to do this,

01:01:41.350 --> 01:01:42.595
usually a more effective way,

01:01:42.595 --> 01:01:46.240
is to do something like maybe take an element-wise max or

01:01:46.240 --> 01:01:50.080
an element-wise mean of all these hidden states to get your sentence encoding,

01:01:50.080 --> 01:01:52.345
um, [NOISE] and, uh,

01:01:52.345 --> 01:01:54.640
this tends to work better than just using the final hidden state.

01:01:54.640 --> 01:01:58.490
[NOISE] Uh, there are some other more advanced things you can do as well.

01:01:59.310 --> 01:02:02.215
Okay. [NOISE] Another thing that you can use RNNs for

01:02:02.215 --> 01:02:05.335
is as a general purpose encoder module.

01:02:05.335 --> 01:02:08.470
Uh, so, here's an example that's question answering,

01:02:08.470 --> 01:02:10.480
but really this idea of RNNs as

01:02:10.480 --> 01:02:15.085
a general purpose encoder module is very common [NOISE] and use it in lots of different,

01:02:15.085 --> 01:02:17.590
um, deep learning [NOISE] architectures for NLP.

01:02:17.590 --> 01:02:21.175
[NOISE] So, here's an example which is question answering.

01:02:21.175 --> 01:02:23.410
Uh, so, let's suppose that the, the task is,

01:02:23.410 --> 01:02:24.670
you've got some kind of context,

01:02:24.670 --> 01:02:26.110
which, in this, uh, situation,

01:02:26.110 --> 01:02:29.365
is the Wikipedia article on Beethoven, and then,

01:02:29.365 --> 01:02:31.210
you have a question which is asking,

01:02:31.210 --> 01:02:33.070
what nationality was Beethoven?

01:02:33.070 --> 01:02:36.400
Uh, and this is actually taken from the SQuAD Challenge,

01:02:36.400 --> 01:02:38.680
which is the subject of the Default Final Project.

01:02:38.680 --> 01:02:41.770
So, um, if you choose to do- to do the Default Final Project,

01:02:41.770 --> 01:02:44.950
you're going to be building systems that solve this problem.

01:02:44.950 --> 01:02:49.930
So, what you might do is, you might use an RNN to process the question,

01:02:49.930 --> 01:02:51.970
what nationality was [NOISE] Beethoven?

01:02:51.970 --> 01:02:56.215
And then, you might use those hidden states that you get from this, uh,

01:02:56.215 --> 01:03:00.280
RNN of the question as a representation of the question.

01:03:00.280 --> 01:03:03.580
And I'm being intentionally vague here [NOISE] about what might happen next, uh,

01:03:03.580 --> 01:03:05.200
but the idea is that you have [NOISE]

01:03:05.200 --> 01:03:08.500
both the context and the question are going to be fed some way,

01:03:08.500 --> 01:03:10.900
and maybe you'll use an RNN on context as well,

01:03:10.900 --> 01:03:14.485
and you're going to have lots more neural architecture in order to get your answer,

01:03:14.485 --> 01:03:15.895
which is, uh, German.

01:03:15.895 --> 01:03:21.355
So, the point here is that the RNN is acting as an encoder for the question,

01:03:21.355 --> 01:03:23.920
that is, the hidden states that you get from running

01:03:23.920 --> 01:03:26.650
the RNN over the question, represent the question.

01:03:26.650 --> 01:03:31.810
[NOISE] Uh, so, the encoder is part of a larger neural system,

01:03:31.810 --> 01:03:33.940
[NOISE] and it's the, the hidden states themselves

01:03:33.940 --> 01:03:36.295
that you're interested in because they contain the information.

01:03:36.295 --> 01:03:38.140
So, you could have, um, taken,

01:03:38.140 --> 01:03:39.700
uh, element-wise max or mean,

01:03:39.700 --> 01:03:41.005
like we showed in the previous slide,

01:03:41.005 --> 01:03:44.170
to get a single vector for the question, but often, you don't do that.

01:03:44.170 --> 01:03:48.160
Often, you'll, uh, do something else which uses the hidden states directly.

01:03:48.160 --> 01:03:53.440
So, the general point here is that RNNs are quite powerful as a way to represent,

01:03:53.440 --> 01:03:54.925
uh, a sequence of text,

01:03:54.925 --> 01:03:57.710
uh, for further computation.

01:03:58.170 --> 01:04:02.935
Okay. Last example. So, going back to RNN language models again, [NOISE] uh,

01:04:02.935 --> 01:04:04.570
they can be used to generate text,

01:04:04.570 --> 01:04:07.300
and there are lots of different, uh, applications for this.

01:04:07.300 --> 01:04:11.020
So, for example, speech recognition, uh, you will have your input,

01:04:11.020 --> 01:04:13.345
which is the audio, and as a student asked earlier,

01:04:13.345 --> 01:04:15.865
this will be, uh, represented in some way,

01:04:15.865 --> 01:04:19.480
and then, uh, maybe you'll do a neural encoding of that, [NOISE] and then,

01:04:19.480 --> 01:04:22.615
you use your RNN language model to generate the output,

01:04:22.615 --> 01:04:24.354
which, in this case, is going to be a transcription

01:04:24.354 --> 01:04:26.275
of what the audio recording is saying.

01:04:26.275 --> 01:04:28.030
So, you will have some way of conditioning,

01:04:28.030 --> 01:04:29.830
and we're gonna talk more about how this works, uh,

01:04:29.830 --> 01:04:31.780
in a later lecture, but you have some way of

01:04:31.780 --> 01:04:35.230
conditioning your RNN language model on the input.

01:04:35.230 --> 01:04:38.920
So, you'll use that to generate your text, [NOISE] and in this case,

01:04:38.920 --> 01:04:41.335
the utterance might be something like, what's the weather,

01:04:41.335 --> 01:04:44.590
question mark. [OVERLAPPING] [NOISE]

01:04:44.590 --> 01:04:54.220
Yeah. [NOISE]

01:04:54.220 --> 01:04:58.120
In speech recognition, [inaudible].

01:04:58.120 --> 01:05:00.100
Okay. So, the question is, in speech recognition,

01:05:00.100 --> 01:05:02.755
we often use word error rates to evaluate,

01:05:02.755 --> 01:05:04.690
but would you use perplexity to evaluate?

01:05:04.690 --> 01:05:07.690
[NOISE] Um, I don't actually know much about that. Do you know, Chris,

01:05:07.690 --> 01:05:09.250
what they use in, uh,

01:05:09.250 --> 01:05:15.010
speech recognition as an eval metric? [NOISE]

01:05:15.010 --> 01:05:23.590
[inaudible] word error rate [inaudible].

01:05:23.590 --> 01:05:25.375
The answer is, you often use WER,

01:05:25.375 --> 01:05:27.550
uh, for eval, but you might also use perplexity.

01:05:27.550 --> 01:05:29.500
Yeah. Any other questions?

01:05:29.500 --> 01:05:35.575
[NOISE] Okay. So, um,

01:05:35.575 --> 01:05:38.350
this is an example of a conditional language model,

01:05:38.350 --> 01:05:39.970
and it's called a conditional language model

01:05:39.970 --> 01:05:41.725
because we have the language model component,

01:05:41.725 --> 01:05:44.740
but crucially, we're conditioning it on some kind of input.

01:05:44.740 --> 01:05:48.580
So, unlike the, uh, fun examples like with the Harry Potter text where we were just, uh,

01:05:48.580 --> 01:05:51.460
generating text basically unconditionally, you know,

01:05:51.460 --> 01:05:52.750
we trained it on the training data, and then,

01:05:52.750 --> 01:05:54.820
we just started [NOISE] with some kind of random seed,

01:05:54.820 --> 01:05:56.305
and then, it generates unconditionally.

01:05:56.305 --> 01:05:58.540
This is called a conditional language model

01:05:58.540 --> 01:06:01.615
because there's some kind of input that we need to condition on.

01:06:01.615 --> 01:06:05.980
Uh, machine translation is an example [NOISE] also of a conditional language model,

01:06:05.980 --> 01:06:07.780
and we're going to see that in much more detail in

01:06:07.780 --> 01:06:09.520
the lecture next week on machine translation.

01:06:09.520 --> 01:06:12.895
[NOISE] All right. Are there any more questions?

01:06:12.895 --> 01:06:14.320
You have a bit of extra time, I think.

01:06:14.320 --> 01:06:17.665
[NOISE] Yeah.

01:06:17.665 --> 01:06:20.350
I have a question about RNNs in general.

01:06:20.350 --> 01:06:25.345
[NOISE] Do people ever combine the RNN,

01:06:25.345 --> 01:06:27.220
uh, patterns of architecture,

01:06:27.220 --> 01:06:29.965
um, with other neural networks?

01:06:29.965 --> 01:06:31.885
Say, [NOISE] you have, um, you know,

01:06:31.885 --> 01:06:34.285
N previous layers that could be doing anything,

01:06:34.285 --> 01:06:35.410
and at the end of your network,

01:06:35.410 --> 01:06:36.880
you wanna run them through,

01:06:36.880 --> 01:06:39.160
uh, five recurrent layers.

01:06:39.160 --> 01:06:40.810
Do people mix and match like that,

01:06:40.810 --> 01:06:42.190
or these, uh, [inaudible]. [NOISE]

01:06:42.190 --> 01:06:46.090
Uh, the question is,

01:06:46.090 --> 01:06:48.580
do you ever combine RNN for the other types of architecture?

01:06:48.580 --> 01:06:49.870
So, I think the answer is yes.

01:06:49.870 --> 01:06:51.595
[NOISE] Uh, you might, [NOISE] you know, uh,

01:06:51.595 --> 01:06:55.210
have- you might have other types of architectures, uh,

01:06:55.210 --> 01:06:58.540
to produce the vectors that are going to be the input to RNN,

01:06:58.540 --> 01:07:00.280
or you might use the output of your RNN

01:07:00.280 --> 01:07:03.320
[NOISE] and feed that into a different type of neural network.

01:07:06.390 --> 01:07:08.620
So, yes. [NOISE] Any other questions?

01:07:08.620 --> 01:07:11.820
[NOISE] Okay.

01:07:11.820 --> 01:07:15.510
Uh, so, before we finish, uh, I have a note on terminology.

01:07:15.510 --> 01:07:17.490
Uh, when you're reading papers,

01:07:17.490 --> 01:07:20.915
you might find often this phrase vanilla RNN,

01:07:20.915 --> 01:07:23.065
and when you see the phrase vanilla RNN,

01:07:23.065 --> 01:07:24.535
that usually means, uh,

01:07:24.535 --> 01:07:26.905
the RNNs that are described in this lecture.

01:07:26.905 --> 01:07:30.460
So, the reason why those are called vanilla RNNs is

01:07:30.460 --> 01:07:34.765
because there are actually other more complex kinds of RNN flavors.

01:07:34.765 --> 01:07:38.005
So, for example, there's GRU and LSTM,

01:07:38.005 --> 01:07:40.330
and we're gonna learn about both of those next week.

01:07:40.330 --> 01:07:42.610
And another thing we're going to learn about next week

01:07:42.610 --> 01:07:45.085
[NOISE] is that you can actually get some multi-layer RNNs,

01:07:45.085 --> 01:07:48.250
which is when you stack multiple RNNs on top of each other.

01:07:48.250 --> 01:07:50.935
[NOISE] So, uh, you're gonna learn about those,

01:07:50.935 --> 01:07:53.875
but we hope that by the time you reach the end of this course,

01:07:53.875 --> 01:07:56.905
you're going to be able to read a research paper and see a phrase like

01:07:56.905 --> 01:08:01.150
stacked bidirectional LSTM with residual connections and self-attention,

01:08:01.150 --> 01:08:02.680
and you'll know exactly what that is.

01:08:02.680 --> 01:08:04.840
[NOISE] That's just an RNN with all of the toppings.

01:08:04.840 --> 01:08:07.840
[LAUGHTER] All right. Thank you. That's it for today.

01:08:07.840 --> 01:08:15.910
[NOISE] Uh, next time- [APPLAUSE] next time,

01:08:15.910 --> 01:08:18.340
we're learning about problems [NOISE] and fancy RNNs.

01:08:18.340 --> 01:08:24.770
[NOISE]

