WEBVTT
Kind: captions
Language: en

00:00:04.430 --> 00:00:07.410
Okay. Hello everyone.

00:00:07.410 --> 00:00:11.265
[LAUGHTER] Okay we should get started.

00:00:11.265 --> 00:00:14.640
Um, they're actually are still quite a few seats left.

00:00:14.640 --> 00:00:15.960
If you wanna be really bold,

00:00:15.960 --> 00:00:18.525
there are a couple of seats right in front of me in the front row.

00:00:18.525 --> 00:00:20.445
If you're less bolder a few over there.

00:00:20.445 --> 00:00:23.940
Um, but they're also on some of the rows are quite a few middle seat.

00:00:23.940 --> 00:00:28.080
So if people wanted to be really civic minded some people could sort of

00:00:28.080 --> 00:00:32.280
squeeze towards the edges and make  more accessible um,

00:00:32.280 --> 00:00:35.685
some of the seats that still exist in the classroom.

00:00:35.685 --> 00:00:39.435
Okay. Um, so, um,

00:00:39.435 --> 00:00:42.890
it's really exciting and great to see so many people here.

00:00:42.890 --> 00:00:47.390
So I'm a hearty welcome to CS224N and occasionally also

00:00:47.390 --> 00:00:52.625
known as Ling 284 which is Natural Language Processing with Deep Learning.

00:00:52.625 --> 00:00:55.420
Um, as just a sort of a  personal anecdote,

00:00:55.420 --> 00:00:59.720
is still sort of blows my mind that so many people turn up to this class these days.

00:00:59.720 --> 00:01:03.980
So, for about the first decade that I taught NLP here,

00:01:03.980 --> 00:01:08.180
you know the number of people I got each year was approximately 45.

00:01:08.180 --> 00:01:11.240
[LAUGHTER] So it's an order of [LAUGHTER] magnitude smaller than

00:01:11.240 --> 00:01:14.360
it is now but guess it says quite a lot

00:01:14.360 --> 00:01:17.450
on about what a revolutionary   impact

00:01:17.450 --> 00:01:20.870
that artificial intelligence in general and machine learning,

00:01:20.870 --> 00:01:25.600
deep learning, NLP are starting to have in modern society.

00:01:25.600 --> 00:01:28.860
Okay. So this is our plan for today.

00:01:28.860 --> 00:01:32.750
So, um, um, we're really gonna get straight down to business today.

00:01:32.750 --> 00:01:37.970
So they'll be a brief, very brief introduction some of the sort of course logistics,

00:01:37.970 --> 00:01:42.140
very brief discussion and talk about human language and

00:01:42.140 --> 00:01:46.370
word meaning and then we wanna get right into talking about um,

00:01:46.370 --> 00:01:50.540
the first thing that we're doing which is coming up with word vectors and looking

00:01:50.540 --> 00:01:55.010
at the word2vec algorithm and that will then sort of fill up the rest of the class.

00:01:55.010 --> 00:01:56.840
There are still two seats right in

00:01:56.840 --> 00:01:59.480
the front row for someone who wants to sit right in front of me,

00:01:59.480 --> 00:02:02.760
just letting you know [LAUGHTER].

00:02:02.760 --> 00:02:06.365
Okay. Okay. So here are the course logistics in brief.

00:02:06.365 --> 00:02:08.345
So I'm Christopher Manning,

00:02:08.345 --> 00:02:15.290
the person who bravely became the head TA is Abigail See is right there.

00:02:15.290 --> 00:02:18.920
And then we have quite a lot of wonderful TA's.

00:02:18.920 --> 00:02:22.700
To the people who are wonderful TA's just sort of stand up for one moment.

00:02:22.700 --> 00:02:26.810
So, um, [LAUGHTER] we have some sense for wonderful TAs.

00:02:26.810 --> 00:02:28.900
[LAUGHTER] Okay great.

00:02:28.900 --> 00:02:31.320
Um, okay.

00:02:31.320 --> 00:02:33.260
So you know when the lecture is because you made it

00:02:33.260 --> 00:02:37.100
here and so welcome also to SCPD people.

00:02:37.100 --> 00:02:41.300
This is also an SCPD class and you can watch it on video.

00:02:41.300 --> 00:02:44.300
But we love for Stanford students to turn

00:02:44.300 --> 00:02:47.300
up and show their beautiful faces in the classroom.

00:02:47.300 --> 00:02:52.810
Okay. So, um, the web-page has all the info about syllabus et cetera et cetera.

00:02:52.810 --> 00:02:56.175
Okay. So this class what do we hope to teach?

00:02:56.175 --> 00:02:59.240
So, one thing that we wanna teach is, uh, you know,

00:02:59.240 --> 00:03:02.495
an understanding of effective modern methods for deep learning.

00:03:02.495 --> 00:03:05.090
Starting off by reviewing some of the basics and then

00:03:05.090 --> 00:03:08.780
particularly talking about the kinds of techniques including um,

00:03:08.780 --> 00:03:11.450
recurrent networks and attention that are widely

00:03:11.450 --> 00:03:14.570
used for natural language processing models.

00:03:14.570 --> 00:03:18.770
A second thing we wanna teach is a big picture understanding of

00:03:18.770 --> 00:03:23.075
human languages and some of the difficulties in understanding and producing them.

00:03:23.075 --> 00:03:25.490
Of course if you wanna know a lot about human languages,

00:03:25.490 --> 00:03:29.060
there's a whole linguistics department and you can do a lot of courses of that.

00:03:29.060 --> 00:03:33.590
Um, but so I wanna give at least some appreciation so you have some clue of what are

00:03:33.590 --> 00:03:38.235
the challenges and difficulties and varieties of human languages.

00:03:38.235 --> 00:03:41.315
And then this is also kind of a practical class.

00:03:41.315 --> 00:03:44.960
Like we actually wanna teach you how you can

00:03:44.960 --> 00:03:49.670
build practical systems that work for some of the major parts of NLP.

00:03:49.670 --> 00:03:53.750
So if you go and get a job at one of those tech firms and they say "Hey,

00:03:53.750 --> 00:03:55.789
could you build us a named entity recognizer?"

00:03:55.789 --> 00:03:58.130
You can say "Sure, I can do that."

00:03:58.130 --> 00:04:00.530
And so for a bunch of problems,

00:04:00.530 --> 00:04:02.090
obviously we can't do everything,

00:04:02.090 --> 00:04:03.230
we're gonna do word meaning,

00:04:03.230 --> 00:04:07.579
dependency parsing, machine translation and you have an option to do question answering,

00:04:07.579 --> 00:04:10.340
I'm actually building systems for those.

00:04:10.340 --> 00:04:15.080
If you'd been talking to friends who did the class in the last couple of years,

00:04:15.080 --> 00:04:18.860
um, here are the differences for this year just to get things straight.

00:04:18.860 --> 00:04:21.830
Um, so we've updated some of the content of the course.

00:04:21.830 --> 00:04:26.285
So, uh, between me and guest lectures there's new content.

00:04:26.285 --> 00:04:28.470
Well that look bad.

00:04:29.030 --> 00:04:32.505
Wonder if that will keep happening, we'll find out.

00:04:32.505 --> 00:04:38.165
There's new content and on various topics that are sort of developing areas.

00:04:38.165 --> 00:04:41.300
One of the problems with this course is really big area of deep learning at

00:04:41.300 --> 00:04:44.755
the moment is still just developing really really quickly.

00:04:44.755 --> 00:04:47.480
So, it's sort of seems like one-year-old content is already

00:04:47.480 --> 00:04:51.290
things kind of data and we're trying to update things.

00:04:51.290 --> 00:04:54.140
A big change that we're making this year is we're

00:04:54.140 --> 00:04:56.930
having five-one week assignments instead of

00:04:56.930 --> 00:04:59.450
three-two week assignments at the beginning of

00:04:59.450 --> 00:05:02.795
the course and I'll say a bit more about that in a minute.

00:05:02.795 --> 00:05:06.215
Um, this year we're gonna use PyTorch instead of TensorFlow,

00:05:06.215 --> 00:05:08.860
and we'll talk about that more later too.

00:05:08.860 --> 00:05:13.880
Um, we're having the assignments due before class on either Tuesday or Thursday.

00:05:13.880 --> 00:05:16.585
So you're not distracted and can come to class.

00:05:16.585 --> 00:05:20.355
So starting off, um, yeah.

00:05:20.355 --> 00:05:22.680
So we're trying to give an easier,

00:05:22.680 --> 00:05:26.510
gentler ramp-up but on the other hand a fast ramp-up.

00:05:26.510 --> 00:05:29.555
So we've got this first assignment which is sort of easy, uh,

00:05:29.555 --> 00:05:34.040
but it's available right now and is due next Tuesday.

00:05:34.040 --> 00:05:37.460
And the final thing is we're not having a midterm this year.

00:05:37.460 --> 00:05:39.395
Um, okay.

00:05:39.395 --> 00:05:40.790
So this is what we're doing.

00:05:40.790 --> 00:05:44.300
So there are five of these assignments that I just mentioned.

00:05:44.300 --> 00:05:46.340
Um, So six percent for the first one,

00:05:46.340 --> 00:05:49.090
12 percent for each of the other ones,

00:05:49.090 --> 00:05:52.185
um, and, I already said that.

00:05:52.185 --> 00:05:54.230
We're gonna use gradescope for grading.

00:05:54.230 --> 00:05:56.780
It'll be really help out the TAs if you could use

00:05:56.780 --> 00:06:01.010
your SUnet ID as your gradescope account ID.

00:06:01.010 --> 00:06:04.205
Um, so then for the second part of the course,

00:06:04.205 --> 00:06:08.795
people do a final project and there are two choices for the final project.

00:06:08.795 --> 00:06:12.080
You can either do our default final project,

00:06:12.080 --> 00:06:14.030
which is a good option for many people,

00:06:14.030 --> 00:06:15.890
or you can do a custom final project and I'll

00:06:15.890 --> 00:06:19.010
talk about that in the more in the beginning.

00:06:19.010 --> 00:06:21.200
This is not working right.

00:06:21.200 --> 00:06:25.130
Um, and so then at the end we have

00:06:25.130 --> 00:06:30.425
a final poster presentation session at which your attendance is expected,

00:06:30.425 --> 00:06:34.580
and we're gonna be having that Wednesday in the evening.

00:06:34.580 --> 00:06:37.460
Probably not quite five hours but it'll be within that window,

00:06:37.460 --> 00:06:39.485
we'll work out the details in a bit.

00:06:39.485 --> 00:06:41.510
Three percent for participation,

00:06:41.510 --> 00:06:43.390
see the website for details.

00:06:43.390 --> 00:06:45.885
Six late days, um,

00:06:45.885 --> 00:06:50.330
collaboration, like always in computer science classes,

00:06:50.330 --> 00:06:55.340
we want you to do your own work and not borrow stuff from other people's Githubs and

00:06:55.340 --> 00:06:57.650
so we really do emphasize that you should

00:06:57.650 --> 00:07:01.150
read and pay attention to collaboration policies.

00:07:01.150 --> 00:07:04.700
Okay. So here's the high level plan for the problem sets.

00:07:04.700 --> 00:07:07.790
So, homework one available right now,

00:07:07.790 --> 00:07:10.130
is a hopefully easy on ramp.

00:07:10.130 --> 00:07:11.720
That's on iPython notebook,

00:07:11.720 --> 00:07:13.565
just help get everyone up to speed.

00:07:13.565 --> 00:07:17.750
Homework two is pure Python plus numpy but that

00:07:17.750 --> 00:07:22.190
will start to kind of teach you more about the sort of underlying,

00:07:22.190 --> 00:07:24.260
how do we do deep learning.

00:07:24.260 --> 00:07:29.430
If you're not so good or a bit rusty or never seen um,

00:07:29.430 --> 00:07:31.155
Python or numpy, um,

00:07:31.155 --> 00:07:34.730
we're gonna have an extra section on Friday.

00:07:34.730 --> 00:07:38.210
So Friday from 1:30 to 2:50 um,

00:07:38.210 --> 00:07:42.710
in Skilling Auditorium, we'll have a section that's a Python review.

00:07:42.710 --> 00:07:44.610
That's our only plan section at the moment,

00:07:44.610 --> 00:07:46.595
we're not gonna have a regular section.

00:07:46.595 --> 00:07:49.550
Um, so encourage to go to that and that will also be

00:07:49.550 --> 00:07:53.515
recorded for SCPD and available for video as well.

00:07:53.515 --> 00:07:56.790
Um, then Homework three um,

00:07:56.790 --> 00:08:00.850
will start us on using PyTorch.

00:08:00.850 --> 00:08:04.760
And then homeworks four and five we're then gonna be using

00:08:04.760 --> 00:08:08.720
py- PyTorch on GPU and we're actually gonna be using

00:08:08.720 --> 00:08:13.520
Microsoft Azure with big thank yous to the kind Microsoft Azure people who have

00:08:13.520 --> 00:08:19.165
sponsored our GPU computing for the last um, three years.

00:08:19.165 --> 00:08:24.995
Um, yes. So basically I mean all of modern deep learning has moved to the use

00:08:24.995 --> 00:08:30.589
of one or other of the large deep learning libraries like PyTorch TensorFlow,

00:08:30.589 --> 00:08:32.210
Chainer or MXNet um,

00:08:32.210 --> 00:08:36.440
et cetera and then doing the computing on GPU.

00:08:36.440 --> 00:08:38.600
So of course since we're in the one building,

00:08:38.600 --> 00:08:40.450
we should of course be using, um,

00:08:40.450 --> 00:08:42.620
GPUs [LAUGHTER] but I mean in general

00:08:42.620 --> 00:08:48.830
the so parallelisms scalability of GPUs is what's powered most of modern deep learning.

00:08:48.830 --> 00:08:50.720
Okay. The final project.

00:08:50.720 --> 00:08:55.460
So for the final project there are two things that you can do.

00:08:55.460 --> 00:09:00.665
So we have a default final project which is essentially our final project in a box.

00:09:00.665 --> 00:09:06.215
And so this is building a question answering system and we do it over the squad dataset.

00:09:06.215 --> 00:09:11.450
So what you build and how you can improve your performance is completely up to you.

00:09:11.450 --> 00:09:14.480
It is open-ended but it has an easier start,

00:09:14.480 --> 00:09:16.910
a clearly defined objective and we can

00:09:16.910 --> 00:09:19.775
have a leaderboard for how well things are working.

00:09:19.775 --> 00:09:24.680
Um, so if you don't have a clear research objective that can be a good choice for you

00:09:24.680 --> 00:09:29.600
or you can propose the custom Final Project and  assuming it's sensible,

00:09:29.600 --> 00:09:32.539
we will approve your custom final project,

00:09:32.539 --> 00:09:34.190
we will give you feedback, um,

00:09:34.190 --> 00:09:36.755
form someone as a mentor, um,

00:09:36.755 --> 00:09:42.410
and either way for only the final project we allow teams of one, two or three.

00:09:42.410 --> 00:09:45.200
For the homework should expect it to do them yourself.

00:09:45.200 --> 00:09:50.020
Of course you can chat to people in a general way about the problems.

00:09:50.020 --> 00:09:53.010
Okay. So that is the course.

00:09:53.010 --> 00:09:55.700
All good, and not even behind schedule yet.

00:09:55.700 --> 00:10:01.730
Okay. So the next section is human language and word meaning.Um.

00:10:01.730 --> 00:10:04.745
You know, if I was um,

00:10:04.745 --> 00:10:10.265
really going to tell you a lot about human language that would take a lot of time um,

00:10:10.265 --> 00:10:12.110
which I don't really have here.

00:10:12.110 --> 00:10:14.015
So I'm just going to tell you um,

00:10:14.015 --> 00:10:16.655
two anecdotes about human language.

00:10:16.655 --> 00:10:19.970
And the first is this XKCD cartoon.

00:10:19.970 --> 00:10:22.520
Um, and I mean this isn't,

00:10:22.520 --> 00:10:25.110
and I don't know why that's happening.

00:10:26.050 --> 00:10:28.250
I'm not sure what to make of that.

00:10:28.250 --> 00:10:34.070
Um, so, I actually really liked this XKCD cartoon.

00:10:34.070 --> 00:10:37.310
It's not one of the classic ones that you see most often around the place,

00:10:37.310 --> 00:10:42.140
but I actually think it says a lot about language and is worth thinking about.

00:10:42.140 --> 00:10:45.650
Like I think a lot of the time for the kind of people who come

00:10:45.650 --> 00:10:49.384
to this class who are mainly people like CS people,

00:10:49.384 --> 00:10:51.950
and EE people and random others.

00:10:51.950 --> 00:10:55.250
There's some other people I know since these people linguists and so on around.

00:10:55.250 --> 00:10:57.050
But for a lot of those people like,

00:10:57.050 --> 00:11:01.610
you've sort of spent your life looking at formal languages and the impression

00:11:01.610 --> 00:11:06.185
is that sort of human language as a sort of somehow a little bit broken formal languages,

00:11:06.185 --> 00:11:08.570
but there's really a lot more to it than that, right?

00:11:08.570 --> 00:11:11.165
That language is this amazing um,

00:11:11.165 --> 00:11:15.110
human created system that is used for

00:11:15.110 --> 00:11:19.520
all sorts of purposes and is adaptable to all sorts of purposes.

00:11:19.520 --> 00:11:23.750
So you can do everything from describing mathematics and human language

00:11:23.750 --> 00:11:28.520
um to sort of nuzzling up to your best friend and getting them to understand you better.

00:11:28.520 --> 00:11:31.910
So there's actually an amazing thing of human language. Anyway, I'll just read it.

00:11:31.910 --> 00:11:34.655
Um, so it's the first person,

00:11:34.655 --> 00:11:36.185
the dark haired person says,

00:11:36.185 --> 00:11:38.105
"Anyway, I could care less."

00:11:38.105 --> 00:11:40.010
And her friend says,

00:11:40.010 --> 00:11:42.440
"I think you mean you couldn't care less."

00:11:42.440 --> 00:11:46.490
Saying you could care less implies you care at least some amount.

00:11:46.490 --> 00:11:49.775
And the dark haired person says, "I don't know,

00:11:49.775 --> 00:11:54.590
we're these unbelievably complicated brains drifting through a void trying

00:11:54.590 --> 00:11:59.630
in vain to connect with one another by blindly flinging words out into the darkness."

00:11:59.630 --> 00:12:02.720
Every choice of phrasing and spelling, and tone,

00:12:02.720 --> 00:12:07.775
and timing carries countless signals and contexts and subtexts and more.

00:12:07.775 --> 00:12:11.435
And every listener interprets those signals in their own way.

00:12:11.435 --> 00:12:13.565
Language isn't a formal system,

00:12:13.565 --> 00:12:16.235
language is glorious chaos.

00:12:16.235 --> 00:12:20.750
You can never know for sure what any words will mean to anyone.

00:12:20.750 --> 00:12:26.150
All you can do is try to get better at guessing how your words affect people so

00:12:26.150 --> 00:12:28.790
you can have a chance of finding the ones that will make

00:12:28.790 --> 00:12:31.790
them feel something like what you want them to feel.

00:12:31.790 --> 00:12:34.235
Everything else is pointless.

00:12:34.235 --> 00:12:37.390
I assume you're giving me tips on how you interpret

00:12:37.390 --> 00:12:41.065
words because you want me to feel less alone.

00:12:41.065 --> 00:12:43.510
If so, thank you.

00:12:43.510 --> 00:12:45.585
That means a lot.

00:12:45.585 --> 00:12:48.440
But if you're just running my sentences past

00:12:48.440 --> 00:12:51.785
some mental checklist so you can show off how well you know it,

00:12:51.785 --> 00:12:53.180
then I could care less.

00:12:53.180 --> 00:13:02.825
[NOISE] Um, and so I think um,

00:13:02.825 --> 00:13:07.790
I think actually this has some nice messages about how language is this uncertain

00:13:07.790 --> 00:13:13.340
evolved system of communication but somehow we have enough agreed meaning that you know,

00:13:13.340 --> 00:13:15.500
we can kind of pretty much communicate.

00:13:15.500 --> 00:13:16.865
But we're doing some kind of you know

00:13:16.865 --> 00:13:20.540
probabilistic inference of guessing what people mean and we're

00:13:20.540 --> 00:13:22.070
using language not just for

00:13:22.070 --> 00:13:26.195
the information functions but for the social functions etc etc.

00:13:26.195 --> 00:13:32.310
Okay. And then here's my one other thought I had review about language.

00:13:33.490 --> 00:13:40.565
So, essentially if we want to have artificial intelligence that's intelligent,

00:13:40.565 --> 00:13:43.940
what we need to somehow get to the point of having

00:13:43.940 --> 00:13:48.560
compu- computers that have the knowledge of human beings, right?

00:13:48.560 --> 00:13:52.430
Because human beings have knowledge that gives them intelligence.

00:13:52.430 --> 00:13:55.460
And if you think about how we sort of

00:13:55.460 --> 00:13:59.270
convey knowledge around the place in our human world,

00:13:59.270 --> 00:14:04.025
mainly the way we do it is through human language.

00:14:04.025 --> 00:14:06.410
You know, some kinds of knowledge you can sort of

00:14:06.410 --> 00:14:09.260
work out for yourself by doing physical stuff right,

00:14:09.260 --> 00:14:11.900
I can hold this and drop that and I've learnt something.

00:14:11.900 --> 00:14:13.760
So I have to learn a bit of knowledge there.

00:14:13.760 --> 00:14:17.180
But sort of most of the knowledge in your heads and why you're sitting in

00:14:17.180 --> 00:14:21.980
this classroom has come from people communicating in human language to you.

00:14:21.980 --> 00:14:24.260
Um, so one of the famous,

00:14:24.260 --> 00:14:26.990
most famous steep learning people Yann Le Cun,

00:14:26.990 --> 00:14:29.165
he likes to say this line about,

00:14:29.165 --> 00:14:33.380
oh, you know really I think that you know there's not much difference

00:14:33.380 --> 00:14:37.965
between the intelligence of human being and orangutan.

00:14:37.965 --> 00:14:40.510
And I actually think he's really wrong on that.

00:14:40.510 --> 00:14:42.790
Like the sense in which he means that is,

00:14:42.790 --> 00:14:45.835
an orangutan has a really good vision system.

00:14:45.835 --> 00:14:48.610
Orangutans have very good you know control of

00:14:48.610 --> 00:14:52.060
their arms just like human beings for picking things up.

00:14:52.060 --> 00:14:58.970
Orangutans um can use tools um and orangutans can make plans so

00:14:58.970 --> 00:15:02.270
that if you sort of put the food somewhere where they have to sort of move

00:15:02.270 --> 00:15:05.960
the plank to get to the island with the food they can do a plan like that.

00:15:05.960 --> 00:15:09.890
So yeah, in a sense they've got a fair bit of intelligence but you know,

00:15:09.890 --> 00:15:13.385
sort of orangutans just aren't like human beings.

00:15:13.385 --> 00:15:16.100
And why aren't they like human beings?

00:15:16.100 --> 00:15:21.605
And I'd like to suggest to you the reason for that is what human beings have achieved is,

00:15:21.605 --> 00:15:25.070
we don't just have sort of one computer like

00:15:25.070 --> 00:15:29.825
a you know dusty old IBM PC in your mother's garage.

00:15:29.825 --> 00:15:33.740
What we have is a human computer network.

00:15:33.740 --> 00:15:37.520
And the way that we've achieved that human computer network is that,

00:15:37.520 --> 00:15:41.285
we use human languages as our networking language.

00:15:41.285 --> 00:15:44.690
Um, and so, when you think about it um,

00:15:44.690 --> 00:15:51.815
so on any kind of evolutionary scale language is super super super super recent, right?

00:15:51.815 --> 00:15:57.470
That um, creatures have had vision for people don't quite know but you know,

00:15:57.470 --> 00:16:00.980
maybe it's 75 million years or maybe it's longer, right?

00:16:00.980 --> 00:16:03.845
A huge length of time.

00:16:03.845 --> 00:16:07.295
How long have human beings have had language?

00:16:07.295 --> 00:16:09.860
You know people don't know that either because it turns out you know,

00:16:09.860 --> 00:16:11.015
when you have fossils,

00:16:11.015 --> 00:16:13.490
you can't knock the skull on the side and say,

00:16:13.490 --> 00:16:15.050
do you not have language.

00:16:15.050 --> 00:16:19.100
Um, but you know, most people estimate that sort of language is

00:16:19.100 --> 00:16:25.985
a very recent invention before current human beings moved out of um, out of Africa.

00:16:25.985 --> 00:16:28.550
So that many people think that we've only had language for

00:16:28.550 --> 00:16:31.460
something like a 100,000 years or something like that.

00:16:31.460 --> 00:16:35.450
So that's sort of you know blink of an eye on the evolutionary timescale.

00:16:35.450 --> 00:16:39.740
But you know, it was the development of language [inaudible]

00:16:39.740 --> 00:16:43.970
that sort of made human beings invisible- [NOISE] in invincible, right?

00:16:43.970 --> 00:16:46.475
It wasn't that, human beings um,

00:16:46.475 --> 00:16:51.410
developed poison fangs or developed ability to run

00:16:51.410 --> 00:16:53.660
faster than any other creature or

00:16:53.660 --> 00:16:56.210
put a big horn on their heads or something like that, right?

00:16:56.210 --> 00:16:59.060
You know, humans are basically pretty puny um,

00:16:59.060 --> 00:17:01.190
but they had this um,

00:17:01.190 --> 00:17:04.310
unbeatable advantage that they could communicate with

00:17:04.310 --> 00:17:07.880
each other and therefore work much more effectively in teams.

00:17:07.880 --> 00:17:11.495
And that sort of basically made human beings invincible.

00:17:11.495 --> 00:17:15.575
But you know, even then humans were kind of limited, right?

00:17:15.575 --> 00:17:18.140
That kind of got you to about the Stone Age right,

00:17:18.140 --> 00:17:20.390
where you could bang on your stones and with

00:17:20.390 --> 00:17:23.240
the right kind of stone make something sharp to cut with.

00:17:23.240 --> 00:17:25.685
Um, what got humans beyond that,

00:17:25.685 --> 00:17:28.100
was that they invented writing.

00:17:28.100 --> 00:17:32.915
So writing was then an ability where you could take knowledge

00:17:32.915 --> 00:17:37.730
not only communicated um mouth to mouth to people that you saw.

00:17:37.730 --> 00:17:41.660
You could put it down on your piece of papyrus so your clay tablet or whatever

00:17:41.660 --> 00:17:45.620
it was at first and that knowledge could then be sent places.

00:17:45.620 --> 00:17:50.270
It could be sent spatially around the world and it could then

00:17:50.270 --> 00:17:55.430
be sent temporally through time.

00:17:55.430 --> 00:17:57.290
And well, how old is writing?

00:17:57.290 --> 00:18:00.890
I mean, we sort of basically know about how old writing is, right?

00:18:00.890 --> 00:18:04.115
That writing is about 5,000 years old.

00:18:04.115 --> 00:18:09.740
It's incredibly incredibly recent on this scale of evolution but you know,

00:18:09.740 --> 00:18:16.730
essentially writing was so powerful as a way of having knowledge that then in those 5,000

00:18:16.730 --> 00:18:24.035
years that enabled human beings to go from stone age sharp piece or flint to you know,

00:18:24.035 --> 00:18:26.240
having iPhones and all of these things,

00:18:26.240 --> 00:18:28.790
all these incredibly sophisticated devices.

00:18:28.790 --> 00:18:32.960
So, language is pretty special thing I'd like to suggest.

00:18:32.960 --> 00:18:37.910
Um, but you know, if I go back to my analogy that sort of it's allowed humans to

00:18:37.910 --> 00:18:43.280
construct a networked computer that is way way more powerful than um,

00:18:43.280 --> 00:18:47.600
just having individual creatures as sort of intelligent like an orangutan.

00:18:47.600 --> 00:18:50.525
Um, and you compare it to our computer networks,

00:18:50.525 --> 00:18:53.045
it's a really funny kind of network, right?

00:18:53.045 --> 00:18:55.745
You know that these days um,

00:18:55.745 --> 00:19:01.805
we have networks that run around where we have sort of large network bandwidth, right?

00:19:01.805 --> 00:19:03.770
You know, we might be frustrated sometimes with

00:19:03.770 --> 00:19:06.530
our Netflix downloads but by and large you know,

00:19:06.530 --> 00:19:09.755
we can download hundreds of megabytes really easily and quickly.

00:19:09.755 --> 00:19:11.570
And we don't think that's fast enough,

00:19:11.570 --> 00:19:13.670
so we're going to be rolling out 5G networks.

00:19:13.670 --> 00:19:16.400
So it's an order of magnitude faster again.

00:19:16.400 --> 00:19:18.800
I mean, by comparison to that, I mean,

00:19:18.800 --> 00:19:23.540
human language is a pathetically slow network, right?

00:19:23.540 --> 00:19:29.465
That the amount of information you can convey by human language is very slow.

00:19:29.465 --> 00:19:33.950
I mean you know, whatever it is I sort of speak at about 15 words a second right,

00:19:33.950 --> 00:19:35.420
you can start doing um,

00:19:35.420 --> 00:19:37.550
your information theory if you know some right?

00:19:37.550 --> 00:19:41.060
But um, you don't actually get much bandwidth at all.

00:19:41.060 --> 00:19:44.405
And that then leads- so you can think of,

00:19:44.405 --> 00:19:45.980
how does it work then?

00:19:45.980 --> 00:19:47.570
So, humans have come up with

00:19:47.570 --> 00:19:53.390
this incredibly impressive system which is essentially form of compression.

00:19:53.390 --> 00:19:56.120
Sort of a very adaptive form of compression,

00:19:56.120 --> 00:19:58.070
so that when we're talking to people,

00:19:58.070 --> 00:20:02.870
we assume that they have an enormous amount of knowledge in their heads which

00:20:02.870 --> 00:20:07.640
isn't the same as but it's broadly similar to mine when I'm talking to you right?

00:20:07.640 --> 00:20:10.565
That you know what English words mean,

00:20:10.565 --> 00:20:13.850
and you know a lot about how the wor- world works.

00:20:13.850 --> 00:20:17.149
And therefore, I can say a short message and communicate

00:20:17.149 --> 00:20:22.820
only a relatively short bit string and you can actually understand a lot. All right?

00:20:22.820 --> 00:20:26.030
So, I can say sort of whatever you know,

00:20:26.030 --> 00:20:28.850
imagine a busy shopping mall and that

00:20:28.850 --> 00:20:31.630
there are two guys standing in front of a makeup counter,

00:20:31.630 --> 00:20:36.290
and you know I've only said whatever that was sort of about 200 bits of

00:20:36.290 --> 00:20:38.960
information but that's enabled you to construct

00:20:38.960 --> 00:20:42.340
a whole visual scene that we're taking megabytes to um,

00:20:42.340 --> 00:20:44.385
represent as an image.

00:20:44.385 --> 00:20:46.625
So, that's why language is good.

00:20:46.625 --> 00:20:49.100
Um, so from that more authorial level,

00:20:49.100 --> 00:20:51.425
I'll now move back to the concrete stuff.

00:20:51.425 --> 00:20:55.925
What we wanna do in this class is not solve the whole of language,

00:20:55.925 --> 00:20:57.950
but we want to represent, um,

00:20:57.950 --> 00:21:00.380
the meaning of words, right?

00:21:00.380 --> 00:21:03.230
So, a lot of language is bound up in words and their meanings

00:21:03.230 --> 00:21:06.200
and words can have really rich meanings, right?

00:21:06.200 --> 00:21:07.970
As soon as you say a word teacher,

00:21:07.970 --> 00:21:12.530
that's kinda quite a lot of rich meaning or you can have actions that have rich meaning.

00:21:12.530 --> 00:21:17.225
So, if I say a word like prognosticate or,

00:21:17.225 --> 00:21:19.070
um, total or something you know,

00:21:19.070 --> 00:21:22.385
these words that have rich meanings and a lot of nuance on them.

00:21:22.385 --> 00:21:24.395
And so we wanna represent meaning.

00:21:24.395 --> 00:21:26.510
And so, the question is what is meaning?

00:21:26.510 --> 00:21:29.360
So, you can of course you can- dictionaries are meant to tell you about meanings.

00:21:29.360 --> 00:21:31.490
So, you can look up dictionaries um,

00:21:31.490 --> 00:21:35.720
and Webster says sort of tries to relate meaning to idea.

00:21:35.720 --> 00:21:39.515
The idea that is represented by a word or a phrase.

00:21:39.515 --> 00:21:44.240
The idea that a person wants to express by word signs et cetera.

00:21:44.240 --> 00:21:46.190
I mean, you know,

00:21:46.190 --> 00:21:49.730
you could think that these definitions are kind of a cop-out because it seems

00:21:49.730 --> 00:21:53.015
like they're rewriting meaning in terms of the word idea,

00:21:53.015 --> 00:21:55.040
and is that really gotten you anywhere.

00:21:55.040 --> 00:21:58.370
Um, how do linguists think about meaning?

00:21:58.370 --> 00:22:03.110
I mean, the most common way that linguists have thought about

00:22:03.110 --> 00:22:05.660
meaning is an idea that's called denotational

00:22:05.660 --> 00:22:08.420
semantics which is also used in programming languages.

00:22:08.420 --> 00:22:14.810
So, the idea of that is we think of meaning as what things represent.

00:22:14.810 --> 00:22:16.955
So, if I say the word chair,

00:22:16.955 --> 00:22:21.140
the denotation of the word chair includes this one here and that one,

00:22:21.140 --> 00:22:22.325
that one, that one, that one.

00:22:22.325 --> 00:22:24.919
And so, the word chair is sort of representing

00:22:24.919 --> 00:22:28.580
all the things that are chairs and you can sort of, um,

00:22:28.580 --> 00:22:33.410
you can then think of something like running as well that you know there's sort of sets

00:22:33.410 --> 00:22:37.985
of actions that people can partake that- that's their denotation.

00:22:37.985 --> 00:22:42.200
And that's sort of what you most commonly see in philosophy or linguistics as denotation.

00:22:42.200 --> 00:22:47.135
It's kind of a hard thing to get your hands on, um, computationally.

00:22:47.135 --> 00:22:50.480
So, um, what type of people most commonly

00:22:50.480 --> 00:22:54.020
do or use the most commonly do I guess I should say now

00:22:54.020 --> 00:22:57.530
for working out the meaning of words on the computer that

00:22:57.530 --> 00:23:01.115
commonly that turn to something that was a bit like a dictionary.

00:23:01.115 --> 00:23:06.200
In particular favorite online thing was this online thesaurus called WordNet which

00:23:06.200 --> 00:23:11.510
sort of tells you about word meanings and relationships between word meanings.

00:23:11.510 --> 00:23:16.445
Um, so this is just giving you the very slices sense of,

00:23:16.445 --> 00:23:19.820
um, of what's in WordNet.

00:23:19.820 --> 00:23:24.485
Um, so this is an actual bit of Python code up there which you can,

00:23:24.485 --> 00:23:28.370
um, type into your computer and run and do this for yourself.

00:23:28.370 --> 00:23:31.040
Um, so this uses a thing called NLTK.

00:23:31.040 --> 00:23:33.725
Um, so NLTK is sort of like

00:23:33.725 --> 00:23:39.364
the "Swiss Army Knife of NLP" meaning that it's not terribly good for anything,

00:23:39.364 --> 00:23:41.570
but it has a lot of basic tools.

00:23:41.570 --> 00:23:46.460
So, if you wanted to do something like just get some stuff out of WordNet and show it,

00:23:46.460 --> 00:23:49.625
it's the perfect thing to use. Um, okay.

00:23:49.625 --> 00:23:54.830
So, um, from NLTK I'm importing WordNet and so then I can say,

00:23:54.830 --> 00:24:01.355
"Okay, um, for the word good tell me about the synonym sets with good participates in."

00:24:01.355 --> 00:24:03.440
And there's good goodness as a noun.

00:24:03.440 --> 00:24:04.760
There is an adjective good.

00:24:04.760 --> 00:24:08.330
There's one estimable good, honorable, respectable.

00:24:08.330 --> 00:24:11.150
Um, this looks really complex and hard to understand.

00:24:11.150 --> 00:24:13.700
But the idea of word- WordNet makes

00:24:13.700 --> 00:24:18.080
these very fine grain distinctions between senses of a word.

00:24:18.080 --> 00:24:20.675
So, what sort of saying for good, um,

00:24:20.675 --> 00:24:23.570
there's what some sensors where it's a noun, right?

00:24:23.570 --> 00:24:24.755
That's where you sort of,

00:24:24.755 --> 00:24:27.200
I bought some goods for my trip, right?

00:24:27.200 --> 00:24:28.880
So, that's sort of, um,

00:24:28.880 --> 00:24:32.780
one of these noun sensors like this one I guess.

00:24:32.780 --> 00:24:35.480
Um, then there are adjective sensors and it's trying to

00:24:35.480 --> 00:24:38.840
distinguish- there's a basic adjective sense of good being good,

00:24:38.840 --> 00:24:41.270
and then in certain, um, sensors,

00:24:41.270 --> 00:24:44.750
there are these extended sensors of good in different directions.

00:24:44.750 --> 00:24:48.515
So, I guess this is good in the sense of beneficial, um,

00:24:48.515 --> 00:24:52.925
and this one is sort of person who is respectable or something.

00:24:52.925 --> 00:24:55.580
He's a good man or something like that, right?

00:24:55.580 --> 00:24:56.855
So, um, but you know,

00:24:56.855 --> 00:24:59.660
part of what's kind of makes us

00:24:59.660 --> 00:25:02.630
think very problematic and practice to use is it tries to make

00:25:02.630 --> 00:25:06.850
all these very fine-grain differences between sensors that are a human being can

00:25:06.850 --> 00:25:11.410
barely understand the difference between them um, and relate to.

00:25:11.410 --> 00:25:13.690
Um, so you can then do other things with WordNet.

00:25:13.690 --> 00:25:18.460
So, this bit of code you can sort of well walk up and is a kind of hierarchy.

00:25:18.460 --> 00:25:21.635
So, it's kinda like a traditional, um, database.

00:25:21.635 --> 00:25:29.030
So, if I start with a panda and say- [NOISE] if I start with a panda.

00:25:29.030 --> 00:25:32.180
Um, and walk up, um,

00:25:32.180 --> 00:25:35.330
the pandas are [inaudible].

00:25:35.330 --> 00:25:37.640
Maybe you'd guys to bio which are carnivores,

00:25:37.640 --> 00:25:39.545
placentals, mammals, blah, blah, blah.

00:25:39.545 --> 00:25:44.135
Okay, so, um, that's the kind of stuff you can get out to- out of WordNet.

00:25:44.135 --> 00:25:47.105
Um, you know, in practice WordNet has been.

00:25:47.105 --> 00:25:49.580
Everyone sort of used to use it because it gave

00:25:49.580 --> 00:25:51.995
you some sort of sense of the meaning of the word.

00:25:51.995 --> 00:25:54.125
But you know it's also sort of well-known.

00:25:54.125 --> 00:25:56.540
It never worked that well.

00:25:56.540 --> 00:26:02.720
Um, so you know that sort of the synonym sets miss a lot of nuance.

00:26:02.720 --> 00:26:05.270
So, you know one of the synonym sets for good has

00:26:05.270 --> 00:26:08.240
proficient in it and good sort of like proficient

00:26:08.240 --> 00:26:11.495
but doesn't proficient have some more connotations and nuance?

00:26:11.495 --> 00:26:13.250
I think it does.

00:26:13.250 --> 00:26:18.080
Um, WordNet like most hand built resources is sort of very incomplete.

00:26:18.080 --> 00:26:21.290
So, as soon as you're coming to new meanings of words,

00:26:21.290 --> 00:26:23.705
or new words and slang words,

00:26:23.705 --> 00:26:25.310
well then, that gives you nothing.

00:26:25.310 --> 00:26:28.985
Um, it's sort of built with human labor,

00:26:28.985 --> 00:26:35.030
um, in ways that you know it's hard to sort of create and adapt.

00:26:35.030 --> 00:26:37.670
And in particular, what we want to focus on is,

00:26:37.670 --> 00:26:41.870
seems like a basic thing you'd like to do with words and it's actually at least

00:26:41.870 --> 00:26:45.920
understand similarities and relations between the meaning of words.

00:26:45.920 --> 00:26:49.520
And it turns out that you know WordNet doesn't actually do that that well

00:26:49.520 --> 00:26:53.600
because it just has these sort of fixed discrete synonym sets.

00:26:53.600 --> 00:26:56.090
So, if you have a words in a synonym said that there's

00:26:56.090 --> 00:26:59.075
sort of a synonym and maybe not exactly the same meaning,

00:26:59.075 --> 00:27:00.800
they're not in the same synonyms set,

00:27:00.800 --> 00:27:04.580
you kind of can't really measure the partial resemblance as a meaning for them.

00:27:04.580 --> 00:27:08.435
So, if something like good and marvelous aren't in the same synonym set,

00:27:08.435 --> 00:27:11.960
but there's something that they share in common that you'd like to represent.

00:27:11.960 --> 00:27:16.880
Okay. So, um, that's kinda turn to lead into

00:27:16.880 --> 00:27:21.935
us wanting to do something different and better for word meaning.

00:27:21.935 --> 00:27:25.730
And, um, before getting there I just sort of wanna again sort

00:27:25.730 --> 00:27:29.495
of build a little from traditional NLP.

00:27:29.495 --> 00:27:33.275
So, traditional NLP in the context of this course sort of means

00:27:33.275 --> 00:27:39.275
Natural Language Processing up until approximately 2012.

00:27:39.275 --> 00:27:43.640
There were some earlier antecedents but as basically, um,

00:27:43.640 --> 00:27:47.600
in 2013 that things really began to change with

00:27:47.600 --> 00:27:53.060
people starting to use neural net style representations for natural language processing.

00:27:53.060 --> 00:27:55.430
So, up until 2012,

00:27:55.430 --> 00:27:58.055
um, standardly you know we had words.

00:27:58.055 --> 00:28:02.210
They are just words. So, we had hotel conference motel.

00:28:02.210 --> 00:28:06.650
They were words, and we'd have you know lexicons and put words into our model.

00:28:06.650 --> 00:28:12.290
Um, and in neural networks land this is referred to as a localist representation.

00:28:12.290 --> 00:28:14.960
I'll come back to those terms again next time.

00:28:14.960 --> 00:28:20.015
But that's sort of meaning that for any concept there's sort of one particular,

00:28:20.015 --> 00:28:24.080
um, place which is the word hotel or the word motel.

00:28:24.080 --> 00:28:26.465
A way of thinking about that is to think

00:28:26.465 --> 00:28:29.615
about what happens when you build a machine learning model.

00:28:29.615 --> 00:28:34.759
So, if you have a categorical variable like you have words with the choice of word

00:28:34.759 --> 00:28:40.130
and you want to stick that into some kind of classifier in a Machine Learning Model,

00:28:40.130 --> 00:28:42.905
somehow you have to code that categorical variable,

00:28:42.905 --> 00:28:46.550
and the standard way of doing it is that you code it by having

00:28:46.550 --> 00:28:51.275
different levels of the variable which means that you have a vector,

00:28:51.275 --> 00:28:53.840
and you have, this is the word house.

00:28:53.840 --> 00:28:55.670
This is the word cat. This is the word dog.

00:28:55.670 --> 00:28:57.020
This is the word some chairs.

00:28:57.020 --> 00:28:58.190
This is the word agreeable.

00:28:58.190 --> 00:28:59.465
This is the word something else.

00:28:59.465 --> 00:29:01.415
This is the word, um,

00:29:01.415 --> 00:29:05.750
hotel, um, and this is another word for something different, right?

00:29:05.750 --> 00:29:08.075
So that you have put a one at the position

00:29:08.075 --> 00:29:11.120
and neural net land we call these one-hot vectors,

00:29:11.120 --> 00:29:12.470
and so these might be, ah,

00:29:12.470 --> 00:29:16.250
one-hot vectors for hotel and motel.

00:29:16.250 --> 00:29:19.040
So, there are a couple of things that are bad here.

00:29:19.040 --> 00:29:21.005
Um, the one that's sort of, ah,

00:29:21.005 --> 00:29:27.140
practical nuisance is you know languages have a lot of words.

00:29:27.140 --> 00:29:30.590
Ah, so, it's sort of one of those dictionaries that you might have still had in

00:29:30.590 --> 00:29:35.450
school that you probably have about 250,000 words in them.

00:29:35.450 --> 00:29:37.400
But you know, if you start getting into

00:29:37.400 --> 00:29:41.855
more technical and scientific English it's easy to get to a million words.

00:29:41.855 --> 00:29:45.690
I mean, actually the number of words that you have in a language, um,

00:29:45.690 --> 00:29:48.620
like English is actually infinite because we have

00:29:48.620 --> 00:29:52.220
these processes which are called derivational morphology,

00:29:52.220 --> 00:29:56.930
um, where you can make more words by adding endings onto existing words.

00:29:56.930 --> 00:29:59.660
So, you know you can start with something like paternalist,

00:29:59.660 --> 00:30:03.470
fatherly, and then you can sort of say from maternal,

00:30:03.470 --> 00:30:06.275
you can say paternalist, or paternalistic,

00:30:06.275 --> 00:30:10.070
paternalism and pa- I did it paternalistically.

00:30:10.070 --> 00:30:14.255
Right? Now all of these ways that you can bake bigger words by adding more stuff into it.

00:30:14.255 --> 00:30:18.905
Um, and so really you end up with an infinite space of words.

00:30:18.905 --> 00:30:22.880
Um, yeah. So that's a minor problem, right?

00:30:22.880 --> 00:30:28.275
We have very big vectors if we want to represent a sensible size vocabulary.

00:30:28.275 --> 00:30:31.990
Um, but there's a much bigger problem than that, which is, well,

00:30:31.990 --> 00:30:35.200
precisely what we want to do all the time, is we want to,

00:30:35.200 --> 00:30:38.590
sort of, understand relationships and the meaning of words.

00:30:38.590 --> 00:30:42.380
So, you know, an obvious example of this is web search.

00:30:42.380 --> 00:30:45.350
So, if I do a search for Seattle motel,

00:30:45.350 --> 00:30:48.710
it'd be useful if it also showed me results that had

00:30:48.710 --> 00:30:52.655
Seattle hotel on the page and vice versa because,

00:30:52.655 --> 00:30:55.415
you know, hotels and motels pretty much the same thing.

00:30:55.415 --> 00:30:59.900
Um, but, you know, if we have these one-hot vectors like we had before they have

00:30:59.900 --> 00:31:04.250
no s- similarity relationship between them, right?

00:31:04.250 --> 00:31:05.675
So, in math terms,

00:31:05.675 --> 00:31:07.775
these two vectors are orthogonal.

00:31:07.775 --> 00:31:10.865
No similarity relationship between them.

00:31:10.865 --> 00:31:12.650
Um, and so you,

00:31:12.650 --> 00:31:14.705
kind of, get nowhere.

00:31:14.705 --> 00:31:16.880
Now, you know, there are things that you could do,

00:31:16.880 --> 00:31:18.710
I- I just showed you WordNet's.

00:31:18.710 --> 00:31:20.840
WordNet's shows you some synonyms and stuff.

00:31:20.840 --> 00:31:22.610
So that might help a bit.

00:31:22.610 --> 00:31:24.035
There are other things you could do.

00:31:24.035 --> 00:31:25.415
You could sort of say, well wait,

00:31:25.415 --> 00:31:29.645
why don't we just build up a big table where we have a big table of,

00:31:29.645 --> 00:31:32.675
um, word similarities, and we could work with that.

00:31:32.675 --> 00:31:34.910
And, you know, people used to try and do that, right?

00:31:34.910 --> 00:31:39.770
You know, that's sort of what Google did in 2005 or something.

00:31:39.770 --> 00:31:42.080
You know, it had word similarity tables.

00:31:42.080 --> 00:31:44.510
The problem with doing that is you know,

00:31:44.510 --> 00:31:48.290
we were talking about how maybe we want 500,000 words.

00:31:48.290 --> 00:31:52.040
And if you want to build up then a word similarity table out

00:31:52.040 --> 00:31:56.060
of our pairs of words from one-hot representations,

00:31:56.060 --> 00:31:58.640
um, you- that means that the size of that table,

00:31:58.640 --> 00:32:00.380
as my math is pretty bad,

00:32:00.380 --> 00:32:02.315
is it 2.5 trillion?

00:32:02.315 --> 00:32:07.130
It's some very big number of cells in your similarity, um, matrix.

00:32:07.130 --> 00:32:09.230
So that's almost impossible to do.

00:32:09.230 --> 00:32:13.715
So, what we're gonna instead do is explore a method in which,

00:32:13.715 --> 00:32:16.670
um, we are going to represent words as vectors,

00:32:16.670 --> 00:32:18.140
in a way I'll show you just, um,

00:32:18.140 --> 00:32:21.770
a minute in such a way that just the representation of

00:32:21.770 --> 00:32:26.480
a word gives you their similarity with no further work.

00:32:26.480 --> 00:32:30.635
Okay. And so that's gonna lead into these different ideas.

00:32:30.635 --> 00:32:34.175
So, I mentioned before denotational semantics.

00:32:34.175 --> 00:32:39.115
Here's another idea for representing the meaning of words,

00:32:39.115 --> 00:32:41.980
um, which is called distributional semantics.

00:32:41.980 --> 00:32:45.140
And so the idea of distributional semantics is, well,

00:32:45.140 --> 00:32:50.900
how are we going to represent the meaning of a word is by looking at the contexts,

00:32:50.900 --> 00:32:52.925
um, in which it appears.

00:32:52.925 --> 00:32:56.510
So, this is a picture of JR Firth who was a British linguist.

00:32:56.510 --> 00:32:58.400
Um, he's famous for this saying,

00:32:58.400 --> 00:33:01.535
"You shall know a word by the company it keeps."

00:33:01.535 --> 00:33:06.950
Um, but another person who's very famous for developing this notion of meaning is, um,

00:33:06.950 --> 00:33:10.670
the philosopher Ludwig- Ludwig Wittgenstein in his later writings,

00:33:10.670 --> 00:33:13.445
which he referred to as a use theory of meeting- meaning.

00:33:13.445 --> 00:33:16.070
Well, actually he's- he used some big German word that I don't know,

00:33:16.070 --> 00:33:18.530
but, um, we'll call it a use theory of meaning.

00:33:18.530 --> 00:33:22.535
And, you know, essentially the point was, well, you know,

00:33:22.535 --> 00:33:26.780
if you can explain every- if- if you can

00:33:26.780 --> 00:33:31.160
explain what contexts it's correct to use a certain word,

00:33:31.160 --> 00:33:34.595
versus in what contexts would be the wrong word to use,

00:33:34.595 --> 00:33:38.135
this maybe gives you bad memories of doing English in high school,

00:33:38.135 --> 00:33:40.490
when people said, ah, that's the wrong word to use there,

00:33:40.490 --> 00:33:43.205
um, well, then you understand the meaning of the word, right?

00:33:43.205 --> 00:33:47.045
Um, and so that's the idea of distributional semantics.

00:33:47.045 --> 00:33:49.790
And it's been- so one of the most successful ideas in

00:33:49.790 --> 00:33:54.005
modern statistical NLP because it gives you a great way to learn about word meaning.

00:33:54.005 --> 00:33:56.615
And so what we're gonna do is we're going to say,

00:33:56.615 --> 00:33:58.925
haha, I want to know what the word banking means.

00:33:58.925 --> 00:34:01.730
So, I'm gonna grab a lot of texts,

00:34:01.730 --> 00:34:04.520
which is easy to do now when we have the World Wide Web,

00:34:04.520 --> 00:34:07.955
I'll find lots of sentences where the word banking is used,

00:34:07.955 --> 00:34:12.770
Government debt problems turning into banking crises as happened in 2009.

00:34:12.770 --> 00:34:15.845
And both these- I'm just going to say all of

00:34:15.845 --> 00:34:19.115
this stuff is the meaning of the word banking.

00:34:19.115 --> 00:34:23.750
Um, that those are the contexts in which the word banking is used.

00:34:23.750 --> 00:34:29.495
And that seems like very simple and perhaps even not quite right idea,

00:34:29.495 --> 00:34:34.880
but it turns out to be a very usable idea that does a great job at capturing meaning.

00:34:34.880 --> 00:34:38.300
And so what we're gonna do is say rather than

00:34:38.300 --> 00:34:42.950
our old localist representation we're now gonna

00:34:42.950 --> 00:34:48.215
represent words in what we call a distributed representation.

00:34:48.215 --> 00:34:51.830
And so, for the distributed representation we're still going

00:34:51.830 --> 00:34:55.655
to [NOISE] represent the meaning of a word as a numeric vector.

00:34:55.655 --> 00:34:59.480
But now we're going to say that the meaning of each word is,

00:34:59.480 --> 00:35:01.520
ah, smallish vector, um,

00:35:01.520 --> 00:35:07.760
but it's going to be a dense vector where by all of the numbers are non-zero.

00:35:07.760 --> 00:35:10.010
So the meaning of banking is going to be

00:35:10.010 --> 00:35:13.340
distributed over the dim- dimensions of this vector.

00:35:13.340 --> 00:35:19.190
Um, now, my vector here is of dimension nine because I want to keep the slide, um, nice.

00:35:19.190 --> 00:35:23.195
Um, life isn't quite that good in practice.

00:35:23.195 --> 00:35:25.970
When we do this we use a larger dimensionality,

00:35:25.970 --> 00:35:29.075
kinda, solid the minimum that people use is 50.

00:35:29.075 --> 00:35:32.330
Um, a typical number that you might use on your laptop is

00:35:32.330 --> 00:35:35.945
300 if you want to really max out performance,

00:35:35.945 --> 00:35:38.885
um, maybe 1,000, 2,000, 4,000.

00:35:38.885 --> 00:35:42.020
But, you know, nevertheless [NOISE] orders of magnitude is

00:35:42.020 --> 00:35:46.320
smaller compared to a length 500,000 vector.

00:35:46.810 --> 00:35:51.890
Okay. So we have words with their vector representations.

00:35:51.890 --> 00:35:55.790
And so since each word is going to have a vector, um,

00:35:55.790 --> 00:36:01.160
representation we then have a vector space in which we can place all of the words.

00:36:01.160 --> 00:36:03.980
Um, and that's completely unreadable, um,

00:36:03.980 --> 00:36:08.135
but if you zoom into the vector space it's still completely unreadable.

00:36:08.135 --> 00:36:10.115
But if you zoom in a bit further,

00:36:10.115 --> 00:36:13.100
um, you can find different parts of this space.

00:36:13.100 --> 00:36:16.820
So here's the part that where countries attending to,

00:36:16.820 --> 00:36:18.950
um, exist Japanese, German,

00:36:18.950 --> 00:36:21.950
French, Russian, British Australian American,

00:36:21.950 --> 00:36:25.130
um, France, Britain, Germany et cetera.

00:36:25.130 --> 00:36:27.770
And you can shift over to a different part of the space.

00:36:27.770 --> 00:36:31.040
So here's a part of the space where various verbs are,

00:36:31.040 --> 00:36:33.485
so has have, had, been, be.

00:36:33.485 --> 00:36:40.880
Oops. Um, um, [inaudible] be always was where.

00:36:40.880 --> 00:36:43.970
You can even see that some morphological forms are grouping together,

00:36:43.970 --> 00:36:46.100
and things that sort of go together like say,

00:36:46.100 --> 00:36:48.770
think expect to things that take those, kind of, compliment.

00:36:48.770 --> 00:36:50.795
He said or thought something.

00:36:50.795 --> 00:36:52.415
Um, they group together.

00:36:52.415 --> 00:36:55.010
Now, what am I actually showing you here?

00:36:55.010 --> 00:36:57.755
Um, you know, really this was built from,

00:36:57.755 --> 00:37:00.575
ah, 100 dimensional word vectors.

00:37:00.575 --> 00:37:05.630
And there is this problem is really hard to visualize 100 dimensional word vectors.

00:37:05.630 --> 00:37:09.860
So, what is actually happening here is these, um,

00:37:09.860 --> 00:37:15.110
100 dimensional word vectors are being projected down into two-dimensions,

00:37:15.110 --> 00:37:17.990
and you're so- seeing the two-dimensional view,

00:37:17.990 --> 00:37:19.790
which I'll get back to later.

00:37:19.790 --> 00:37:22.400
Um, so, on the one hand, um,

00:37:22.400 --> 00:37:24.410
whenever you see these pictures you should hold on to

00:37:24.410 --> 00:37:26.840
the your wallet because there's a huge amount of

00:37:26.840 --> 00:37:31.535
detail on the original vector space that got completely killed and went away, um,

00:37:31.535 --> 00:37:32.839
in the 2D projection,

00:37:32.839 --> 00:37:37.070
and indeed some of what push things together in the 2D,

00:37:37.070 --> 00:37:39.875
um, projection may really, really,

00:37:39.875 --> 00:37:42.590
really misrepresent what's in the original space.

00:37:42.590 --> 00:37:45.740
Um, but even looking at these 2D representations,

00:37:45.740 --> 00:37:46.850
the overall feeling is,

00:37:46.850 --> 00:37:48.920
my gosh this actually sort of works, doesn't it?

00:37:48.920 --> 00:37:54.365
Um, we can sort of see similarities, um, between words.

00:37:54.365 --> 00:38:02.375
Okay. So, um, ha- so that was the idea of what we want to do.

00:38:02.375 --> 00:38:04.310
Um, the next part, um,

00:38:04.310 --> 00:38:07.940
is then how do we actually go about doing it?

00:38:07.940 --> 00:38:10.445
I'll pause for breath for half a minute.

00:38:10.445 --> 00:38:12.710
Has anyone got a question they're dying to ask?

00:38:12.710 --> 00:38:20.300
[NOISE] Yeah.

00:38:20.300 --> 00:38:26.720
Where were the- the vectors is each, um,

00:38:26.720 --> 00:38:28.460
had a different order in each contact,

00:38:28.460 --> 00:38:30.530
like, say the first decimal vector,

00:38:30.530 --> 00:38:32.840
second decimal vector, are those standard

00:38:32.840 --> 00:38:35.475
across all theory or people choose them themselves?

00:38:35.475 --> 00:38:42.340
Um, they're not standards across NLP um and they're not chosen at all.

00:38:42.340 --> 00:38:45.055
So what we're gonna present is a learning algorithm.

00:38:45.055 --> 00:38:48.430
So where we just sort of shuffle in lots of text

00:38:48.430 --> 00:38:51.970
and miraculously these word vectors come out.

00:38:51.970 --> 00:38:57.760
And so the l- learning algorithm itself decides the dimensions.

00:38:57.760 --> 00:39:03.085
But um that actually reminds me of something I sort of meant to say which was yeah,

00:39:03.085 --> 00:39:05.425
I mean, since this is a vector space,

00:39:05.425 --> 00:39:09.580
in some sense the dimensions over the arbitrary right,

00:39:09.580 --> 00:39:12.570
because you can you know just have your basis vectors in

00:39:12.570 --> 00:39:15.945
any different direction and you could sort of re-represent,

00:39:15.945 --> 00:39:19.715
um the words in the vector space with a different set of basics,

00:39:19.715 --> 00:39:22.930
basis vectors and it'd be exactly the same vector space

00:39:22.930 --> 00:39:26.380
just sort of rotate around to your new um, vectors.

00:39:26.380 --> 00:39:30.580
So, you know, you shouldn't read too much into the sort of elements.

00:39:30.580 --> 00:39:32.860
So, it actually turns out that because of the way a lot of

00:39:32.860 --> 00:39:36.070
deep learning um operations work,

00:39:36.070 --> 00:39:38.170
some things they do, do element-wise.

00:39:38.170 --> 00:39:42.775
So that the dimensions do actually tend to get some meaning to them it turns out.

00:39:42.775 --> 00:39:46.900
But um, though I think I really wanted to say was,

00:39:46.900 --> 00:39:52.240
that you know one thing we can just think of is how close things

00:39:52.240 --> 00:39:54.250
are in the vector space and that's

00:39:54.250 --> 00:39:57.805
a notion of meaning similarity that we are going to exploit.

00:39:57.805 --> 00:40:00.640
But you might hope that you get more than that,

00:40:00.640 --> 00:40:03.010
and you might actually think that there's meaning in

00:40:03.010 --> 00:40:06.925
different dimensions and directions in the word vector space.

00:40:06.925 --> 00:40:11.335
And the answer to that is there is and I'll come back to that a bit later.

00:40:11.335 --> 00:40:17.770
Okay. Um, so in some sense this thing that had

00:40:17.770 --> 00:40:22.240
the biggest impact um in sort of turning the world of

00:40:22.240 --> 00:40:27.625
NLP in a neural networks direction was that picture.

00:40:27.625 --> 00:40:32.260
Um, was this um algorithm that um

00:40:32.260 --> 00:40:37.345
Thomas Mikolov came up with in 2013 called the word2vec algorithm.

00:40:37.345 --> 00:40:43.210
So it wasn't the first work and having distributed representations of words.

00:40:43.210 --> 00:40:45.730
So there was older work from Yoshua Bengio that went

00:40:45.730 --> 00:40:48.370
back to about the sort of turn on the millennium,

00:40:48.370 --> 00:40:52.780
that somehow it's sort of hadn't really sort of hit the world over their head and had

00:40:52.780 --> 00:40:57.730
a huge impact and has really sort of Thomas Mikolov showed this very simple,

00:40:57.730 --> 00:41:00.070
very scalable way of learning

00:41:00.070 --> 00:41:05.005
vector representations of um words and that sort of really opened the flood gates.

00:41:05.005 --> 00:41:08.650
And so that's the algorithm that I'm going to um show now.

00:41:08.650 --> 00:41:15.775
Okay. So the idea of this algorithm is you start with a big pile of text.

00:41:15.775 --> 00:41:20.650
Um, so wherever you find you know web pages on newspaper articles or something,

00:41:20.650 --> 00:41:22.480
a lot of continuous text, right?

00:41:22.480 --> 00:41:26.350
Actual sentences because we want to learn wo- word meaning context.

00:41:26.350 --> 00:41:32.470
Um, NLP people call a large pile of text a corpus.

00:41:32.470 --> 00:41:35.890
And I mean that's just the Latin word for body, right?

00:41:35.890 --> 00:41:37.915
It's a body of text.

00:41:37.915 --> 00:41:43.224
Important things to note if you want to seem really educated is in Latin,

00:41:43.224 --> 00:41:46.690
this is a fourth declensions noun.

00:41:46.690 --> 00:41:49.900
So the plural of corpus is corpora.

00:41:49.900 --> 00:41:51.190
And whereas if you say

00:41:51.190 --> 00:41:55.390
core Pi everyone will know that you didn't study Latin in high school.

00:41:55.390 --> 00:42:00.490
[LAUGHTER] Um, okay.

00:42:00.490 --> 00:42:06.460
Um, so right- so we then want to say that every word um

00:42:06.460 --> 00:42:08.890
in a- in a fixed vocabulary which would just be

00:42:08.890 --> 00:42:12.445
the vocabulary the corpus is um represented by a vector.

00:42:12.445 --> 00:42:16.615
And we just start those vectors off as random vectors.

00:42:16.615 --> 00:42:18.340
And so then what we're going to do is do

00:42:18.340 --> 00:42:22.585
this big iterative algorithm where we go through each position in the text.

00:42:22.585 --> 00:42:24.715
We say, here's a word in the text.

00:42:24.715 --> 00:42:30.520
Let's look at the words around it and what we're going to want to do is say well,

00:42:30.520 --> 00:42:32.890
the meaning of a word is its contexts of use.

00:42:32.890 --> 00:42:35.290
So we want the representation of the word

00:42:35.290 --> 00:42:37.870
in the middle to be able to predict the words that are

00:42:37.870 --> 00:42:43.720
around it and so we're gonna achieve that by moving the position of the word vector.

00:42:43.720 --> 00:42:47.500
And we just repeat that a billion times and

00:42:47.500 --> 00:42:51.190
somehow a miracle occurs and outcomes at the end we have

00:42:51.190 --> 00:42:54.790
a word vector space that looks like a picture I showed where it has

00:42:54.790 --> 00:42:59.530
a good meaning of word meet good representation of word meaning.

00:42:59.530 --> 00:43:03.090
So slightly more, um,

00:43:03.090 --> 00:43:07.240
um, slightly more um graphically right.

00:43:07.240 --> 00:43:08.440
So here's the situation.

00:43:08.440 --> 00:43:12.835
So we've got part of our corpus problems turning into banking crisis,

00:43:12.835 --> 00:43:14.290
and so what we want to say is well,

00:43:14.290 --> 00:43:17.725
we want to know the meaning of the word into and so we're going to hope that

00:43:17.725 --> 00:43:21.400
its representation can be used in a way that'll

00:43:21.400 --> 00:43:24.820
make precise to predict what words appear in

00:43:24.820 --> 00:43:28.600
the context of into because that's the meaning of into.

00:43:28.600 --> 00:43:31.525
And so we're going to try and make those predictions,

00:43:31.525 --> 00:43:34.855
see how well we can predict and then change

00:43:34.855 --> 00:43:39.475
the vector representations of words in a way that we can do that prediction better.

00:43:39.475 --> 00:43:41.320
And then once we've dealt with into,

00:43:41.320 --> 00:43:43.765
we just go onto the next word and we say,

00:43:43.765 --> 00:43:46.060
okay, let's take banking as the word.

00:43:46.060 --> 00:43:49.795
The meaning of banking is predicting the contexts in which banking occurs.

00:43:49.795 --> 00:43:51.265
Here's one context.

00:43:51.265 --> 00:43:54.550
Let's try and predict these words that occur around banking and

00:43:54.550 --> 00:43:58.735
see how we do and then we'll move on again from there.

00:43:58.735 --> 00:44:02.470
Okay. Um, sounds easy so far.

00:44:02.470 --> 00:44:06.100
Um, [NOISE] now we go on and sort of do a bit more stuff.

00:44:06.100 --> 00:44:12.460
Okay. So overall, we have a big long corpus of capital T words.

00:44:12.460 --> 00:44:17.125
So if we have a whole lot of documents we just concatenate them all together and we say,

00:44:17.125 --> 00:44:19.014
okay, here's a billion words,

00:44:19.014 --> 00:44:21.745
and so big long list of words.

00:44:21.745 --> 00:44:23.305
And so what we're gonna do,

00:44:23.305 --> 00:44:26.875
is for the first um product we're going to sort of

00:44:26.875 --> 00:44:30.954
go through all the words and then for the second product,

00:44:30.954 --> 00:44:34.630
we're gonna say- we're gonna choose some fixed size window, you know,

00:44:34.630 --> 00:44:37.990
it might be five words on each side or something and we're going to try and

00:44:37.990 --> 00:44:42.010
predict the 10 words that are around that center word.

00:44:42.010 --> 00:44:44.200
And we're going to predict in the sense of trying to

00:44:44.200 --> 00:44:46.780
predict that word given the center word.

00:44:46.780 --> 00:44:48.460
That's our probability model.

00:44:48.460 --> 00:44:51.175
And so if we multiply all those things together,

00:44:51.175 --> 00:44:54.610
that's our model likelihood is how good a job it

00:44:54.610 --> 00:44:58.375
does at predicting the words around every word.

00:44:58.375 --> 00:45:01.600
And that model likelihood is going to depend

00:45:01.600 --> 00:45:05.185
on the parameters of our model which we write as theta.

00:45:05.185 --> 00:45:07.855
And in this particular model,

00:45:07.855 --> 00:45:10.690
the only parameters in it is actually

00:45:10.690 --> 00:45:13.810
going to be the vector representations we give the words.

00:45:13.810 --> 00:45:16.945
The model has absolutely no other parameters to it.

00:45:16.945 --> 00:45:20.050
So, we're just going to say we're representing

00:45:20.050 --> 00:45:23.695
a word with a vector in a vector space and that

00:45:23.695 --> 00:45:27.880
representation of it is its meaning and we're then going to be able to

00:45:27.880 --> 00:45:32.335
use that to predict what other words occur in a way I'm about to show you.

00:45:32.335 --> 00:45:37.240
Okay. So, um, that's our likelihood and so what we do in all of

00:45:37.240 --> 00:45:42.280
these models is we sort of define an objective function and then we're going to be,

00:45:42.280 --> 00:45:45.880
I want to come up with vector representations of words in

00:45:45.880 --> 00:45:50.740
such a way as to minimize our objective function.

00:45:50.740 --> 00:45:56.380
Um, so objective function is basically the same as what's on the top half of the slide,

00:45:56.380 --> 00:45:58.045
but we change a couple of things.

00:45:58.045 --> 00:46:03.040
We stick a minus sign in front of it so we can do minimization rather than maximization.

00:46:03.040 --> 00:46:05.515
Completely arbitrary makes no difference.

00:46:05.515 --> 00:46:08.125
Um, we stick a one and T in front of it,

00:46:08.125 --> 00:46:11.800
so that we're working out the sort of average

00:46:11.800 --> 00:46:16.150
as of a goodness of predicting for each choice of center word.

00:46:16.150 --> 00:46:19.360
Again, that sort of makes no difference but it kinda keeps the scale of

00:46:19.360 --> 00:46:23.095
things ah not dependent on the size of the corpus.

00:46:23.095 --> 00:46:27.235
Um, the bit that's actually important is we stick a log in front of

00:46:27.235 --> 00:46:31.690
the function that was up there um because it turns out that everything always gets nice.

00:46:31.690 --> 00:46:33.805
So when you stick logs and find the products

00:46:33.805 --> 00:46:36.370
um when you're doing things like optimization.

00:46:36.370 --> 00:46:38.860
So, when we do that we then got a log of

00:46:38.860 --> 00:46:42.430
all these products which will allow us to turn things you know,

00:46:42.430 --> 00:46:46.300
into a sums of the log of this probability

00:46:46.300 --> 00:46:50.755
and we'll go through that again um in just a minute.

00:46:50.755 --> 00:46:55.420
Okay. Um, and so if we can mi- if we can change

00:46:55.420 --> 00:47:00.865
our vector representations of these words so as to minimize this J of theta,

00:47:00.865 --> 00:47:06.110
that means we'll be good at predicting words in the context of another word.

00:47:06.540 --> 00:47:10.450
So then, that all sounded good but it was all

00:47:10.450 --> 00:47:13.960
dependent on having this probability function where you wanna

00:47:13.960 --> 00:47:17.020
predict the probability of a word in

00:47:17.020 --> 00:47:20.635
the context given the center word and the question is,

00:47:20.635 --> 00:47:23.620
how can you possibly do that?

00:47:23.620 --> 00:47:28.390
Um, well um, remember what I said is actually our model is just gonna

00:47:28.390 --> 00:47:33.655
have vector representations of words and that was the only parameters of the model.

00:47:33.655 --> 00:47:35.650
Now, that's, that's almost true.

00:47:35.650 --> 00:47:37.105
It's not quite true.

00:47:37.105 --> 00:47:39.220
Um, we actually cheat slightly.

00:47:39.220 --> 00:47:42.400
Since we actually propose two vector representations for

00:47:42.400 --> 00:47:46.600
each word and this makes it simpler to do this.

00:47:46.600 --> 00:47:48.070
Um, you cannot do this,

00:47:48.070 --> 00:47:50.620
there are ways to get around it but this is the simplest way to do it.

00:47:50.620 --> 00:47:54.610
So we have one vector for word when it's the center word that's predicting

00:47:54.610 --> 00:47:59.500
other words but we have a second vector for each word when it's a context word,

00:47:59.500 --> 00:48:01.225
so that's one of the words in context.

00:48:01.225 --> 00:48:02.680
So for each word type,

00:48:02.680 --> 00:48:06.850
we have these two vectors as center word, as context word.

00:48:06.850 --> 00:48:12.700
Um, so then we're gonna work out this probability of a word in the context,

00:48:12.700 --> 00:48:14.574
given the center word,

00:48:14.574 --> 00:48:22.105
purely in terms of these vectors and the way we do it is with this equation right here,

00:48:22.105 --> 00:48:25.165
which I'll explain more in just a moment.

00:48:25.165 --> 00:48:29.650
So we're still on exactly the same situation, right?

00:48:29.650 --> 00:48:32.050
That we're wanting to work out probabilities of

00:48:32.050 --> 00:48:35.665
words occurring in the context of our center word.

00:48:35.665 --> 00:48:38.785
So the center word is C and the context words represented with

00:48:38.785 --> 00:48:42.370
O and these [inaudible] slide notation but sort of,

00:48:42.370 --> 00:48:44.890
we're basically saying there's one kind of

00:48:44.890 --> 00:48:47.590
vector for center words is a different kind of vector

00:48:47.590 --> 00:48:53.665
for context words and we're gonna work out this probabilistic prediction um,

00:48:53.665 --> 00:48:56.470
in terms of these word vectors.

00:48:56.470 --> 00:48:59.260
Okay. So how can we do that?

00:48:59.260 --> 00:49:02.950
Well, the way we do it is with this um,

00:49:02.950 --> 00:49:07.870
formula here which is the sort of shape that you see over and over again um,

00:49:07.870 --> 00:49:10.300
in deep learning with categorical staff.

00:49:10.300 --> 00:49:12.670
So for the very center bit of it,

00:49:12.670 --> 00:49:17.815
the bit in orange are more the same thing occurs in the um, denominator.

00:49:17.815 --> 00:49:21.130
What we're doing there is calculating a dot product.

00:49:21.130 --> 00:49:24.460
So, we're gonna go through the components of our vector and we're gonna

00:49:24.460 --> 00:49:28.750
multiply them together and that means if um,

00:49:28.750 --> 00:49:32.800
different words have B components of the same sign,

00:49:32.800 --> 00:49:35.620
plus or minus, in the same positions,

00:49:35.620 --> 00:49:38.920
the dot product will be big and if

00:49:38.920 --> 00:49:42.460
they have different signs or one is big and one is small,

00:49:42.460 --> 00:49:44.410
the dot product will be a lot smaller.

00:49:44.410 --> 00:49:48.100
So that orange part directly calculates uh,

00:49:48.100 --> 00:49:51.670
sort of a similarity between words where

00:49:51.670 --> 00:49:55.345
the similarity is the sort of vectors looking the same, right?

00:49:55.345 --> 00:49:57.610
Um, and so that's the heart of it, right?

00:49:57.610 --> 00:50:00.130
So we're gonna have words that have similar vectors,

00:50:00.130 --> 00:50:04.240
IS close together in the vector space have similar meaning.

00:50:04.240 --> 00:50:06.580
Um, so for the rest of it- um,

00:50:06.580 --> 00:50:10.330
so the next thing we do is take that number and put an X around it.

00:50:10.330 --> 00:50:12.100
So, um, the exponential has

00:50:12.100 --> 00:50:15.295
this nice property that no matter what number you stick into it,

00:50:15.295 --> 00:50:17.845
because the dot product might be positive or negative,

00:50:17.845 --> 00:50:20.890
it's gonna come out as a positive number and if

00:50:20.890 --> 00:50:24.160
we eventually wanna get a probability, um, that's really good.

00:50:24.160 --> 00:50:28.450
If we have positive numbers and not negative numbers, um, so that's good.

00:50:28.450 --> 00:50:33.370
Um, then the third part of which is the bid in blue is we wanted to have

00:50:33.370 --> 00:50:36.070
probabilities and probabilities are meant to add up to

00:50:36.070 --> 00:50:39.970
one and so we do that in the standard, dumbest possible way.

00:50:39.970 --> 00:50:42.205
We sum up what this quantity is,

00:50:42.205 --> 00:50:47.080
that every different word in our vocabulary and we divide through by

00:50:47.080 --> 00:50:52.315
it and so that normalizes things and turns them into a probability distribution.

00:50:52.315 --> 00:50:54.685
Yeah, so there's sort of in practice,

00:50:54.685 --> 00:50:55.990
there are two parts.

00:50:55.990 --> 00:50:59.110
There's the orange part which is this idea of using

00:50:59.110 --> 00:51:03.580
dot product and a vector space as our similarity measure between words

00:51:03.580 --> 00:51:07.480
and then the second part is all the rest of it where we feed it

00:51:07.480 --> 00:51:11.665
through what we refer to a news all the time as a softmax distribution.

00:51:11.665 --> 00:51:17.530
So the two parts of the expen normalizing gives you a softmax distribution.

00:51:17.530 --> 00:51:22.120
Um, and softmax functions will sort of map any numbers into

00:51:22.120 --> 00:51:26.950
a probability distribution always for the two reasons that I gave and so,

00:51:26.950 --> 00:51:30.000
it's referred to as a softmax um,

00:51:30.000 --> 00:51:33.525
because it works like a softmax, right?

00:51:33.525 --> 00:51:35.040
So if you have numbers,

00:51:35.040 --> 00:51:39.735
you could just say what's the max of these numbers, um,

00:51:39.735 --> 00:51:46.810
and you know that's sort of a hot- if you sort of map your original numbers into,

00:51:46.810 --> 00:51:49.390
if it's the max of the max and everything else is zero,

00:51:49.390 --> 00:51:51.160
that's sort of a hard max.

00:51:51.160 --> 00:51:56.935
Um, soft- this is a softmax because the exponenti- you know,

00:51:56.935 --> 00:52:00.310
if you sort of imagine this but- if we just ignore the problem

00:52:00.310 --> 00:52:04.315
negative numbers for a moment and you got rid of the exp, um,

00:52:04.315 --> 00:52:06.220
then you'd sort of coming out with

00:52:06.220 --> 00:52:09.640
a probability distribution but by and large it's so be fairly

00:52:09.640 --> 00:52:12.070
flat and wouldn't particularly pick out the max of

00:52:12.070 --> 00:52:15.310
the different XI numbers whereas when you exponentiate them,

00:52:15.310 --> 00:52:18.670
that sort of makes big numbers way bigger and so, this,

00:52:18.670 --> 00:52:25.990
this softmax sort of mainly puts mass where the max's or the couple of max's are.

00:52:25.990 --> 00:52:29.920
Um, so that's the max part and a soft part is that this isn't

00:52:29.920 --> 00:52:34.900
a hard decisions still spreads a little bit of probability mass everywhere else.

00:52:34.900 --> 00:52:40.540
Okay, so now we have uh, loss function.

00:52:40.540 --> 00:52:45.160
We have a loss function with a probability model on the inside that we can

00:52:45.160 --> 00:52:50.230
build and so what we want to be able to do is then um,

00:52:50.230 --> 00:52:55.690
move our vector representations of words around

00:52:55.690 --> 00:53:01.075
so that they are good at predicting what words occur in the context of other words.

00:53:01.075 --> 00:53:06.400
Um, and so, at this point what we're gonna do is optimization.

00:53:06.400 --> 00:53:10.465
So, we have vector components of different words.

00:53:10.465 --> 00:53:13.180
We have a very high-dimensional space again but here,

00:53:13.180 --> 00:53:16.270
I've just got two for the picture and we're gonna wanna

00:53:16.270 --> 00:53:19.510
say how- how can we minimize this function and we're going to

00:53:19.510 --> 00:53:23.920
want to jiggle the numbers that are used in the word representations in

00:53:23.920 --> 00:53:28.990
such a way that we're walking down the slope of this space.

00:53:28.990 --> 00:53:32.095
I walking down the gradient and um,

00:53:32.095 --> 00:53:37.330
then we're gonna minimize the function we found good representations for words.

00:53:37.330 --> 00:53:39.775
So doing this for this case,

00:53:39.775 --> 00:53:42.070
we want to make a very big vector in

00:53:42.070 --> 00:53:45.400
a very high-dimensional vector space of all the parameters of

00:53:45.400 --> 00:53:48.730
our model and the only parameters that this model

00:53:48.730 --> 00:53:53.095
has is literally the vector space representations of words.

00:53:53.095 --> 00:53:56.170
So if there are a 100 dimensional word representations,

00:53:56.170 --> 00:53:59.320
they're sort of a 100 parameters for aardvark and context,

00:53:59.320 --> 00:54:03.400
100 parameters for the word a- in context et cetera going through,

00:54:03.400 --> 00:54:08.020
100 parameters for the word aardvark [NOISE] as a center word et cetera,

00:54:08.020 --> 00:54:12.520
et cetera through that gives us a high big vector of parameters to

00:54:12.520 --> 00:54:18.265
optimize and we're gonna run this optimization and then um, move them down.

00:54:18.265 --> 00:54:23.740
Um, [NOISE] yeah so that's essentially what you do.

00:54:23.740 --> 00:54:26.365
Um, I sort of wanted to go through um,

00:54:26.365 --> 00:54:28.990
the details of this um,

00:54:28.990 --> 00:54:32.440
just so we've kind of gone through things concretely to

00:54:32.440 --> 00:54:36.070
make sure everyone is on the same page.

00:54:36.070 --> 00:54:39.475
Um, so I suspect that, you know,

00:54:39.475 --> 00:54:43.510
if I try and do this concretely,

00:54:43.510 --> 00:54:45.865
um, there are a lot of people um,

00:54:45.865 --> 00:54:50.830
that this will bore and some people that are- will bore very badly,

00:54:50.830 --> 00:54:54.415
um, so I apologize for you,

00:54:54.415 --> 00:54:55.810
um, but you know,

00:54:55.810 --> 00:54:59.140
I'm hoping and thinking that there's probably

00:54:59.140 --> 00:55:02.650
some people who haven't done as much of this stuff recently

00:55:02.650 --> 00:55:05.740
and it might just actually be good to do it concretely

00:55:05.740 --> 00:55:09.760
and get everyone up to speed right at the beginning. Yeah?

00:55:09.760 --> 00:55:14.680
[inaudible] how do we calculate [inaudible] specifically?

00:55:14.680 --> 00:55:20.275
Well, so, we- so the way we calculate the,

00:55:20.275 --> 00:55:26.050
the U and V vectors is we're literally going to start with a random vector for

00:55:26.050 --> 00:55:33.010
each word and then we iteratively going to change those vectors a little bit as we learn.

00:55:33.010 --> 00:55:37.135
And the way we're going to work out how to change them is we're gonna say,

00:55:37.135 --> 00:55:42.400
"I want to do optimization," and that is going to be implemented as okay.

00:55:42.400 --> 00:55:44.830
We have the current vectors for each word.

00:55:44.830 --> 00:55:51.550
Let me do some calculus to work out how I could change the word vectors, um, to mean,

00:55:51.550 --> 00:55:55.780
that the word vectors would calculate a higher probability for

00:55:55.780 --> 00:56:00.160
the words that actually occur in contexts of this center word.

00:56:00.160 --> 00:56:01.855
And we will do that,

00:56:01.855 --> 00:56:03.925
and we'll do it again and again and again,

00:56:03.925 --> 00:56:06.760
and then will eventually end up with good word vectors.

00:56:06.760 --> 00:56:08.260
Thank you for that question,

00:56:08.260 --> 00:56:10.780
cause that's a concept that you're meant to have understood.

00:56:10.780 --> 00:56:13.330
Is that how this works and maybe I didn't

00:56:13.330 --> 00:56:16.645
explain that high-level recipe well enough, yeah.

00:56:16.645 --> 00:56:20.410
Okay, so yeah, so let's just go through it. So, we've seen it, right?

00:56:20.410 --> 00:56:24.070
So, we had this formula that we wanted to maximize, you know,

00:56:24.070 --> 00:56:32.410
our original function which was the product of T equals one to T,

00:56:32.410 --> 00:56:35.995
and then the product of the words, uh,

00:56:35.995 --> 00:56:40.720
position minus M less than or equal to J,

00:56:40.720 --> 00:56:42.460
less than or equal to M,

00:56:42.460 --> 00:56:46.000
J not equal to zero of, um,

00:56:46.000 --> 00:56:51.640
the probability of W. At prime at T

00:56:51.640 --> 00:56:57.700
plus J given WT according to the parameters of our model.

00:56:57.700 --> 00:57:01.330
Okay, and then we'd already seen that we were gonna convert that

00:57:01.330 --> 00:57:05.515
into the function that we're going to use where we have J of Theta,

00:57:05.515 --> 00:57:15.490
where we had the minus one on T. Of the sum of T equals one to T of the sum of minus M,

00:57:15.490 --> 00:57:17.770
less than or equal to J less than or equal to M,

00:57:17.770 --> 00:57:27.400
J not equal to zero of the log of the probability of W times T, plus J, W,

00:57:27.400 --> 00:57:31.840
T. Okay, so we had that and then we'd had

00:57:31.840 --> 00:57:36.490
this formula that the probability of the outside word given

00:57:36.490 --> 00:57:46.360
the context word is this formula we just went through of xu ot vc over

00:57:46.360 --> 00:57:56.770
the sum of W equals one to the vocabulary size of xu wt vc.

00:57:56.770 --> 00:57:59.530
Okay, so that's sort of our model.

00:57:59.530 --> 00:58:03.835
We want to min- minimize this.

00:58:03.835 --> 00:58:11.230
So, we wanna minimize this and we want to minimize that by changing these parameters.

00:58:11.230 --> 00:58:15.415
And these parameters are the contents of these vectors.

00:58:15.415 --> 00:58:17.635
And so, what we want to do now,

00:58:17.635 --> 00:58:23.560
is do calculus and we wanna say let's work out in terms of these parameters which are,

00:58:23.560 --> 00:58:25.960
u and v vectors, um,

00:58:25.960 --> 00:58:30.115
for the current values of the parameters which we initialized randomly.

00:58:30.115 --> 00:58:32.050
Like what's the slope of the space?

00:58:32.050 --> 00:58:33.490
Where is downhill?

00:58:33.490 --> 00:58:35.770
Because if we can work out downhill is,

00:58:35.770 --> 00:58:39.115
we got just gotta walk downhill and our model gets better.

00:58:39.115 --> 00:58:42.010
So, we're gonna take derivatives and work out what

00:58:42.010 --> 00:58:45.610
direction downhill is and then we wanna walk that way, yeah.

00:58:45.610 --> 00:58:50.230
So, why do we wanna maximize that probable edge and like,

00:58:50.230 --> 00:58:51.805
like going through every word,

00:58:51.805 --> 00:58:57.640
it's like [inaudible] given the [inaudible]

00:58:57.640 --> 00:58:59.665
So, well, so, so,

00:58:59.665 --> 00:59:02.905
I'm wanting to achieve this, um,

00:59:02.905 --> 00:59:08.395
what I want to achieve for my distributional notion of meaning is,

00:59:08.395 --> 00:59:11.500
I have a meaningful word, a vector.

00:59:11.500 --> 00:59:16.810
And that vector knows what words occur in the context of,

00:59:16.810 --> 00:59:19.525
um, a word- of itself.

00:59:19.525 --> 00:59:23.020
And knowing what words occur in its context means,

00:59:23.020 --> 00:59:24.790
it can accurately give

00:59:24.790 --> 00:59:28.945
a high probability estimate to those words that occur in the context,

00:59:28.945 --> 00:59:32.319
and it will give low probability estimates

00:59:32.319 --> 00:59:35.050
to words that don't typically occur in the context.

00:59:35.050 --> 00:59:37.240
So, you know, if the word is bank,

00:59:37.240 --> 00:59:39.549
I'm hoping that words like branch,

00:59:39.549 --> 00:59:41.575
and open, and withdrawal,

00:59:41.575 --> 00:59:43.360
will be given high probability,

00:59:43.360 --> 00:59:45.445
cause they tend to occur with the word bank.

00:59:45.445 --> 00:59:49.950
And I'm hoping that some other words, um,

00:59:49.950 --> 00:59:52.740
like neural network or something have

00:59:52.740 --> 00:59:57.970
a lower probability because they don't tend to occur with the word bank.

00:59:58.290 --> 01:00:01.525
Okay, um, does that make sense?

01:00:01.525 --> 01:00:01.780
Yeah.

01:00:01.780 --> 01:00:03.730
Yeah. And the other thing I was,

01:00:03.730 --> 01:00:06.865
I'd forgotten meant to comment was, you know, obviously,

01:00:06.865 --> 01:00:10.480
we're not gonna be able to do this super well or it's just not gonna be able,

01:00:10.480 --> 01:00:13.180
that we can say all the words in the context is going to

01:00:13.180 --> 01:00:15.880
be this word with probability 0.97, right?

01:00:15.880 --> 01:00:19.750
Because we're using this one simple probability distribution

01:00:19.750 --> 01:00:23.230
to predict all words in our context.

01:00:23.230 --> 01:00:27.880
So, in particular, we're using it to predict 10 different words generally, right?

01:00:27.880 --> 01:00:32.425
So, at best, we can kind of be giving sort of five percent chance to one of them, right?

01:00:32.425 --> 01:00:33.820
We can't possibly be,

01:00:33.820 --> 01:00:35.950
so guessing right every time.

01:00:35.950 --> 01:00:37.390
Um, and well, you know,

01:00:37.390 --> 01:00:40.255
they're gonna be different contexts with different words in them.

01:00:40.255 --> 01:00:44.610
So, you know, it's gonna be a very loose model,

01:00:44.610 --> 01:00:48.660
but nevertheless, we wanna capture the fact that, you know,

01:00:48.660 --> 01:00:51.330
withdrawal is much more likely, um,

01:00:51.330 --> 01:00:57.580
to occur near the word bank than something like football.

01:00:57.580 --> 01:01:01.030
That's, you know, basically what our goal is.

01:01:01.030 --> 01:01:07.360
Okay, um, yes, so we want to maximize this,

01:01:07.360 --> 01:01:12.610
by minimizing this, which means we then want to do some calculus to work this out.

01:01:12.610 --> 01:01:14.740
So, what we're then gonna do is,

01:01:14.740 --> 01:01:16.720
that we're going to say, well,

01:01:16.720 --> 01:01:19.495
these parameters are our word vectors

01:01:19.495 --> 01:01:22.630
and we're gonna sort of want to move these word vectors,

01:01:22.630 --> 01:01:28.180
um, to, um, work things out as to how to, um, walk downhill.

01:01:28.180 --> 01:01:32.440
So, the case that I'm going to do now is gonna look at the parameters of

01:01:32.440 --> 01:01:38.275
this center word vc and work out how to do things with respect to it.

01:01:38.275 --> 01:01:40.750
Um, now, that's not the only thing that you wanna do,

01:01:40.750 --> 01:01:44.905
you also want to work out the slope with respect to the uo vector.

01:01:44.905 --> 01:01:47.965
Um, but I'm not gonna do that because time in class is going to run out.

01:01:47.965 --> 01:01:49.750
So, it'd be really good if you did that one at

01:01:49.750 --> 01:01:51.715
home and then you'd feel much more competent.

01:01:51.715 --> 01:01:57.130
Right, so then, um, so what I'm wanting you to do is work out the partial derivative with

01:01:57.130 --> 01:02:03.205
respect to my vc vector representation of this quantity,

01:02:03.205 --> 01:02:04.810
that we were just looking at.

01:02:04.810 --> 01:02:08.290
Which is, um, the quantity in here,

01:02:08.290 --> 01:02:11.980
um, where we're taking the log of that quantity.

01:02:11.980 --> 01:02:17.560
Right, the log of the x of u,

01:02:17.560 --> 01:02:20.140
o, T, v, c,

01:02:20.140 --> 01:02:26.830
over the sum of W equals one to V of the x of u,

01:02:26.830 --> 01:02:30.220
o, T, v, c. Okay,

01:02:30.220 --> 01:02:33.219
so this, um, so now we have a log of the division,

01:02:33.219 --> 01:02:35.695
so that's easy to rewrite, um,

01:02:35.695 --> 01:02:39.595
that we have a partial derivative of the log of

01:02:39.595 --> 01:02:47.560
the numerator minus and

01:02:47.560 --> 01:02:49.690
I can distribute the partial derivative.

01:02:49.690 --> 01:02:53.395
So, I can have minus the partial derivative,

01:02:53.395 --> 01:02:56.680
um, of the denominator,

01:02:56.680 --> 01:02:59.710
um, which is log of this thing.

01:02:59.710 --> 01:03:08.865
[NOISE]

01:03:08.865 --> 01:03:18.190
Okay. Um, so this is sort of what was the numerator and this is what was the denominator.

01:03:19.190 --> 01:03:27.060
Okay. So, um, the part that was the numerator is really easy.

01:03:27.060 --> 01:03:29.130
In fact maybe I can fit it in here.

01:03:29.130 --> 01:03:33.450
Um, so log on exp are just inverses of each other,

01:03:33.450 --> 01:03:34.800
so they cancel out.

01:03:34.800 --> 01:03:43.650
So, we've got the partial derivative of U_o T V_c.

01:03:43.650 --> 01:03:47.460
Okay, so this point I should, um, just, um,

01:03:47.460 --> 01:03:51.630
remind people right that this V_c here's a vector of- um,

01:03:51.630 --> 01:03:56.130
it's still a vector right because we had a 100 dimensional representation of a word.

01:03:56.130 --> 01:04:00.330
Um, so this is doing multivariate calculus.

01:04:00.330 --> 01:04:02.790
Um, so you know, if you're,

01:04:02.790 --> 01:04:04.530
if you at all, um,

01:04:04.530 --> 01:04:06.105
remember any of this stuff,

01:04:06.105 --> 01:04:08.175
you can say, "Ha this is trivial".

01:04:08.175 --> 01:04:12.390
The answer to that is you are done, um and that's great.

01:04:12.390 --> 01:04:14.955
But you know, if you're, um, feeling, um,

01:04:14.955 --> 01:04:17.550
not so good on all of this stuff, um,

01:04:17.550 --> 01:04:19.125
and you wanna sort of, um,

01:04:19.125 --> 01:04:22.440
cheat a little on the side and try and work out what it is,

01:04:22.440 --> 01:04:24.180
um, you can sort of say,

01:04:24.180 --> 01:04:25.980
"Well, let me um,,

01:04:25.980 --> 01:04:28.380
work out the partial derivative,

01:04:28.380 --> 01:04:34.200
um with respect to one element of this vector like the first element of this vector".

01:04:34.200 --> 01:04:42.869
Well, what I actually got here for this dot product is I have U_o one times V_c one,

01:04:42.869 --> 01:04:49.560
plus U_o two times V_c two plus dot, dot,

01:04:49.560 --> 01:04:56.910
dot plus U_o 100 times V_c 100, right,

01:04:56.910 --> 01:05:02.535
and I'm finding the partial derivative of this with respect to V_c one,

01:05:02.535 --> 01:05:05.490
and hopefully remember that much calculus from high school

01:05:05.490 --> 01:05:09.135
of none of these terms involve V_c one.

01:05:09.135 --> 01:05:12.660
So, the only thing that's left is this U_o one,

01:05:12.660 --> 01:05:15.960
and that's what I've got there for this dimension.

01:05:15.960 --> 01:05:17.850
So, this particular parameter.

01:05:17.850 --> 01:05:23.265
But I don't only want to do the first component of the V_c vector,

01:05:23.265 --> 01:05:26.745
I also want to do the second component of the V_c vector et cetera,

01:05:26.745 --> 01:05:30.630
which means I'm going to end up with all of them

01:05:30.630 --> 01:05:35.685
turning up in precisely one of these things.

01:05:35.685 --> 01:05:41.190
Um, and so the end result is I get the vector U_o.

01:05:41.190 --> 01:05:43.620
Okay. Um, but you know,

01:05:43.620 --> 01:05:47.220
if you're sort of getting confused and your brain is falling apart,

01:05:47.220 --> 01:05:52.050
I think it can be sort of kind of useful to re- reduce things to sort of um,

01:05:52.050 --> 01:05:58.275
single dimensional calculus and actually sort of play out what's actually happening.

01:05:58.275 --> 01:06:00.840
Um, anyway, this part was easy.

01:06:00.840 --> 01:06:03.540
The numerator, we get um, U_o.

01:06:03.540 --> 01:06:08.085
Um, so things aren't quite so nice when we do the denominator.

01:06:08.085 --> 01:06:11.640
So we now want to have this, um, B_d,

01:06:11.640 --> 01:06:17.010
V_c of the log of the sum of W equals

01:06:17.010 --> 01:06:22.845
one to the P_x of U_o T V_c.

01:06:22.845 --> 01:06:25.800
Okay. So, now at this point,

01:06:25.800 --> 01:06:27.450
I'm not quite so pretty.

01:06:27.450 --> 01:06:31.035
We've got this log sum X combination that you see a lot,

01:06:31.035 --> 01:06:35.640
and so at this point you have to remember that there was E, the chain rule.

01:06:35.640 --> 01:06:38.520
Okay. So, what we can say is here's you know,

01:06:38.520 --> 01:06:42.540
our function F and here is the body of the function,

01:06:42.540 --> 01:06:46.245
and so what we want to do is um,

01:06:46.245 --> 01:06:48.630
do it in two stages.

01:06:48.630 --> 01:06:51.570
Um, so that at the end of the day,

01:06:51.570 --> 01:06:53.430
we've got this V_c at the end.

01:06:53.430 --> 01:06:57.105
So, we have sort of some function here.

01:06:57.105 --> 01:06:59.910
There's ultimately a function of V_c,

01:06:59.910 --> 01:07:02.220
and so we gonna do with a chain rule.

01:07:02.220 --> 01:07:05.040
We'll say the chain rule is we first take

01:07:05.040 --> 01:07:09.135
the derivative of this outside thing putting in this body,

01:07:09.135 --> 01:07:13.680
and then we remember that the derivative of log is one on X.

01:07:13.680 --> 01:07:22.920
So, we have one over the sum of W equals one to V of the exp of U_o T V_c

01:07:22.920 --> 01:07:26.640
and then we need to multiply that by then taking

01:07:26.640 --> 01:07:32.610
the derivative of the inside part which is um,

01:07:32.610 --> 01:07:34.750
what we have here.

01:07:40.490 --> 01:07:44.850
Okay. Times the derivative of the inside part with

01:07:44.850 --> 01:07:48.600
the important reminder that you need to do a change of variables,

01:07:48.600 --> 01:07:53.460
and for the inside part use a different variable that you're summing over.

01:07:53.460 --> 01:08:00.810
Okay. So, now we're trying to find the derivative of a sum of X.

01:08:00.810 --> 01:08:05.055
The first thing that we can do is v-very easy.

01:08:05.055 --> 01:08:08.865
We can move the derivative inside a sum.

01:08:08.865 --> 01:08:14.430
So, we can rewrite that and have at the sum first of the X equals one to

01:08:14.430 --> 01:08:20.430
V of the partial derivatives with respect to V_c of the [inaudible].

01:08:20.430 --> 01:08:22.575
Um, so that's a little bit of progress.

01:08:22.575 --> 01:08:26.730
Um and that point we have to sort of do the chain rule again, right.

01:08:26.730 --> 01:08:33.210
So, here is our function and here's the thing in it again which is some function of V_c.

01:08:33.210 --> 01:08:37.605
So, we again want to do um, the chain rule.

01:08:37.605 --> 01:08:41.340
So, [NOISE] we then have well,

01:08:41.340 --> 01:08:45.720
the derivative of X um, is exp.

01:08:45.720 --> 01:08:54.630
So, we gonna have the sum of X equals one to V of exp of U_x T V_c,

01:08:54.630 --> 01:09:00.150
and then we're going to multiply that by the partial derivative with

01:09:00.150 --> 01:09:05.700
respect to T V_c of the inside U_x T V_c.

01:09:05.700 --> 01:09:08.160
Well, we saw that one before, so,

01:09:08.160 --> 01:09:13.200
the derivative of that is U- well,

01:09:13.200 --> 01:09:16.320
yeah, U_x because we're doing it through a different X, right.

01:09:16.320 --> 01:09:18.780
This then becomes out as U_x,

01:09:18.780 --> 01:09:23.850
and so we have the sum of the X equals one to

01:09:23.850 --> 01:09:30.030
V of this exp U X T B C times the U_of X.

01:09:30.030 --> 01:09:34.995
Okay. So, by doing the chain rule twice, we've got that.

01:09:34.995 --> 01:09:38.190
So, now if we put it together, you know,

01:09:38.190 --> 01:09:43.050
the derivative of V_c with respect of the whole thing,

01:09:43.050 --> 01:09:46.500
this log of the probability of O given C, right.

01:09:46.500 --> 01:09:51.210
That for the numerator it was just U_o,

01:09:51.210 --> 01:09:54.030
and then we're subtracting,

01:09:54.030 --> 01:09:57.645
we had this term here, um,

01:09:57.645 --> 01:09:59.730
which is sort of a denominator,

01:09:59.730 --> 01:10:03.870
and then we have this term here which is the numerator.

01:10:03.870 --> 01:10:07.725
So, we're subtracting in the numerator,

01:10:07.725 --> 01:10:12.270
we have the sum of X equals one to V of

01:10:12.270 --> 01:10:18.765
the exp of U_x T V_c times U_x,

01:10:18.765 --> 01:10:25.395
and then in the denominator, we have um,

01:10:25.395 --> 01:10:35.920
the sum of W equals one to V of exp of U_w T V_c.

01:10:36.350 --> 01:10:40.035
Um, okay, so we kind of get that.

01:10:40.035 --> 01:10:44.025
Um, oh wait. Yeah. Yeah, I've gotten.

01:10:44.025 --> 01:10:45.900
Yeah, that's right. Um, okay.

01:10:45.900 --> 01:10:52.170
We kind of get that and then we can sort of just re-arrange this a little.

01:10:52.170 --> 01:10:56.475
So, we can have this sum right out front,

01:10:56.475 --> 01:11:03.284
and we can say that this is sort of a big sum of X equals one to V,

01:11:03.284 --> 01:11:09.870
and we can sort of take that U_x out the end and say, okay.

01:11:09.870 --> 01:11:13.155
Let's call that put over here a U_x,

01:11:13.155 --> 01:11:15.090
and if we do that,

01:11:15.090 --> 01:11:20.295
sort of an interesting thing has happened because look right here,

01:11:20.295 --> 01:11:25.830
we've rediscovered exactly the same form

01:11:25.830 --> 01:11:31.425
that we use as our probability distribution for predicting the probability of words.

01:11:31.425 --> 01:11:37.860
So, this is now simply the probability of X given C according to our model.

01:11:37.860 --> 01:11:46.155
Um, so we can rewrite this and say that what we're getting is U_o minus the sum of

01:11:46.155 --> 01:11:54.795
X equals one to V of the probability of X given C times U_x.

01:11:54.795 --> 01:11:58.755
This has a kind of an interesting meaning if you think about it.

01:11:58.755 --> 01:12:01.365
So, this is actually giving us, you know,

01:12:01.365 --> 01:12:04.200
our slope in this multi-dimensional space

01:12:04.200 --> 01:12:07.215
and how we're getting that slope is we're taking

01:12:07.215 --> 01:12:11.280
the observed representation of

01:12:11.280 --> 01:12:18.450
the context word and we're subtracting from that what our model thinks um,

01:12:18.450 --> 01:12:20.955
the context should look like.

01:12:20.955 --> 01:12:24.465
What does the model think that the context should look like?

01:12:24.465 --> 01:12:27.330
This part here is formal in expectation.

01:12:27.330 --> 01:12:31.395
So, what you're doing is you're finding the weighted average

01:12:31.395 --> 01:12:36.375
of the models of the representations of each word,

01:12:36.375 --> 01:12:39.990
multiplied by the probability of it in the current model.

01:12:39.990 --> 01:12:45.315
So, this is sort of the expected context word according to our current model,

01:12:45.315 --> 01:12:47.460
and so we're taking the difference between

01:12:47.460 --> 01:12:52.170
the expected context word and the actual context word that showed up,

01:12:52.170 --> 01:12:55.560
and that difference then turns out to exactly give

01:12:55.560 --> 01:12:58.890
us the slope as to which direction we should be

01:12:58.890 --> 01:13:01.050
walking changing the words

01:13:01.050 --> 01:13:06.715
representation in order to improve our model's ability to predict.

01:13:06.715 --> 01:13:11.565
Okay. Um, so we'll,

01:13:11.565 --> 01:13:14.100
um, assignment two, um, yeah.

01:13:14.100 --> 01:13:18.060
So, um, it'll be a great exercise for you guys,

01:13:18.060 --> 01:13:20.115
um, to in- um,

01:13:20.115 --> 01:13:22.830
to try and do that for the cen-, wait,

01:13:22.830 --> 01:13:26.640
um I did the center words trying to look context words as well

01:13:26.640 --> 01:13:31.125
and show you that you can do the same kind of piece of math and have it work out.

01:13:31.125 --> 01:13:35.655
Um, if I've just got a few minutes left at the end.

01:13:35.655 --> 01:13:43.320
Um, what I just wanted to show you if I can get all of this to work right.

01:13:43.320 --> 01:13:49.950
Um, let's go [inaudible] this way.

01:13:49.950 --> 01:13:53.590
Okay, find my.

01:13:54.200 --> 01:14:00.075
Okay. Um, so I just wanted to just show you a quick example.

01:14:00.075 --> 01:14:01.935
So, for the first assignment,

01:14:01.935 --> 01:14:04.170
um, again it's an iPython Notebook.

01:14:04.170 --> 01:14:09.015
So, if you're all set up you sort of can do Jupyter Notebook.

01:14:09.015 --> 01:14:12.945
Um, and you have some notebook.

01:14:12.945 --> 01:14:16.630
Um, here's my little notebook I'm gonna show you,

01:14:17.180 --> 01:14:23.620
um, and the trick will be to make this big enough that people can see it.

01:14:30.940 --> 01:14:35.530
That readable? [LAUGHTER] Okay, um,

01:14:35.530 --> 01:14:39.210
so right so, so Numpy is the sort of,

01:14:39.210 --> 01:14:41.930
um, do math package in Python.

01:14:41.930 --> 01:14:43.120
You'll want to know about that.

01:14:43.120 --> 01:14:44.445
If you don't know about it.

01:14:44.445 --> 01:14:46.440
Um, Matplotlib is sort of the,

01:14:46.440 --> 01:14:49.040
one of the most basic graphing package

01:14:49.040 --> 01:14:51.755
if you don't know about that you're going to want to know about it.

01:14:51.755 --> 01:14:55.900
This is sort of an IPython or Jupyter special that

01:14:55.900 --> 01:14:59.755
lets you have an interactive matplotlib um, inside.

01:14:59.755 --> 01:15:03.675
And if you want to get fancy you can play it- play with your graphic styles.

01:15:03.675 --> 01:15:06.615
Um, there's that.

01:15:06.615 --> 01:15:10.465
Scikit-learn is kind of a general machine learning package.

01:15:10.465 --> 01:15:13.350
Um, Gensim isn't a deep learning package.

01:15:13.350 --> 01:15:17.590
Gensim is kind of a word similarity package which started off um,

01:15:17.590 --> 01:15:20.760
with um, methods like Latent Dirichlet analysis.

01:15:20.760 --> 01:15:22.530
If you know about that from modelling words

01:15:22.530 --> 01:15:25.940
similarities that sort of grown as a good package um,

01:15:25.940 --> 01:15:28.570
for doing um, word vectors as well.

01:15:28.570 --> 01:15:31.650
So, it's quite often used for word vectors and

01:15:31.650 --> 01:15:36.105
word similarities that sort of efficient for doing things at large-scale.

01:15:36.105 --> 01:15:37.720
Um, yeah.

01:15:37.720 --> 01:15:41.360
So, I haven't yet told you about will next time we have

01:15:41.360 --> 01:15:46.400
our own homegrown form of word vectors which are the GloVe word vectors.

01:15:46.400 --> 01:15:51.270
I'm using them not because it really matters for what I'm showing but you know,

01:15:51.270 --> 01:15:55.745
these vectors are conveniently small.

01:15:55.745 --> 01:16:00.470
It turns out that the vectors that Facebook and Google

01:16:00.470 --> 01:16:05.940
distribute are extremely large vocabulary and extremely high dimensional.

01:16:05.940 --> 01:16:08.940
So take me just too long to load them in

01:16:08.940 --> 01:16:12.860
the last five minutes of this class where conveniently uh,

01:16:12.860 --> 01:16:16.865
in our Stanford vectors we have a 100 dimensional vectors, um,

01:16:16.865 --> 01:16:19.160
and 50 dimensional vectors which are kinda

01:16:19.160 --> 01:16:21.755
good for doing small things on a laptop frankly.

01:16:21.755 --> 01:16:27.330
Um, so, what I'm doing here is Gensim doesn't natively support

01:16:27.330 --> 01:16:30.210
GloVe vectors but they actually provide a utility that

01:16:30.210 --> 01:16:33.390
converts the GloVe file format to the word2vec file format.

01:16:33.390 --> 01:16:40.285
So I've done that. And then I've loaded a pre-trained model of word vectors.

01:16:40.285 --> 01:16:44.430
Um, and, so this is what they call a keyed vector.

01:16:44.430 --> 01:16:46.890
And so, the keyed vector is nothing fancy.

01:16:46.890 --> 01:16:51.660
It's just you have words like potato and there's a vector that hangs off each one.

01:16:51.660 --> 01:16:55.435
So it's really just sort of a big dictionary with a vector for each thing.

01:16:55.435 --> 01:16:58.690
But, so this model has been a trained model where

01:16:58.690 --> 01:17:02.230
we just use the kind of algorithm we looked at and,

01:17:02.230 --> 01:17:06.730
you know, trained at billions of times fiddling our word vectors.

01:17:06.730 --> 01:17:11.255
Um, and once we have one we can then, um,

01:17:11.255 --> 01:17:14.265
ask questions like, we can say,

01:17:14.265 --> 01:17:17.110
what is the most similar word to some other words?

01:17:17.110 --> 01:17:19.650
So we could take something like, um,

01:17:19.650 --> 01:17:23.175
what are the most similar words to Obama let's say?

01:17:23.175 --> 01:17:25.770
And we get back Barrack, Bush, Clinton,

01:17:25.770 --> 01:17:29.040
McCain, Gore, Hillary Dole, Martin, Henry.

01:17:29.040 --> 01:17:31.425
That seems actually kind of interesting.

01:17:31.425 --> 01:17:34.050
These factors from a few years ago.

01:17:34.050 --> 01:17:37.150
So we don't have a post- post-Obama staff.

01:17:37.150 --> 01:17:40.750
I mean if you put in another word, um, you know,

01:17:40.750 --> 01:17:44.100
we can put in something like banana and we get coconut,

01:17:44.100 --> 01:17:46.600
mango, bananas, potato, pineapple.

01:17:46.600 --> 01:17:49.430
We get kind of tropical food.

01:17:49.430 --> 01:17:54.070
So, you can actually- you can actually ask uh,

01:17:54.070 --> 01:17:56.995
for being dissimilar to words.

01:17:56.995 --> 01:17:59.700
By itself dissimilar isn't very useful.

01:17:59.700 --> 01:18:04.550
So if I ask most similar and I say um,

01:18:04.550 --> 01:18:09.290
negative equals, um, banana,

01:18:09.290 --> 01:18:14.720
um, I'm not sure what your concept of what's most dissimilar to,

01:18:14.720 --> 01:18:16.620
um, banana is, but you know,

01:18:16.620 --> 01:18:22.655
actually by itself you don't get anything useful out of this, um,

01:18:22.655 --> 01:18:28.000
because, um, you just so get these weird really rare words um,

01:18:28.000 --> 01:18:31.440
which, um, [LAUGHTER] definitely weren't the ones who are thinking of.

01:18:31.440 --> 01:18:37.570
Um, but it turns out you can do something really useful with this negative idea

01:18:37.570 --> 01:18:39.000
which was one of

01:18:39.000 --> 01:18:44.175
the highly celebrated results of word vectors when they first started off.

01:18:44.175 --> 01:18:50.205
And that was this idea that there is actually dimensions of meaning in this space.

01:18:50.205 --> 01:18:54.820
And so this was the most celebrated example um, which was look,

01:18:54.820 --> 01:18:59.985
what we could do is we could start with the word king and subtract

01:18:59.985 --> 01:19:05.355
from it the meaning of man and then we could add to it the meaning of woman.

01:19:05.355 --> 01:19:09.110
And then we could say which word in our vector space as

01:19:09.110 --> 01:19:13.060
most similar in meaning to that word.

01:19:13.060 --> 01:19:15.810
And that would be a way of sort of doing analogies.

01:19:15.810 --> 01:19:18.640
Would be able to do the, um, analogy,

01:19:18.640 --> 01:19:22.050
man is the king as woman is to what?

01:19:22.050 --> 01:19:26.500
And so, the way we're gonna do that is to say we want to be similar to king

01:19:26.500 --> 01:19:31.220
and woman because they're both positive ones and far away from man.

01:19:31.220 --> 01:19:35.185
And so, we could do that manually,

01:19:35.185 --> 01:19:36.950
here is said manually,

01:19:36.950 --> 01:19:41.050
most similar positive woman king, negative man.

01:19:41.050 --> 01:19:45.410
And we can run this and lo and behold it produces queen.

01:19:45.410 --> 01:19:48.570
To make that a little bit easier I defined this analogy,

01:19:48.570 --> 01:19:53.485
um, analogy predicates so I can run other ones.

01:19:53.485 --> 01:19:59.095
And so I can run another one like analogy Japan Japanese,

01:19:59.095 --> 01:20:01.160
Austria is to Austrian.

01:20:01.160 --> 01:20:03.125
Um, and you know,

01:20:03.125 --> 01:20:07.150
I think it's fair to say that when people first

01:20:07.150 --> 01:20:10.950
saw that you could have this simple piece of math and run it,

01:20:10.950 --> 01:20:12.950
and learn meanings of words.

01:20:12.950 --> 01:20:18.465
I mean it actually just sort of blew people's minds how effective this was.

01:20:18.465 --> 01:20:22.030
You know. Like there- there's is no mirrors and strings here, right?

01:20:22.030 --> 01:20:24.225
You know it's not that I have a separate-

01:20:24.225 --> 01:20:28.330
a special sort of list in my Python where there's a difficult I'm looking up,

01:20:28.330 --> 01:20:30.240
er, for Austria Austrian,

01:20:30.240 --> 01:20:31.910
uh, and things like that.

01:20:31.910 --> 01:20:35.310
But somehow these vector representations are

01:20:35.310 --> 01:20:38.760
such that it is actually encoding these semantic relationships,

01:20:38.760 --> 01:20:40.920
you know, so you can try different ones,

01:20:40.920 --> 01:20:43.360
you know, like it's not that only this one works.

01:20:43.360 --> 01:20:46.185
I can put in France, it says French.

01:20:46.185 --> 01:20:49.780
I can put in Germany, it says German,

01:20:49.780 --> 01:20:54.590
I can put in Australia not Austria and it says Australian,

01:20:54.590 --> 01:20:59.480
you know that somehow if you want this vector representations of words that

01:20:59.480 --> 01:21:04.815
for sort of these ideas like understanding the relationships between words,

01:21:04.815 --> 01:21:10.595
you're just doing this vector space manipulation on these 100 dimensional numbers,

01:21:10.595 --> 01:21:15.830
that it actually knows about them.This not only the similarities of word meanings but

01:21:15.830 --> 01:21:18.255
actually different semantic relationships

01:21:18.255 --> 01:21:21.635
between words like country names and their peoples.

01:21:21.635 --> 01:21:23.850
And yeah that's actually pretty amazing.

01:21:23.850 --> 01:21:31.340
It really-you know, it's sort of surprising that running such a dumb algorithm on um,

01:21:31.340 --> 01:21:35.100
vectors of numbers could capture so well the meaning of words.

01:21:35.100 --> 01:21:38.160
And so that's sort of became the foundation of a lot of sort

01:21:38.160 --> 01:21:41.355
of modern distributed neural representations of words.

01:21:41.355 --> 01:21:42.630
Okay I'll stop there.

01:21:42.630 --> 01:21:46.700
Thanks a lot guys and see you on Thursday. [NOISE]

