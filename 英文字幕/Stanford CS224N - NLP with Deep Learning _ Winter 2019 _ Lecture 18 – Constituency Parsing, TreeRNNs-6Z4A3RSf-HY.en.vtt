WEBVTT
Kind: captions
Language: en

00:00:04.850 --> 00:00:07.410
Okay. Hi everyone.

00:00:07.410 --> 00:00:11.430
Let's get started [NOISE] Okay.

00:00:11.430 --> 00:00:14.280
So, so for today's lecture,

00:00:14.280 --> 00:00:20.025
what we're gonna do is look at the topic of having Tree Recursive Neural Networks.

00:00:20.025 --> 00:00:22.110
I mean, this is actually, uh,

00:00:22.110 --> 00:00:26.660
a topic which I feel especially fond of and attached to,

00:00:26.660 --> 00:00:32.975
because actually when we started doing deep learning for NLP here at Stanford in 2010,

00:00:32.975 --> 00:00:37.940
really for the sort of period from 2010 to 2015,

00:00:37.940 --> 00:00:42.320
the dominant set of ideas that we were working on was this topic of how you

00:00:42.320 --> 00:00:46.945
could build a recur- recursive tree structure into neural networks.

00:00:46.945 --> 00:00:50.645
So in a way, it's kind of funny that I'm only getting to it now.

00:00:50.645 --> 00:00:53.420
I mean, there are sort of reasons for that,

00:00:53.420 --> 00:00:56.720
but I think there are a bunch of interesting ideas

00:00:56.720 --> 00:01:00.365
here which relate closely to linguistic structure,

00:01:00.365 --> 00:01:02.720
and so it's good stuff to have seen.

00:01:02.720 --> 00:01:04.625
But in practice, um,

00:01:04.625 --> 00:01:08.105
these ideas have proven kind of hard to scale

00:01:08.105 --> 00:01:11.870
and not necessarily to work better in practice than

00:01:11.870 --> 00:01:15.140
the kind of things that we've spent more time on

00:01:15.140 --> 00:01:19.100
meaning things like looking at LSTMs and looking at transformers,

00:01:19.100 --> 00:01:20.570
and things like that.

00:01:20.570 --> 00:01:25.860
And so that's kinda why we sort of shunted them towards the end of the curriculum.

00:01:25.860 --> 00:01:29.090
But I want to sort of say something about the motivations,

00:01:29.090 --> 00:01:31.190
and the ways you can build tree structures,

00:01:31.190 --> 00:01:34.220
and neural networks, and look at some of the possibilities,

00:01:34.220 --> 00:01:36.065
um, we explored um,

00:01:36.065 --> 00:01:38.170
in during this class.

00:01:38.170 --> 00:01:44.690
Um, another fact about this class is actually this is the last class I'm going to give.

00:01:44.690 --> 00:01:47.060
Um, so two more classes next week.

00:01:47.060 --> 00:01:49.320
Don't forget about next week, um,

00:01:49.320 --> 00:01:52.275
CS224N classes, um,

00:01:52.275 --> 00:01:54.220
but on Tuesday, um,

00:01:54.220 --> 00:01:57.280
we've gotten the final invited speaker, ,

00:01:57.280 --> 00:02:00.410
who's a great speaker and has tons of interesting stuff

00:02:00.410 --> 00:02:04.865
to say about fairness and ethics in NLP and AI.

00:02:04.865 --> 00:02:06.425
And then for the final lecture,

00:02:06.425 --> 00:02:09.650
one of my- another of my PhD students  is

00:02:09.650 --> 00:02:13.270
gonna give that and talk about some of the recent,

00:02:13.270 --> 00:02:16.965
what's been happening in deep learning in 2018, '19,

00:02:16.965 --> 00:02:20.975
of some of the sort of recent developments in NLP and deep learning. Um,

00:02:20.975 --> 00:02:24.875
so, um, let's- I'll say my farewells at the end of this one.

00:02:24.875 --> 00:02:28.730
Um, so hopefully, everyone has submitted, um,

00:02:28.730 --> 00:02:33.380
their, um, milestone for their final project.

00:02:33.380 --> 00:02:37.310
If you haven't, you should really begin your milestone in- um,

00:02:37.310 --> 00:02:40.640
you know, it's inevitable that somewhere around here,

00:02:40.640 --> 00:02:46.290
there start to be problems that people have the situation that nothing works,

00:02:46.290 --> 00:02:48.855
and everything is too slow, and you panic.

00:02:48.855 --> 00:02:52.095
Um, and, um, this happens.

00:02:52.095 --> 00:02:54.030
Um, I wish you luck, of course.

00:02:54.030 --> 00:02:55.900
I mean, what can you do about it?

00:02:55.900 --> 00:03:00.785
I mean, it can be really hard when you have things that don't work as to work out,

00:03:00.785 --> 00:03:02.255
why they don't work,

00:03:02.255 --> 00:03:03.800
and how to fix them.

00:03:03.800 --> 00:03:08.660
I mean, I think often the best thing to do is really to go back to something

00:03:08.660 --> 00:03:14.405
simple that you can get working and to work forward from there again.

00:03:14.405 --> 00:03:18.980
It also really helps to have really small data sets.

00:03:18.980 --> 00:03:23.540
I really recommend the strategy of sort of having a 10-item,

00:03:23.540 --> 00:03:28.510
or 20-item data set and checking that your model works perfectly,

00:03:28.510 --> 00:03:31.320
over-trains to 100 percent accuracy on that kind of data

00:03:31.320 --> 00:03:34.610
set saves you huge amounts of time,

00:03:34.610 --> 00:03:39.170
and it's sort of after you've gotten something simple working on a small amount of data,

00:03:39.170 --> 00:03:41.780
that's the right time to sort of then,

00:03:41.780 --> 00:03:44.600
um, expand forward again.

00:03:44.600 --> 00:03:47.330
Um, you should definitely always make sure that

00:03:47.330 --> 00:03:49.970
you can completely overfit on your training data set.

00:03:49.970 --> 00:03:51.180
That's sort of, um,

00:03:51.180 --> 00:03:52.370
not quite a proof,

00:03:52.370 --> 00:03:56.780
but it's at least a first good requirement for your model being implemented properly.

00:03:56.780 --> 00:04:01.130
Um, you, you know part of the trick of being

00:04:01.130 --> 00:04:03.920
a successful deep learning researcher is actually

00:04:03.920 --> 00:04:07.565
managing to get things done and not wasting a ton of time.

00:04:07.565 --> 00:04:10.300
And so it definitely always helps just to be, you know,

00:04:10.300 --> 00:04:12.975
plotting as you go along your training and

00:04:12.975 --> 00:04:16.160
dev errors so that you can sort of tell if things are working,

00:04:16.160 --> 00:04:17.900
or if things aren't working,

00:04:17.900 --> 00:04:20.930
and you should abandon and start again with a new experiment,

00:04:20.930 --> 00:04:25.505
tha- that just things like that save you hours and get you, uh, more done.

00:04:25.505 --> 00:04:27.350
And so then once things are working,

00:04:27.350 --> 00:04:30.080
there's sort of a whole bunch of things to make it work better.

00:04:30.080 --> 00:04:33.410
There's regularization with L2 and Dropout,

00:04:33.410 --> 00:04:36.969
there's time to do hyperparameter search,

00:04:36.969 --> 00:04:38.530
um, and, you know,

00:04:38.530 --> 00:04:42.620
often doing these things and make quite a lot of difference to what

00:04:42.620 --> 00:04:46.820
your final results are and so it's good to have time to do those things.

00:04:46.820 --> 00:04:48.680
But clearly, you want to get things, um,

00:04:48.680 --> 00:04:51.590
working first before you go on to that, um,

00:04:51.590 --> 00:04:54.380
and sort of really encourage people to still

00:04:54.380 --> 00:04:57.470
stop by in office hours if you've got any problems,

00:04:57.470 --> 00:05:00.530
and we'll try our best to help out here within

00:05:00.530 --> 00:05:05.205
the limitations of what we can do from just being hit cold with problems.

00:05:05.205 --> 00:05:08.240
Okay, um, yeah.

00:05:08.240 --> 00:05:13.595
So, I wanted to sort of just say some general remarks about, um,

00:05:13.595 --> 00:05:16.680
language and theories of language,

00:05:16.680 --> 00:05:22.790
um, that, in the context that motivate these tree recursive networks.

00:05:22.790 --> 00:05:27.485
Um, so this is an art installation at Carnegie Mellon University.

00:05:27.485 --> 00:05:29.150
And as an NLP person,

00:05:29.150 --> 00:05:31.700
I really love this art installation.

00:05:31.700 --> 00:05:36.805
Um, so we need better art installations around the Stanford School of Engineering.

00:05:36.805 --> 00:05:40.510
Um, so this is the bag-of-words art Installation.

00:05:40.510 --> 00:05:42.710
There's the bag with a lot of words in it.

00:05:42.710 --> 00:05:44.120
And you see down here,

00:05:44.120 --> 00:05:47.015
there were the stop words, the the, and the us,

00:05:47.015 --> 00:05:49.135
that had fallen out of the bag,

00:05:49.135 --> 00:05:52.175
and are represented on the ground as the stop words.

00:05:52.175 --> 00:05:55.730
Beautiful artwork, right? So, um,

00:05:55.730 --> 00:06:02.690
one of the interesting things that has been found about NLP models of language,

00:06:02.690 --> 00:06:04.670
and I think this is even more true in

00:06:04.670 --> 00:06:08.600
the deep learning world than it used to be previously is,

00:06:08.600 --> 00:06:11.510
boy, you can do a lot with bag-of-words models, right?

00:06:11.510 --> 00:06:15.920
That you can just often get a lot of power by saying,

00:06:15.920 --> 00:06:18.400
well, let's get our neural word vectors,

00:06:18.400 --> 00:06:20.670
we're gonna average them or max pool them,

00:06:20.670 --> 00:06:21.975
or something like this,

00:06:21.975 --> 00:06:23.580
and do nothing more,

00:06:23.580 --> 00:06:26.300
and that gives me a pretty good sentence representation or

00:06:26.300 --> 00:06:30.035
document representation that I could use in a classifier or something.

00:06:30.035 --> 00:06:33.830
And sometimes, you can do not much more than that and get even better.

00:06:33.830 --> 00:06:37.910
So people have done things like deep averaging networks where you're taking

00:06:37.910 --> 00:06:40.400
the output of a bag-of-words model and sort of

00:06:40.400 --> 00:06:43.475
feeding it through a couple more layers and improving things.

00:06:43.475 --> 00:06:47.300
So that is in complete distinction to

00:06:47.300 --> 00:06:51.560
what's been dominant in linguistics of looking at language structure.

00:06:51.560 --> 00:06:58.340
That typically in linguistics the emphasis has been on identifying kind of

00:06:58.340 --> 00:07:05.480
huge amounts of structure of linguistic utterances through very complex formalisms.

00:07:05.480 --> 00:07:10.455
I guess this is sort of a bit of a picture of a Chomsky minimalism syntactic tree,

00:07:10.455 --> 00:07:15.260
and the one up at the top is a bit of a picture of head-driven phrase structure grammar.

00:07:15.260 --> 00:07:18.035
Which was a theory that was predominantly, um,

00:07:18.035 --> 00:07:21.935
developed at Stanford in the '90s.

00:07:21.935 --> 00:07:25.070
Um, but sort of very complex data structures and

00:07:25.070 --> 00:07:28.895
articulated structures used to describe linguistics.

00:07:28.895 --> 00:07:32.645
And there's a huge gap between these two things.

00:07:32.645 --> 00:07:36.260
And you might think that, you know, surely,

00:07:36.260 --> 00:07:41.180
there's some good points in the middle where we have a certain amount of structure,

00:07:41.180 --> 00:07:43.460
and that's going to help us do what we want.

00:07:43.460 --> 00:07:46.525
And so in particular, um,

00:07:46.525 --> 00:07:49.745
that if we're wanting to semantically interpret language,

00:07:49.745 --> 00:07:53.300
it seems like we don't just want to have word vectors,

00:07:53.300 --> 00:07:56.120
we want to have meanings of bigger phrases.

00:07:56.120 --> 00:07:59.705
So here's the snowboarders leaping over a mogul,

00:07:59.705 --> 00:08:03.170
and a person on a snowboard jumps into the air.

00:08:03.170 --> 00:08:06.635
And what we'd like to be able to say is that the snowboarder

00:08:06.635 --> 00:08:10.790
means basically the same thing as a person on the snowboard.

00:08:10.790 --> 00:08:13.220
So we wanted to have these chunks of

00:08:13.220 --> 00:08:17.255
language which in linguistics will be constituent  phrases,

00:08:17.255 --> 00:08:19.430
and say that they have a meaning,

00:08:19.430 --> 00:08:21.830
and we'd like to be able to compare their meaning.

00:08:21.830 --> 00:08:26.870
Now, we've looked at at least one tool that allows us to have chunks of language, right?

00:08:26.870 --> 00:08:30.320
Because we looked at convolutional neural networks where you could take

00:08:30.320 --> 00:08:34.805
three words and make a representation of the convolutional neural network,

00:08:34.805 --> 00:08:38.090
but the fundamental difference is that in

00:08:38.090 --> 00:08:41.254
human languages you have these chunks that have meaning,

00:08:41.254 --> 00:08:43.055
that are of different sizes.

00:08:43.055 --> 00:08:45.935
So we'd like to say the snowboarder

00:08:45.935 --> 00:08:50.120
is pretty much semantically equivalent to a person on the snowboard,

00:08:50.120 --> 00:08:52.790
but the top one is two words long,

00:08:52.790 --> 00:08:55.295
and the bottom one is five words long.

00:08:55.295 --> 00:08:59.200
And so if we're gonna be able to do that, um,

00:08:59.200 --> 00:09:03.230
we somehow wanted to have these sort of constituent chunks and be

00:09:03.230 --> 00:09:07.070
able to work with and represent them in neural networks.

00:09:07.070 --> 00:09:08.630
And that's sort of, um,

00:09:08.630 --> 00:09:11.870
the central idea of what

00:09:11.870 --> 00:09:16.235
motivated some of the sort of tree structured neural networks that I'm about to show you.

00:09:16.235 --> 00:09:21.380
There's another related thing that you might wanna think about is, you know,

00:09:21.380 --> 00:09:23.240
a person on a snowboard,

00:09:23.240 --> 00:09:27.950
how do human beings manage to understand what that means?

00:09:27.950 --> 00:09:31.055
And then a person on a snowboard jumps into the air,

00:09:31.055 --> 00:09:35.655
how does people manage to understand what that means?

00:09:35.655 --> 00:09:41.030
And it sort of seems like the only possible answer to

00:09:41.030 --> 00:09:46.595
this is what's normally referred to as the principle of compositionality.

00:09:46.595 --> 00:09:49.040
That people know the word person,

00:09:49.040 --> 00:09:50.240
they know the word on,

00:09:50.240 --> 00:09:52.760
they know the word snowboard, therefore,

00:09:52.760 --> 00:09:55.880
they can work out what on a snowboard means, um,

00:09:55.880 --> 00:10:00.170
and they can work out what person on a snowboard means by knowing

00:10:00.170 --> 00:10:05.900
the meanings of components and putting them together into bigger pieces.

00:10:05.900 --> 00:10:08.295
There's a f- there's a famous,

00:10:08.295 --> 00:10:12.500
um, applied mathematician statistician, um,

00:10:12.500 --> 00:10:17.235
at Brown University, Stu Geman, and I guess the way he summarized this is,

00:10:17.235 --> 00:10:21.680
either the principle of compositionality is true, or God exists.

00:10:21.680 --> 00:10:24.285
Um, for [LAUGHTER] which he was, um,

00:10:24.285 --> 00:10:27.200
well you can take that as- as you want but, you know,

00:10:27.200 --> 00:10:30.480
um, I think what he meant was well, you know,

00:10:30.480 --> 00:10:32.880
you can just make these infinite number of

00:10:32.880 --> 00:10:36.085
infinitely long sentences and human beings understand them,

00:10:36.085 --> 00:10:39.365
that it just has to be that people can know about

00:10:39.365 --> 00:10:43.650
words and ways to combine meanings and-and make bigger meanings because,

00:10:43.650 --> 00:10:47.945
you know, how else could it possibly work that people could understand sentences.

00:10:47.945 --> 00:10:50.375
And so we want to be able to do that.

00:10:50.375 --> 00:10:54.664
We want to be able to work out semantic compositions of smaller elements,

00:10:54.664 --> 00:10:57.415
to work out the meanings of bigger pieces.

00:10:57.415 --> 00:11:01.200
And that this obviously isn't only a linguistic thing,

00:11:01.200 --> 00:11:05.050
compositionality, um, appears in other places as well, right.

00:11:05.050 --> 00:11:10.620
So, um, if you want to understand how some piece of machinery works,

00:11:10.620 --> 00:11:14.195
what you kind of wanna know is it has different sub-components.

00:11:14.195 --> 00:11:16.140
And if you can understand how

00:11:16.140 --> 00:11:19.370
the different sub-components work and how they're fitted together,

00:11:19.370 --> 00:11:24.605
um, then you might have some understanding of how the whole scene works.

00:11:24.605 --> 00:11:31.170
Um, and, um, compositionality seems to be wor- at work in vision as well.

00:11:31.170 --> 00:11:35.950
So here is a scene and again it seems like this scene has parts.

00:11:35.950 --> 00:11:38.650
So there are little parts that go together, right.

00:11:38.650 --> 00:11:41.725
So there are people that go together into a crowd of people,

00:11:41.725 --> 00:11:44.860
and there's a roof and a second floor and another bit of roof.

00:11:44.860 --> 00:11:48.850
and a first floor that go together into a picture of this church.

00:11:48.850 --> 00:11:54.275
And so this is also kind of a compositional scene in which pieces go together.

00:11:54.275 --> 00:11:58.695
So it sort of seems like certainly for language understanding,

00:11:58.695 --> 00:12:02.810
and then really for a lot of the other things that we use for intelligence,

00:12:02.810 --> 00:12:05.495
that we somehow need to be able to understand

00:12:05.495 --> 00:12:09.335
bigger things from knowing about smaller parts.

00:12:09.335 --> 00:12:14.870
Um, yeah, so computational- so the most- I mentioned this earlier,

00:12:14.870 --> 00:12:16.850
sometime the most famous, um,

00:12:16.850 --> 00:12:20.480
linguist is Noam Chomsky at MIT and,

00:12:20.480 --> 00:12:24.480
um, you know, really computational linguists,

00:12:24.480 --> 00:12:28.355
a lot of the time haven't been that friendly to, um,

00:12:28.355 --> 00:12:32.590
linguistics linguists and in particular some of Noam Chomsky's, um,

00:12:32.590 --> 00:12:36.250
theories of language because really he's never been

00:12:36.250 --> 00:12:39.970
sympathetic to the idea of machine learning.

00:12:39.970 --> 00:12:44.075
Or in general does some of the empirical ability to learn from data.

00:12:44.075 --> 00:12:46.020
He's sort of always been, um,

00:12:46.020 --> 00:12:48.575
[NOISE] wanting to refuse to that exists.

00:12:48.575 --> 00:12:51.470
But, um, if we nevertheless look for a little bit of,

00:12:51.470 --> 00:12:53.565
um, insight on that.

00:12:53.565 --> 00:12:58.115
Um, you know, this is a recent paper of Chomsky's with authors and that they're sort

00:12:58.115 --> 00:13:03.120
of trying to give a version of what is unique about human language.

00:13:03.120 --> 00:13:05.280
And essentially what they, um,

00:13:05.280 --> 00:13:07.985
zero in on is that well,

00:13:07.985 --> 00:13:09.910
if you're sort of looking at, you know,

00:13:09.910 --> 00:13:13.730
humans versus other fairly intelligent creatures.

00:13:13.730 --> 00:13:17.790
They suggest that the defining difference of human beings, um,

00:13:17.790 --> 00:13:22.095
is that they have this ability to model recursion.

00:13:22.095 --> 00:13:27.450
And so the- this paper argues that the- the singular distinction that allowed

00:13:27.450 --> 00:13:30.390
language to develop in human beings was that we

00:13:30.390 --> 00:13:33.555
could put together smaller parts to make bigger things,

00:13:33.555 --> 00:13:38.495
in a recursive process and that that was the sort of defining new ability.

00:13:38.495 --> 00:13:41.135
Um, not sure I- not sure I believe that or not,

00:13:41.135 --> 00:13:43.920
um, [LAUGHTER] you can decide what you think.

00:13:43.920 --> 00:13:46.325
But what I think, um,

00:13:46.325 --> 00:13:51.390
is certainly the case is that- it's just incontrovertible that

00:13:51.390 --> 00:13:56.990
the structure of human language sentences have these pieces,

00:13:56.990 --> 00:14:01.260
um, constituents that then form together hierarchically or

00:14:01.260 --> 00:14:05.555
recursively into bigger pieces as you go up in the tree.

00:14:05.555 --> 00:14:11.190
And in particular you get this recursion where you get a little noun phrase meat,

00:14:11.190 --> 00:14:15.375
which then appears in a bigger noun phrase like spaghetti with meat.

00:14:15.375 --> 00:14:17.640
And you can repeat that several times,

00:14:17.640 --> 00:14:19.530
giving you a recursive structure.

00:14:19.530 --> 00:14:22.710
And I have an example of that in blue up at the top.

00:14:22.710 --> 00:14:25.180
So the person standing next to the man from

00:14:25.180 --> 00:14:28.035
the company that purchased the firm that you used to work at,

00:14:28.035 --> 00:14:32.675
um, that whole thing is a big noun phrase.

00:14:32.675 --> 00:14:36.280
Um, but inside that there's a noun phrase,

00:14:36.280 --> 00:14:39.780
the man from the company that purchased the firm that you used to work at,

00:14:39.780 --> 00:14:41.880
which is another big noun phrase.

00:14:41.880 --> 00:14:43.820
And well inside that, um,

00:14:43.820 --> 00:14:47.260
there are smaller noun phrases like,

00:14:47.260 --> 00:14:49.890
the company that purchased the firm you used to work at.

00:14:49.890 --> 00:14:53.190
But, you know, it's still got inside that noun phrases like,

00:14:53.190 --> 00:14:55.055
the firm that you used to work at.

00:14:55.055 --> 00:14:57.660
And actually even that's got it inside,

00:14:57.660 --> 00:14:59.365
the smaller noun phrase,

00:14:59.365 --> 00:15:02.035
which is just the word you.

00:15:02.035 --> 00:15:06.945
So an individual pronoun is also a noun phrase.

00:15:06.945 --> 00:15:11.000
Um, so just kind of structuring of

00:15:11.000 --> 00:15:13.565
language where you get this sort of

00:15:13.565 --> 00:15:16.895
hierarchical structure and the same kind of things inside them.

00:15:16.895 --> 00:15:20.465
I think that's just sort of totally, totally correct.

00:15:20.465 --> 00:15:23.130
Um, the- the claim then that,

00:15:23.130 --> 00:15:26.210
you know, our language is recursive, I mean,

00:15:26.210 --> 00:15:29.615
in a formal sense is not quite clear that that's,

00:15:29.615 --> 00:15:33.365
uh, it's a clear thing.

00:15:33.365 --> 00:15:36.850
And that's the reason- to say something is recursive,

00:15:36.850 --> 00:15:39.500
it has to repeat out to infinity, right.

00:15:39.500 --> 00:15:43.180
So as soon as you put any bound on something,

00:15:43.180 --> 00:15:48.819
and you say, "Look that's a noun phrase you just gave me with five levels of nesting."

00:15:48.819 --> 00:15:52.615
That's pretty implausible that someone is going to say that.

00:15:52.615 --> 00:15:54.630
And so as soon as you sort of,

00:15:54.630 --> 00:15:56.570
um, want to make an argument like,

00:15:56.570 --> 00:15:57.960
okay even if they said that,

00:15:57.960 --> 00:16:01.110
no one is going to say a noun phrase with 10 levels of nesting.

00:16:01.110 --> 00:16:04.340
And if you put some hard limit on it like that, um,

00:16:04.340 --> 00:16:08.970
then in some sense it's not truly recursive because it doesn't go out to infinity.

00:16:08.970 --> 00:16:10.290
Um, but, you know,

00:16:10.290 --> 00:16:12.280
regardless what you think about that,

00:16:12.280 --> 00:16:16.090
that doesn't negate the basic argument that you get this hierarchical

00:16:16.090 --> 00:16:19.900
structuring with the same kinds of things like noun phrases,

00:16:19.900 --> 00:16:26.780
sentences, verb phrases, appearing inside each other in a way that has no clear bound.

00:16:26.780 --> 00:16:30.200
Like to the extent that I show you a complex sentence,

00:16:30.200 --> 00:16:35.430
you can say I can make that an even bigger, more complex sentence by putting it inside,

00:16:35.430 --> 00:16:38.315
you said to me that, and then saying,

00:16:38.315 --> 00:16:39.940
um, my sentence, right.

00:16:39.940 --> 00:16:44.880
So that's the sense in which it does appear to be a recursive generative process,

00:16:44.880 --> 00:16:50.510
even though practically there are limits to how complex sentences people say.

00:16:50.510 --> 00:16:53.625
And so that's the kind of structure that gets

00:16:53.625 --> 00:16:57.605
captured in these constituency, um, structure trees.

00:16:57.605 --> 00:17:02.780
So before the early time when we talked about parsing and you guys did some of it,

00:17:02.780 --> 00:17:05.365
I emphasized dependency parsing.

00:17:05.365 --> 00:17:08.220
Um, but the other kind of parsing which is actually

00:17:08.220 --> 00:17:12.035
the kind that the models I'm going to talk about today was using,

00:17:12.035 --> 00:17:15.605
was this idea of what's often called constituency

00:17:15.605 --> 00:17:19.790
parsing or linguists often call it phrase structure grammars,

00:17:19.790 --> 00:17:24.635
um, or in sort of computer science formal language theory.

00:17:24.635 --> 00:17:27.185
These are context-free grammars, where, um,

00:17:27.185 --> 00:17:29.525
we're having, um, these,

00:17:29.525 --> 00:17:32.129
um, non-terminals like noun phrase,

00:17:32.129 --> 00:17:35.105
and verb phrase, and that's inside another noun phrases,

00:17:35.105 --> 00:17:36.715
it's inside another verb phrase,

00:17:36.715 --> 00:17:38.879
which is inside more verb phrases,

00:17:38.879 --> 00:17:40.775
heading up the sentence.

00:17:40.775 --> 00:17:43.895
And so these are our constituency grammars.

00:17:43.895 --> 00:17:47.585
And when we've occasionally mentioned the Penn Treebank tree,

00:17:47.585 --> 00:17:52.495
this was kind of an original Penn Treebank tree which is basically, uh,

00:17:52.495 --> 00:17:54.005
phrase structure grammar like,

00:17:54.005 --> 00:17:56.705
this with sort of various extra annotations,

00:17:56.705 --> 00:17:58.850
um, put on the nodes.

00:17:58.850 --> 00:18:04.775
Okay, so what did seem- what- what do you- to capture some of these properties,

00:18:04.775 --> 00:18:07.925
it seems like we'd like to have a neural model

00:18:07.925 --> 00:18:11.405
that can make use of some of this same kind of tree structure.

00:18:11.405 --> 00:18:17.720
And so what we'd like to do for working out semantic similarity of constituents,

00:18:17.720 --> 00:18:20.255
is we want to not only have

00:18:20.255 --> 00:18:25.185
a word vector space like we started off with right at the beginning of the quarter,

00:18:25.185 --> 00:18:30.065
but we'd like to be able to take bigger constituents like noun phrases,

00:18:30.065 --> 00:18:31.565
the country of my birth,

00:18:31.565 --> 00:18:33.475
and the place where I was born,

00:18:33.475 --> 00:18:35.535
and also give them a meaning.

00:18:35.535 --> 00:18:39.370
And so it seems like what we'd like to do is have a method of

00:18:39.370 --> 00:18:44.140
computing the meaning of any phrase in a compositional manner,

00:18:44.140 --> 00:18:47.320
such that the end result is also that

00:18:47.320 --> 00:18:52.410
these phrases could be stuck inside our vector space models.

00:18:52.410 --> 00:18:56.300
So we're still going to stick with our vector space semantics of phrases,

00:18:56.300 --> 00:18:59.625
and we wanna comp- compute the meanings of phrases.

00:18:59.625 --> 00:19:01.530
And so then the question is,

00:19:01.530 --> 00:19:04.715
how could we go about doing that?

00:19:04.715 --> 00:19:07.865
And well answer number one is we're gonna use the principle of

00:19:07.865 --> 00:19:11.375
compositionality since we're sure it's right,

00:19:11.375 --> 00:19:15.740
and so, well, what the principle of compositionality essentially says,

00:19:15.740 --> 00:19:20.170
if you want to work out the meaning- or here it says of a sentence.

00:19:20.170 --> 00:19:24.340
But the meaning of any phrase, any constituent is you're going to

00:19:24.340 --> 00:19:29.050
build it by knowing the meanings of its words,

00:19:29.050 --> 00:19:31.565
and then having rules that combine these meanings.

00:19:31.565 --> 00:19:34.280
So starting off with the country of my birth,

00:19:34.280 --> 00:19:37.310
I should be able to calculate a meaning of my birth,

00:19:37.310 --> 00:19:39.105
and meaning of the country,

00:19:39.105 --> 00:19:43.540
and meaning of of the- my birth and then a meaning of the country of my birth.

00:19:43.540 --> 00:19:47.680
So we'd have meaning composition rules which will let us calculate

00:19:47.680 --> 00:19:52.520
meanings upwards for larger constituents or sentences.

00:19:52.520 --> 00:19:57.290
Um, so that seems kind of the right thing to do.

00:19:57.290 --> 00:20:00.140
And so then the question is well, can we, um,

00:20:00.140 --> 00:20:04.470
then build a model of how to do that?

00:20:04.470 --> 00:20:08.630
Well, here's sort of a straightforward way of doing this, okay.

00:20:08.630 --> 00:20:16.105
So we- we have word vectors for the words that we've calculated.

00:20:16.105 --> 00:20:20.405
And what we'd like to do is work out, um-

00:20:20.405 --> 00:20:23.625
Then a meaning representation of this sentence.

00:20:23.625 --> 00:20:26.680
And at this point we sort of have two things to do.

00:20:26.680 --> 00:20:31.590
We have parsing to do of working out what's the right structure of the sentence,

00:20:31.590 --> 00:20:35.250
and then we have meaning computation to do of

00:20:35.250 --> 00:20:39.515
working out what is the meaning representation of this sentence.

00:20:39.515 --> 00:20:42.900
Um, so for parsing we'd sort of be building,

00:20:42.900 --> 00:20:45.280
sort of noun phrase, prepositional phrase,

00:20:45.280 --> 00:20:48.120
verb phrase, sentence kind of units, um,

00:20:48.120 --> 00:20:49.895
to get "the cat sat on the mat",

00:20:49.895 --> 00:20:51.380
and then will, what,

00:20:51.380 --> 00:20:52.935
we, if we had that,

00:20:52.935 --> 00:20:57.220
we could then run some kind of meaning computation program,

00:20:57.220 --> 00:20:59.195
and give us sort of a vector space,

00:20:59.195 --> 00:21:01.310
um, meaning of these sentences.

00:21:01.310 --> 00:21:02.970
So that's kind of what we want,

00:21:02.970 --> 00:21:04.270
is to do both of those,

00:21:04.270 --> 00:21:07.355
and in a little bit I'll show you an example of the kind

00:21:07.355 --> 00:21:10.660
of one way that you go about approaching that.

00:21:10.660 --> 00:21:13.085
But before I do that, just sort of stepping back for

00:21:13.085 --> 00:21:15.935
a moment as to what's different here, right?

00:21:15.935 --> 00:21:18.395
That here we had our

00:21:18.395 --> 00:21:21.630
recurrent neural network which in some sense has been

00:21:21.630 --> 00:21:25.125
our workhorse tool in this class up to now,

00:21:25.125 --> 00:21:26.465
and it gives you,

00:21:26.465 --> 00:21:30.605
it gives you a representation of the meaning of the country of my birth sort of,

00:21:30.605 --> 00:21:32.920
you could either say that's the meaning of,

00:21:32.920 --> 00:21:34.740
um, the country of my birth,

00:21:34.740 --> 00:21:36.980
or we talked about other tricks like,

00:21:36.980 --> 00:21:39.979
doing max pooling across all of these,

00:21:39.979 --> 00:21:42.820
or you could have a separate node out here,

00:21:42.820 --> 00:21:44.620
which so does attention over these.

00:21:44.620 --> 00:21:49.240
So it does give you a sort of representation, um,

00:21:49.240 --> 00:21:50.950
of the meaning of this,

00:21:50.950 --> 00:21:54.785
of any, um, sub-sequence of words as well.

00:21:54.785 --> 00:21:57.410
Um, but they, they're sort of different, right?

00:21:57.410 --> 00:21:59.290
That this what, the top,

00:21:59.290 --> 00:22:01.435
the tree recursive neural network,

00:22:01.435 --> 00:22:07.775
it requires a sentence or any kind of phrase to have a tree structure.

00:22:07.775 --> 00:22:10.390
So we know what its component parts are,

00:22:10.390 --> 00:22:14.935
but then we're working out meaning representations

00:22:14.935 --> 00:22:20.800
for the phrase that is sensitive to what its syntactic structure is,

00:22:20.800 --> 00:22:24.215
that how the words go together to build phrases.

00:22:24.215 --> 00:22:27.875
Whereas for the recurrent neural network we're

00:22:27.875 --> 00:22:31.549
just in an oblivious way running a sequence model along,

00:22:31.549 --> 00:22:33.500
and say and compute things,

00:22:33.500 --> 00:22:35.120
and in the obvious,

00:22:35.120 --> 00:22:38.390
it doesn't in any obvious way give a meaning representation of,

00:22:38.390 --> 00:22:41.970
of my birth, or my birth contained inside that.

00:22:41.970 --> 00:22:46.240
We sort of only have a meaning representation for the whole sequence,

00:22:46.240 --> 00:22:48.680
whereas if we're doing things this way, um,

00:22:48.680 --> 00:22:54.705
we do have meaning representations for the different meaningful parts of the sentence.

00:22:54.705 --> 00:22:58.730
Okay. That makes sense of what we're trying to do?

00:22:59.120 --> 00:23:01.625
Okay. So how could we do,

00:23:01.625 --> 00:23:03.240
go about doing that?

00:23:03.240 --> 00:23:08.385
Um, well, the idea of how we could go about doing that is,

00:23:08.385 --> 00:23:09.859
if we work bottom-up,

00:23:09.859 --> 00:23:14.245
at the very bottom we have word vectors,

00:23:14.245 --> 00:23:19.890
and so we want to recursively compute the meaning of bigger constituents.

00:23:19.890 --> 00:23:25.280
So if we wanted to compute the meaning of "on the mat" what we can do is say,

00:23:25.280 --> 00:23:30.135
well, we have, already have a meaning representation of, on and mat.

00:23:30.135 --> 00:23:33.910
So if we could feed those into a neural network, because that's our

00:23:33.910 --> 00:23:37.820
one tool, we could maybe get out of it two things.

00:23:37.820 --> 00:23:42.055
We could get out of it a goodness score.

00:23:42.055 --> 00:23:44.420
So this is what we're going to use for parsing.

00:23:44.420 --> 00:23:49.450
We're going to say, "Do you belie- do you believe you can put together "on" and the

00:23:49.450 --> 00:23:55.055
"mat" to form a good constituent that's part of a parse tree?

00:23:55.055 --> 00:23:58.055
And this will be a big positive number if the answer is true,

00:23:58.055 --> 00:23:59.865
and negative if it's not true,

00:23:59.865 --> 00:24:03.305
and then we have a meaning composition device,

00:24:03.305 --> 00:24:05.270
which says, "Okay, um,

00:24:05.270 --> 00:24:07.480
if you put together these two things,

00:24:07.480 --> 00:24:11.965
what would be the meaning representation of what we put together?"

00:24:11.965 --> 00:24:16.200
And so this is the first model that we explored which

00:24:16.200 --> 00:24:19.660
was doing this in a pretty simple way, right?

00:24:19.660 --> 00:24:22.750
So here was our meaning composition, um,

00:24:22.750 --> 00:24:27.665
device that we concatenated the two vectors of the constituents,

00:24:27.665 --> 00:24:30.230
we multiply them by a matrix, add a

00:24:30.230 --> 00:24:31.840
bias as usual,

00:24:31.840 --> 00:24:33.955
put it through a tan h. Uh,

00:24:33.955 --> 00:24:35.345
this work is old enough,

00:24:35.345 --> 00:24:36.815
it's sort of before things, like,

00:24:36.815 --> 00:24:38.110
ReLUs became popular,

00:24:38.110 --> 00:24:41.735
but maybe it's better to have a tan h anyway, um, fit more like,

00:24:41.735 --> 00:24:43.235
a recurrent neural network,

00:24:43.235 --> 00:24:47.580
and so this was our meaning composition that gave the meaning of the parent.

00:24:47.580 --> 00:24:52.305
And then to the side, what the score of it was as to whether this was a good phrase,

00:24:52.305 --> 00:24:55.370
we were taking that parent vector representation,

00:24:55.370 --> 00:24:58.949
and multiplying it by another vector,

00:24:58.949 --> 00:25:01.990
and that was giving us out a number.

00:25:02.100 --> 00:25:06.180
Um, if you think about it a bit while we're doing this,

00:25:06.180 --> 00:25:10.699
you might think that this isn't quite a perfect model of meaning composition,

00:25:10.699 --> 00:25:14.800
and later on in the class I'll talk about some more complex models,

00:25:14.800 --> 00:25:17.880
um, that we then started to explore.

00:25:17.880 --> 00:25:21.815
Um, but this is sort of enough to get us going,

00:25:21.815 --> 00:25:24.520
and this gave us a way of building

00:25:24.520 --> 00:25:29.805
a recursive neural network parser which both found parsers,

00:25:29.805 --> 00:25:33.530
and worked out a meaning representation for them.

00:25:33.530 --> 00:25:37.565
And so the way we did this was in the simplest possible way really,

00:25:37.565 --> 00:25:39.580
which was to have a greedy parser.

00:25:39.580 --> 00:25:42.620
So if we start off with the "cat sat on the mat",

00:25:42.620 --> 00:25:43.960
what we could do is say,

00:25:43.960 --> 00:25:47.040
well, maybe you should join "the" and "cat" together.

00:25:47.040 --> 00:25:48.220
Let's try that.

00:25:48.220 --> 00:25:49.865
Run it through our neural network,

00:25:49.865 --> 00:25:53.315
it'll get a score and a meaning representation,

00:25:53.315 --> 00:25:55.565
and while we could try doing that for "cat" and

00:25:55.565 --> 00:25:58.635
"sat" we could try doing it for "sat" and "on".

00:25:58.635 --> 00:26:03.170
We could try doing it for "on" and "the" we could try doing it for "the" and "mat".

00:26:03.170 --> 00:26:06.665
And then at this point we'd say, okay, well the,

00:26:06.665 --> 00:26:12.725
the best phrase that we can make combining these word vectors is the one for "the cat".

00:26:12.725 --> 00:26:14.770
So let's just commit to that one,

00:26:14.770 --> 00:26:17.360
and it has this semantic representation,

00:26:17.360 --> 00:26:20.825
and at this point we can essentially repeat.

00:26:20.825 --> 00:26:25.190
Now, all the work we did over there we can just reuse because nothing has changed,

00:26:25.190 --> 00:26:28.850
but we can also consider now joining the "cat"

00:26:28.850 --> 00:26:32.965
as a constituent with "sat" and get a score for that.

00:26:32.965 --> 00:26:34.930
And so at this point we decide, okay,

00:26:34.930 --> 00:26:37.140
the mat is the best constituent to build,

00:26:37.140 --> 00:26:41.455
commit to that, calculate a meaning representation for "on the mat".

00:26:41.455 --> 00:26:44.025
That looks good, commit to that,

00:26:44.025 --> 00:26:46.080
and kind of keep on chugging up,

00:26:46.080 --> 00:26:51.200
and so we've got a mechanism for sort of choosing a parse of a sentence in a,

00:26:51.200 --> 00:26:52.655
in a greedy manner.

00:26:52.655 --> 00:26:55.300
But, you know, when we looked at the dependency parsing,

00:26:55.300 --> 00:26:57.230
we're also doing that greedily, right?

00:26:57.230 --> 00:27:00.955
Um, and coming up with a meaning representation.

00:27:00.955 --> 00:27:06.105
Okay. So that was our first model of having a tree recursive neural network,

00:27:06.105 --> 00:27:07.630
and using it for parsing.

00:27:07.630 --> 00:27:12.630
Um, there are a few more details here,

00:27:12.630 --> 00:27:16.200
some of which probably aren't super,

00:27:16.200 --> 00:27:18.335
um, important at this point, right?

00:27:18.335 --> 00:27:22.790
So we could score a tree by summing the scores at each node,

00:27:22.790 --> 00:27:25.290
um, for working out,

00:27:25.290 --> 00:27:27.730
for the optimization we were working out,

00:27:27.730 --> 00:27:33.545
we're using this kind of max-margin loss that we've looked at in other places.

00:27:33.545 --> 00:27:37.780
Um, the simplest way to do things is completely greedily.

00:27:37.780 --> 00:27:41.825
You just, um, find the best local decision at each point,

00:27:41.825 --> 00:27:42.845
and make that structure,

00:27:42.845 --> 00:27:43.985
and keep on going.

00:27:43.985 --> 00:27:45.610
But if you wanna do things a bit better,

00:27:45.610 --> 00:27:47.100
and we explored this,

00:27:47.100 --> 00:27:48.750
um, you could say,

00:27:48.750 --> 00:27:50.910
um, we could do beam search.

00:27:50.910 --> 00:27:54.020
We could explore out several good ways of merging,

00:27:54.020 --> 00:27:59.480
and then decide later higher up the tree as to which was the best way, um, to merge.

00:27:59.480 --> 00:28:03.155
Um, we haven't talked about it in this class,

00:28:03.155 --> 00:28:05.240
but just to mention, um,

00:28:05.240 --> 00:28:08.490
something in case people have seen it is, um,

00:28:08.490 --> 00:28:12.545
traditional constituency parsing where you have symbols here,

00:28:12.545 --> 00:28:14.405
like, NP or VP.

00:28:14.405 --> 00:28:19.240
Um, there exist efficient dynamic programming algorithms where you can

00:28:19.240 --> 00:28:24.660
find the optimal parse of a sentence in polynomial time.

00:28:24.660 --> 00:28:26.190
So in, in cubic time.

00:28:26.190 --> 00:28:29.375
So if you have a regular context-free grammar, and well,

00:28:29.375 --> 00:28:32.615
so regular probabilistic context-free grammar, um,

00:28:32.615 --> 00:28:35.010
and if you want to know what is the best parse of

00:28:35.010 --> 00:28:38.385
the sentence according to the probabilistic context-free grammar,

00:28:38.385 --> 00:28:43.265
you can write a cubic time dynamic programming algorithm and you can find it.

00:28:43.265 --> 00:28:47.560
That's good. And in the old days of CS224N,

00:28:47.560 --> 00:28:51.605
um, before neural networks we used to have everyone do that.

00:28:51.605 --> 00:28:57.240
The, the most, the most brain-breaking assignment of the old CS224N

00:28:57.240 --> 00:29:02.380
was writing this dynamic program to do context-free grammar parsing of a sentence.

00:29:02.380 --> 00:29:05.620
Um, the slightly sad fact is,

00:29:05.620 --> 00:29:08.810
once you go to these kind of neural network representations,

00:29:08.810 --> 00:29:12.975
you can't write clever dynamic programming algorithms anymore,

00:29:12.975 --> 00:29:18.094
because clever dynamic programming algorithms only work when you have symbols

00:29:18.094 --> 00:29:23.580
from a reasonably small set for your non-terminals because if that's the case,

00:29:23.580 --> 00:29:26.110
you can, you kind of have collisions, right?

00:29:26.110 --> 00:29:29.075
You have lots of ways of parsing stuff lower down,

00:29:29.075 --> 00:29:30.760
which kind of, uh,

00:29:30.760 --> 00:29:33.485
turn out to be different ways to make a noun phrase,

00:29:33.485 --> 00:29:35.850
or different ways to make a prepositional phrase,

00:29:35.850 --> 00:29:38.825
and therefore you can save work with dynamic programming.

00:29:38.825 --> 00:29:40.645
If you've got a model like this,

00:29:40.645 --> 00:29:44.490
since everything that you build is going through layers of neural network,

00:29:44.490 --> 00:29:47.745
and you've got a meaning representation, some high-dimensional vector,

00:29:47.745 --> 00:29:49.760
things are never going to collide,

00:29:49.760 --> 00:29:53.070
and so you can never save work by doing dynamic programming.

00:29:53.070 --> 00:29:58.520
And so, um, you're either doing exponential work to explore out everything,

00:29:58.520 --> 00:30:03.950
or else you're using some kind of beam to explore a bunch of likely stuff.

00:30:04.090 --> 00:30:09.260
Yeah. Um, we actually also applied this,

00:30:09.260 --> 00:30:11.915
um, to vision at the same time.

00:30:11.915 --> 00:30:14.330
So it wasn't just sort of completely

00:30:14.330 --> 00:30:17.300
a vague motivation of, um,

00:30:17.300 --> 00:30:23.255
visual scenes have parts that we actually started exploring that well you could take, um,

00:30:23.255 --> 00:30:27.590
these pieces of scenes and then work out, um,

00:30:27.590 --> 00:30:32.885
representations for scenes using a similar form of compositionality.

00:30:32.885 --> 00:30:35.405
And so in particular,

00:30:35.405 --> 00:30:40.715
um, there was sort of this dataset that was being used for, um,

00:30:40.715 --> 00:30:43.894
multi-class segmentation in vision,

00:30:43.894 --> 00:30:48.830
where you start off with very small patches and then you wanna combine them

00:30:48.830 --> 00:30:51.050
up into parts of a scene of sort of

00:30:51.050 --> 00:30:54.170
recognizing which part of the picture was the building,

00:30:54.170 --> 00:30:58.025
the sky, the road, various other classes.

00:30:58.025 --> 00:31:02.435
And we were actually at the time able to do this really rather well, um,

00:31:02.435 --> 00:31:06.140
using one of these tree recursive structured neural networks better

00:31:06.140 --> 00:31:11.435
than preceding work in vision had done in the late 2000s decade.

00:31:11.435 --> 00:31:17.225
Okay. So how can we- how can we build neural networks,

00:31:17.225 --> 00:31:19.550
um, that do this kind of stuff?

00:31:19.550 --> 00:31:25.730
Um, so when- when we started off exploring these tree structured neural networks, um,

00:31:25.730 --> 00:31:29.480
we thought that this was a cool original idea and no one

00:31:29.480 --> 00:31:33.305
had worked on tree structured neural networks successfully before.

00:31:33.305 --> 00:31:39.560
Um, but it turned out we were wrong, that there were a couple of Germans in the mid-1990s,

00:31:39.560 --> 00:31:44.765
um, had actually started looking at tree structured neural networks and had worked out,

00:31:44.765 --> 00:31:45.950
um, the math of them.

00:31:45.950 --> 00:31:49.535
So corresponding to the backpropagation through time algorithm,

00:31:49.535 --> 00:31:52.865
um, that Abby talked about when we were doing recurrent neural networks.

00:31:52.865 --> 00:31:55.310
They worked out the tree structured case which they

00:31:55.310 --> 00:31:58.310
called backpropagation, um, through structure.

00:31:58.310 --> 00:32:02.810
Um, there are several slides on this in

00:32:02.810 --> 00:32:07.505
the slides but I think I'm gonna sort of skip them.

00:32:07.505 --> 00:32:08.765
If anyone wants to look at them,

00:32:08.765 --> 00:32:10.610
they're on the web and you can look at them.

00:32:10.610 --> 00:32:14.720
I mean, there isn't actually anything that's new.

00:32:14.720 --> 00:32:17.420
So if you remember with- with

00:32:17.420 --> 00:32:21.725
bad scarring or something that was early lectures of this class of working out,

00:32:21.725 --> 00:32:26.450
um, the derivatives of neural networks and how it worked with recurrent neural networks.

00:32:26.450 --> 00:32:28.130
It's sort of the same, right.

00:32:28.130 --> 00:32:32.585
You have this recurrent matrix at different levels of tree structure.

00:32:32.585 --> 00:32:37.220
You're summing the derivatives of everywhere it turns up.

00:32:37.220 --> 00:32:40.370
The only difference is sort of because we now have tree structure,

00:32:40.370 --> 00:32:43.040
you're sort of splitting things downwards.

00:32:43.040 --> 00:32:45.410
Um, so yes.

00:32:45.410 --> 00:32:48.560
So forward prop we kind of compute it forwards.

00:32:48.560 --> 00:32:51.245
And then when we're doing back prop,

00:32:51.245 --> 00:32:55.910
when we've had the backward propagation we have the error signal coming from above.

00:32:55.910 --> 00:32:58.025
We then, um, combine it,

00:32:58.025 --> 00:33:00.530
um, with the calculations at this node.

00:33:00.530 --> 00:33:03.350
And then we're sort of sending it back in a tree structure

00:33:03.350 --> 00:33:06.845
down to each of the branches underneath us.

00:33:06.845 --> 00:33:11.960
So that was our first version of things and we got some decent results.

00:33:11.960 --> 00:33:16.985
We got this good vision results that I showed you and it sort of seemed to do,

00:33:16.985 --> 00:33:19.490
um, some good for, um,

00:33:19.490 --> 00:33:23.030
language both for parsing and doing- We had

00:33:23.030 --> 00:33:27.410
some results I haven't actually included here of sort of doing paraphrase,

00:33:27.410 --> 00:33:33.545
um, judgment between sentences and it- it modeled things, um, fairly well.

00:33:33.545 --> 00:33:37.760
But once we started thinking about it more it seemed like

00:33:37.760 --> 00:33:41.780
that very simple neural net function couldn't possibly

00:33:41.780 --> 00:33:46.955
compute the kind of meanings that we wanted to compute for sentence meanings.

00:33:46.955 --> 00:33:49.760
And so we then sort of set about trying to come up with

00:33:49.760 --> 00:33:52.970
some more complex ways of working out kind

00:33:52.970 --> 00:33:56.180
of meaning composition functions and nodes that

00:33:56.180 --> 00:33:59.630
could then be used to build a better neural network.

00:33:59.630 --> 00:34:04.520
And sort of some- some of the essence of that is on this slide.

00:34:04.520 --> 00:34:07.280
But, you know, for the first version we just

00:34:07.280 --> 00:34:10.130
didn't have enough complexity of neural network, frankly, right?

00:34:10.130 --> 00:34:13.730
So when we had two constituents we concatenated

00:34:13.730 --> 00:34:19.040
them and multiply that by a weight, uh, weight matrix.

00:34:19.040 --> 00:34:21.590
Um, and that was sort of essentially all we had.

00:34:21.590 --> 00:34:27.740
And, um, as I hope you've gotten more of a sense of in this class.

00:34:27.740 --> 00:34:31.610
If you just concatenate and multiply by a weight matrix,

00:34:31.610 --> 00:34:36.200
you're not actually modeling the interaction between these two vectors, right.

00:34:36.200 --> 00:34:39.500
Because you can think of this weight matrix as just sort of being

00:34:39.500 --> 00:34:43.490
divided in two and half of it multiplies this vector,

00:34:43.490 --> 00:34:45.605
and half of it multiplies this vector.

00:34:45.605 --> 00:34:49.685
So the meanings of these two things don't act on each other.

00:34:49.685 --> 00:34:52.280
And so somehow you have to make your neural network,

00:34:52.280 --> 00:34:54.620
um, more complex than that.

00:34:54.620 --> 00:34:59.944
But the other way in which this seemed too simple is in the first model,

00:34:59.944 --> 00:35:04.505
we had just one weight matrix which we use for everything.

00:35:04.505 --> 00:35:08.120
And, ah, at least if you're a linguist and you're

00:35:08.120 --> 00:35:12.050
thinking about the structure of language you might start thinking of well,

00:35:12.050 --> 00:35:14.630
wait a minute, sometimes you're gonna be putting

00:35:14.630 --> 00:35:17.645
together a verb and an object noun phrase.

00:35:17.645 --> 00:35:19.490
Um, hit the ball.

00:35:19.490 --> 00:35:24.125
Sometimes you're gonna be putting together an article and a noun, uh, ball.

00:35:24.125 --> 00:35:29.150
Sometimes you're gonna be doing adjectival modification blue ball.

00:35:29.150 --> 00:35:32.705
These things are very different in their semantics.

00:35:32.705 --> 00:35:36.980
Can it really be the case that you can just have one weight matrix that is

00:35:36.980 --> 00:35:41.645
this universal composition function for putting together the meaning of phrases?

00:35:41.645 --> 00:35:43.280
Could that possibly work?

00:35:43.280 --> 00:35:45.020
And you sort of might suspect,

00:35:45.020 --> 00:35:46.820
um, it doesn't work.

00:35:46.820 --> 00:35:49.355
Um, and so I'm gonna go on and, um,

00:35:49.355 --> 00:35:53.150
show, um, some of those different things.

00:35:53.150 --> 00:35:57.965
But really, um, before I show the different things,

00:35:57.965 --> 00:36:02.810
um, I'm gonna show one more version that's sort of related to the first thing,

00:36:02.810 --> 00:36:07.355
which actually gave a pretty successful and good parser,

00:36:07.355 --> 00:36:10.250
um, for doing, um,

00:36:10.250 --> 00:36:14.195
context-free style constituency parsing.

00:36:14.195 --> 00:36:21.950
And so this was another way of getting away from the parsing being completely greedy.

00:36:21.950 --> 00:36:26.900
Um, which was to actually split apart the two parts of g. We

00:36:26.900 --> 00:36:31.880
have to come up with a tree structure for our sentence from,

00:36:31.880 --> 00:36:34.805
'Let's compute the meaning of the sentence'.

00:36:34.805 --> 00:36:37.280
And so the thinking was, well,

00:36:37.280 --> 00:36:42.965
in terms of deciding what's a good tree structure for a sentence,

00:36:42.965 --> 00:36:47.015
that's actually something you can do pretty well with the symbolic grammar.

00:36:47.015 --> 00:36:49.940
But the problems with symbolic grammars aren't

00:36:49.940 --> 00:36:53.270
that they can't put tree structures over sentences.

00:36:53.270 --> 00:36:55.790
The problems you have with those grammars is that,

00:36:55.790 --> 00:36:59.390
they can't compute meaning representation and they're not

00:36:59.390 --> 00:37:03.710
very good at choosing between alternative tree structures.

00:37:03.710 --> 00:37:07.175
But we can divide up the two parts.

00:37:07.175 --> 00:37:08.750
So what we can do is say, well,

00:37:08.750 --> 00:37:13.175
let's just use a regular probabilistic Context-Free Grammar

00:37:13.175 --> 00:37:16.580
to generate possible tree structures for sentences.

00:37:16.580 --> 00:37:19.205
We can generate a k best list and say,

00:37:19.205 --> 00:37:21.110
what are the 50 best, um,

00:37:21.110 --> 00:37:24.230
context-free grammar structures for this sentence?

00:37:24.230 --> 00:37:28.775
And that's something we can do very efficiently with dynamic programming algorithms.

00:37:28.775 --> 00:37:33.125
And then we can work out a neural net,

00:37:33.125 --> 00:37:38.105
um, that will work out the meaning representation of the sentence.

00:37:38.105 --> 00:37:41.675
Um, and so that led to this, um,

00:37:41.675 --> 00:37:46.670
what's called syntactically untied recursive neural network.

00:37:46.670 --> 00:37:51.260
Um, so essentially what this is saying is that we

00:37:51.260 --> 00:37:55.925
ha- for each node and the sentence it's got a category,

00:37:55.925 --> 00:37:58.940
um, of a symbolic context-free grammar.

00:37:58.940 --> 00:38:02.630
So they're category A and B and C. So when

00:38:02.630 --> 00:38:06.500
we put things together we'll be able to say, okay.

00:38:06.500 --> 00:38:10.430
We've got a rule that says, um,

00:38:10.430 --> 00:38:12.620
X goes to BC,

00:38:12.620 --> 00:38:15.365
so that licenses this node here.

00:38:15.365 --> 00:38:18.800
So that part of the parsing is symbolic.

00:38:18.800 --> 00:38:21.050
Then- then we want to, um,

00:38:21.050 --> 00:38:24.110
work out the meaning of this phrase.

00:38:24.110 --> 00:38:27.770
Um, and well, the second problem I talked about

00:38:27.770 --> 00:38:32.075
was surely just having a one way of doing composition

00:38:32.075 --> 00:38:35.990
is expecting a lot too much to be able to have

00:38:35.990 --> 00:38:40.535
sort of verb and object versus adjective and noun composed the same way.

00:38:40.535 --> 00:38:42.980
So we have this idea of well,

00:38:42.980 --> 00:38:46.190
since we now know about the syntactic categories of

00:38:46.190 --> 00:38:51.215
the children that we maybe know that this is an adjective and this is a noun.

00:38:51.215 --> 00:38:55.010
What we could do is have different weight matrices for

00:38:55.010 --> 00:38:58.895
composition depending on what the categories are.

00:38:58.895 --> 00:39:01.790
So rather than where before there was

00:39:01.790 --> 00:39:07.325
just this one universal weight matrix which was meant to do all meaning composition.

00:39:07.325 --> 00:39:08.810
Here we can have,

00:39:08.810 --> 00:39:11.840
this is the weight matrix for combining together

00:39:11.840 --> 00:39:15.439
the meanings of an adjective and a noun and it will compute,

00:39:15.439 --> 00:39:17.810
um, the meaning of this constituent.

00:39:17.810 --> 00:39:21.590
But then we'll have a different weight matrix for combining

00:39:21.590 --> 00:39:27.870
together the meanings of a determiner and a noun phrase or something like that.

00:39:30.090 --> 00:39:34.450
Okay. Um, yes.

00:39:34.450 --> 00:39:37.360
So I sort of always said this one I guess,

00:39:37.360 --> 00:39:40.825
um, we wanted to be able to do things quickly.

00:39:40.825 --> 00:39:44.830
And so our solution to be able to do that is we sort of

00:39:44.830 --> 00:39:49.525
used a probabilistic context-free grammar to find likely parses,

00:39:49.525 --> 00:39:55.120
um, and then only worked out our meaning for ones that were, um, quite probable.

00:39:55.120 --> 00:39:59.050
And so we call this result a compositional vector grammar which was

00:39:59.050 --> 00:40:03.745
a combination of a PCFG and a tree recursive neural network.

00:40:03.745 --> 00:40:07.440
Um, and yeah.

00:40:07.440 --> 00:40:11.010
So, um, essentially at the time,

00:40:11.010 --> 00:40:14.235
this actually gave a pretty good constituency parser.

00:40:14.235 --> 00:40:16.845
So there are sort of lots of results here.

00:40:16.845 --> 00:40:20.040
The top ones are kind of our classic older, um,

00:40:20.040 --> 00:40:25.285
Stanford Parser which is a PCFG,  the kind of parsers that people had built.

00:40:25.285 --> 00:40:32.080
This is our compositional vector grammar that the time of this being done in 2013,

00:40:32.080 --> 00:40:34.690
it wasn't the very best parser available.

00:40:34.690 --> 00:40:38.155
There had been some better work by Eugene Charniak at Brown.

00:40:38.155 --> 00:40:41.980
But we actually had a pretty good parser coming out of that system.

00:40:41.980 --> 00:40:46.495
But what was perhaps a bit more interesting was we,

00:40:46.495 --> 00:40:50.845
we didn't only have a parser that was meant to give the right parse trees.

00:40:50.845 --> 00:40:54.850
We are also computing meaning representations of nodes.

00:40:54.850 --> 00:40:58.210
And as a kind of a consequence of that,

00:40:58.210 --> 00:41:02.140
you can look at not only meaning representations of nodes.

00:41:02.140 --> 00:41:06.355
You could learn about the weight matrices that these models were learning,

00:41:06.355 --> 00:41:08.980
um, when they combine together meanings.

00:41:08.980 --> 00:41:13.600
So remember we had these sort of category-specific W matrices,

00:41:13.600 --> 00:41:17.455
that were going together with the children to work out the meaning.

00:41:17.455 --> 00:41:20.875
Um, so these are a little bit hard to interpret.

00:41:20.875 --> 00:41:24.130
But the deal is, when we load these matrices,

00:41:24.130 --> 00:41:27.310
we initialize them as a pair of diagonal matrices.

00:41:27.310 --> 00:41:32.125
So these are sort of two by one rectangular matrices because there are two children.

00:41:32.125 --> 00:41:35.335
Um, so half of it is, um,

00:41:35.335 --> 00:41:37.090
multiplying the left child,

00:41:37.090 --> 00:41:39.535
the other half is multiplying the right child.

00:41:39.535 --> 00:41:45.520
And we initialize them as sort of like a compi- two identity matrices next to each

00:41:45.520 --> 00:41:48.895
other which would give us the sort of default semantics

00:41:48.895 --> 00:41:52.840
of just averaging until something different was learned in the,

00:41:52.840 --> 00:41:55.690
in the, in the weight vectors.

00:41:55.690 --> 00:42:02.070
And to the extent that sort of nothing interesting has been learned by the model,

00:42:02.070 --> 00:42:08.325
you'll get yellow along the diagonal and this sort of sky blue in the rest of the field.

00:42:08.325 --> 00:42:11.100
And to the extent that it's learned something

00:42:11.100 --> 00:42:14.355
interesting to take out of the semantics of a child,

00:42:14.355 --> 00:42:17.770
you will then start to see reds and oranges on the diagonal,

00:42:17.770 --> 00:42:22.465
and dark blues and greens and stuff in the rest of the field.

00:42:22.465 --> 00:42:26.200
So what you find is that if you train this model,

00:42:26.200 --> 00:42:34.225
it's learning about which children of a phrase are actually the important ones.

00:42:34.225 --> 00:42:36.370
Um, so these ones are saying that if you're

00:42:36.370 --> 00:42:39.310
combining together a noun phrase and the coordination,

00:42:39.310 --> 00:42:41.830
so something like "the cat and",

00:42:41.830 --> 00:42:45.055
that most of the semantics have to be found in "the cat"

00:42:45.055 --> 00:42:48.760
and not much of the semantics is going to be found in "and".

00:42:48.760 --> 00:42:52.825
Whereas if you are combining together a possessive pronoun,

00:42:52.825 --> 00:42:55.045
something like her or his,

00:42:55.045 --> 00:42:58.615
um, with a noun phrase inside it like,

00:42:58.615 --> 00:43:02.425
um, her tabby cat or something like that.

00:43:02.425 --> 00:43:06.595
Then most of the meaning is to be found inside the tabby cat constituent.

00:43:06.595 --> 00:43:10.555
So it's actually learning where the important semantics of sentences is.

00:43:10.555 --> 00:43:18.460
Um, and there're lots of examples of that. Um, yeah.

00:43:18.460 --> 00:43:21.850
This one sort of- so this one shows a variety of

00:43:21.850 --> 00:43:26.575
modification structures where adjectives or adverbs,

00:43:26.575 --> 00:43:31.330
um, modify either a noun phrase or an adjective phrase or

00:43:31.330 --> 00:43:35.995
just a single adjective is multiplying a noun phrase.

00:43:35.995 --> 00:43:40.120
And the thing that you seem to notice is that there are particular dimensions which are

00:43:40.120 --> 00:43:44.200
kind of capturing sort of modification meanings.

00:43:44.200 --> 00:43:50.395
So dimension 6 and dimension 11 is sort of showing up in these different,

00:43:50.395 --> 00:43:53.830
um, combinations here, as sort of capturing meaning components.

00:43:53.830 --> 00:43:55.645
So that was kind of neat.

00:43:55.645 --> 00:44:00.430
And so this slightly more complex model actually worked pretty

00:44:00.430 --> 00:44:05.035
well at capturing a meaning of phrases and sentences.

00:44:05.035 --> 00:44:07.105
So in this test here,

00:44:07.105 --> 00:44:11.920
we were giving it- the system a test sentence and saying well,

00:44:11.920 --> 00:44:17.785
what are the other- what are sentences that are most similar in meaning,

00:44:17.785 --> 00:44:22.120
nearest to paraphrases in our corpus for this sentence?

00:44:22.120 --> 00:44:25.990
So if all the figures are adjusted for seasonal variations,

00:44:25.990 --> 00:44:29.650
the two most similar other sentences in the corpus were,

00:44:29.650 --> 00:44:32.995
all the numbers are adjusted for seasonal vet fluctuation.

00:44:32.995 --> 00:44:34.270
That's a pretty easy one.

00:44:34.270 --> 00:44:37.960
Or all the figures are adjusted to remove usual seasonal patterns.

00:44:37.960 --> 00:44:40.240
So that seems to be working pretty well.

00:44:40.240 --> 00:44:43.000
"Knight-Ridder wouldn't a comment on the author,

00:44:43.000 --> 00:44:46.360
Harsco declined to say what country placed the order."

00:44:46.360 --> 00:44:48.640
The semantics there are a bit more different but it

00:44:48.640 --> 00:44:51.490
seems like it is capturing something similar.

00:44:51.490 --> 00:44:53.860
"Um, Coastal wouldn't disclose the terms."

00:44:53.860 --> 00:44:55.630
That's kind of a really interesting one,

00:44:55.630 --> 00:45:00.010
because that one is actually very similar in meaning but it's expressed in

00:45:00.010 --> 00:45:05.215
a very different way in terms of the words and the syntactic structure that are used.

00:45:05.215 --> 00:45:09.820
Okay, so that was progress because now

00:45:09.820 --> 00:45:14.815
we could have different matrices for different constituent types.

00:45:14.815 --> 00:45:22.015
Um, but there's still some reason to think that we didn't have enough power,

00:45:22.015 --> 00:45:28.225
and that was we're still at heart using this very simple compositional structure

00:45:28.225 --> 00:45:34.960
where we're just concatenating the two children's vectors and multiplying it by a matrix.

00:45:34.960 --> 00:45:37.885
So that means the two words, um,

00:45:37.885 --> 00:45:41.470
didn't interact with each other in terms of their meaning.

00:45:41.470 --> 00:45:49.450
Um, but, um, it seems like we want to have them interact in their meaning, right?

00:45:49.450 --> 00:45:54.100
So in particular if you if you think about

00:45:54.100 --> 00:45:59.305
human languages and the kind of things that people look at in linguistic semantics,

00:45:59.305 --> 00:46:04.465
you get words that appear to be kind of modifiers or operators.

00:46:04.465 --> 00:46:06.970
So the word very,

00:46:06.970 --> 00:46:09.580
sort of doesn't mean much by itself.

00:46:09.580 --> 00:46:14.980
I mean it means something like strengthening or more so or something like that,

00:46:14.980 --> 00:46:18.160
but you know, it doesn't really have a meaning, right?

00:46:18.160 --> 00:46:20.050
It doesn't have any denotation.

00:46:20.050 --> 00:46:22.360
You can't show me very things, right?

00:46:22.360 --> 00:46:25.135
You can show me chairs and pens and, um,

00:46:25.135 --> 00:46:27.910
children but you can't show me very things,

00:46:27.910 --> 00:46:32.695
that the meaning of very seems to be that something comes after it, good.

00:46:32.695 --> 00:46:39.580
And this has a sort of an operator meaning of increase on the scale of this thing,

00:46:39.580 --> 00:46:42.490
and it can increase on the scale in either direction.

00:46:42.490 --> 00:46:45.115
You can have very good or very bad.

00:46:45.115 --> 00:46:49.990
So if we want to capture that kind of semantics,

00:46:49.990 --> 00:46:53.425
it seems like we can't capture that kind of semantics by just

00:46:53.425 --> 00:46:58.330
concatenating two vectors and multiplying them by a matrix.

00:46:58.330 --> 00:47:04.300
It seems like what we really want to say is very is gonna grab

00:47:04.300 --> 00:47:06.880
hold of the meaning of good and

00:47:06.880 --> 00:47:10.915
modify it in some ways to produce a new meaning for very good.

00:47:10.915 --> 00:47:15.130
And indeed, that's the kind of approach that's typically,

00:47:15.130 --> 00:47:17.980
um, been done in linguistic semantics.

00:47:17.980 --> 00:47:20.530
So in linguistic theories of semantic,

00:47:20.530 --> 00:47:22.120
you would normally say, okay,

00:47:22.120 --> 00:47:23.620
good has a meaning,

00:47:23.620 --> 00:47:29.530
very is a function that takes in the meaning of good and returns a meaning of very good.

00:47:29.530 --> 00:47:31.825
And so we wanted to have, um,

00:47:31.825 --> 00:47:35.050
a way of putting that into a neural network.

00:47:35.050 --> 00:47:40.750
And so to try and come up with a new composition function as to how to do that.

00:47:40.750 --> 00:47:44.530
And there are various ways that you could think about

00:47:44.530 --> 00:47:48.340
doing that and other people have had a couple of different attempts.

00:47:48.340 --> 00:47:52.915
But essentially what was in our head is well,

00:47:52.915 --> 00:47:55.120
we have word vectors,

00:47:55.120 --> 00:48:01.990
and if we want to say that very takes the meaning of good and returns a new meaning,

00:48:01.990 --> 00:48:05.410
the kind of obvious thing to do is to say very

00:48:05.410 --> 00:48:08.830
has a matrix attached to it because then we could use the,

00:48:08.830 --> 00:48:13.990
the very matrix and multiply it by the good vector and we get a new,

00:48:13.990 --> 00:48:16.360
um, vector coming out.

00:48:16.360 --> 00:48:19.045
And so then, well,

00:48:19.045 --> 00:48:21.310
the problem is, uh,

00:48:21.310 --> 00:48:25.810
which- then which words have vectors and which words have matrices?

00:48:25.810 --> 00:48:27.700
And that's kind of, um,

00:48:27.700 --> 00:48:30.190
hard to know the answer to.

00:48:30.190 --> 00:48:32.485
I mean, in particular, um,

00:48:32.485 --> 00:48:35.875
words that act as operators can,

00:48:35.875 --> 00:48:39.550
um, often themselves be modified.

00:48:39.550 --> 00:48:44.305
Um, and so, um, that you know,

00:48:44.305 --> 00:48:49.680
good can also- good also is a operator, right?

00:48:49.680 --> 00:48:52.740
So that from a sort of a person,

00:48:52.740 --> 00:48:56.115
you can have a good person and that's sort of also an operator,

00:48:56.115 --> 00:48:58.290
and very is modifying that good.

00:48:58.290 --> 00:49:03.460
So the idea we came up with is let's not try and predetermine all of this.

00:49:03.460 --> 00:49:07.630
Why don't we say that every word and every phrase has

00:49:07.630 --> 00:49:12.385
connected to it both matrix and vector.

00:49:12.385 --> 00:49:15.175
So here's our very good movie.

00:49:15.175 --> 00:49:16.645
So for each word,

00:49:16.645 --> 00:49:19.945
we have a vector meaning and it has a matrix meaning,

00:49:19.945 --> 00:49:23.530
and then as we start to build up phrases like very good,

00:49:23.530 --> 00:49:28.795
they're also going to have a vector meaning and a matrix meaning.

00:49:28.795 --> 00:49:32.635
And so what we proposed was,

00:49:32.635 --> 00:49:34.390
um, so first of all,

00:49:34.390 --> 00:49:37.015
we, we would like to be able, um,

00:49:37.015 --> 00:49:40.765
to calculate, um, the vector meanings.

00:49:40.765 --> 00:49:47.005
So to work out the vector meaning of a phrase like very good.

00:49:47.005 --> 00:49:49.540
Each word has a matrix meaning.

00:49:49.540 --> 00:49:53.680
And so we're going to combine their opposing matrix and vector meaning.

00:49:53.680 --> 00:49:56.860
So we're going to take the matrix meaning of

00:49:56.860 --> 00:50:00.610
good and multiply it by the vector meaning of very.

00:50:00.610 --> 00:50:03.910
And we're going to take the matrix meaning of very and

00:50:03.910 --> 00:50:07.525
multiply it by the vector meaning of good.

00:50:07.525 --> 00:50:11.185
And so we're going to have both of those two things.

00:50:11.185 --> 00:50:17.620
And then we're going to have a neural network layer like before that combine those together.

00:50:17.620 --> 00:50:19.540
And so that's sort of in the red box.

00:50:19.540 --> 00:50:22.045
Then those two things were concatenated,

00:50:22.045 --> 00:50:25.840
and put through the kind of neural network layer we had before to give us

00:50:25.840 --> 00:50:30.235
a final vector meaning on this, for the phrase.

00:50:30.235 --> 00:50:34.675
And then we also needed a matrix meaning for the phrase.

00:50:34.675 --> 00:50:38.395
And so for the matrix meaning for the phrase, um.

00:50:38.395 --> 00:50:44.185
We did this kind of simple model which maybe actually wasn't very good which was to say,

00:50:44.185 --> 00:50:50.260
let's just concatenate the two matrices of the- um,

00:50:50.260 --> 00:50:53.560
the constituents, multiply them by

00:50:53.560 --> 00:50:57.280
another matrix and that's then going to give us a matrix,

00:50:57.280 --> 00:50:59.920
um, version of the parent node.

00:50:59.920 --> 00:51:05.605
And so this was gave us our new more compo- more powerful composition procedure.

00:51:05.605 --> 00:51:11.620
Um, this did seem like it could do some kind of good things that captured,

00:51:11.620 --> 00:51:17.980
uh, uh, sort of operator semantics where one word modified the meaning of another word.

00:51:17.980 --> 00:51:25.610
Um, so here's a kind of a neat thing that we were able to do with this.

00:51:25.620 --> 00:51:30.040
Um, that we are wanting to be able to work out

00:51:30.040 --> 00:51:34.330
the semantics of an operator modifying another word.

00:51:34.330 --> 00:51:40.450
So unbelievably annoying, unbelievably awesome, unbelievably sad.

00:51:40.450 --> 00:51:44.260
Um, not annoying, not awesome, not sad.

00:51:44.260 --> 00:51:48.340
[NOISE] And so this was contrasting,

00:51:48.340 --> 00:51:54.205
our, um, old model versus the new model.

00:51:54.205 --> 00:51:58.345
And this scale is a scale of positive to negative.

00:51:58.345 --> 00:52:03.210
So this is completely negative to completely positive, all right?

00:52:03.210 --> 00:52:06.750
And so the kind of contrast you get,

00:52:06.750 --> 00:52:09.910
uh, that for, um,

00:52:09.910 --> 00:52:15.290
not annoying that the simple model thought that this was pretty negative,

00:52:15.290 --> 00:52:19.070
whereas the new model thinks this is pretty neutral in meaning,

00:52:19.070 --> 00:52:22.615
and that seems to be reasonably correct.

00:52:22.615 --> 00:52:25.210
Um, but not sad,

00:52:25.210 --> 00:52:31.180
that means it's a little bit positive and both models were trying to capt- capture that,

00:52:31.180 --> 00:52:34.775
that- you know, the results here are a little bit ambivalent,

00:52:34.775 --> 00:52:36.970
but- but it sort of seems that they sort of go a

00:52:36.970 --> 00:52:40.105
little bit in the direction of what we want. Yes.

00:52:40.105 --> 00:52:42.640
What is the ground truth in the "not sad" example?

00:52:42.640 --> 00:52:45.745
Oh, yeah. So this ground truth

00:52:45.745 --> 00:52:49.510
was- we actually asked a whole bunch of human beings to say,

00:52:49.510 --> 00:52:53.215
um, rate the [LAUGHTER] meaning of not sad,

00:52:53.215 --> 00:52:55.210
on this scale of 1 to 10.

00:52:55.210 --> 00:52:58.390
Maybe this wasn't a very good clear task because as you can see it,

00:52:58.390 --> 00:53:01.630
bounced around a lot [LAUGHTER] as to,

00:53:01.630 --> 00:53:05.350
um, what kind of ratings we were getting for things.

00:53:05.350 --> 00:53:08.230
But yeah, that was actually kind of getting human judgments.

00:53:08.230 --> 00:53:13.464
Um, we also then use this,

00:53:13.464 --> 00:53:15.220
um, model to say, "Well,

00:53:15.220 --> 00:53:18.910
could we do, um, semantic classification tasks?"

00:53:18.910 --> 00:53:24.965
So if we wanted to understand relations between different noun phrases,

00:53:24.965 --> 00:53:28.255
so this was a dataset where, um,

00:53:28.255 --> 00:53:32.695
there were relations marked between two noun phrases.

00:53:32.695 --> 00:53:37.390
My apartment has a pretty large kitchen that that was seen as an example of

00:53:37.390 --> 00:53:43.840
a component-whole, a part of relationship between the two noun phrases,

00:53:43.840 --> 00:53:49.210
and there were other relationships between different kinds of noun phrases.

00:53:49.210 --> 00:53:52.240
So if it was the movie showed wars, um,

00:53:52.240 --> 00:53:54.535
that that was then a message topic,

00:53:54.535 --> 00:53:59.455
so there's some communication medium that contains some topic relationship.

00:53:59.455 --> 00:54:01.930
And so we were using this kind of

00:54:01.930 --> 00:54:05.860
neural network to sort of build our meaning representations and

00:54:05.860 --> 00:54:06.940
then putting them through

00:54:06.940 --> 00:54:12.395
another neural network layer as a classifier to see how well we did.

00:54:12.395 --> 00:54:15.580
And so we got some sort of fairly good results on that.

00:54:15.580 --> 00:54:18.970
So this was a dataset that people had worked on with

00:54:18.970 --> 00:54:24.070
traditional NLP systems of different kinds of machine learning methods.

00:54:24.070 --> 00:54:26.170
But in some sense, you know,

00:54:26.170 --> 00:54:29.710
what we were interested in was we seem to be making progress in having

00:54:29.710 --> 00:54:32.440
a better semantic composition system that

00:54:32.440 --> 00:54:37.180
our old recursive neural network was getting about 75 percent,

00:54:37.180 --> 00:54:40.255
and then our new one was getting about 79 percent,

00:54:40.255 --> 00:54:45.280
which we could sort of push up further by putting more features into our system.

00:54:45.280 --> 00:54:47.770
So that was progress,

00:54:47.770 --> 00:54:50.815
um, but we didn't stop there.

00:54:50.815 --> 00:54:55.375
We kept on trying to come up with better ways of doing things.

00:54:55.375 --> 00:54:59.660
And so even though things worked fairly well here,

00:54:59.660 --> 00:55:07.500
it sort of seemed like this way of doing matrices wasn't necessarily very good.

00:55:07.500 --> 00:55:09.675
It sort of had two problems.

00:55:09.675 --> 00:55:15.910
One problem was it introduced a humongous number of parameters because,

00:55:15.910 --> 00:55:19.630
you know, for just about everything that we've done, otherwise,

00:55:19.630 --> 00:55:22.360
words have had a vector and well,

00:55:22.360 --> 00:55:28.090
maybe sometimes we use quite high dimensional vectors like 1,024,

00:55:28.090 --> 00:55:31.900
um, [NOISE] but, you know, that's a relatively modest number of parameters.

00:55:31.900 --> 00:55:35.305
Whereas once we introduce this matrix here,

00:55:35.305 --> 00:55:40.540
we've got that number of squared additional parameters for every word.

00:55:40.540 --> 00:55:43.300
And essentially because of that number of

00:55:43.300 --> 00:55:46.690
parameters to be able to compute this model at all,

00:55:46.690 --> 00:55:49.180
we were making the vector size small.

00:55:49.180 --> 00:55:51.250
So what we were actually using was that these were

00:55:51.250 --> 00:55:55.570
just 25-dimensional vectors so that the 25 squared,

00:55:55.570 --> 00:56:01.495
625, still safe, sort of decently within the range in which we could compute.

00:56:01.495 --> 00:56:03.775
So that was the first problem.

00:56:03.775 --> 00:56:05.485
The second problem is,

00:56:05.485 --> 00:56:08.620
we didn't really have very good ways of

00:56:08.620 --> 00:56:13.210
sort of building up the matrix meaning of bigger phrases.

00:56:13.210 --> 00:56:14.350
I mean, you know,

00:56:14.350 --> 00:56:17.830
this sort of seems something simple we could do but it didn't,

00:56:17.830 --> 00:56:21.970
you know, feel a very good way of getting a matrix meaning of a phrase.

00:56:21.970 --> 00:56:24.760
So we sort of wanted to come up with some other way of doing

00:56:24.760 --> 00:56:28.375
things that could fix both of those problems.

00:56:28.375 --> 00:56:33.940
And then, that led into work on recursive neural tensor networks.

00:56:33.940 --> 00:56:39.415
Um, and there's a kind of a nice idea here of these neural tensors,

00:56:39.415 --> 00:56:44.590
which is an idea that's actually been used in other places including, um,

00:56:44.590 --> 00:56:49.210
work on sort of putting vector embeddings of knowledge graphs and so on,

00:56:49.210 --> 00:56:51.550
which is a kind of a bit of a nice idea.

00:56:51.550 --> 00:56:55.570
So I wanted to sort of show a bit of how this model works.

00:56:55.570 --> 00:56:58.930
Um, and but just to say, first,

00:56:58.930 --> 00:57:03.670
a place where we applied this model was on the problem of sentiment analysis.

00:57:03.670 --> 00:57:08.650
Now, I think the term sentiment analysis has come up a few times as something you can

00:57:08.650 --> 00:57:13.720
do and actually which I then mentioned in the last, um, lecture.

00:57:13.720 --> 00:57:17.320
But I think we've never really talked for five minutes, um,

00:57:17.320 --> 00:57:19.450
in this class on sentiment analysis,

00:57:19.450 --> 00:57:22.645
so I'll, um, give you this as an example of that.

00:57:22.645 --> 00:57:24.910
Um, sentiment analysis has actually been

00:57:24.910 --> 00:57:30.940
a really common and important application in natural language processing.

00:57:30.940 --> 00:57:34.705
Um, you're looking at a piece of text and you're sort of saying,

00:57:34.705 --> 00:57:37.810
"Is it, um, positive or negative?"

00:57:37.810 --> 00:57:42.160
Um, and that's just something that's very useful for lots of, um,

00:57:42.160 --> 00:57:46.955
commercial applications of looking at product reviews or doing brand,

00:57:46.955 --> 00:57:51.530
um, awareness and things like that of sort of looking at sentiment connected to things.

00:57:51.530 --> 00:57:55.540
And to some extent doing sentiment analysis is easy, right?

00:57:55.540 --> 00:57:57.220
That you can kind of say,

00:57:57.220 --> 00:57:58.840
"Well, look at a piece of text.

00:57:58.840 --> 00:58:00.700
If you see words like loved,

00:58:00.700 --> 00:58:03.265
great, impressed, marvelous, then it's positive.

00:58:03.265 --> 00:58:04.570
It's a positive review.

00:58:04.570 --> 00:58:06.880
And if it's saying, bad and awful,

00:58:06.880 --> 00:58:08.440
then it's a negative review."

00:58:08.440 --> 00:58:13.420
And to some extent that's the baseline of sentiment analysis that you can use

00:58:13.420 --> 00:58:18.805
just either selected word features or all words in a bag of words.

00:58:18.805 --> 00:58:20.035
And if you do that,

00:58:20.035 --> 00:58:22.780
you don't actually do that badly,

00:58:22.780 --> 00:58:25.150
um, in sentiment analysis.

00:58:25.150 --> 00:58:26.650
If you have longer documents,

00:58:26.650 --> 00:58:31.135
just looking at bags of words can give you 90 percent in sentiment analysis.

00:58:31.135 --> 00:58:32.335
But on the other hand,

00:58:32.335 --> 00:58:35.140
things often do get trickier, right?

00:58:35.140 --> 00:58:38.020
So, um, this is from Rotten Tomatoes.

00:58:38.020 --> 00:58:40.450
With this cast and the subject matter,

00:58:40.450 --> 00:58:43.480
the movie should have been funnier and more entertaining.

00:58:43.480 --> 00:58:47.004
And if you sort of pretend you're a bag of words model,

00:58:47.004 --> 00:58:52.840
the only words in this that are sort of clearly sentiment-laden words, uh,

00:58:52.840 --> 00:58:57.969
entertaining and funnier, and both of those are pretty positive words,

00:58:57.969 --> 00:59:04.615
um, but it's fairly obvious that this actually is meant to be a bad review of the movie.

00:59:04.615 --> 00:59:07.360
And so well, how are we meant to know that?

00:59:07.360 --> 00:59:11.320
Well, it sort of seems again like what we have to do is meaning composition.

00:59:11.320 --> 00:59:15.310
We have to get sort of phrases like "should have been

00:59:15.310 --> 00:59:21.070
funnier," and then realized that that's actually a negative meaning for a phrase.

00:59:21.070 --> 00:59:25.690
And so we wanted to explore how we could look at those sort of meanings for

00:59:25.690 --> 00:59:33.310
phrases and explore building up those meanings as doing meaning composition over trees.

00:59:33.310 --> 00:59:36.400
Um, so the first thing we did, um,

00:59:36.400 --> 00:59:42.490
was we built a treebank of sentiment trees where we got people to rate sentiment.

00:59:42.490 --> 00:59:45.910
And so this led to the Stanford Sentiment Treebank,

00:59:45.910 --> 00:59:49.675
which is still a dataset you often see used in, um,

00:59:49.675 --> 00:59:54.280
various of evaluations with a whole bunch of datasets. Indeed,

00:59:54.280 --> 00:59:57.175
it showed up in decaNLP last week.

00:59:57.175 --> 01:00:00.545
Um, so what we were doing in this was taking,

01:00:00.545 --> 01:00:06.265
um, sentences which were Rotten Tomatoes sentences from movies.

01:00:06.265 --> 01:00:13.450
We were parsing them to give tree structure and then we were asking Mechanical Turkers to

01:00:13.450 --> 01:00:16.745
rate the different phra- the different words and

01:00:16.745 --> 01:00:21.460
phrases on a sentiment scale of very positive to very negative.

01:00:21.460 --> 01:00:25.660
So lots of stuff is white because it's just not sentiment-laden, right?

01:00:25.660 --> 01:00:27.575
There's words that are the,

01:00:27.575 --> 01:00:29.710
and there's phrases like the movie and

01:00:29.710 --> 01:00:33.325
the movie was- which don't really have any sentiment,

01:00:33.325 --> 01:00:37.180
but then you have pieces that are sort of very positives pieces of

01:00:37.180 --> 01:00:42.025
tree and negative pieces of tree that are then shown in the blue and the red.

01:00:42.025 --> 01:00:45.265
And- so typically in sentiment datasets,

01:00:45.265 --> 01:00:49.720
people have only labeled the entire sentence to say,

01:00:49.720 --> 01:00:53.140
"This is a positive sentence or a very positive sentence.

01:00:53.140 --> 01:00:55.840
This is a negative sentence or a very negative sentence."

01:00:55.840 --> 01:01:01.810
Crucially, what we were doing differently here is every phrase in the sentence

01:01:01.810 --> 01:01:08.170
according to our tree structure was being given a label for its positivity or negativity.

01:01:08.170 --> 01:01:11.080
Um, and perhaps not surprisingly,

01:01:11.080 --> 01:01:14.990
just the fact that you have a lot more annotations like that, um,

01:01:14.990 --> 01:01:20.400
just improves the behavior of classifiers because you kind of can

01:01:20.400 --> 01:01:26.735
do better attribution of which words in a sentence are positive or negative. Um.

01:01:26.735 --> 01:01:32.810
So, these were um were results of sort of preceding models.

01:01:32.810 --> 01:01:39.650
So the green is a Naive Bayes model except it not only uses individual words,

01:01:39.650 --> 01:01:41.630
but it uses pairs of words.

01:01:41.630 --> 01:01:45.590
It turns out if you're building a traditional classifier and you

01:01:45.590 --> 01:01:49.940
wanna do sentiment analysis as opposed to something like topic classification,

01:01:49.940 --> 01:01:54.500
you get a lot better results if you also use word pair features.

01:01:54.500 --> 01:01:58.850
And that's because it does a baby bit of um composition for you.

01:01:58.850 --> 01:02:01.659
You don't only have features for not an interesting,

01:02:01.659 --> 01:02:03.610
but you can have a feature for not

01:02:03.610 --> 01:02:07.030
interesting and that lets you model a certain amount of stuff.

01:02:07.030 --> 01:02:10.630
Um, and then these are our older generations of neural networks,

01:02:10.630 --> 01:02:14.935
our ori- original tree structured neural network and our matrix vector one.

01:02:14.935 --> 01:02:19.070
And so simply having- for these sort of fixed models,

01:02:19.070 --> 01:02:23.810
simply having the richer supervision that comes from our new treebank,

01:02:23.810 --> 01:02:26.330
it's sort of moved up the performance of every model.

01:02:26.330 --> 01:02:29.450
So even um, for just the um,

01:02:29.450 --> 01:02:33.695
Naive Bayes model's performances going up about four percent um,

01:02:33.695 --> 01:02:35.940
because of the fact um,

01:02:35.940 --> 01:02:38.405
that it now knows more about which

01:02:38.405 --> 01:02:42.005
particular words are positive or negative in the sentences.

01:02:42.005 --> 01:02:47.075
Um, but still none of these performances are really great.

01:02:47.075 --> 01:02:53.120
Um, so we still thought that well can we build better models of how to do this.

01:02:53.120 --> 01:02:56.390
Um, in particular, if you look at sentences with

01:02:56.390 --> 01:02:59.450
sort of various kinds of negation you know,

01:02:59.450 --> 01:03:01.505
things like should've been funnier,

01:03:01.505 --> 01:03:06.170
these models in general still couldn't capture the right meanings for them.

01:03:06.170 --> 01:03:11.600
And so that led into our fourth model of how to do this,

01:03:11.600 --> 01:03:15.965
which is this idea of recursive neural tensor networks.

01:03:15.965 --> 01:03:22.550
Um, and so what we wanted to be able to do is go back to just having um,

01:03:22.550 --> 01:03:26.660
meanings of words be vectors,

01:03:26.660 --> 01:03:30.335
but nevertheless despite that to be able to

01:03:30.335 --> 01:03:34.355
have a meaningful phrase where the two vectors um,

01:03:34.355 --> 01:03:36.140
acted on each other.

01:03:36.140 --> 01:03:39.245
And well, you know, this kind of,

01:03:39.245 --> 01:03:41.810
this is the picture of what we did when we were

01:03:41.810 --> 01:03:44.615
doing attention in a bi-linear way, right?

01:03:44.615 --> 01:03:46.625
We had vectors for two words.

01:03:46.625 --> 01:03:50.330
We stuck a matrix in between and we used

01:03:50.330 --> 01:03:54.775
that and gave an attention and got an attention score out.

01:03:54.775 --> 01:03:59.245
So that let these two vectors interact with each other,

01:03:59.245 --> 01:04:02.635
but it only produced one number as the output.

01:04:02.635 --> 01:04:04.630
But there's a way to fix that,

01:04:04.630 --> 01:04:10.295
which is to say well rather than having a matrix here,

01:04:10.295 --> 01:04:14.555
what we could stick here is a three-dimensional cube,

01:04:14.555 --> 01:04:19.220
which physicists and deep learning people now call a tensor, right?

01:04:19.220 --> 01:04:22.550
So a tensor is just higher multi-dimensional array um,

01:04:22.550 --> 01:04:24.560
in computer science terms.

01:04:24.560 --> 01:04:28.595
Um, so if we sort of made that a tensor,

01:04:28.595 --> 01:04:33.035
you know, it's like we have sort of multiple layers of matrix here.

01:04:33.035 --> 01:04:38.795
And so the end result of that is we get one number here and one number here.

01:04:38.795 --> 01:04:42.979
So in total, we get out a size two vector,

01:04:42.979 --> 01:04:46.355
which is all we need in my baby example where

01:04:46.355 --> 01:04:50.300
baby examples, where we only have these two component vectors for words.

01:04:50.300 --> 01:04:52.250
But in general, we have a tensor with

01:04:52.250 --> 01:04:55.835
the extra mention dimension of the size of our word vector.

01:04:55.835 --> 01:04:58.985
And so therefore, we will get a word vector, w hat,

01:04:58.985 --> 01:05:03.800
we will get a phrase vector out from the composition that's the same size of

01:05:03.800 --> 01:05:06.980
the input vectors and will allow them to

01:05:06.980 --> 01:05:12.450
interact with each other in working out the meaning of the entire thing.

01:05:12.910 --> 01:05:17.390
Okay. Um, all right.

01:05:17.390 --> 01:05:19.220
So at that point um,

01:05:19.220 --> 01:05:23.250
we use the resulting vectors um um,

01:05:24.610 --> 01:05:28.265
so we had our neural tensor network.

01:05:28.265 --> 01:05:33.695
We actually combined it together with the sort of previous kind of layer we used to have,

01:05:33.695 --> 01:05:37.310
our sort of first RNN, maybe you  didn't need to do this,

01:05:37.310 --> 01:05:39.485
but we just decided to add that in as well,

01:05:39.485 --> 01:05:42.470
put things through a nonlinearity and that was then

01:05:42.470 --> 01:05:45.770
giving us our new representation of phrases.

01:05:45.770 --> 01:05:49.190
We built that up the tree and then at the end,

01:05:49.190 --> 01:05:53.120
we could classify the meaning of any phrase um,

01:05:53.120 --> 01:05:56.900
in the same kind of way with softmax regression and we could

01:05:56.900 --> 01:06:00.585
train these weights with gradient descent to predict sentiment.

01:06:00.585 --> 01:06:03.910
And so this actually worked pretty nicely.

01:06:03.910 --> 01:06:05.245
I mean in particular,

01:06:05.245 --> 01:06:09.820
it didn't so really work any better with just the sentence labels.

01:06:09.820 --> 01:06:13.194
But if we train the model with our treebank,

01:06:13.194 --> 01:06:15.820
we could then get a kind of- of whatever

01:06:15.820 --> 01:06:18.700
that is about another couple of percent in performance,

01:06:18.700 --> 01:06:20.575
and so that seemed good.

01:06:20.575 --> 01:06:21.880
And so in particular,

01:06:21.880 --> 01:06:26.920
it seemed to do a much better job of actually understanding meaning composition.

01:06:26.920 --> 01:06:32.095
So here's the kind of sentence where you have there are slow and repetitive parts,

01:06:32.095 --> 01:06:35.245
but it has just enough spice to keep it interesting.

01:06:35.245 --> 01:06:38.470
And the model seen here is pretty good at understanding.

01:06:38.470 --> 01:06:41.400
Okay, this part of the sentence is negative,

01:06:41.400 --> 01:06:43.880
this part of the sentence is positive,

01:06:43.880 --> 01:06:46.310
and actually when you stick the two halves together,

01:06:46.310 --> 01:06:50.450
the end result is a sentence that's positive in meaning.

01:06:50.450 --> 01:06:54.170
But focusing in a little bit more what seems

01:06:54.170 --> 01:06:58.610
like it's especially good was for the first time this actually

01:06:58.610 --> 01:07:02.270
did seem like it could do a better job of

01:07:02.270 --> 01:07:07.220
working out sort of what happens when you do things like negation.

01:07:07.220 --> 01:07:11.975
So here we have it's just incredibly dull and it's definitely not dull.

01:07:11.975 --> 01:07:14.000
So if it's definitely not dull,

01:07:14.000 --> 01:07:16.130
that's actually means it's good, right?

01:07:16.130 --> 01:07:20.480
Can we work out the meaning of, it's definitely not dull?

01:07:20.480 --> 01:07:25.610
And so um, these, this is sort of

01:07:25.610 --> 01:07:31.505
showing sort of what happens when you have a negative,

01:07:31.505 --> 01:07:34.790
a negative sentence that's further negated.

01:07:34.790 --> 01:07:39.890
So if you go from um,

01:07:39.890 --> 01:07:42.305
so if you sort of do

01:07:42.305 --> 01:07:48.710
a annex- negation of a negative thing should become moderately positive, right?

01:07:48.710 --> 01:07:54.065
So that if you have dull is negative and if you say not dull,

01:07:54.065 --> 01:07:56.105
it doesn't mean it's fantastic,

01:07:56.105 --> 01:07:58.400
but it means it's moderately positive.

01:07:58.400 --> 01:08:04.070
And so for either a kind of Naive Bayes model or our preceding models,

01:08:04.070 --> 01:08:09.755
they weren't capable of capturing that of sort of going from dull to not dull your,

01:08:09.755 --> 01:08:14.135
your meaning computation did not come out any more positive.

01:08:14.135 --> 01:08:17.000
Whereas this sort of neural tensor network was

01:08:17.000 --> 01:08:22.470
capturing the fact that not dull meant that it was reasonably good.

01:08:22.750 --> 01:08:26.960
So that was progress. Um, yes.

01:08:26.960 --> 01:08:31.460
So I think that's as much as I'll um show you really now about applying

01:08:31.460 --> 01:08:37.590
these tree structured neural networks um, to natural language.

01:08:37.810 --> 01:08:43.190
Um, you know, I think the summary I sort of said at the beginning um is that I

01:08:43.190 --> 01:08:48.275
think you know, they're kind of interesting ideas and linguistic connections here.

01:08:48.275 --> 01:08:52.325
I mean, for various reasons,

01:08:52.325 --> 01:08:55.100
these ideas haven't been um,

01:08:55.100 --> 01:08:59.570
pursued a ton in recent years of natural language processing.

01:08:59.570 --> 01:09:04.610
You know, one is in all honesty people have found that um,

01:09:04.610 --> 01:09:08.090
once you have high dimensional vectors

01:09:08.090 --> 01:09:11.480
in things like the kind of sequence models that we've looked at,

01:09:11.480 --> 01:09:15.980
whether it's meaning things like the sort of LSTM models or any of

01:09:15.980 --> 01:09:21.200
the more recent contextual language models, those work incredibly well um,

01:09:21.200 --> 01:09:24.890
and it's not, it's not clear that overall these models work better.

01:09:24.890 --> 01:09:28.399
The second reason is sort of a computational reason,

01:09:28.399 --> 01:09:35.495
which is um, GPUs work great when you're doing uniform computation.

01:09:35.495 --> 01:09:40.130
And the beauty of having something like a sequence model is that there's uh,

01:09:40.130 --> 01:09:43.595
there's just one determinant computation you are doing

01:09:43.595 --> 01:09:47.180
along the sequence or in the convolutional neural network,

01:09:47.180 --> 01:09:49.010
there's one determinant um,

01:09:49.010 --> 01:09:51.530
computation you're doing up um,

01:09:51.530 --> 01:09:54.170
through your convolutional layers and therefore,

01:09:54.170 --> 01:09:58.805
things can be represented and computed efficiently on a GPU.

01:09:58.805 --> 01:10:03.020
The huge problem with these kind of models was what computations you are

01:10:03.020 --> 01:10:07.475
going to do depended on which structure you are assigning to the sentence,

01:10:07.475 --> 01:10:12.110
and every sentence was going to have a different structure, and so therefore,

01:10:12.110 --> 01:10:15.200
there was no way to batch the computations over a group of

01:10:15.200 --> 01:10:18.860
sentences and have the same computations being done for

01:10:18.860 --> 01:10:22.310
different sentences, it sort of undermined the ability

01:10:22.310 --> 01:10:26.365
to sort of efficiently build these models in the large.

01:10:26.365 --> 01:10:31.145
The thing I thought I'd just sort of say a moment about at the end.

01:10:31.145 --> 01:10:36.195
Um, the funny thing is that although these haven't been used much for,

01:10:36.195 --> 01:10:39.250
um, language in the last few years, um,

01:10:39.250 --> 01:10:45.650
that they've actually had some use and found different applications in different places,

01:10:45.650 --> 01:10:48.215
um, which is just sort of seen kind of cute.

01:10:48.215 --> 01:10:52.850
Um, so this is actually an application from physics.

01:10:52.850 --> 01:10:56.020
Um, and I think I'm going to just have to read this and

01:10:56.020 --> 01:10:58.890
so I have no idea what half the words mean.

01:10:58.890 --> 01:11:04.295
Um, but, um, what it says is by far the most common structures seen in collisions at

01:11:04.295 --> 01:11:10.295
the Large Hadron Collider are collimated sprays of energetic hadrons referred to as jets.

01:11:10.295 --> 01:11:14.135
These jets are produced from the fragmentation and hadronization of

01:11:14.135 --> 01:11:19.200
quarks and gluons as described by quantum chromodynamics.

01:11:19.200 --> 01:11:21.420
Anyone knows what that means?

01:11:21.420 --> 01:11:23.550
Um, I hope you're following along here.

01:11:23.550 --> 01:11:26.970
Um, one compelling physics challenge is to search for

01:11:26.970 --> 01:11:32.000
highly boosted standard model particles decaying hadronically.

01:11:32.000 --> 01:11:36.935
Unfortunately there's a large background from jets produced by more mundane,

01:11:36.935 --> 01:11:41.090
um, QCD, that's quantum chromodynamics processes.

01:11:41.090 --> 01:11:44.610
In this work, we propose instead a solution for

01:11:44.610 --> 01:11:48.215
jet classification based on an analogy between

01:11:48.215 --> 01:11:52.470
quantum chromodynamics and natural languages as inspired by

01:11:52.470 --> 01:11:56.775
several works from natural language, um, processing.

01:11:56.775 --> 01:11:59.520
Much like a sentence is composed of words

01:11:59.520 --> 01:12:02.865
following a syntactic structure organized as a parse tree,

01:12:02.865 --> 01:12:08.050
a jet is also composed of 4-momenta following a structure dictated by

01:12:08.050 --> 01:12:09.760
QCD and organized via

01:12:09.760 --> 01:12:14.100
the clustering history of a sequential co- combination jet algorithm.

01:12:14.100 --> 01:12:18.005
Um, so anyway um, yeah with these jets you see they're getting

01:12:18.005 --> 01:12:23.794
a tree structure over them and they're using the tree recursive neural network,

01:12:23.794 --> 01:12:25.100
um, to model it.

01:12:25.100 --> 01:12:31.435
Um, well that's a little bit far afield but to show you just one more example, um,

01:12:31.435 --> 01:12:35.320
that another place where these models have actually being quite

01:12:35.320 --> 01:12:39.840
useful is for doing things in programming languages.

01:12:39.840 --> 01:12:42.020
And I think in part this,

01:12:42.020 --> 01:12:46.545
this is because the application is easier in programming languages.

01:12:46.545 --> 01:12:51.150
So unlike in natural language where we have this uncertainty as to what is

01:12:51.150 --> 01:12:55.775
the correct parse tree because there's a lot of ambiguity in natural language,

01:12:55.775 --> 01:12:58.295
in programming languages, um,

01:12:58.295 --> 01:13:01.175
the parse trees are actually pretty determinant.

01:13:01.175 --> 01:13:07.560
Um, so a group of people at Berkeley, Dawn Song and her students have worked on doing

01:13:07.560 --> 01:13:10.870
programming language translation by building

01:13:10.870 --> 01:13:14.490
tree recursive neural network encoder-decoders.

01:13:14.490 --> 01:13:17.375
So that you're building up a tree structured

01:13:17.375 --> 01:13:22.120
neural network representation of a program in one language.

01:13:22.120 --> 01:13:26.345
This is a CoffeeScript program and then you're wanting to build a tree to

01:13:26.345 --> 01:13:31.760
tree model which is then translating that to a program in a different language.

01:13:31.760 --> 01:13:35.150
And they've been able to do that and get good results.

01:13:35.150 --> 01:13:38.760
Um, I was too lazy to retype this table.

01:13:38.760 --> 01:13:40.610
So this is probably a bit,

01:13:40.610 --> 01:13:42.010
bit hard to read.

01:13:42.010 --> 01:13:46.320
But what's it's contrasting is for a number of programs this is the sort of

01:13:46.320 --> 01:13:51.650
CoffeeScript to JavaScript, um, um, translation.

01:13:51.650 --> 01:13:54.980
They're comparing using tree to tree models.

01:13:54.980 --> 01:13:57.130
Um, and then using sequence to sequence

01:13:57.130 --> 01:14:00.120
models and then they tried both other combinations,

01:14:00.120 --> 01:14:03.345
sequence to tree and tree to sequence.

01:14:03.345 --> 01:14:06.215
Um, and what they find is you can get the best

01:14:06.215 --> 01:14:10.175
results with the tree to tree neural network models.

01:14:10.175 --> 01:14:13.665
And in particular these tree to tree models are

01:14:13.665 --> 01:14:17.345
augmented with attention so they have attention like we talked about

01:14:17.345 --> 01:14:21.490
the sequence to sequence models where you're then being able to do attention back to

01:14:21.490 --> 01:14:26.990
nodes in the tree structure which is a pretty natural way of doing translation.

01:14:26.990 --> 01:14:31.320
And indeed what these results show is if you don't have- that's right these results

01:14:31.320 --> 01:14:36.035
show is if you don't have the attention operation it doesn't work at all.

01:14:36.035 --> 01:14:37.680
It's too difficult, um,

01:14:37.680 --> 01:14:39.060
to get things, um,

01:14:39.060 --> 01:14:41.050
sort of done if you've just sort of trying to create

01:14:41.050 --> 01:14:45.810
a single tree representation and then say generate the tra- the translation from that.

01:14:45.810 --> 01:14:48.245
But if you can do it with this sort of putting attention

01:14:48.245 --> 01:14:51.735
into the different modes, um, that's great.

01:14:51.735 --> 01:14:56.385
Um, you might- If you know what CoffeeScript is you might, um,

01:14:56.385 --> 01:14:58.740
feel like wait that's cheating slightly because

01:14:58.740 --> 01:15:02.125
CoffeeScript is a bit too similar to JavaScript.

01:15:02.125 --> 01:15:03.725
Um, but they've also, um,

01:15:03.725 --> 01:15:05.310
done it in other languages.

01:15:05.310 --> 01:15:11.090
So this is going between Java and C# and this is a sort of

01:15:11.090 --> 01:15:14.490
handwritten Java to C# converter that you can

01:15:14.490 --> 01:15:18.820
download from GitHub if you want but it doesn't actually work that well.

01:15:18.820 --> 01:15:20.570
Um, and they're able to show,

01:15:20.570 --> 01:15:23.120
the- they're able to build a far better, um,

01:15:23.120 --> 01:15:25.580
Java to C# translator,

01:15:25.580 --> 01:15:28.080
um, doing that.

01:15:28.080 --> 01:15:30.390
Um, so that's actually kind of cool.

01:15:30.390 --> 01:15:33.110
And it's good to know that tree structured recursive neural networks

01:15:33.110 --> 01:15:34.905
are good for some things.

01:15:34.905 --> 01:15:37.820
Um, so I'm pleased to see work like this.

01:15:37.820 --> 01:15:41.135
Okay. I'm, I'm, just about done but I thought,

01:15:41.135 --> 01:15:43.515
um, before, um, finishing,

01:15:43.515 --> 01:15:47.135
I'd just mention one other [NOISE] thing which is sort of nothing to do

01:15:47.135 --> 01:15:50.850
with natural language processing precisely but it's about AI.

01:15:50.850 --> 01:15:54.570
Um, but I wanted to sort of put in a little bit of advertisement.

01:15:54.570 --> 01:15:57.335
Um, that's something that a number of us have been

01:15:57.335 --> 01:16:00.855
working on very hard for the last year or so,

01:16:00.855 --> 01:16:06.710
is developing, um, a new Stanford Institute for Human-Centered Artificial Intelligence.

01:16:06.710 --> 01:16:11.880
And actually the launch of this institute is going to be on Monday of exam week,

01:16:11.880 --> 01:16:16.365
just when you're maximally concentrating on things such as this.

01:16:16.365 --> 01:16:19.680
Um, but our hope is that we can have a lot of

01:16:19.680 --> 01:16:23.495
new activity around artificial intelligence,

01:16:23.495 --> 01:16:27.510
taking a much broader perspective to artificial intelligence, um,

01:16:27.510 --> 01:16:34.770
which is centrally viewing it from the viewpoint of humans and working out, um,

01:16:34.770 --> 01:16:38.080
I'll- exploring a much broader range of issues that

01:16:38.080 --> 01:16:41.490
embrace a lot of the interests of the rest of the university whether

01:16:41.490 --> 01:16:45.100
it's the social sciences and humanities, or also variously

01:16:45.100 --> 01:16:48.945
in professional schools like the law school and the business school.

01:16:48.945 --> 01:16:52.760
Um, so let's just quickly say a minute about that.

01:16:52.760 --> 01:16:58.980
Um, that the, the sort of motivating idea is that sort of for most of my life sort

01:16:58.980 --> 01:17:01.270
of AI seemed like a kind of

01:17:01.270 --> 01:17:03.670
a fun intellectual quest as

01:17:03.670 --> 01:17:06.420
to whether you could write bits of software that did anything,

01:17:06.420 --> 01:17:10.245
um, halfway intelligent but that's clearly not what's going to be,

01:17:10.245 --> 01:17:12.820
what's happening for the next 25 years.

01:17:12.820 --> 01:17:14.830
That we're now at this point in which

01:17:14.830 --> 01:17:19.680
artificial intelligence systems are being unleashed on society.

01:17:19.680 --> 01:17:23.730
Um, and well hopefully they do some good things but as we've

01:17:23.730 --> 01:17:26.120
increasingly been seeing there are lots of

01:17:26.120 --> 01:17:29.070
also lots of opportunities for them to do bad things.

01:17:29.070 --> 01:17:32.510
And even if we're not imagining Terminator scenarios,

01:17:32.510 --> 01:17:35.530
there are just lots of places where people are using

01:17:35.530 --> 01:17:39.545
machine learning and AI algorithms for making decisions.

01:17:39.545 --> 01:17:42.570
Some of the worst ones are things like sentencing guidelines in

01:17:42.570 --> 01:17:46.950
courts where you have very biased algorithms making bad decisions and

01:17:46.950 --> 01:17:51.425
people are starting to become a lot more aware of the issues and so

01:17:51.425 --> 01:17:54.075
effectively we are wanting to have this institute sort of

01:17:54.075 --> 01:17:56.940
embracing a lot of the work of social scientists,

01:17:56.940 --> 01:18:01.420
the ethicists and other people to actually explore how to have an AI

01:18:01.420 --> 01:18:06.030
that's really improving human lives rather than having the opposite effect.

01:18:06.030 --> 01:18:07.860
And so the three themes,

01:18:07.860 --> 01:18:09.390
um, that we're, um,

01:18:09.390 --> 01:18:15.870
mainly emphasizing for this institute is the first one in the top left is

01:18:15.870 --> 01:18:19.329
developing AI technologies but we're particularly

01:18:19.329 --> 01:18:22.770
interested in making linkages back to human intelligence.

01:18:22.770 --> 01:18:25.575
So cognitive science and neuroscience

01:18:25.575 --> 01:18:28.925
that when a lot of the early formative work in AI was

01:18:28.925 --> 01:18:31.410
done including all of

01:18:31.410 --> 01:18:35.330
the early work in neural networks like the development of back propagation,

01:18:35.330 --> 01:18:38.400
it was actually largely done in the context of cognitive science.

01:18:38.400 --> 01:18:42.090
Right? And that was sort of a linkage that tended to get lost in

01:18:42.090 --> 01:18:47.140
the '90s and 2000s statistical machine learning emphasis.

01:18:47.140 --> 01:18:49.030
And I think it would be good to renew that.

01:18:49.030 --> 01:18:51.240
Um, the top right, um,

01:18:51.240 --> 01:18:53.910
there's paying much more attention to

01:18:53.910 --> 01:18:59.310
the human and societal impact of AI and so this is looking at legal issues,

01:18:59.310 --> 01:19:01.920
economic issues, labor forces,

01:19:01.920 --> 01:19:05.905
ethics, um, green power, politics, whatever you are.

01:19:05.905 --> 01:19:09.520
But then down at the bottom is something where it seems like

01:19:09.520 --> 01:19:13.725
there's just kind of enormous opportunities to do more which is,

01:19:13.725 --> 01:19:18.825
um, how can we build technology that actually augments human lives.

01:19:18.825 --> 01:19:25.860
Like to some extent here tech- we've got technology with AI augmenting human lives.

01:19:25.860 --> 01:19:29.200
So all of your cell phones have speech recognition in them now.

01:19:29.200 --> 01:19:31.235
So you know that's AI, um,

01:19:31.235 --> 01:19:34.055
that can augment, um, your human lives.

01:19:34.055 --> 01:19:37.205
But there's a sense of which not very much of

01:19:37.205 --> 01:19:43.055
artificial intelligence has actually been put into the service of augmenting human lives.

01:19:43.055 --> 01:19:45.840
Like most of what a cell phone has on it is still

01:19:45.840 --> 01:19:48.160
sort of clever and cute stuff done by

01:19:48.160 --> 01:19:51.270
HCI people and designers which is very nice a lot of

01:19:51.270 --> 01:19:55.470
the time when you're using your map program or something but we don't really have

01:19:55.470 --> 01:20:00.455
much AI inside these devices helping to make people's lives better.

01:20:00.455 --> 01:20:05.880
And so we're hoping not only for individuals when applications like health care,

01:20:05.880 --> 01:20:08.240
um, to be doing much more of sort of putting

01:20:08.240 --> 01:20:12.420
artificial intelligence into human-centered applications.

01:20:12.420 --> 01:20:14.755
Um, anyway that's my brief advertisement.

01:20:14.755 --> 01:20:17.840
Um, look it out for this while you're not studying for your exams.

01:20:17.840 --> 01:20:20.700
And I think there'll be sort of lots of opportunities, um,

01:20:20.700 --> 01:20:24.315
for students and others to be getting more involved in this in the coming months.

01:20:24.315 --> 01:20:26.290
Okay. Thank you very much.

01:20:26.290 --> 01:20:37.290
Um, and I will see you later, um, at the poster session.

01:20:37.290 --> 01:20:37.400
[APPLAUSE].

