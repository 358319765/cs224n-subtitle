WEBVTT
Kind: captions
Language: en

00:00:04.970 --> 00:00:08.670
Okay. Let's get started again.

00:00:08.670 --> 00:00:11.445
Okay. So welcome back to, um,

00:00:11.445 --> 00:00:14.895
week three of CS224N.

00:00:14.895 --> 00:00:20.760
Okay. So we- we've got a bit of a change of pace today after week two.

00:00:20.760 --> 00:00:24.190
So, um, this week in week three,

00:00:24.190 --> 00:00:28.205
we're actually going to have some human language,

00:00:28.205 --> 00:00:32.630
and so this lecture has no partial derivative signs in it.

00:00:32.630 --> 00:00:35.030
And so we'll be moving away, um,

00:00:35.030 --> 00:00:39.715
from sort of working out the so technicalities of doing, um,

00:00:39.715 --> 00:00:42.735
new networks and back propagation,

00:00:42.735 --> 00:00:45.435
um, and a sort of math heavy week two.

00:00:45.435 --> 00:00:46.680
So then, this week,

00:00:46.680 --> 00:00:48.855
what we actually want- well,

00:00:48.855 --> 00:00:51.165
in today's lecture, we want to look at, well,

00:00:51.165 --> 00:00:55.455
what kind of structures do human language sentences have,

00:00:55.455 --> 00:00:58.515
and how we can build models that,

00:00:58.515 --> 00:01:02.990
um, build that kind of structure for sentences that we see.

00:01:02.990 --> 00:01:05.090
Um, so first of all,

00:01:05.090 --> 00:01:08.795
I'm gonna sort of explain and motivate a bit about,

00:01:08.795 --> 00:01:11.150
um, structure of human language sentences.

00:01:11.150 --> 00:01:13.250
So, that's kind of like, um,

00:01:13.250 --> 00:01:16.040
linguistics in 20 minutes or something.

00:01:16.040 --> 00:01:19.850
Um, then going particularly focusing on dependency grammars,

00:01:19.850 --> 00:01:23.495
and then gonna present a method for doing dependency structure,

00:01:23.495 --> 00:01:27.905
dependency grammar parsing called transition-based dependency parsing.

00:01:27.905 --> 00:01:33.795
And then talk about how you can make neural, um, dependency parsers.

00:01:33.795 --> 00:01:36.390
Um, so, um, going on just,

00:01:36.390 --> 00:01:38.130
you know, a couple of announcements.

00:01:38.130 --> 00:01:42.120
So, assignment two was due one minute ago,

00:01:42.120 --> 00:01:44.595
so I hope everyone's succeeded,

00:01:44.595 --> 00:01:47.430
um, in getting assignment two out of the way.

00:01:47.430 --> 00:01:49.200
If you're still working on it,

00:01:49.200 --> 00:01:50.720
do make sure to make, um,

00:01:50.720 --> 00:01:52.790
use of the office hours and get help for that.

00:01:52.790 --> 00:01:56.060
Coming out just today is assignment three.

00:01:56.060 --> 00:01:58.640
Um, assignment three, um,

00:01:58.640 --> 00:02:01.385
is basically about this lecture.

00:02:01.385 --> 00:02:03.350
Um, so, [LAUGHTER] in assignment three,

00:02:03.350 --> 00:02:06.994
what you're doing is building a neural dependency parser,

00:02:06.994 --> 00:02:09.980
and so we hope that you can put together what you learned about

00:02:09.980 --> 00:02:13.445
neural networks last week and the content of today,

00:02:13.445 --> 00:02:17.750
and jump straight right in to building a neural dependency parser.

00:02:17.750 --> 00:02:21.830
Um, the other thing that happens in assignment three is that,

00:02:21.830 --> 00:02:25.080
we start using a deep learning framework PyTorch.

00:02:25.080 --> 00:02:28.680
So, for doing assignment three, instruction zero,

00:02:28.680 --> 00:02:30.945
and this is in the PDF for the assignment,

00:02:30.945 --> 00:02:34.515
is to install PyTorch as a Python package,

00:02:34.515 --> 00:02:36.255
and start using that.

00:02:36.255 --> 00:02:43.710
Um, so we've attempted to make assignment three sort of be a highly scaffolded tutorial,

00:02:43.710 --> 00:02:47.360
where you can start to learn how to do things in PyTorch by just,

00:02:47.360 --> 00:02:50.730
um, writing a few lines of code at a time.

00:02:50.730 --> 00:02:53.130
Hopefully that works out for people.

00:02:53.130 --> 00:02:55.020
Um, if you have any issues with,

00:02:55.020 --> 00:02:56.580
with that, um, well,

00:02:56.580 --> 00:02:58.830
obviously, you can send Piazza messages,

00:02:58.830 --> 00:03:00.180
come to office hours.

00:03:00.180 --> 00:03:02.830
I mean, the one other thing you could think of doing is that there's sort of

00:03:02.830 --> 00:03:06.220
a one hour introduction to PyTorch on the PyTorch site,

00:03:06.220 --> 00:03:09.895
where you down- where you're directed for installing PyTorch,

00:03:09.895 --> 00:03:13.000
and you could also look at that if that was maybe helpful.

00:03:13.000 --> 00:03:16.710
Um, now the final mentions, yes.

00:03:16.710 --> 00:03:19.320
So, um, final projects, um, you know,

00:03:19.320 --> 00:03:22.060
we're going to sort of focus on those more in week five,

00:03:22.060 --> 00:03:25.120
but if it's not bad to be thinking about things you could do,

00:03:25.120 --> 00:03:27.145
if you're under a custom final project.

00:03:27.145 --> 00:03:30.400
You're certainly encouraged to come and talk to me or the TAs.

00:03:30.400 --> 00:03:34.315
We have under the sort of office hours page on the website,

00:03:34.315 --> 00:03:37.810
a listing of the expertise of some of the different TAs.

00:03:37.810 --> 00:03:42.220
Um, since I missed my office hours yesterday,

00:03:42.220 --> 00:03:46.520
I'm gonna have a shortened office hour tomorrow from 1:00 to 2:20.

00:03:46.520 --> 00:03:49.490
Um, that's at the same time as the,

00:03:49.490 --> 00:03:53.045
um, normal CS224N, um,

00:03:53.045 --> 00:03:56.300
office hours, so you can kind of come for any reason you want,

00:03:56.300 --> 00:03:58.430
but it might be especially good to come to me if you want

00:03:58.430 --> 00:04:00.910
to talk about, um, final projects.

00:04:00.910 --> 00:04:07.475
Okay. So, let's leap in and start talking about the structure of sentences.

00:04:07.475 --> 00:04:15.650
And so, I just sort of want to explain something about human language sentence structure,

00:04:15.650 --> 00:04:18.530
and how people think about that structure,

00:04:18.530 --> 00:04:22.775
and what kind of goals then people in natural language processing

00:04:22.775 --> 00:04:27.725
have of sort of building structure to understand the meaning of sentences.

00:04:27.725 --> 00:04:31.890
Um, all of the examples I'm going to give today are in English,

00:04:31.890 --> 00:04:36.390
um, because that's the language that you're all expected to have some competence in.

00:04:36.390 --> 00:04:39.545
But this really isn't meant to be sort of facts about English.

00:04:39.545 --> 00:04:43.460
This is meant to be sort of ideas of how you can think about the structure of

00:04:43.460 --> 00:04:48.145
human language sentences that are applied to all sorts of languages.

00:04:48.145 --> 00:04:51.575
Okay. So in general,

00:04:51.575 --> 00:04:54.410
there are two different ways that

00:04:54.410 --> 00:04:57.650
linguists have thought about the structure of sentences,

00:04:57.650 --> 00:04:59.420
though there's some relations to them.

00:04:59.420 --> 00:05:02.165
One of them is called phrase structure,

00:05:02.165 --> 00:05:04.085
or phrase structure grammars.

00:05:04.085 --> 00:05:08.480
And if you vaguely remember from CS103 if you did that,

00:05:08.480 --> 00:05:12.900
when you spent about the lecture on context-free grammars, um,

00:05:12.900 --> 00:05:15.050
phrase structure grammars are using the tools of

00:05:15.050 --> 00:05:17.930
context-free grammars to put structures over sentences.

00:05:17.930 --> 00:05:22.670
So, I'm first of all going to just briefly introduce that, so you've seen it,

00:05:22.670 --> 00:05:24.590
but actually the main tool that we're going to

00:05:24.590 --> 00:05:27.700
use in this class and for assignment three,

00:05:27.700 --> 00:05:30.560
is to do put dependency structures over,

00:05:30.560 --> 00:05:33.110
um, sentences, so I'll then go about that.

00:05:33.110 --> 00:05:36.290
So, the idea of phrase structure is to say that

00:05:36.290 --> 00:05:40.810
sentences are built out of units that progressively nest.

00:05:40.810 --> 00:05:44.140
So, we start off with words that, cat, cuddly,

00:05:44.140 --> 00:05:48.280
et cetera, and then we're gonna put them into bigger units that we call phrases,

00:05:48.280 --> 00:05:50.425
like "The cuddly cat by the door",

00:05:50.425 --> 00:05:54.190
and then you can keep on combining those up into even bigger phrases,

00:05:54.190 --> 00:05:56.465
like, "The cuddly cat by the door."

00:05:56.465 --> 00:06:00.660
Um, [NOISE] Okay, that's that.

00:06:00.660 --> 00:06:02.235
So, how does this work?

00:06:02.235 --> 00:06:04.230
Well, so the idea of it,

00:06:04.230 --> 00:06:06.570
and this is sort of the way linguists thinks,

00:06:06.570 --> 00:06:08.205
is to say, "Well,

00:06:08.205 --> 00:06:10.260
here's this language, which,

00:06:10.260 --> 00:06:12.465
you know, might not be English.

00:06:12.465 --> 00:06:15.055
It might be Oaxacan or some other language.

00:06:15.055 --> 00:06:17.320
What kind of structure does it have?

00:06:17.320 --> 00:06:21.985
And well, we could look at lots of sentences of the language.

00:06:21.985 --> 00:06:24.205
And so the linguist is gonna think,

00:06:24.205 --> 00:06:25.585
"Well, I can see,

00:06:25.585 --> 00:06:28.485
um, patterns, like the cat,

00:06:28.485 --> 00:06:29.820
a dog, the dog,

00:06:29.820 --> 00:06:31.280
a cat, et cetera.

00:06:31.280 --> 00:06:34.940
So, it's sort of seems like there's one word class here,

00:06:34.940 --> 00:06:37.940
which linguists often referred to as determiners.

00:06:37.940 --> 00:06:40.700
Um, they're also referred to as article sometimes in English.

00:06:40.700 --> 00:06:44.390
There's another word class here of nouns.

00:06:44.390 --> 00:06:48.295
And so, what I- to capture this pattern here,

00:06:48.295 --> 00:06:51.230
it seems like we can make this unit, um,

00:06:51.230 --> 00:06:55.025
that I see all over the place in language, um,

00:06:55.025 --> 00:06:57.095
which is made of a,

00:06:57.095 --> 00:07:00.340
um, a determiner, followed by a noun.

00:07:00.340 --> 00:07:02.190
So, I've write, um,

00:07:02.190 --> 00:07:04.275
a phrase structure grammar role,

00:07:04.275 --> 00:07:07.490
a context-free grammar role of- I can have

00:07:07.490 --> 00:07:11.245
a noun phrase that goes to a determiner, and a noun.

00:07:11.245 --> 00:07:12.795
Okay. But, you know,

00:07:12.795 --> 00:07:16.980
that's not the only thing that I can, um, see.

00:07:16.980 --> 00:07:20.870
So, I can also see, um,

00:07:20.870 --> 00:07:24.470
other examples in my language of the large cat,

00:07:24.470 --> 00:07:25.955
or a barking dog,

00:07:25.955 --> 00:07:28.400
or the cuddly cat, the cuddly dog.

00:07:28.400 --> 00:07:33.080
So, that seems that I need to put a bit more stuff into my grammar.

00:07:33.080 --> 00:07:38.430
So, maybe I can say from my grammar that a noun phrase goes to a determiner,

00:07:38.430 --> 00:07:41.195
and then optionally, you can put in an adjective,

00:07:41.195 --> 00:07:42.710
and then you can have a noun.

00:07:42.710 --> 00:07:45.800
And then I poke around a little bit further and I

00:07:45.800 --> 00:07:48.995
can find examples like the cat in a crate,

00:07:48.995 --> 00:07:52.220
or a barking dog by the door.

00:07:52.220 --> 00:07:55.100
And I can see lots of sentences like this.

00:07:55.100 --> 00:07:58.470
And so I want to put those into my grammar.

00:07:58.470 --> 00:08:01.410
But at that point, I noticed something special, because look,

00:08:01.410 --> 00:08:04.245
here are some other things,

00:08:04.245 --> 00:08:08.540
and these things look a lot like the things I started off with.

00:08:08.540 --> 00:08:09.590
So, it seems like,

00:08:09.590 --> 00:08:12.230
which sort of having a phrase with

00:08:12.230 --> 00:08:17.840
the same expansion potential that's nested inside this bigger phrase,

00:08:17.840 --> 00:08:21.150
because these ones can also be, um, expanded, right?

00:08:21.150 --> 00:08:24.720
I could have something like the green door something in here.

00:08:24.720 --> 00:08:27.315
So, I just wanna capture that in some way.

00:08:27.315 --> 00:08:32.885
So, maybe I could say that a noun phrase goes to a determiner,

00:08:32.885 --> 00:08:36.315
optionally an adjective, a noun,

00:08:36.315 --> 00:08:37.650
and then a something else,

00:08:37.650 --> 00:08:39.930
which I'll call a prepositional phrase.

00:08:39.930 --> 00:08:42.530
And then I'm gonna write a second rule saying that

00:08:42.530 --> 00:08:46.820
a prepositional phrase goes to a preposition,

00:08:46.820 --> 00:08:49.665
that's gonna be these words here,

00:08:49.665 --> 00:08:53.595
um, followed by a noun phrase.

00:08:53.595 --> 00:09:00.980
So then I'm reuse- [NOISE] I'm reusing my noun phrase that I defined up here.

00:09:00.980 --> 00:09:04.205
So then I could immediately generate other stuff.

00:09:04.205 --> 00:09:05.870
I can sort of say,

00:09:05.870 --> 00:09:10.515
"The cat by the, the large door."

00:09:10.515 --> 00:09:11.970
Or indeed I could say,

00:09:11.970 --> 00:09:15.190
"The cat by the large crate."

00:09:15.190 --> 00:09:18.990
Um, "The cat by the large crate on the table",

00:09:18.990 --> 00:09:20.160
or something like that,

00:09:20.160 --> 00:09:23.900
because once I can have the prepositional phrase includes a noun phrase,

00:09:23.900 --> 00:09:26.960
and a noun phrase includes a prepositional phrase,

00:09:26.960 --> 00:09:29.990
I've already got something that I can kind of

00:09:29.990 --> 00:09:33.650
recursively go back and forth between noun phrases,

00:09:33.650 --> 00:09:36.265
and I can make infinitely big sentences, right?

00:09:36.265 --> 00:09:39.090
Yeah?

00:09:39.090 --> 00:09:42.530
Yeah? So, I could write something like, yeah,

00:09:42.530 --> 00:09:46.690
"The cat by

00:09:46.690 --> 00:09:55.110
the large crate on the,

00:09:55.120 --> 00:10:02.410
um, large table, um, by the door."

00:10:02.410 --> 00:10:06.365
Right. I can keep on going and make big sentences.

00:10:06.365 --> 00:10:08.150
And I could say, well,

00:10:08.150 --> 00:10:12.260
I've got a- I don't have space to fit it on this slide,

00:10:12.260 --> 00:10:15.680
but I've got an analysis of this according to my grammar,

00:10:15.680 --> 00:10:21.110
where that's a noun phrase goes to a determiner noun prepositional phrase.

00:10:21.110 --> 00:10:23.855
The prepositional phrase goes to a preposition,

00:10:23.855 --> 00:10:25.520
and a noun phrase,

00:10:25.520 --> 00:10:27.920
and this noun phrase goes to a determiner,

00:10:27.920 --> 00:10:32.385
adjective, noun prepositional phrase.

00:10:32.385 --> 00:10:35.040
And that goes to a preposition,

00:10:35.040 --> 00:10:36.415
and another noun phrase,

00:10:36.415 --> 00:10:40.270
and I keep on going and I can produce big sentences.

00:10:40.270 --> 00:10:45.570
Okay. You know, that kind of then continues on,

00:10:45.570 --> 00:10:47.835
because, um, you know,

00:10:47.835 --> 00:10:50.970
I can then start seeing more bits of grammar.

00:10:50.970 --> 00:10:52.290
So, I could say, "Well,

00:10:52.290 --> 00:10:54.450
I can now talk to the cat."

00:10:54.450 --> 00:10:56.760
Um, and so if I wanna capture,

00:10:56.760 --> 00:10:59.970
um, this talking to a cat here, well,

00:10:59.970 --> 00:11:02.100
that now means I've got a verb,

00:11:02.100 --> 00:11:05.760
because words like talk and walk are verbs.

00:11:05.760 --> 00:11:07.830
And then talk to the cat,

00:11:07.830 --> 00:11:09.180
it seems like after that,

00:11:09.180 --> 00:11:11.210
it could become a prepositional phrase.

00:11:11.210 --> 00:11:14.555
And so I could write another rule saying that a verb phrase

00:11:14.555 --> 00:11:18.410
goes to a verb followed by a prepositional phrase.

00:11:18.410 --> 00:11:21.110
And then I can make more bigger sentences like that.

00:11:21.110 --> 00:11:27.060
And I could look at more sentences of the language and start building up these,

00:11:27.060 --> 00:11:32.310
these context-free grammar rules to describe the structure of the language.

00:11:32.310 --> 00:11:34.285
And that's part of what linguists do,

00:11:34.285 --> 00:11:38.465
and different languages, um, have different structures.

00:11:38.465 --> 00:11:40.890
So, um, for example,

00:11:40.890 --> 00:11:43.235
like in this, uh,

00:11:43.235 --> 00:11:46.620
little grammar I've had and in general in English, um,

00:11:46.620 --> 00:11:51.995
what you do, what you find is that prepositional phrases following the verb.

00:11:51.995 --> 00:11:54.940
But if you go to a different language like Chinese,

00:11:54.940 --> 00:11:57.980
what you find is the prepositional phrases come before the verb.

00:11:57.980 --> 00:11:59.310
And so, we could say okay,

00:11:59.310 --> 00:12:02.400
there are different rules for Chinese, um,

00:12:02.400 --> 00:12:07.305
and I could start writing a context-free grammar for them. Okay, beauty.

00:12:07.305 --> 00:12:10.169
Um,so that's the idea of context-free grammars,

00:12:10.169 --> 00:12:12.265
and actually, you know,

00:12:12.265 --> 00:12:15.980
this is the dominant approached linguistic structure

00:12:15.980 --> 00:12:20.570
that you'll see if you go and do a linguistics class in the linguistics department,

00:12:20.570 --> 00:12:24.180
people make these kinds of Phrase Structure Grammar trees.

00:12:24.180 --> 00:12:26.160
Um, but just to be contrary,

00:12:26.160 --> 00:12:28.205
no, it's not actually just to be contrary,

00:12:28.205 --> 00:12:30.660
it's because this alternative approach has been

00:12:30.660 --> 00:12:33.555
very dominant in computational linguistics.

00:12:33.555 --> 00:12:36.625
What I'm going to show you instead, um,

00:12:36.625 --> 00:12:40.655
is the view point of dependency structure.

00:12:40.655 --> 00:12:44.100
So, the idea of dependency structure

00:12:44.100 --> 00:12:47.750
is rather than having these sort of phrasal categories,

00:12:47.750 --> 00:12:50.314
like, noun phrases and prepositional phrases,

00:12:50.314 --> 00:12:51.745
and things like that,

00:12:51.745 --> 00:12:54.775
we are going to directly, um,

00:12:54.775 --> 00:12:58.945
represent the structure of sentences by saying,

00:12:58.945 --> 00:13:05.630
how words, how arguments or modifiers of other words in a recursive faction.

00:13:05.630 --> 00:13:09.895
Which is sort of another way of saying how the dependence on other words.

00:13:09.895 --> 00:13:11.100
So, we have a sentence,

00:13:11.100 --> 00:13:13.840
''Look in the large crate in the kitchen by the door''.

00:13:13.840 --> 00:13:16.300
And if we want to we can give these word,

00:13:16.300 --> 00:13:19.865
words word classes, so we can still say this is a verb,

00:13:19.865 --> 00:13:21.375
and this is a preposition,

00:13:21.375 --> 00:13:22.829
and this is a determiner,

00:13:22.829 --> 00:13:24.465
and this is an adjective,

00:13:24.465 --> 00:13:25.940
and this is a noun.

00:13:25.940 --> 00:13:27.920
But to represent the structure,

00:13:27.920 --> 00:13:30.305
what we're going to say is, "Well,

00:13:30.305 --> 00:13:35.105
look here is the the root of this whole sentence."

00:13:35.105 --> 00:13:37.440
So, that's where things start.

00:13:37.440 --> 00:13:42.530
Um, and then, well, where are we going to look is in the large crate,

00:13:42.530 --> 00:13:46.875
so that is a dependent of look.

00:13:46.875 --> 00:13:52.799
And well, if we- then we have for the crate,

00:13:52.799 --> 00:13:55.890
it's got some modifies its a large crate.

00:13:55.890 --> 00:13:57.660
So, that's a dependent of crate.

00:13:57.660 --> 00:13:59.515
Its the large crate,

00:13:59.515 --> 00:14:01.395
that's a dependence of crate.

00:14:01.395 --> 00:14:05.285
And in this system of dependencies I'm going to show you,

00:14:05.285 --> 00:14:08.425
we've got in as kind of,

00:14:08.425 --> 00:14:11.360
um, a modifier of crate in the large crate.

00:14:11.360 --> 00:14:13.020
I could come back to that.

00:14:13.020 --> 00:14:16.355
Well, but this crate has its own modification,

00:14:16.355 --> 00:14:18.385
because it's a crate in the kitchen.

00:14:18.385 --> 00:14:21.665
So, we have, in the kitchen,

00:14:21.665 --> 00:14:23.910
as a modifier of crate.

00:14:23.910 --> 00:14:26.740
And it's the kitchen in the kitchen,

00:14:26.740 --> 00:14:29.735
these are dependence of crate.

00:14:29.735 --> 00:14:34.115
And well, then we have this next bit by the door.

00:14:34.115 --> 00:14:36.075
And as I'll discuss in a minute, well,

00:14:36.075 --> 00:14:39.345
what does the by the door modifying?

00:14:39.345 --> 00:14:40.950
It's still modifying the crate,

00:14:40.950 --> 00:14:42.770
it saying, ''It's the crate by the door.''

00:14:42.770 --> 00:14:47.465
Okay. So, the by the door is also a dependent of crate,

00:14:47.465 --> 00:14:53.775
and then we've got the structure of dependencies coming off of it.

00:14:53.775 --> 00:14:56.265
Okay. And so that's then, um,

00:14:56.265 --> 00:14:59.010
the structure you get may be drawn a little bit more

00:14:59.010 --> 00:15:02.110
neatly when I did that in advance like this.

00:15:02.110 --> 00:15:05.790
And so we call these things, uh, dependency structure.

00:15:05.790 --> 00:15:08.670
And so crucially, what we're doing here,

00:15:08.670 --> 00:15:14.145
um, is that we're- sorry,

00:15:14.145 --> 00:15:15.750
I had two different examples.

00:15:15.750 --> 00:15:17.010
[NOISE] different examples.

00:15:17.010 --> 00:15:19.330
[LAUGHTER] Um, um,

00:15:19.330 --> 00:15:21.275
what we're doing is saying, what,

00:15:21.275 --> 00:15:24.840
what words modify other words?

00:15:24.840 --> 00:15:28.365
And so, that allows us to sort of

00:15:28.365 --> 00:15:32.570
understand how the different parts of the sentence relate to each other.

00:15:32.570 --> 00:15:34.945
And so, overall, you know,

00:15:34.945 --> 00:15:37.225
then- let me just so say here,

00:15:37.225 --> 00:15:39.920
you might want to why do we need sentence structure?

00:15:39.920 --> 00:15:41.760
You know, the way, um,

00:15:41.760 --> 00:15:44.280
language seems to work when you're talking to

00:15:44.280 --> 00:15:47.429
your friends is that you just blab of something,

00:15:47.429 --> 00:15:50.735
and I understand what you're saying, and, um,

00:15:50.735 --> 00:15:52.995
what goes on beyond that, um,

00:15:52.995 --> 00:15:55.905
is sort of not really accessible to consciousness.

00:15:55.905 --> 00:16:01.780
But well, to be able to have machines that interpret language correctly,

00:16:01.780 --> 00:16:05.660
we sort of need to understand the structure of these sentences,

00:16:05.660 --> 00:16:10.485
because unless we know what words are arguments and modifiers of other words,

00:16:10.485 --> 00:16:13.560
we can't actually work out what sentences mean.

00:16:13.560 --> 00:16:17.260
And I'll show some examples of that as to how things go wrong immediately,

00:16:17.260 --> 00:16:19.410
because actually, a lot of the time there are

00:16:19.410 --> 00:16:22.230
different possible interpretations you can have.

00:16:22.230 --> 00:16:23.495
And so, in general,

00:16:23.495 --> 00:16:24.845
our goal is, you know,

00:16:24.845 --> 00:16:27.815
up until now we've sort of looked at the meaning of words, right?

00:16:27.815 --> 00:16:29.140
We did word vectors,

00:16:29.140 --> 00:16:31.325
and we found that words there was similar meaning,

00:16:31.325 --> 00:16:32.550
and things like that.

00:16:32.550 --> 00:16:36.960
Um, and you can get somewhere in human languages with just saying words.

00:16:36.960 --> 00:16:40.085
I mean you can say, "Hi",

00:16:40.085 --> 00:16:44.350
and friendly, um, and things like that,

00:16:44.350 --> 00:16:46.695
but you can't get very far with just words, right?

00:16:46.695 --> 00:16:48.890
The way human beings can express

00:16:48.890 --> 00:16:52.550
complex ideas and explain and teach things to each other,

00:16:52.550 --> 00:16:57.500
is you can put together words to express more complex meanings.

00:16:57.500 --> 00:17:00.570
And then, you can do that over and over again

00:17:00.570 --> 00:17:04.120
recursively to build up more and more complex meanings,

00:17:04.120 --> 00:17:07.420
so that by the time you're reading the morning newspaper,

00:17:07.420 --> 00:17:10.805
you know most sentences are sort of 20-30 words long,

00:17:10.805 --> 00:17:12.225
and they're saying, um,

00:17:12.225 --> 00:17:14.335
some complex meaning, like you know,

00:17:14.335 --> 00:17:17.965
"Overnight Senate Republicans resolve that they would not do blah blah blah blah.''

00:17:17.965 --> 00:17:20.070
And you understand that flawlessly,

00:17:20.070 --> 00:17:22.850
by just sort of putting together those meanings of words.

00:17:22.850 --> 00:17:25.230
And so, we need to be able to know what is connected to

00:17:25.230 --> 00:17:28.010
what in order to be able to do that.

00:17:28.010 --> 00:17:30.740
And one of the ways of saying, um,

00:17:30.740 --> 00:17:32.765
that's important is saying,

00:17:32.765 --> 00:17:34.805
''What can go wrong?''

00:17:34.805 --> 00:17:38.675
Okay. So here, is a newspaper article.

00:17:38.675 --> 00:17:42.380
Uh, ''San Jose cop kills man with knife''.

00:17:42.380 --> 00:17:46.605
Um, now, this has two meanings and the two meanings, um,

00:17:46.605 --> 00:17:50.630
depend on, well, what you decide depends on what,

00:17:50.630 --> 00:17:52.190
you know, what modifies what?

00:17:52.190 --> 00:17:55.535
So, what are the two meanings. Meaning one.

00:17:55.535 --> 00:17:57.580
The cop stabs the guy. [LAUGHTER]

00:17:57.580 --> 00:17:59.310
The cop stabs the guy.

00:17:59.310 --> 00:18:02.235
Right. So, meaning one is the cop stabs that guy.

00:18:02.235 --> 00:18:04.625
So, what we've got here is,

00:18:04.625 --> 00:18:07.330
we've got the cops that are killing.

00:18:07.330 --> 00:18:12.070
So, this is what we'll say is the subject of kill,

00:18:12.070 --> 00:18:15.635
is the cops, and I'll just call them the San Jose cops here.

00:18:15.635 --> 00:18:19.365
And well, there's what they kill which say that,

00:18:19.365 --> 00:18:24.155
the man is an object of killing.

00:18:24.155 --> 00:18:27.615
Um, and then while one person is the,

00:18:27.615 --> 00:18:30.460
the cop using knife to kill the person.

00:18:30.460 --> 00:18:34.790
And so that's then that this is, um,

00:18:34.790 --> 00:18:38.739
modifier and here if we complex we call it an instrumental

00:18:38.739 --> 00:18:43.200
modifier to say that the cops are killing people with a knife.

00:18:43.200 --> 00:18:45.300
That's one possible analysis.

00:18:45.300 --> 00:18:49.055
Okay. Then, there's a second meaning sentence can have.

00:18:49.055 --> 00:18:52.275
The second meaning sentence can have. [NOISE]

00:18:52.275 --> 00:18:55.380
Okay. The second meaning the sentence can have is,

00:18:55.380 --> 00:18:57.225
that's the man has a knife.

00:18:57.225 --> 00:18:59.490
So, um, in that case,

00:18:59.490 --> 00:19:01.350
what we wanna say is, well, you know,

00:19:01.350 --> 00:19:03.165
is this word man,

00:19:03.165 --> 00:19:06.540
and this man has, uh,

00:19:06.540 --> 00:19:12.600
noun modifier, um, which is sort of saying something that the man possesses,

00:19:12.600 --> 00:19:14.595
and then this dependency is the same,

00:19:14.595 --> 00:19:16.860
and it's a man with a knife.

00:19:16.860 --> 00:19:23.430
Okay. And so, the interpretations of these sentences that you can get depend on putting

00:19:23.430 --> 00:19:30.405
different structures over the sentences in terms of who is- what is modifying what?

00:19:30.405 --> 00:19:33.675
Um, here is another one that's just like that one.

00:19:33.675 --> 00:19:37.020
Um, scientists count whales from space.

00:19:37.020 --> 00:19:38.850
[LAUGHTER] Okay.

00:19:38.850 --> 00:19:42.960
So again, this sentence has two possible structures, right?

00:19:42.960 --> 00:19:46.860
[LAUGHTER] That we have, the scientists are the subject that are

00:19:46.860 --> 00:19:51.330
counting and the whales are the object.

00:19:51.330 --> 00:19:57.135
Um, and, well, one possibility is that this is how they're doing the counting,

00:19:57.135 --> 00:20:02.520
um, so that they're counting the whales from space using something like a satellite.

00:20:02.520 --> 00:20:06.000
Um, but the other possibility is that these parts are the same,

00:20:06.000 --> 00:20:07.230
this is the subject,

00:20:07.230 --> 00:20:08.835
and this is the object,

00:20:08.835 --> 00:20:12.750
but these are whales from space which, you know,

00:20:12.750 --> 00:20:16.875
we could have analyzed as a noun phrase goes to,

00:20:16.875 --> 00:20:19.050
um, and now, on a PP,

00:20:19.050 --> 00:20:21.330
you know, um, constituency grammar,

00:20:21.330 --> 00:20:23.340
but its dependency grammar we saying, "Oh,

00:20:23.340 --> 00:20:28.275
this is now a modifier of the whales,

00:20:28.275 --> 00:20:31.200
and that they are whales from space, um,

00:20:31.200 --> 00:20:34.184
that are starting to turn up as in the bottom example."

00:20:34.184 --> 00:20:40.440
Right? So, obviously what you want is this one is correct and this one is here wrong.

00:20:40.440 --> 00:20:46.455
Um, and so this choice is referred to as a prepositional phrase attachment ambiguity,

00:20:46.455 --> 00:20:51.810
and it's one of the most common ambiguities in the parsing of English, right?

00:20:51.810 --> 00:20:55.140
So, here's our prepositional phrase from space.

00:20:55.140 --> 00:20:56.370
And so in general,

00:20:56.370 --> 00:21:00.840
when you have prepositional phrases and before it you have verbs,

00:21:00.840 --> 00:21:03.480
and noun phrases, or nouns,

00:21:03.480 --> 00:21:06.405
that the prepositional phrase can modify

00:21:06.405 --> 00:21:09.525
either of the things that come beforehand, right?

00:21:09.525 --> 00:21:12.330
And so this is a crucial way in which

00:21:12.330 --> 00:21:16.200
human languages are different from programming languages, right?

00:21:16.200 --> 00:21:20.360
In programming languages, we have hard rules

00:21:20.360 --> 00:21:24.890
as to how you meant to interpret things that dangle afterwards, right?

00:21:24.890 --> 00:21:27.140
So, in programming languages,

00:21:27.140 --> 00:21:31.765
you have an else is always construed with the closest if.

00:21:31.765 --> 00:21:33.930
Well, if that's not what you want, um,

00:21:33.930 --> 00:21:37.110
you have to use parentheses or indentation or something like that.

00:21:37.110 --> 00:21:39.870
I guess, it's different in Python because you have to use indentation.

00:21:39.870 --> 00:21:43.365
But if we think of something like C or a similar language, right?

00:21:43.365 --> 00:21:45.165
Um, if you haven't used,

00:21:45.165 --> 00:21:46.965
um, braces to indicate,

00:21:46.965 --> 00:21:51.060
it's just deterministically, the else goes with the closest if.

00:21:51.060 --> 00:21:54.105
Um, but that's not how human languages are.

00:21:54.105 --> 00:21:55.920
Human languages are, um,

00:21:55.920 --> 00:21:59.940
this prepositional phrase can go with anything proceeding,

00:21:59.940 --> 00:22:04.485
and the hearer is assumed to be smart enough to work out the right one.

00:22:04.485 --> 00:22:07.050
And, you know, that's actually a pa- large part of why

00:22:07.050 --> 00:22:10.440
human communication is so efficient, right?

00:22:10.440 --> 00:22:14.250
Like, um, we can do such a good job at communicating with

00:22:14.250 --> 00:22:18.360
each other because most of the time we don't have to say very much,

00:22:18.360 --> 00:22:21.495
and there's this really smart person on the other end, um,

00:22:21.495 --> 00:22:25.905
who can interpret the words that we say in the right way.

00:22:25.905 --> 00:22:31.800
Um, so, that's where if you want to have artificial intelligence and smart computers,

00:22:31.800 --> 00:22:37.065
we then start to need to build language understanding devices who can also,

00:22:37.065 --> 00:22:38.910
um, work on that basis.

00:22:38.910 --> 00:22:44.985
That they can just decide what would be the right thing for form space to modify.

00:22:44.985 --> 00:22:46.560
And if we have that working really well,

00:22:46.560 --> 00:22:49.050
we can then apply it back to programming languages,

00:22:49.050 --> 00:22:52.725
and you could just not put in any braces in your programming languages,

00:22:52.725 --> 00:22:55.110
and the compiler would work out what you meant.

00:22:55.110 --> 00:22:58.575
Um, okay. So, this is prepositional phrase attachment.

00:22:58.575 --> 00:23:02.445
It's sort of seems maybe not that hard there,

00:23:02.445 --> 00:23:04.965
but you know, it, it gets worse, I mean,

00:23:04.965 --> 00:23:06.990
this isn't as fun an example,

00:23:06.990 --> 00:23:12.465
but it's a real example of a sentence from The Wall Street Journal actually.

00:23:12.465 --> 00:23:18.120
The board approved this acquisition by Royal Trustco Limited of Toronto for $0.27,

00:23:18.120 --> 00:23:20.955
$27 a share at its monthly meeting.

00:23:20.955 --> 00:23:22.635
Boring sentence, but, um,

00:23:22.635 --> 00:23:24.660
what is the structure of this sentence?

00:23:24.660 --> 00:23:26.670
Well, you know, we've got a verb here,

00:23:26.670 --> 00:23:29.805
and we've got exactly the same subject,

00:23:29.805 --> 00:23:32.295
and for this noun,

00:23:32.295 --> 00:23:34.965
um, object coming after it.

00:23:34.965 --> 00:23:36.570
But then what happens after that?

00:23:36.570 --> 00:23:38.670
Well, here, we've got a prepositional phrase.

00:23:38.670 --> 00:23:40.575
Here, we've got a prepositional phrase.

00:23:40.575 --> 00:23:44.700
You've just got a see four prepositional phrases in a row.

00:23:44.700 --> 00:23:48.600
And so, well, what we wanna

00:23:48.600 --> 00:23:52.845
do is say for each of these prepositional phrases what they modify,

00:23:52.845 --> 00:23:55.589
and starting off there only two choices,

00:23:55.589 --> 00:23:58.020
the verb and the noun proceeding as before.

00:23:58.020 --> 00:24:01.590
But it's gonna get more complicated as we go in, because look,

00:24:01.590 --> 00:24:02.910
there's another noun here,

00:24:02.910 --> 00:24:04.245
and another noun here,

00:24:04.245 --> 00:24:06.825
and another noun here.

00:24:06.825 --> 00:24:11.475
Um, so once we start getting further in there'll be more possibilities.

00:24:11.475 --> 00:24:13.185
Okay. So, let's see if we can,

00:24:13.185 --> 00:24:14.400
um, work it out.

00:24:14.400 --> 00:24:18.720
So, um, by Royal Trustco Limited, what's that modifying?

00:24:18.720 --> 00:24:25.709
[NOISE] Right. You see acquisition,

00:24:25.709 --> 00:24:28.800
so it's not the board approved by Royal Trustco Limited,

00:24:28.800 --> 00:24:32.145
it's an acquisition by Royal Trustco Limited.

00:24:32.145 --> 00:24:36.750
Okay. So, this one is a dependent of the acquisition.

00:24:36.750 --> 00:24:39.975
Okay. Um, now, we went to of Toronto,

00:24:39.975 --> 00:24:41.580
and we have three choices,

00:24:41.580 --> 00:24:43.875
that could be this, this, or this.

00:24:43.875 --> 00:24:48.340
Okay. So, of Toronto is modifying.

00:24:50.060 --> 00:24:51.540
Acquisition. [NOISE]

00:24:51.540 --> 00:24:53.010
Its acquisition of Toronto?

00:24:53.010 --> 00:24:59.055
[LAUGHTER] No, I think that's a wrong answer.

00:24:59.055 --> 00:25:04.080
Um. [LAUGHTER] Is there another guess for what of Toronto is modifying?

00:25:04.080 --> 00:25:05.640
Royal Trustco.

00:25:05.640 --> 00:25:09.180
Royal Trustco, right. So, it's Royal Trustco Limited of Toronto.

00:25:09.180 --> 00:25:14.550
So, this of Toronto is a dependent of Royal Trustco Limited.

00:25:14.550 --> 00:25:16.170
And Royal Trustco Limited,

00:25:16.170 --> 00:25:17.580
right, that's this again,

00:25:17.580 --> 00:25:18.840
sort of this noun phrase,

00:25:18.840 --> 00:25:21.840
so it can also have modifiers by prepositional phrase.

00:25:21.840 --> 00:25:29.400
Okay. For $27 a share is modifying acquisition, right?

00:25:29.400 --> 00:25:32.220
[NOISE] So now, we leap right back.

00:25:32.220 --> 00:25:34.320
[NOISE] I'm drawing this wrong.

00:25:34.320 --> 00:25:37.170
Now, we leap right back and,

00:25:37.170 --> 00:25:40.350
um, is now the acquisition that's being modified.

00:25:40.350 --> 00:25:45.600
And then finally, we have at its monthly meeting is modifying?

00:25:45.600 --> 00:25:47.940
[NOISE]

00:25:47.940 --> 00:25:48.120
Approved.

00:25:48.120 --> 00:25:49.410
Well, the approved, right?

00:25:49.410 --> 00:25:50.520
It's approved, yeah.

00:25:50.520 --> 00:25:53.055
It's approved that its monthly meeting.

00:25:53.055 --> 00:25:55.110
Okay. [NOISE] I drew that on,

00:25:55.110 --> 00:25:59.730
[NOISE] I drew that one the wrong way around with the arrow.

00:25:59.730 --> 00:26:01.830
Sorry, it should have been done this way.

00:26:01.830 --> 00:26:05.260
I'm getting my arrows wrong. [NOISE] Um, um.

00:26:06.390 --> 00:26:13.670
Okay. So that we've got this pattern of how things are modifying.

00:26:13.770 --> 00:26:17.175
Um, [NOISE] and so actually, you know,

00:26:17.175 --> 00:26:21.390
once you start having a lot of things that have choices like this,

00:26:21.390 --> 00:26:25.165
you stop having- if I wanna put an analysis ac-

00:26:25.165 --> 00:26:29.680
on to this sentence I've to work out the, the right structure,

00:26:29.680 --> 00:26:36.415
I have to potentially consider an exponential number of possible structures because,

00:26:36.415 --> 00:26:40.435
I've got this situation where for the first prepositional phrase,

00:26:40.435 --> 00:26:43.540
there were two places that could have modified.

00:26:43.540 --> 00:26:45.610
For the second prepositional phrase,

00:26:45.610 --> 00:26:47.965
there are three places that could have modified.

00:26:47.965 --> 00:26:49.180
For the fourth one,

00:26:49.180 --> 00:26:51.580
there are five places that could have modified.

00:26:51.580 --> 00:26:53.605
That just sounds like a factorial.

00:26:53.605 --> 00:26:57.325
It's not quite as bad as the factorial, because normally,

00:26:57.325 --> 00:27:01.930
once you've let back that kind of closes off the ones in the middle.

00:27:01.930 --> 00:27:04.900
And so, further prepositional phrases have to be

00:27:04.900 --> 00:27:08.140
at least as far back in terms of what they modify.

00:27:08.140 --> 00:27:13.000
And so, if you get into this sort of combinatorics stuff the number of analyses you get

00:27:13.000 --> 00:27:17.830
when you get multiple prepositional phrases is the sequence called the Catalan numbers.

00:27:17.830 --> 00:27:20.890
Ah, but that's still an exponential series.

00:27:20.890 --> 00:27:26.080
And it's sort of one that turns up in a lot of places when they're tree-like contexts.

00:27:26.080 --> 00:27:30.910
So, if any of you are doing or have done CS228,

00:27:30.910 --> 00:27:32.140
where you see, um,

00:27:32.140 --> 00:27:35.575
triangular- triangulation of, ah,

00:27:35.575 --> 00:27:40.210
probabilistic graphical models and you ask how many triangulations there are,

00:27:40.210 --> 00:27:43.375
that's sort of like making a tree over your variables.

00:27:43.375 --> 00:27:47.605
And that's, again, gives you the number of them as the Catalan series.

00:27:47.605 --> 00:27:49.180
Okay. But- so the point is,

00:27:49.180 --> 00:27:52.315
we ha- end up with a lot of ambiguities.

00:27:52.315 --> 00:27:55.060
Okay. So, that's prepositional phrase attachments.

00:27:55.060 --> 00:27:56.950
A lot of those going on.

00:27:56.950 --> 00:27:59.320
They are far from the only kind of ambiguity.

00:27:59.320 --> 00:28:01.900
So, I wanted to tell you about a few others.

00:28:01.900 --> 00:28:08.950
Um, okay, shuttle veteran and longtime NASA executive Fred Gregory appointed to board.

00:28:08.950 --> 00:28:12.790
Um, why is this sentence ambiguous?

00:28:12.790 --> 00:28:14.929
What are the different reading of this statement?

00:28:14.929 --> 00:28:16.400
[NOISE].

00:28:16.400 --> 00:28:20.800
Yes?

00:28:20.800 --> 00:28:22.150
Uh, it's a better [inaudible]

00:28:22.150 --> 00:28:24.700
Okay. So, um, right answer.

00:28:24.700 --> 00:28:26.920
So, yeah there are two possibilities, right?

00:28:26.920 --> 00:28:30.235
That is either that there's somebody who's

00:28:30.235 --> 00:28:34.030
a shuttle veteran and a long time NASA executive,

00:28:34.030 --> 00:28:35.875
and their name is Fred Gregory,

00:28:35.875 --> 00:28:38.395
and that they've been appointed to the board.

00:28:38.395 --> 00:28:42.550
Um, or, um, the other possibility

00:28:42.550 --> 00:28:46.570
is that there's a shuttle veteran and there's a long time NASA executive,

00:28:46.570 --> 00:28:51.175
Fred Gregory, and both of them have been appointed to the board.

00:28:51.175 --> 00:28:56.980
And so, again, we can start to indicate the structure of that using our dependency.

00:28:56.980 --> 00:28:58.360
So, we can ether,

00:28:58.360 --> 00:29:02.980
um, say, okay, um,

00:29:02.980 --> 00:29:09.955
there's Fred Gregory and then this person is, um,

00:29:09.955 --> 00:29:15.205
a shuttle veteran and long ta- and whoops,

00:29:15.205 --> 00:29:17.680
and longtime NASA executive.

00:29:17.680 --> 00:29:20.140
Or we can say, well,

00:29:20.140 --> 00:29:28.690
we're doing appointment of a veteran and the longtime NASA executive, Fred Gregory.

00:29:28.690 --> 00:29:31.060
And so, we can represent by dependencies,

00:29:31.060 --> 00:29:33.715
um, these two different structures.

00:29:33.715 --> 00:29:37.645
Okay. Um, that's, um, one.

00:29:37.645 --> 00:29:40.120
Um, That one is not very funny again.

00:29:40.120 --> 00:29:45.775
So- so, here's a funnier example that illustrates the same ambiguity effectively.

00:29:45.775 --> 00:29:49.630
Um, so, here's precedence first physical.

00:29:49.630 --> 00:29:52.570
Doctor: No heart, cognitive issues.

00:29:52.570 --> 00:29:56.590
[LAUGHTER] Um, so, there isn't actually an explicit,

00:29:56.590 --> 00:29:59.620
um, coordination word here.

00:29:59.620 --> 00:30:02.830
But effectively in, um,

00:30:02.830 --> 00:30:06.445
a natural language or certainly English, um,

00:30:06.445 --> 00:30:11.170
you can use kind of just comma of sort of list intonation

00:30:11.170 --> 00:30:16.300
to effectively act as if it was an "And" or an "Or", right?

00:30:16.300 --> 00:30:22.645
So, here, um, we have again two possibilities that either we have

00:30:22.645 --> 00:30:26.650
issues and the dep- and the dependencies

00:30:26.650 --> 00:30:31.240
of- the dependencies of issues is that there are no issues.

00:30:31.240 --> 00:30:36.025
So, that's actually a determiner, ah, no issues.

00:30:36.025 --> 00:30:40.210
Um, and then it's sort of like no heart or cognitive issues.

00:30:40.210 --> 00:30:42.790
So, heart is another dependent.

00:30:42.790 --> 00:30:45.610
It's sort of a non-compound heart issues.

00:30:45.610 --> 00:30:48.745
And so, we refer to that as an independency,

00:30:48.745 --> 00:30:53.890
and then it's heart or, um, cognitive.

00:30:53.890 --> 00:30:57.040
Um, so that heart or cognitive is

00:30:57.040 --> 00:31:03.400
a conjoined phrase inside of this "No heart" or "Cognitive issues".

00:31:03.400 --> 00:31:05.410
But there's another possibility,

00:31:05.410 --> 00:31:07.075
um, which is, um,

00:31:07.075 --> 00:31:14.470
that the coordination is at the top level that we have "No heart" and "Cognitive issues".

00:31:14.470 --> 00:31:16.885
And, um, at that point,

00:31:16.885 --> 00:31:23.604
we ha- have the "Cognitive" as an adjective modifier of the "Issues" and the "No heart",

00:31:23.604 --> 00:31:26.410
the determiner is just a modifier of "Heart",

00:31:26.410 --> 00:31:29.530
and then these being conjoined together.

00:31:29.530 --> 00:31:36.920
So, um, "Heart" has a depend- has a coordinated dependency of "Issues".

00:31:37.020 --> 00:31:40.880
Okay. That's one one.

00:31:40.980 --> 00:31:44.005
Um, I've got more funny ones.

00:31:44.005 --> 00:31:48.580
Susan gets- [NOISE] [LAUGHTER] Okay.

00:31:48.580 --> 00:31:52.240
So, what the person [LAUGHTER] who wrote this intended to

00:31:52.240 --> 00:31:57.460
have is that there- we- Here we've got an adjective modifier ambiguity.

00:31:57.460 --> 00:32:00.490
So, the intended reading was, um,

00:32:00.490 --> 00:32:08.935
that "First" is an adjectival modifier of "First hand" and it's firsthand experience.

00:32:08.935 --> 00:32:12.700
Um, so, the "First hand" is a modifier of

00:32:12.700 --> 00:32:18.295
"Experience" and the "Job" is also a modifier of "Experience".

00:32:18.295 --> 00:32:21.265
And then we have the same kind of subject,

00:32:21.265 --> 00:32:26.275
object, um, reading on that one.

00:32:26.275 --> 00:32:31.810
Um, but unfortunately, um, this sentence, um,

00:32:31.810 --> 00:32:34.585
has a different reading, um,

00:32:34.585 --> 00:32:38.050
where you change the modification relationships.

00:32:38.050 --> 00:32:47.830
Um, and you have it's the first experience and it goes like this. Um. [LAUGHTER] Okay.

00:32:47.830 --> 00:32:50.395
[NOISE] One more example.

00:32:50.395 --> 00:32:57.265
Um, "Mutilated body washes up on Rio beach to be used for Olympics beach volleyball."

00:32:57.265 --> 00:33:01.780
Um, wha- what are- [LAUGHTER]

00:33:01.780 --> 00:33:06.100
what are the two ambigui- What are the two readings that you can get for this one?

00:33:06.100 --> 00:33:11.860
[NOISE]

00:33:11.860 --> 00:33:15.610
We've got this big phrase that I want to try and put

00:33:15.610 --> 00:33:20.305
a structure of to be used for Olympic beach volleyball,

00:33:20.305 --> 00:33:22.300
um, and then, you know,

00:33:22.300 --> 00:33:26.170
this is sort of like a prepositional phrase attachment ambiguity

00:33:26.170 --> 00:33:30.715
but this time instead of it's a prepositional phrase that's being attached,

00:33:30.715 --> 00:33:34.060
we've now got this big verb phrase we call it, right,

00:33:34.060 --> 00:33:37.960
so that when you've sort of got most of a sentence but without any subject to it,

00:33:37.960 --> 00:33:40.510
that's sort of a verb phrase to be used for

00:33:40.510 --> 00:33:44.095
Olympic beach volleyball which might be then infinitive form.

00:33:44.095 --> 00:33:48.850
Sometimes it's in part of CPO form like being used for beach volleyball.

00:33:48.850 --> 00:33:54.970
And really, those kind of verb phrases they sort of just like, um, prepositional phrases.

00:33:54.970 --> 00:33:58.120
Whenever they appear towards the right end of sentences,

00:33:58.120 --> 00:34:01.840
they can modify various things like verbs or nouns.

00:34:01.840 --> 00:34:05.830
Um, so, here, um, we have two possibilities.

00:34:05.830 --> 00:34:09.070
So, this to be used for Olympics beach volleyball.

00:34:09.070 --> 00:34:14.650
Um, what the right answer is meant to be is that that is a dependent of the Rio beach.

00:34:14.650 --> 00:34:15.835
So, it's a, um,

00:34:15.835 --> 00:34:18.475
modifier of the Rio Beach.

00:34:18.475 --> 00:34:20.845
Um, but the funny reading is,

00:34:20.845 --> 00:34:23.380
um, that instead of that, um,

00:34:23.380 --> 00:34:28.135
we can have here is another noun phrase muti- mutilated body,

00:34:28.135 --> 00:34:32.425
um, and it's the mutilated body that's going to be used.

00:34:32.425 --> 00:34:35.590
Um, and so then this would be, uh,

00:34:35.590 --> 00:34:39.820
a noun phrase modifier [NOISE] of that.

00:34:39.820 --> 00:34:45.430
Okay. Um, so knowing the right structure of sentences is

00:34:45.430 --> 00:34:48.130
important to understand the interpretations you're

00:34:48.130 --> 00:34:51.775
meant to get and the interpretations you're not meant to get.

00:34:51.775 --> 00:34:55.330
Okay. But it's, it's sort of, um, okay,

00:34:55.330 --> 00:34:58.780
you know, I was using funny examples for the obvious reason, but, you know,

00:34:58.780 --> 00:35:01.720
this is sort of essential to all the things that

00:35:01.720 --> 00:35:05.140
we'd like to get out of language most of the time.

00:35:05.140 --> 00:35:07.180
So, you know, this is back to the kind of

00:35:07.180 --> 00:35:10.240
boring stuff that we often work with of reading through

00:35:10.240 --> 00:35:14.170
biomedical research articles and trying to extract facts

00:35:14.170 --> 00:35:18.220
about protein-protein interactions from them or something like that.

00:35:18.220 --> 00:35:19.720
So, you know, this is, um,

00:35:19.720 --> 00:35:28.465
the results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB.

00:35:28.465 --> 00:35:33.550
Um, and well, [NOISE] I turned the notification's off.

00:35:33.550 --> 00:35:40.705
[NOISE] Um, so, if we wanna get out sort of protein-protein interaction,

00:35:40.705 --> 00:35:42.130
um, facts, you know, well,

00:35:42.130 --> 00:35:47.395
we have this KaiC that's interacting with these other proteins over there.

00:35:47.395 --> 00:35:53.560
And well, the way we can do that is looking at patterns in our dependency analysis,

00:35:53.560 --> 00:35:56.245
and so that we can sort of, um,

00:35:56.245 --> 00:36:00.055
see this repeated pattern where you have, um,

00:36:00.055 --> 00:36:07.930
the noun subject here interacts with a noun modifier,

00:36:07.930 --> 00:36:12.760
and then it's going to be these things that are beneath that of the SasA

00:36:12.760 --> 00:36:18.055
and its conjoin things KaiA and KaiB are the things that interacts with.

00:36:18.055 --> 00:36:24.340
So, we can kind of think of these two things as essentially, um, patterns.

00:36:24.340 --> 00:36:26.920
[NOISE] I actually mis-edited this.

00:36:26.920 --> 00:36:29.170
Sorry. This should also be nmod:with.

00:36:29.170 --> 00:36:33.910
[NOISE] Um, we can kind of think of

00:36:33.910 --> 00:36:36.010
these two things as sort of patterns and

00:36:36.010 --> 00:36:40.315
dependencies that we could look for to find examples of,

00:36:40.315 --> 00:36:46.495
um, just protein-protein interactions that appear in biomedical text.

00:36:46.495 --> 00:36:51.940
Okay. Um, so that's the general idea of what we wanna do,

00:36:51.940 --> 00:36:55.690
and so the total we want to do it with is these Dependency Grammars.

00:36:55.690 --> 00:36:59.305
And so, I've sort of shown you some Dependency Grammars.

00:36:59.305 --> 00:37:03.280
I just want us to sort of motivate Dependency Grammar a bit more,

00:37:03.280 --> 00:37:05.830
um, formally and fully, right?

00:37:05.830 --> 00:37:08.365
So, Dependency Grammar, um,

00:37:08.365 --> 00:37:13.210
postulates the what is syntactic structure is is that you have, um,

00:37:13.210 --> 00:37:15.970
relations between lexical items that are sort of

00:37:15.970 --> 00:37:19.690
binary asymmetric relations which we draw as arrows,

00:37:19.690 --> 00:37:21.520
because they are binary and asymmetric,

00:37:21.520 --> 00:37:23.890
and we call dependencies.

00:37:23.890 --> 00:37:26.290
And there's sort of two ways, common ways,

00:37:26.290 --> 00:37:29.290
of writing them, and I've sort of shown both now.

00:37:29.290 --> 00:37:33.565
One way is you sort of put the words in a line and that makes it.

00:37:33.565 --> 00:37:35.680
He see, let's see the whole sentence.

00:37:35.680 --> 00:37:38.470
You draw this sort of loopy arrows above them and

00:37:38.470 --> 00:37:41.905
the other way is you sort of more represent it as a tree,

00:37:41.905 --> 00:37:44.470
where you put the head of the whole sentence at the top,

00:37:44.470 --> 00:37:49.240
submitted and then you say the dependence of submitted,

00:37:49.240 --> 00:37:51.970
uh, bills were in Brownback and then you say,

00:37:51.970 --> 00:37:54.250
um, the dependence of each of those.

00:37:54.250 --> 00:37:58.240
Um, so, it was bills on ports and immigration.

00:37:58.240 --> 00:38:01.840
So, the dependence of bills and were submitted words,

00:38:01.840 --> 00:38:05.755
the dependent of submitted and you're giving this kind of tree structure.

00:38:05.755 --> 00:38:12.700
Okay. Um, so, in addition to the arrows commonly what we do is we

00:38:12.700 --> 00:38:19.120
put a type on each arrow which says what grammatical relations holding them between them.

00:38:19.120 --> 00:38:21.640
So, is this the subject of the sentence?

00:38:21.640 --> 00:38:23.620
Is it the object of the verb?

00:38:23.620 --> 00:38:25.225
Is that a, um,

00:38:25.225 --> 00:38:27.280
a conjunct and things like that?

00:38:27.280 --> 00:38:30.550
We have a system of dependency labels.

00:38:30.550 --> 00:38:32.815
Um, so, for the assignment,

00:38:32.815 --> 00:38:36.910
what we're gonna do is use universal dependencies,

00:38:36.910 --> 00:38:38.140
which I'll show you more,

00:38:38.140 --> 00:38:39.955
a little bit more in a minute.

00:38:39.955 --> 00:38:41.125
And if you think,

00:38:41.125 --> 00:38:42.790
"Man, this stuff is fascinating.

00:38:42.790 --> 00:38:45.250
I wanna learn all about these linguist structures."

00:38:45.250 --> 00:38:47.830
Um, there's a universal dependency site, um,

00:38:47.830 --> 00:38:50.860
that you go and can go off and look at it and learn all about them.

00:38:50.860 --> 00:38:54.100
But, if you don't think that's fascinating, um,

00:38:54.100 --> 00:38:56.365
for what we're doing for this class,

00:38:56.365 --> 00:38:59.095
we're never gonna make use of these labels.

00:38:59.095 --> 00:39:02.620
All we're doing is making use of the arrows.

00:39:02.620 --> 00:39:04.165
And for the arrows,

00:39:04.165 --> 00:39:08.590
you should be able to interpret things like prepositional phrases as to what they're

00:39:08.590 --> 00:39:10.930
modifying just in terms of where

00:39:10.930 --> 00:39:15.430
the prepositional phrases are connected and whether that's right or wrong.

00:39:15.430 --> 00:39:18.070
Okay. Yes. So formally,

00:39:18.070 --> 00:39:20.695
when we have this kind of Dependency Grammar,

00:39:20.695 --> 00:39:24.310
we've sort of drawing these arrows and we sort of refer to

00:39:24.310 --> 00:39:28.390
the thing at this end as the head of a dependency.

00:39:28.390 --> 00:39:33.025
And the thing at this end as the dependent of the dependency.

00:39:33.025 --> 00:39:36.910
And as in these examples are normal expectation

00:39:36.910 --> 00:39:41.170
and what our policies are gonna do is the dependencies form a tree.

00:39:41.170 --> 00:39:44.274
So, it's a connected acyclic single,

00:39:44.274 --> 00:39:47.635
um, rooted graph at the end of the day.

00:39:47.635 --> 00:39:52.855
Okay. So, Dependency Grammar has an enormously long history.

00:39:52.855 --> 00:39:59.110
So, basically, the famous first linguists that human beings know about his Panini who,

00:39:59.110 --> 00:40:02.125
um, wrote in the fifth century before the Common Era

00:40:02.125 --> 00:40:05.470
and tried to describe the structure of Sanskrit.

00:40:05.470 --> 00:40:09.610
And a lot of what Panini did was working out things about all of

00:40:09.610 --> 00:40:14.035
the morphology of Sanskrit that I'm not gonna touch at the moment.

00:40:14.035 --> 00:40:19.330
But beyond that, he started trying to describe the structure of Sanskrit sentences.

00:40:19.330 --> 00:40:23.335
And, um, the notation was sort of different but, essentially,

00:40:23.335 --> 00:40:26.290
the mechanism he used for describing the structure of

00:40:26.290 --> 00:40:29.770
Sanskrit was dependencies of sort of working out these,

00:40:29.770 --> 00:40:35.740
um, what are arguments in modifies of what relationships like we've been looking at.

00:40:35.740 --> 00:40:40.840
And indeed, if you look at kind of the history of humankind, um,

00:40:40.840 --> 00:40:44.380
most of attempts to understand the structure of

00:40:44.380 --> 00:40:48.010
human languages are essentially Dependency Grammars.

00:40:48.010 --> 00:40:52.870
Um, so, sort of in the later parts of the first millennium,

00:40:52.870 --> 00:40:56.680
there was a ton of work by Arabic grammarians and essentially what

00:40:56.680 --> 00:41:00.670
they used is also kind of basically a Dependency Grammar.

00:41:00.670 --> 00:41:03.325
Um, so compared to that, you know,

00:41:03.325 --> 00:41:05.740
the idea of context-free grammars and

00:41:05.740 --> 00:41:10.120
phrase structure grammars is incredibly incredibly new.

00:41:10.120 --> 00:41:12.430
I mean, you can basically, um, totally date it.

00:41:12.430 --> 00:41:16.570
There was this guy Wells in 1947 who first proposed

00:41:16.570 --> 00:41:20.980
this idea of having these constituents and phrase structure grammars,

00:41:20.980 --> 00:41:25.510
and where it then became really famous is through the work of Chomsky, um,

00:41:25.510 --> 00:41:29.725
which love him or hate him is by far the most famous, um,

00:41:29.725 --> 00:41:34.015
linguist and also variously contributed to Computer Science.

00:41:34.015 --> 00:41:35.800
Who's head of the Chomsky hierarchy?

00:41:35.800 --> 00:41:37.615
Do people remember that 103?

00:41:37.615 --> 00:41:40.210
Yeah. Okay, the Chomsky hierarchy,

00:41:40.210 --> 00:41:46.375
the Chomsky hierarchy was not invented to torture beginning computer science students.

00:41:46.375 --> 00:41:51.265
The Chomsky hierarchy was invented because Chomsky wanted to make

00:41:51.265 --> 00:41:57.020
arguments as to what the complexity of human languages was, um.

00:41:57.020 --> 00:42:00.120
Okay. Yeah. So, in modern work,

00:42:00.120 --> 00:42:03.180
uh, there's this guy Lucie Tesniere.

00:42:03.180 --> 00:42:06.120
Um, and he sort of formalized

00:42:06.120 --> 00:42:09.755
the kind of version of dependency grammar that I've been showing you.

00:42:09.755 --> 00:42:13.375
So, um we sort of often talk about his work.

00:42:13.375 --> 00:42:18.250
And you know it's- it's long-term being influential and computational linguistics.

00:42:18.250 --> 00:42:20.315
Some of the earliest parsing work in

00:42:20.315 --> 00:42:23.435
US Computational Linguistics was dependency grammars.

00:42:23.435 --> 00:42:27.030
But I won't go on about that um more now.

00:42:27.030 --> 00:42:29.780
Okay. Um, just one,

00:42:29.780 --> 00:42:32.635
two little things um, to note.

00:42:32.635 --> 00:42:37.434
I mean, if you somehow start looking at other papers where their dependency grammars,

00:42:37.434 --> 00:42:42.230
people aren't consistent on which way to have the arrows point.

00:42:42.230 --> 00:42:46.115
There's sort of two ways of thinking about this um,

00:42:46.115 --> 00:42:49.090
that you can either think okay,

00:42:49.090 --> 00:42:53.575
I'm gonna start at the head and point to the dependent.

00:42:53.575 --> 00:42:57.660
Or you can say I'm going to start at the dependent and say what its head is,

00:42:57.660 --> 00:42:59.080
and you find both of them.

00:42:59.080 --> 00:43:04.425
Uh, the way we're gonna do it in this class is to do it the way Tesniere did it,

00:43:04.425 --> 00:43:08.490
which was she started the head and pointed to the dependent.

00:43:08.490 --> 00:43:11.360
Uh, sorry. I'm drawing that wrong.

00:43:11.360 --> 00:43:14.920
Whoops, um because discussion of the outstanding issues.

00:43:14.920 --> 00:43:19.495
So, really um, the dependent is sort of discussion.

00:43:19.495 --> 00:43:22.065
Um, okay. We go from heads to dependence.

00:43:22.065 --> 00:43:26.880
And usually, it's convenient to serve in addition to the sentence to

00:43:26.880 --> 00:43:31.790
sort of have a fake root node that points to the head of the whole sentence.

00:43:31.790 --> 00:43:34.250
So, we use that as well.

00:43:34.250 --> 00:43:42.825
Okay. Um, so to build a dependency pauses or to indeed build

00:43:42.825 --> 00:43:47.679
any kind of human language structure

00:43:47.679 --> 00:43:51.530
finders including kind of constituency grammar pauses,

00:43:51.530 --> 00:43:55.470
the central tool in recent work,

00:43:55.470 --> 00:44:03.185
where recent work kind of means the last 25 years has been this idea of tree banks.

00:44:03.185 --> 00:44:08.160
Um, and the idea of tree banks is to say we are going to get

00:44:08.160 --> 00:44:15.605
human beings to sit around and [NOISE] put grammatical structures over sentences.

00:44:15.605 --> 00:44:17.660
So, here are some examples I'm showing you from

00:44:17.660 --> 00:44:22.070
Universal Dependencies where here are some um, English sentences.

00:44:22.070 --> 00:44:25.445
I think Miramar was a famous goat trainer or something.

00:44:25.445 --> 00:44:28.530
And some human being has sat and put

00:44:28.530 --> 00:44:31.760
a dependency structure over this sentence and all the rest.

00:44:31.760 --> 00:44:34.854
Um, and with the name Universal Dependencies,

00:44:34.854 --> 00:44:36.385
this is just an aside.

00:44:36.385 --> 00:44:40.655
Um, Universal Dependencies is actually project I've been strongly involved with.

00:44:40.655 --> 00:44:43.735
But precisely what the goal of universal dependencies

00:44:43.735 --> 00:44:47.435
was is to say what we'd like to do is have

00:44:47.435 --> 00:44:50.090
a uniform parallel system of

00:44:50.090 --> 00:44:55.035
dependency description which could be used for any human language.

00:44:55.035 --> 00:44:58.490
So, if you go to the Universal Dependencies website,

00:44:58.490 --> 00:45:00.385
it's not only about English.

00:45:00.385 --> 00:45:05.290
You can find Universal Dependency analyses of you know, French,

00:45:05.290 --> 00:45:07.235
or German, or Finish,

00:45:07.235 --> 00:45:09.850
or Carsac, or Indonesian,

00:45:09.850 --> 00:45:11.440
um, lots of languages.

00:45:11.440 --> 00:45:14.070
Of course, there are um, even more languages

00:45:14.070 --> 00:45:16.800
which there aren't Universal Dependencies analyses of.

00:45:16.800 --> 00:45:19.835
So, if you have a- a big calling to say I'm gonna

00:45:19.835 --> 00:45:23.110
build a Swahili Universal Dependencies um,

00:45:23.110 --> 00:45:25.415
treebank, um, you can get in touch.

00:45:25.415 --> 00:45:27.190
Um, but anyway.

00:45:27.190 --> 00:45:29.580
So, this is the idea of treebank.

00:45:29.580 --> 00:45:37.230
You know, historically, tree banks wasn't something that people thought of immediately.

00:45:37.230 --> 00:45:40.960
This so- an idea that took quite a long time to develop, right?

00:45:40.960 --> 00:45:44.490
That um, people started thinking about grammars

00:45:44.490 --> 00:45:48.340
of languages even in modern times in the fifties,

00:45:48.340 --> 00:45:55.760
and people started building parses for languages in the 19, early 1960s.

00:45:55.760 --> 00:45:59.549
So, there was decades of work in the 60s,

00:45:59.549 --> 00:46:03.130
70s, 80s, and no one had tree banks.

00:46:03.130 --> 00:46:07.140
The way people did this work is that they wrote grammars,

00:46:07.140 --> 00:46:10.775
that they either wrote grammars like the one I did for constituency of

00:46:10.775 --> 00:46:14.630
noun phrase goes to determiner, optional adjective noun.

00:46:14.630 --> 00:46:16.995
Noun goes to goat um,

00:46:16.995 --> 00:46:21.350
or the equivalent kind of grammars and a dependency format,

00:46:21.350 --> 00:46:26.565
and they hand built these grammars and then train,

00:46:26.565 --> 00:46:29.930
had parsers that could parse these sentences.

00:46:29.930 --> 00:46:37.480
Going into things, having a human being write a grammar feels more efficient.

00:46:37.480 --> 00:46:39.275
Because if you write uh,

00:46:39.275 --> 00:46:43.590
a rule like noun phrase goes to determiner optional adjective noun.

00:46:43.590 --> 00:46:46.085
I mean, that- that describes

00:46:46.085 --> 00:46:49.570
a huge number of phrases or actually infinite number of phrases.

00:46:49.570 --> 00:46:50.920
Um, so that you know,

00:46:50.920 --> 00:46:53.200
this is the structure of you know, the cat, the dog,

00:46:53.200 --> 00:46:56.910
or cat or dog, or large dog all those things we saw at the beginning.

00:46:56.910 --> 00:47:01.160
So, it's really efficient you're capturing lots of stuff with one rule.

00:47:01.160 --> 00:47:07.475
Um, but it sort of turned out that in practice that wasn't such a good idea,

00:47:07.475 --> 00:47:10.440
and it turned out to be much better to have

00:47:10.440 --> 00:47:14.210
these kind of treebank supporting structures over sentences.

00:47:14.210 --> 00:47:17.135
It's often a bit more subtle was to why that

00:47:17.135 --> 00:47:20.320
is because it sounds like pretty menial work um,

00:47:20.320 --> 00:47:22.850
building tree banks, and in some sense it is.

00:47:22.850 --> 00:47:25.060
Um, but you know,

00:47:25.060 --> 00:47:27.815
it turns out to be much more useful.

00:47:27.815 --> 00:47:33.535
I mean, so one huge benefit is that treebanks are very reusable.

00:47:33.535 --> 00:47:36.190
That effectively what they was in 60s, 70s,

00:47:36.190 --> 00:47:39.450
and 80s was that every different you know,

00:47:39.450 --> 00:47:42.965
people who started about building a parser invented

00:47:42.965 --> 00:47:46.970
their own notation for grammar rules which got more and more complex,

00:47:46.970 --> 00:47:50.675
and it was only used by their parser and nobody else's parser.

00:47:50.675 --> 00:47:54.610
So, there was no sharing and reuse of the work those done by human beings.

00:47:54.610 --> 00:47:55.990
Well, once you have a treebank,

00:47:55.990 --> 00:48:01.510
it's reusable for all sorts of purposes that lots of people build parsers format.

00:48:01.510 --> 00:48:04.890
But also other people use it as well like linguists now often used

00:48:04.890 --> 00:48:08.650
tree banks to find examples of different constructions.

00:48:08.650 --> 00:48:10.400
Um, but beyond that,

00:48:10.400 --> 00:48:14.695
this sort of just became necessary once we wanted to do machine learning.

00:48:14.695 --> 00:48:17.405
So that if we want to do machine learning,

00:48:17.405 --> 00:48:20.240
we want to have data that we can build models on.

00:48:20.240 --> 00:48:21.730
In particular, a lot of what

00:48:21.730 --> 00:48:27.015
our machine learning models exploit is how common are different structures.

00:48:27.015 --> 00:48:30.645
So, we want to know about the commoners and the frequency of things.

00:48:30.645 --> 00:48:34.335
Um, but then treebanks gave us another big thing which is,

00:48:34.335 --> 00:48:37.365
well, lots of sentences are ambiguous,

00:48:37.365 --> 00:48:44.530
and what we want to do is build models that find the right structure for sentences.

00:48:44.530 --> 00:48:48.065
If all you do is have a grammar you have no way of

00:48:48.065 --> 00:48:51.805
telling what is the right structure for ambiguous sentences.

00:48:51.805 --> 00:48:54.620
All you can do is say hey that sentence with

00:48:54.620 --> 00:48:58.780
four prepositional phrases after it that I showed you earlier,

00:48:58.780 --> 00:49:00.430
it has 14 different parsers.

00:49:00.430 --> 00:49:02.075
Let me show you all of them.

00:49:02.075 --> 00:49:04.890
Um, but once you have um,

00:49:04.890 --> 00:49:12.185
treebank examples, you can say this is the right structure for this sentence in context.

00:49:12.185 --> 00:49:17.255
So, you should be building a machine learning model which will recover that structure,

00:49:17.255 --> 00:49:19.020
and if you don't that you're wrong.

00:49:19.020 --> 00:49:23.980
[NOISE]. Okay. Um, so that's treebanks.

00:49:23.980 --> 00:49:28.320
Um, so how are we gonna do build dependency parsers?

00:49:28.320 --> 00:49:34.790
Well, somehow we want models that can kind of capture what's the right parse.

00:49:34.790 --> 00:49:37.240
Just thinking about abstractly, you know,

00:49:37.240 --> 00:49:40.340
there's sort of different things that we can pay attention to.

00:49:40.340 --> 00:49:46.030
So, one thing that we can pay attention to is the sort of actual words, right?

00:49:46.030 --> 00:49:47.490
Discussion of issues.

00:49:47.490 --> 00:49:49.590
That's a reasonable thing.

00:49:49.590 --> 00:49:55.235
So, it's reasonable to have issues as dependent of discussion um,

00:49:55.235 --> 00:49:58.090
where you know, discussion of outstanding.

00:49:58.090 --> 00:49:59.105
That sounds weird.

00:49:59.105 --> 00:50:01.570
So, you probably don't want that dependency.

00:50:01.570 --> 00:50:05.045
Um, there's a question of how far apart words are.

00:50:05.045 --> 00:50:07.480
Most dependencies are fairly short distance.

00:50:07.480 --> 00:50:08.960
They not all of them are.

00:50:08.960 --> 00:50:11.385
There's a question of what's in between.

00:50:11.385 --> 00:50:13.385
Um, if there's a semicolon in between,

00:50:13.385 --> 00:50:15.335
there probably is an a dependency across that.

00:50:15.335 --> 00:50:20.190
Um, and the other issue is sort of how many arguments do things take?

00:50:20.190 --> 00:50:22.355
So, here we have was completed.

00:50:22.355 --> 00:50:24.590
If you see the words was completed,

00:50:24.590 --> 00:50:29.410
you sort of expect that there'll be a subject before of the something was completed,

00:50:29.410 --> 00:50:31.145
and it would be wrong if there wasn't.

00:50:31.145 --> 00:50:34.470
So, you're expecting an argument on that side.

00:50:34.470 --> 00:50:37.960
But on the other side, hand it won't have object after it.

00:50:37.960 --> 00:50:41.630
You won't say the discussion was completed the goat.

00:50:41.630 --> 00:50:44.080
Um, that's not a good sentence, right?

00:50:44.080 --> 00:50:47.135
So, you won't have ah, um, an object after it.

00:50:47.135 --> 00:50:49.060
So, there's sort of information of that sort,

00:50:49.060 --> 00:50:54.305
and we want to have our dependency parsers be able to make use of that structure.

00:50:54.305 --> 00:50:56.830
[NOISE] Okay.

00:50:56.830 --> 00:51:04.360
Um, so effectively what we do when we build a dependency parser is going to say,

00:51:04.360 --> 00:51:11.560
for each word is- is going to be the dependent of some other word or the root.

00:51:11.560 --> 00:51:15.600
So, this give here is actually the head of the sentence.

00:51:15.600 --> 00:51:17.570
So, it's a dependent of root,

00:51:17.570 --> 00:51:20.620
the talk is a dependent of give,

00:51:20.620 --> 00:51:24.040
'll is a dependent of talk.

00:51:24.040 --> 00:51:28.660
And so, for each word we want to choose what is

00:51:28.660 --> 00:51:34.600
the dependent of and we want to do it in such a way that the dependencies form a tree.

00:51:34.600 --> 00:51:39.565
So that means it would be a bad idea if we made a cycle.

00:51:39.565 --> 00:51:43.435
So, if we sort of said, Bootstrapping, um,

00:51:43.435 --> 00:51:47.860
was a dependent of, um, talk,

00:51:47.860 --> 00:51:52.420
um, but then we had things sort of move around.

00:51:52.420 --> 00:51:53.935
So,this goes to here,

00:51:53.935 --> 00:51:55.600
but then talk is a dependent that,

00:51:55.600 --> 00:51:57.430
and so I'm gonna cycle that's bad news,

00:51:57.430 --> 00:52:00.145
we don't want cycles, we want a tree.

00:52:00.145 --> 00:52:03.175
And there's one final issue,

00:52:03.175 --> 00:52:06.970
um, which is we don't want things that,

00:52:06.970 --> 00:52:11.680
um, is whether we want to allow dependencies to cross or not,

00:52:11.680 --> 00:52:14.035
um, and this is an example of this.

00:52:14.035 --> 00:52:16.405
So, most of the time, um,

00:52:16.405 --> 00:52:19.180
dependencies don't cross each other.

00:52:19.180 --> 00:52:22.180
Uh, but sometimes they do,

00:52:22.180 --> 00:52:26.770
and this example here is actually an instance for that.

00:52:26.770 --> 00:52:31.195
So, I'll give a talk tomorrow, um, on bootstrapping.

00:52:31.195 --> 00:52:35.710
So, we're giving a talk that's the object,

00:52:35.710 --> 00:52:39.025
and when it's being given is tomorrow,

00:52:39.025 --> 00:52:43.240
but this talk has a modifier that's on bootstrapping.

00:52:43.240 --> 00:52:50.905
So, we actually have another dependency here that crosses, um, that dependency.

00:52:50.905 --> 00:52:52.150
And that's sort of rare,

00:52:52.150 --> 00:52:53.890
that doesn't happen a ton in English,

00:52:53.890 --> 00:52:57.085
but it happens sometimes in some structures like that.

00:52:57.085 --> 00:52:59.770
And so, this is the question of whether, um,

00:52:59.770 --> 00:53:04.900
what we say is that the positive sentence is projective if there

00:53:04.900 --> 00:53:10.015
no crossing dependencies and it's non-projective if there are crossing dependencies,

00:53:10.015 --> 00:53:12.370
and most of the time, English's projective and it's

00:53:12.370 --> 00:53:15.130
parses of sentences, but occasionally not.

00:53:15.130 --> 00:53:16.780
And when it's not is when you kind of have

00:53:16.780 --> 00:53:20.740
these constituents that are delayed to the end of the sentence, right?

00:53:20.740 --> 00:53:24.055
You could've said, I'll give a talk on bootstrapping tomorrow,

00:53:24.055 --> 00:53:27.490
and then a [inaudible] have a projective parse, but if you want to,

00:53:27.490 --> 00:53:30.940
you can kind of delay that extra modifier and say I'll give a talk

00:53:30.940 --> 00:53:34.780
tomorrow on bootstrapping and then the parse becomes non-projective.

00:53:34.780 --> 00:53:38.410
Um, okay.

00:53:38.410 --> 00:53:40.450
So, that's that.

00:53:40.450 --> 00:53:43.285
Um, there are various ways of,

00:53:43.285 --> 00:53:46.435
um, doing dependency parsing,

00:53:46.435 --> 00:53:50.200
but basically what I am gonna tell you about today is this one called

00:53:50.200 --> 00:53:54.205
transition-based or deterministic dependency parsing,

00:53:54.205 --> 00:53:55.555
and this is, um,

00:53:55.555 --> 00:54:01.030
the one that's just been enormously influential in practical deployments of parsing.

00:54:01.030 --> 00:54:04.375
So, when Google goes off and parses every web page,

00:54:04.375 --> 00:54:07.690
what they're using is a transition based parser.

00:54:07.690 --> 00:54:12.040
Um, and so, this was a notion of parsing that, um,

00:54:12.040 --> 00:54:14.800
was mainly popularized by this guy,

00:54:14.800 --> 00:54:17.980
walk him Joakim Nivre, he is a Swedish computational linguists.

00:54:17.980 --> 00:54:25.075
Um, and what you do it's- it's sort of inspired by shift-reduce parsing.

00:54:25.075 --> 00:54:29.980
So, probably in- in our CS103 or compilers class or something,

00:54:29.980 --> 00:54:32.650
you saw a little bit of shift-reduce parsing.

00:54:32.650 --> 00:54:35.965
And this is sort of like a shift-reduce parser,

00:54:35.965 --> 00:54:38.710
apart from when we reduce,

00:54:38.710 --> 00:54:42.430
we build dependencies instead of constituent.

00:54:42.430 --> 00:54:46.270
Um, and this has a lot of very technical description that

00:54:46.270 --> 00:54:50.515
doesn't help you at all to look at in terms of understanding what,

00:54:50.515 --> 00:54:53.080
um, a shift-reduce parser does.

00:54:53.080 --> 00:54:55.570
And here's a formal description of a

00:54:55.570 --> 00:54:59.890
transition-based shift-reduce parser and which also doesn't help you at all.

00:54:59.890 --> 00:55:02.770
Um, so, instead we kinda look at this example,

00:55:02.770 --> 00:55:05.335
uh, [LAUGHTER] because that will hopefully help you.

00:55:05.335 --> 00:55:10.030
So, what I wanna to do is parse the sentence "I ate fish".

00:55:10.030 --> 00:55:14.530
And yet formally what I have is I have a why I start,

00:55:14.530 --> 00:55:17.500
there are three actions I can take and I have

00:55:17.500 --> 00:55:20.895
a finished condition for formal parse, parse.

00:55:20.895 --> 00:55:23.410
Um, and so here's what I do.

00:55:23.410 --> 00:55:29.080
So, I have a stack which is on this side and I have a buffer.

00:55:29.080 --> 00:55:32.455
Um, so, the stack is what I have built,

00:55:32.455 --> 00:55:36.055
and the buffer is all the words in the sentence I haven't dealt with yet.

00:55:36.055 --> 00:55:38.020
So, I stop the parse,

00:55:38.020 --> 00:55:41.575
and that's the sort of instruction here, by putting route,

00:55:41.575 --> 00:55:44.395
my root for my whole sentence onto my stack,

00:55:44.395 --> 00:55:47.200
and my buffer is the whole sentence,

00:55:47.200 --> 00:55:49.630
and I haven't found any dependencies yet.

00:55:49.630 --> 00:55:51.055
Okay, and so then,

00:55:51.055 --> 00:55:55.645
the actions I can take is to shift things onto the stack

00:55:55.645 --> 00:56:01.570
or to do the equivalent of a Reduce where I build dependencies.

00:56:01.570 --> 00:56:03.945
So, starting off, um,

00:56:03.945 --> 00:56:07.755
I can't build a dependency because I only have root on the stack,

00:56:07.755 --> 00:56:09.840
so the only thing I can do is shift,

00:56:09.840 --> 00:56:12.215
so I can shift I onto the stack.

00:56:12.215 --> 00:56:14.800
Um, now, I could at this point say,

00:56:14.800 --> 00:56:16.150
let's build a dependency,

00:56:16.150 --> 00:56:17.680
I is a dependent of root,

00:56:17.680 --> 00:56:19.660
but that would be the wrong analysis,

00:56:19.660 --> 00:56:23.080
because really the head of this sentence is I ate.

00:56:23.080 --> 00:56:26.065
So, I'm a clever boy and I shift again.

00:56:26.065 --> 00:56:30.205
And now I have root I ate on the stack.

00:56:30.205 --> 00:56:33.280
Okay, and so, at this point,

00:56:33.280 --> 00:56:34.720
I'm in a position where,

00:56:34.720 --> 00:56:40.105
hey, what I'm gonna do is reductions that build structure, because look,

00:56:40.105 --> 00:56:44.125
I have I ate here and I want to be able to say

00:56:44.125 --> 00:56:49.600
that I is the subject of dependency of ate,

00:56:49.600 --> 00:56:51.130
and I will do that by,

00:56:51.130 --> 00:56:54.595
um, by doing a reduction.

00:56:54.595 --> 00:57:00.730
And so, what I'm gonna do is the left-arc reduction, which says, look,

00:57:00.730 --> 00:57:04.494
I'm gonna treat the second from top thing on the stack

00:57:04.494 --> 00:57:08.605
as a dependent of the thing that's on top of the stack.

00:57:08.605 --> 00:57:10.210
And so, I do that,

00:57:10.210 --> 00:57:12.520
and so, when I do that,

00:57:12.520 --> 00:57:17.575
I create the second from the head thing as a subject dependent of ate,

00:57:17.575 --> 00:57:21.085
and I leave the head on the stack ate,

00:57:21.085 --> 00:57:25.675
but I sort of add this dependencies as other dependencies I've built.

00:57:25.675 --> 00:57:29.305
Okay, um, so, I do that.

00:57:29.305 --> 00:57:34.720
Um, now, I could immediately reduce again and say ate is a dependent of root,

00:57:34.720 --> 00:57:37.855
but my sentence's actually I ate fish.

00:57:37.855 --> 00:57:40.990
So, what I want to do is say, "Oh,

00:57:40.990 --> 00:57:45.610
if it's still fish on the buffer," so what I should first do is shift again,

00:57:45.610 --> 00:57:48.265
have root ate fish in my sentence,

00:57:48.265 --> 00:57:49.960
and then I'll be able to say, Look,

00:57:49.960 --> 00:57:52.795
I want to now build, um,

00:57:52.795 --> 00:57:55.705
the thing on the top of this stack as

00:57:55.705 --> 00:57:59.905
a right dependent of the thing that's second from top of the stack,

00:57:59.905 --> 00:58:02.664
and so that's referred to as a Right-Arc move,

00:58:02.664 --> 00:58:05.680
and so, I say Right Arc, and so,

00:58:05.680 --> 00:58:08.410
I do a reduction where I've generated

00:58:08.410 --> 00:58:14.245
a new dependency and I take the two things that are on top of the stack and say,

00:58:14.245 --> 00:58:17.245
um, fish is a dependent of ate,

00:58:17.245 --> 00:58:20.575
and so therefore, I just keep the head.

00:58:20.575 --> 00:58:25.915
I always just keep the hit on the stack and the- and I generate this new Arc.

00:58:25.915 --> 00:58:27.640
And so, at this point,

00:58:27.640 --> 00:58:33.685
I'm in the same position I want to say that this ate is a right dependent of my route,

00:58:33.685 --> 00:58:36.985
and so, I'm again going to do Right Arc,

00:58:36.985 --> 00:58:40.840
um, and make this extra dependency here.

00:58:40.840 --> 00:58:43.450
Okay. So, then my finished condition of having

00:58:43.450 --> 00:58:46.420
successfully parsed the sentence is my buffer is

00:58:46.420 --> 00:58:52.600
empty and I just have root left on my stack because that's what I sort of said back here,

00:58:52.600 --> 00:58:55.675
that was, buffer is empty as my finished condition.

00:58:55.675 --> 00:58:59.230
Okay. So, I've parsed the sentence.

00:58:59.230 --> 00:59:01.870
So that worked well but, you know,

00:59:01.870 --> 00:59:07.870
I actually had different choices of when to pa- when to shift and when to reduce.

00:59:07.870 --> 00:59:12.325
And I just miraculously made the right choice at each point.

00:59:12.325 --> 00:59:16.630
And well, one thing you could do at this point is say, well,

00:59:16.630 --> 00:59:20.155
you could have explored every choice and,

00:59:20.155 --> 00:59:24.355
um, seen what happened and gone different parsers.

00:59:24.355 --> 00:59:25.765
And I could have,

00:59:25.765 --> 00:59:28.269
but if that's what I'd done,

00:59:28.269 --> 00:59:35.680
I would've explored this exponential size tree of different possible parsers.

00:59:35.680 --> 00:59:37.570
And if that was what I was doing,

00:59:37.570 --> 00:59:39.610
I wouldn't be able to parse efficiently.

00:59:39.610 --> 00:59:44.110
And indeed that's not what people did in the 60s, 70s and 80s.

00:59:44.110 --> 00:59:47.305
Uh, clever people in the 60s said,

00:59:47.305 --> 00:59:50.305
uh, rather than doing a crummy search here,

00:59:50.305 --> 00:59:54.430
we can come up with clever dynamic programming algorithms and you

00:59:54.430 --> 00:59:58.870
can relatively efficiently explore the space of all possible parsers.

00:59:58.870 --> 01:00:03.295
Uh, and that was sort of the mainstay of parsing in those decades.

01:00:03.295 --> 01:00:06.700
But when Joakim Nivre came along,

01:00:06.700 --> 01:00:10.420
he said "Yeah, that's true, um, but hey,

01:00:10.420 --> 01:00:12.595
I've got a clever idea, uh,

01:00:12.595 --> 01:00:18.220
because now it's the 2000s and I know machine learning."

01:00:18.220 --> 01:00:21.550
Um, so, what I could do instead,

01:00:21.550 --> 01:00:26.890
is say I'm at a particular position in the parse and I'm gonna build

01:00:26.890 --> 01:00:30.340
a machine learning classifier and that machine learning

01:00:30.340 --> 01:00:33.970
classifier is gonna tell me the next thing to do.

01:00:33.970 --> 01:00:36.250
It's gonna tell me whether to shift,

01:00:36.250 --> 01:00:39.955
um, with left arc or right arc.

01:00:39.955 --> 01:00:42.550
So, if we're only just so talking about, well,

01:00:42.550 --> 01:00:43.750
how to build the arrows,

01:00:43.750 --> 01:00:45.160
they're just three actions,

01:00:45.160 --> 01:00:47.215
shift, left arc or right arc.

01:00:47.215 --> 01:00:50.770
Um, if we also wanted to put labels on the dependencies,

01:00:50.770 --> 01:00:53.140
and we have our different labels, um,

01:00:53.140 --> 01:00:56.110
there are then sort of 2R plus actions because she is

01:00:56.110 --> 01:01:00.790
sort of left arc subject or left arc object or something like that.

01:01:00.790 --> 01:01:04.060
But anyway, there's a set of actions and so you gonna build

01:01:04.060 --> 01:01:07.570
a classifier with machine learning somehow which will predict

01:01:07.570 --> 01:01:14.685
the right action and Joakim Nivre showed the sort of slightly surprising fact

01:01:14.685 --> 01:01:22.530
that actually you could predict the correct action to take with high accuracy.

01:01:22.530 --> 01:01:27.265
So, um, in the simplest version of this,

01:01:27.265 --> 01:01:29.440
um, there's absolutely no search.

01:01:29.440 --> 01:01:31.720
You just run a classifier at each step and it

01:01:31.720 --> 01:01:34.090
says "What you should do next is shift" and you shift,

01:01:34.090 --> 01:01:36.820
and then it says "What you should do is left arc" and you left arc

01:01:36.820 --> 01:01:39.685
and you run that through and he proved, no,

01:01:39.685 --> 01:01:42.385
he showed empirically, that even doing that,

01:01:42.385 --> 01:01:45.565
you could parse sentences with high accuracy.

01:01:45.565 --> 01:01:47.380
Now if you wanna do some searching around,

01:01:47.380 --> 01:01:48.505
you can do a bit better,

01:01:48.505 --> 01:01:50.440
but it's not necessary.

01:01:50.440 --> 01:01:54.700
Um, and we're not gonna do it for our, um, assignment.

01:01:54.700 --> 01:01:58.480
But so if you're doing this just sort of run classify,

01:01:58.480 --> 01:02:01.434
predict action, run classify, predict action,

01:02:01.434 --> 01:02:04.030
we then get this wonderful result which

01:02:04.030 --> 01:02:07.645
you're meant to explain a bit honest on your assignment 3,

01:02:07.645 --> 01:02:11.275
is that what we've built is a linear time parser.

01:02:11.275 --> 01:02:15.370
Right? That because we are gonna be sort of- as we chug through a sentence,

01:02:15.370 --> 01:02:17.485
where we're only doing a linear amount of work for

01:02:17.485 --> 01:02:21.220
each word and that was sort of an enormous breakthrough.

01:02:21.220 --> 01:02:23.380
Because although people in the 60s hadn't come

01:02:23.380 --> 01:02:25.840
up with these dynamic programming algorithms,

01:02:25.840 --> 01:02:31.705
dynamic programming algorithms for sentences were always cubic or worse.

01:02:31.705 --> 01:02:34.375
And that's not very good if you want to parse the whole web,

01:02:34.375 --> 01:02:37.450
whereas if you have something that's linear time,

01:02:37.450 --> 01:02:40.105
that's really getting you places.

01:02:40.105 --> 01:02:45.085
Okay. So this is the conventional way in which this was done.

01:02:45.085 --> 01:02:48.010
Was, you know, we have a stack,

01:02:48.010 --> 01:02:50.750
we might have already built some structure if we

01:02:50.750 --> 01:02:53.410
hadn't working out something's dependent of something.

01:02:53.410 --> 01:02:57.790
We have a buffer of words that we don't deal with and we want to predict the next action.

01:02:57.790 --> 01:03:00.910
So the conventional way to do this is to say well,

01:03:00.910 --> 01:03:02.665
we want to have features.

01:03:02.665 --> 01:03:06.100
And well, the kind of features you wanted was so

01:03:06.100 --> 01:03:09.535
the usually some kind of conjunction or multiple things so

01:03:09.535 --> 01:03:13.720
that if the top word of the stack is good,

01:03:13.720 --> 01:03:18.760
um, and something else is true, right,

01:03:18.760 --> 01:03:22.930
that the second top word of the stack it has,

01:03:22.930 --> 01:03:25.120
and it's part of speech is verb,

01:03:25.120 --> 01:03:27.580
then maybe that's an indicator of do some action.

01:03:27.580 --> 01:03:31.705
So ha- had these very complex binary indicator features

01:03:31.705 --> 01:03:35.710
and you'd build- you literally have millions of

01:03:35.710 --> 01:03:39.130
these binary indicator features and you'd feed them into

01:03:39.130 --> 01:03:41.530
some big logistic regression or

01:03:41.530 --> 01:03:46.705
support vector machine or something like that and you would build parses.

01:03:46.705 --> 01:03:48.775
And these parses worked pretty well.

01:03:48.775 --> 01:03:55.600
Um, but you sort of had these sort of very complex hand engineered binary features.

01:03:55.600 --> 01:04:00.520
Um, so in the last bit of lecture I want to show you what people have done in the,

01:04:00.520 --> 01:04:02.980
um, neural dependency parsing world.

01:04:02.980 --> 01:04:04.270
But before I do that,

01:04:04.270 --> 01:04:05.890
let me just explain how you,

01:04:05.890 --> 01:04:11.220
um, how you evaluate, um, dependency parses.

01:04:11.220 --> 01:04:13.200
And that's actually very simple, right?

01:04:13.200 --> 01:04:16.130
So, what you do is well,

01:04:16.130 --> 01:04:18.880
you assume because the human wrote it down,

01:04:18.880 --> 01:04:22.300
that there is a correct dependency parse for a sentence.

01:04:22.300 --> 01:04:24.580
She saw the video lecture like this.

01:04:24.580 --> 01:04:29.560
And so these are the correct arcs and to evaluate our dependency parser,

01:04:29.560 --> 01:04:32.140
we're simply gonna say,

01:04:32.140 --> 01:04:34.465
uh, which arcs are correct.

01:04:34.465 --> 01:04:36.505
So, there are the gold arcs,

01:04:36.505 --> 01:04:38.410
so there's a gold arc,

01:04:38.410 --> 01:04:41.439
um, from two to one,

01:04:41.439 --> 01:04:47.110
She saw subject, and there's a gold arc from zero to two,

01:04:47.110 --> 01:04:48.190
the root of the sentence,

01:04:48.190 --> 01:04:49.825
these the gold arcs.

01:04:49.825 --> 01:04:52.480
Um, if we generate a parse,

01:04:52.480 --> 01:04:56.935
we're gonna propose some arcs as to what is the head of each word.

01:04:56.935 --> 01:05:00.685
And we're simply going to count up how many of them are correct,

01:05:00.685 --> 01:05:02.785
treating each arc individually.

01:05:02.785 --> 01:05:04.900
And there are two ways we can do that.

01:05:04.900 --> 01:05:07.885
We can either, as we're going to do,

01:05:07.885 --> 01:05:11.500
ignore the labels and that's then,

01:05:11.500 --> 01:05:15.865
uh, referred to as the unlabeled attachment score.

01:05:15.865 --> 01:05:19.000
So here in my example, my dependency paths,

01:05:19.000 --> 01:05:23.860
I've got most of the arcs right but it got this one wrong.

01:05:23.860 --> 01:05:28.690
So I say my unlabeled attachment score is 80 percent or we can also

01:05:28.690 --> 01:05:33.820
look at the labels and then my parser wasn't very good at getting the labels rights,

01:05:33.820 --> 01:05:35.380
so I'm only getting 40 percent.

01:05:35.380 --> 01:05:40.810
And so we can just count up the number of dependencies and how many we get correct.

01:05:40.810 --> 01:05:44.080
And that's in our accuracy and in the assignment,

01:05:44.080 --> 01:05:49.225
you're meant to build a dependency parser with a certain accuracy.

01:05:49.225 --> 01:05:51.265
I forget the number now is saying,

01:05:51.265 --> 01:05:55.645
some number 80 something or something that you're meant to get to.

01:05:55.645 --> 01:05:58.855
Okay. Um, maybe I'll skip that.

01:05:58.855 --> 01:06:03.370
Okay. Um, so, now I wanted to sort of explain to you just a bit

01:06:03.370 --> 01:06:07.780
about neural dependency parses and why they are motivated.

01:06:07.780 --> 01:06:11.845
So I'd mentioned to you already that the conventional model, uh,

01:06:11.845 --> 01:06:16.360
had these sort of indicated features of, um,

01:06:16.360 --> 01:06:19.180
on the top of the stack is the word good and the second thing on

01:06:19.180 --> 01:06:22.360
the stack is the verb has or on

01:06:22.360 --> 01:06:28.720
the top of the stack is some other word and the second top is of some part of speech.

01:06:28.720 --> 01:06:30.670
And that part of speech has already been

01:06:30.670 --> 01:06:32.620
joined with the dependency of another part of speech.

01:06:32.620 --> 01:06:35.500
People hand-engineer these features.

01:06:35.500 --> 01:06:36.760
And the problems with that,

01:06:36.760 --> 01:06:39.040
was these features were very sparse.

01:06:39.040 --> 01:06:42.100
Each of these features matches very few things.

01:06:42.100 --> 01:06:48.865
Um, they match some configurations but not others so the features tend to be incomplete.

01:06:48.865 --> 01:06:51.760
Um, and there are a lot of them,

01:06:51.760 --> 01:06:54.655
they're are commonly millions of features.

01:06:54.655 --> 01:06:57.040
And so it turned out that actually computing

01:06:57.040 --> 01:07:01.690
these features was just expensive so that you had some configuration on

01:07:01.690 --> 01:07:05.260
your stack and the buffer and then you wanted to know which of

01:07:05.260 --> 01:07:10.945
these features were active for that stack and buffer configuration.

01:07:10.945 --> 01:07:13.315
And so you had to compute features format.

01:07:13.315 --> 01:07:14.500
And it turned out that

01:07:14.500 --> 01:07:19.795
conventional dependency parsers spent most of their time computing features,

01:07:19.795 --> 01:07:24.640
then went into the machine learning model rather than doing the sort of shifting and,

01:07:24.640 --> 01:07:28.810
which you're are seeing, are just a pure parser operation.

01:07:28.810 --> 01:07:34.030
And so that seemed like it left open the possibility that, well,

01:07:34.030 --> 01:07:38.110
what if we could get rid of all of this stuff and we could run

01:07:38.110 --> 01:07:43.360
a neural network directly on the stack and buffer configuration,

01:07:43.360 --> 01:07:48.550
then maybe that would allow us to build a dependency parser which was

01:07:48.550 --> 01:07:55.705
faster and suffer less from issues of sparseness than the conventional dependency parser.

01:07:55.705 --> 01:08:02.785
And so that was a project that Dan Chi Chen and me tried to do in 2014,

01:08:02.785 --> 01:08:06.310
uh, we used to build a neural dependency parser.

01:08:06.310 --> 01:08:09.565
And, you know, effectively what we found,

01:08:09.565 --> 01:08:12.730
is that that's exactly what you could do.

01:08:12.730 --> 01:08:15.940
So, here's sort of a few stats here.

01:08:15.940 --> 01:08:18.940
So these are these same UAS and LAS.

01:08:18.940 --> 01:08:24.430
Uh, so MaltParser was Joakim Nivre's Parser that I sort of,

01:08:24.430 --> 01:08:27.490
uh, we started showing before.

01:08:27.490 --> 01:08:28.735
And they've got, um,

01:08:28.735 --> 01:08:33.625
a UAS on this data of 89.8.

01:08:33.625 --> 01:08:35.500
But everybody loved that.

01:08:35.500 --> 01:08:41.590
And the reason they loved it is it could parse at 469 sentences a second.

01:08:41.590 --> 01:08:44.500
There had been other people that have worked out

01:08:44.500 --> 01:08:47.350
different more complex ways

01:08:47.350 --> 01:08:52.360
of doing parsing with so-called graph-based dependency parsers.

01:08:52.360 --> 01:08:56.380
So this is another famous dependency parser from the 90s.

01:08:56.380 --> 01:08:58.660
So it was actually, you know,

01:08:58.660 --> 01:09:01.600
a bit more accurate but it was a bit more

01:09:01.600 --> 01:09:05.680
accurate at the cost of being two orders of magnitude slower.

01:09:05.680 --> 01:09:07.780
And, you know, people have worked on top of that.

01:09:07.780 --> 01:09:11.680
So, here is an even more complex graph-based parser, uh,

01:09:11.680 --> 01:09:14.350
from the 2000s and well, you know,

01:09:14.350 --> 01:09:17.890
it's a little bit more accurate again but it's gotten even slower.

01:09:17.890 --> 01:09:19.450
Um, okay.

01:09:19.450 --> 01:09:25.210
So, what we were able to show is that using the idea of instead using

01:09:25.210 --> 01:09:33.055
a neural network to make the decisions of Joakim Nivre Style shift-reduce parser,

01:09:33.055 --> 01:09:37.180
we could produce something that was almost

01:09:37.180 --> 01:09:41.380
as accurate as the very best parsers available at that time.

01:09:41.380 --> 01:09:45.745
I mean, strictly we won over here and we are a fraction behind on UAS.

01:09:45.745 --> 01:09:47.610
Um, but, you know,

01:09:47.610 --> 01:09:51.840
it was not only just as fast as Nivre's parser,

01:09:51.840 --> 01:09:54.795
it was actually faster than Nivre's parser,

01:09:54.795 --> 01:09:59.520
because we didn't have to spend as much time on feature computation.

01:09:59.520 --> 01:10:02.175
And that's actually almost a surprising result, right?

01:10:02.175 --> 01:10:04.690
It's not that we didn't have to do anything.

01:10:04.690 --> 01:10:08.050
We had to do matrix multiplies in our neural network,

01:10:08.050 --> 01:10:09.805
but it turned out, um,

01:10:09.805 --> 01:10:13.330
you could do the matrix multiplies more quickly than

01:10:13.330 --> 01:10:17.350
the feature computation that he was doing even though at the end of the day,

01:10:17.350 --> 01:10:21.235
it was sort of looking at weights that went into a support vector machine.

01:10:21.235 --> 01:10:23.020
So that was kind of cool.

01:10:23.020 --> 01:10:25.420
And so the secret was we're gonna make use of

01:10:25.420 --> 01:10:31.025
distributed representations like we've already seen for words.

01:10:31.025 --> 01:10:32.970
So for each word,

01:10:32.970 --> 01:10:35.645
we're going to represent it as a word embedding,

01:10:35.645 --> 01:10:38.005
like we've all what already seen.

01:10:38.005 --> 01:10:40.175
And in particular, um,

01:10:40.175 --> 01:10:44.260
we are gonna make use of word vectors

01:10:44.260 --> 01:10:49.075
and use them as the represent- the starting representations of words in our Parser.

01:10:49.075 --> 01:10:53.145
But well, if we're interested in distributed representations,

01:10:53.145 --> 01:10:58.550
it seem to us like maybe you should only have distributed representations of words.

01:10:58.550 --> 01:11:03.260
Um, maybe it also be good temp distributed representations of other things.

01:11:03.260 --> 01:11:05.440
So we had parts of speech like,

01:11:05.440 --> 01:11:08.085
you know, nouns and verbs and adjectives and so on.

01:11:08.085 --> 01:11:13.230
Well some of those parts of speech have more to do with each other than others.

01:11:13.230 --> 01:11:15.310
I mean, [NOISE] in particular, um,

01:11:15.310 --> 01:11:20.320
most NLP work uses fine-grained parts of speech.

01:11:20.320 --> 01:11:23.620
So you don't only have a part of speech like noun or verb,

01:11:23.620 --> 01:11:26.890
you have parts of speech like singular noun versus

01:11:26.890 --> 01:11:31.650
plural noun and you have different parts of speech for, you know,

01:11:31.650 --> 01:11:36.070
work, works, working, kind of the different forms of

01:11:36.070 --> 01:11:41.170
verbs are given different parts of speech, um, as well.

01:11:41.170 --> 01:11:45.355
So there's sort of sets of parts of speech labels that kind of clusters.

01:11:45.355 --> 01:11:47.940
So maybe we could have distributed representations,

01:11:47.940 --> 01:11:50.955
a part of speech that represent their similarity.

01:11:50.955 --> 01:11:53.670
Why not? Um, well if we're gonna do that,

01:11:53.670 --> 01:11:57.140
why not just keep on going and say the dependency labels.

01:11:57.140 --> 01:12:00.875
They also, um, have a distributed representation.

01:12:00.875 --> 01:12:04.405
And so, we built a representation that did that.

01:12:04.405 --> 01:12:10.290
So the idea is that we have in our stack,

01:12:10.290 --> 01:12:13.185
the sort of the top positions of the stack,

01:12:13.185 --> 01:12:17.695
the first positions of the buffer and for each of those positions,

01:12:17.695 --> 01:12:23.275
we have a word and a part of speech and if we've already built structure as here,

01:12:23.275 --> 01:12:27.525
we kind of know about a dependency that's already been built.

01:12:27.525 --> 01:12:31.790
And so we've got a triple for each position and we're gonna convert

01:12:31.790 --> 01:12:36.295
all of those into a distributed representation,

01:12:36.295 --> 01:12:41.960
um, which we are learning and we're gonna use those distributed representations,

01:12:41.960 --> 01:12:45.065
um, to build our parser.

01:12:45.065 --> 01:12:47.750
Okay. Now for- so,

01:12:47.750 --> 01:12:52.440
you know starting from- starting from the next lecture forward,

01:12:52.440 --> 01:12:58.835
we're gonna sort of s- start using a more complex forms of neural models.

01:12:58.835 --> 01:13:01.250
But for this model, um,

01:13:01.250 --> 01:13:05.780
we did it in a sort of a very simple straightforward way.

01:13:05.780 --> 01:13:10.960
We said, well, we could just use exactly the same model,

01:13:10.960 --> 01:13:15.160
exactly the same parser structure that Nivre used, right?

01:13:15.160 --> 01:13:18.620
Doing those shifts and left arcs and right arcs.

01:13:18.620 --> 01:13:21.010
Um, the only part we're gonna turn into

01:13:21.010 --> 01:13:25.590
a neural network is we're gonna have the decision of what to do next,

01:13:25.590 --> 01:13:28.815
um, being controlled by our neural network.

01:13:28.815 --> 01:13:31.070
So our neural network is

01:13:31.070 --> 01:13:37.040
just a very simple classifier of the kind that we are talking about last week.

01:13:37.040 --> 01:13:39.665
So based on the configuration,

01:13:39.665 --> 01:13:44.400
we create an input layer which means we're sort

01:13:44.400 --> 01:13:49.140
of taking the stuff in these boxers and turn- and looking up

01:13:49.140 --> 01:13:54.580
a vector representation for each one and concatenating them together to produce

01:13:54.580 --> 01:13:59.050
a input representation that's sort of similar to when we were making

01:13:59.050 --> 01:14:03.935
those window classifiers and then we can concatenate a bunch of stuff together.

01:14:03.935 --> 01:14:06.040
So that gives us in our input layer.

01:14:06.040 --> 01:14:08.490
[NOISE] Um, so from there,

01:14:08.490 --> 01:14:12.200
we put things through a hidden layer just like last week.

01:14:12.200 --> 01:14:18.130
We do Wx plus b and then put it through a ReLU or a non-linearity to a hidden layer.

01:14:18.130 --> 01:14:19.910
And then on top of that,

01:14:19.910 --> 01:14:23.605
we're simply gonna stick a softmax output layer.

01:14:23.605 --> 01:14:25.820
So multiplying by another matrix,

01:14:25.820 --> 01:14:28.905
adding another, um, bias term,

01:14:28.905 --> 01:14:32.000
and then that goes into the softmax which is gonna give

01:14:32.000 --> 01:14:37.780
a probability over our actions as to whether it's shift left arc or right arc,

01:14:37.780 --> 01:14:40.055
or the corresponding one with labels.

01:14:40.055 --> 01:14:45.800
And then we're gonna use the same kind of cross entropy loss to say how good a job did we

01:14:45.800 --> 01:14:48.440
do at guessing the action that we should have

01:14:48.440 --> 01:14:52.050
taken according to the tree bank parse of the sentence.

01:14:52.050 --> 01:14:56.490
And so each step of the shift-reduce parser,

01:14:56.490 --> 01:15:01.120
we're making a decision as what to do next and we're doing it by this classifier

01:15:01.120 --> 01:15:03.120
and we're getting a loss to

01:15:03.120 --> 01:15:06.815
the extent that we don't give probability one to the right action.

01:15:06.815 --> 01:15:11.380
Um, and so that's what we did using the tree bank.

01:15:11.380 --> 01:15:14.145
We trained up our parser, um,

01:15:14.145 --> 01:15:19.825
and it was then able to predict the sentences.

01:15:19.825 --> 01:15:23.680
And the cool thing- the cool thing was,

01:15:23.680 --> 01:15:26.410
um, that this, um,

01:15:26.410 --> 01:15:30.160
had all the good things of Nivre's parser but, you know,

01:15:30.160 --> 01:15:33.890
by having it use these dense representations,

01:15:33.890 --> 01:15:36.280
it meant that we could get greater accuracy and

01:15:36.280 --> 01:15:39.845
speed than Nivre's parser at the same time.

01:15:39.845 --> 01:15:43.270
So here is sort of some results on that.

01:15:43.270 --> 01:15:46.230
I mean, I already showed you some earlier results, right?

01:15:46.230 --> 01:15:49.215
So this was showing, um, the fact, um,

01:15:49.215 --> 01:15:53.310
that, you know, we're outperforming these earlier parsers basically.

01:15:53.310 --> 01:15:56.445
But subsequent to us doing this work,

01:15:56.445 --> 01:15:58.880
um, people at Google,

01:15:58.880 --> 01:16:03.050
um, these papers here by Weiss and Andor,

01:16:03.050 --> 01:16:05.625
um, they said, "Well, this is pretty cool.

01:16:05.625 --> 01:16:11.460
Um, maybe we can get the numbers even better if we make our neural network,

01:16:11.460 --> 01:16:17.705
um, bigger and deeper and we spend a lot more time tuning our hyper-parameters."

01:16:17.705 --> 01:16:19.690
Um, sad but true.

01:16:19.690 --> 01:16:21.900
All of these things help when you're building

01:16:21.900 --> 01:16:25.015
neural networks and when you're doing your final project.

01:16:25.015 --> 01:16:29.185
Sometimes the answer to making the results better is to make it bigger,

01:16:29.185 --> 01:16:32.795
deeper and spend more time choosing the hyper-parameters.

01:16:32.795 --> 01:16:36.205
Um, they put in Beam search as I sort of mentioned.

01:16:36.205 --> 01:16:37.950
Um, Beam search can really help.

01:16:37.950 --> 01:16:39.490
So in Beam search,

01:16:39.490 --> 01:16:42.320
you know, rather than just saying,

01:16:42.320 --> 01:16:44.850
"Let's work out what's the best next action,

01:16:44.850 --> 01:16:46.530
do that one and repeat over",

01:16:46.530 --> 01:16:49.300
you allow yourself to do a little bit of search.

01:16:49.300 --> 01:16:53.520
You sort of say, "Well, let's consider two actions and explore what happens."

01:16:53.520 --> 01:16:55.780
Um, quick question.

01:16:55.780 --> 01:17:00.480
Do humans always agree on how to build this trees and if they don't,

01:17:00.480 --> 01:17:05.525
what will be the [inaudible] or agreement of humans relative to [inaudible] [OVERLAPPING] [NOISE]

01:17:05.525 --> 01:17:08.685
So that's a good question which I haven't addressed.

01:17:08.685 --> 01:17:11.485
Um, humans don't always agree.

01:17:11.485 --> 01:17:15.010
There are sort of two reasons they can't agree fundamentally.

01:17:15.010 --> 01:17:16.980
One is that, uh, humans,

01:17:16.980 --> 01:17:18.480
um, sort of mess up, right?

01:17:18.480 --> 01:17:20.950
Because human work is doing this aren't perfect.

01:17:20.950 --> 01:17:25.360
And the other one is they generally think that there should be different structures.

01:17:25.360 --> 01:17:27.030
Um, so, you know,

01:17:27.030 --> 01:17:30.455
it depend- varies depending on the circumstances and so on.

01:17:30.455 --> 01:17:33.840
If you just get humans to parse sentences and say,

01:17:33.840 --> 01:17:37.220
"Well, what is the agreement and what they produced?"

01:17:37.220 --> 01:17:41.055
You know, maybe you're only getting something like 92 percent.

01:17:41.055 --> 01:17:45.465
But, you know, if you then do an adjudication phase and you say, "Um,

01:17:45.465 --> 01:17:47.955
look at these differences,

01:17:47.955 --> 01:17:50.340
um, is one of them right or wrong?"

01:17:50.340 --> 01:17:52.050
There are a lot of them where, you know,

01:17:52.050 --> 01:17:53.830
one of the person is effectively saying,

01:17:53.830 --> 01:17:55.000
"Oh yeah, I goofed.

01:17:55.000 --> 01:17:57.570
Um, wasn't paying attention or whatever."

01:17:57.570 --> 01:17:58.950
Um, and so then,

01:17:58.950 --> 01:18:01.450
what's the residual rate in which,

01:18:01.450 --> 01:18:05.350
um, people can actually disagree about possible parses?

01:18:05.350 --> 01:18:07.760
I think that's sort of more around three percent.

01:18:07.760 --> 01:18:09.400
Um, yeah.

01:18:09.400 --> 01:18:11.790
But there certainly are cases and that includes

01:18:11.790 --> 01:18:14.685
some of the prepositional phrase attachment ambiguities.

01:18:14.685 --> 01:18:17.510
Sometimes there are multiple attachments

01:18:17.510 --> 01:18:19.290
that sort of same clause although it's not really

01:18:19.290 --> 01:18:21.600
clear which one is right even though there are lots of

01:18:21.600 --> 01:18:24.710
other circumstances where one of them is very clearly wrong.

01:18:24.710 --> 01:18:27.120
Um, yeah.

01:18:27.120 --> 01:18:28.280
[inaudible].

01:18:28.950 --> 01:18:32.210
There's- there's still room to do better.

01:18:32.210 --> 01:18:34.400
I mean, at the unlabeled attachment score,

01:18:34.400 --> 01:18:35.960
it's actually starting to get pretty good.

01:18:35.960 --> 01:18:38.570
But there's still room to do better. Um, yeah.

01:18:38.570 --> 01:18:40.460
Um, yeah.

01:18:40.460 --> 01:18:41.910
So Beam search,

01:18:41.910 --> 01:18:45.110
the final thing that they did was- that we're not gonna talk about here,

01:18:45.110 --> 01:18:49.480
is the sort of more global inference to make sure, um, it's sensible.

01:18:49.480 --> 01:18:51.610
Um, and so, um,

01:18:51.610 --> 01:18:56.250
that then led to Google developing these models that they gave silly names to,

01:18:56.250 --> 01:18:59.095
especially the Parsey McPa- parseFace,

01:18:59.095 --> 01:19:01.355
um, model of parsing.

01:19:01.355 --> 01:19:03.385
Um, and so, yeah.

01:19:03.385 --> 01:19:08.395
So that then- that's sort of pushed up the numbers even further so that they were sort of

01:19:08.395 --> 01:19:13.775
getting close to 95 percent unlabeled accuracy score from these models.

01:19:13.775 --> 01:19:15.905
And actually, this work has kind of,

01:19:15.905 --> 01:19:19.485
you know, deep learning people like to optimize.

01:19:19.485 --> 01:19:21.730
Um, this work [LAUGHTER] has continued along

01:19:21.730 --> 01:19:24.345
in the intervening two years and the numbers are sort of getting,

01:19:24.345 --> 01:19:26.310
um, a bit higher again.

01:19:26.310 --> 01:19:29.345
But, you know, so this actually, um,

01:19:29.345 --> 01:19:37.440
led to ah sort of a new era of sort of better parsers because so effectively this was the

01:19:37.440 --> 01:19:42.350
90's- the 90's era of parsers that was sort of where

01:19:42.350 --> 01:19:48.320
around 90 percent and then going into this sort of new generation of,

01:19:48.320 --> 01:19:51.580
um, neural transition based dependency parsers.

01:19:51.580 --> 01:19:55.260
We sort of have gone down that we've halve that error- error rate.

01:19:55.260 --> 01:19:59.120
And we're now down to sort of about a five percent error rate.

01:19:59.120 --> 01:20:01.430
Yeah. I'm basically out of time now but, you know,

01:20:01.430 --> 01:20:04.080
there is further work including, you know, at Stanford.

01:20:04.080 --> 01:20:07.660
Um, another student, Tim Dossad has some sort of more recent work.

01:20:07.660 --> 01:20:09.590
It's more accurate than 95 percent, right?

01:20:09.590 --> 01:20:13.180
So we- we're still going on but I think I'd better stop here today,

01:20:13.180 --> 01:20:16.310
um, and that's neural dependency parsing. [NOISE].

