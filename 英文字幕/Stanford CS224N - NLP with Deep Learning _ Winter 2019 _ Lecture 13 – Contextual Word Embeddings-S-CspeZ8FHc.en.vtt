WEBVTT
Kind: captions
Language: en

00:00:05.660 --> 00:00:08.145
Okay hi everyone.

00:00:08.145 --> 00:00:11.110
Let's get started again.

00:00:11.210 --> 00:00:17.010
Um. Okay. So, first of all for a couple of announcements.

00:00:17.010 --> 00:00:20.040
Um, first of all thanks to everyone, um,

00:00:20.040 --> 00:00:23.955
who filled in our mid-quarter survey we've actually gotten,

00:00:23.955 --> 00:00:27.320
um, great participation in that.

00:00:27.320 --> 00:00:29.855
Here are my two little Pac-Man figures.

00:00:29.855 --> 00:00:31.640
So, the Pac-Man figures thinks,

00:00:31.640 --> 00:00:34.580
means that almost everyone thinks the lectures are at

00:00:34.580 --> 00:00:38.390
the right pace and those that don't are pretty much evenly divided.

00:00:38.390 --> 00:00:42.680
Um, if we go for how challenging was Assignment three,

00:00:42.680 --> 00:00:46.190
slightly more people thought it was too easy than too hard.

00:00:46.190 --> 00:00:48.680
So, I guess we're setting about rectifying that with

00:00:48.680 --> 00:00:53.460
assignments four and five, um, [NOISE].

00:00:53.460 --> 00:00:56.030
So, though there are a whole bunch of other questions and we've

00:00:56.030 --> 00:00:59.180
been trying to absorb all the feedback.

00:00:59.180 --> 00:01:03.980
I mean one of the questions was what people wanted most from the remaining lectures.

00:01:03.980 --> 00:01:09.260
I guess the good news here is really we're very good at predicting, um,

00:01:09.260 --> 00:01:11.780
what people wanted, that or else everybody

00:01:11.780 --> 00:01:14.660
just looked ahead in the syllabus and wrote down what it said was

00:01:14.660 --> 00:01:19.160
ahead in the syllabus but I guess the most popular four answers to

00:01:19.160 --> 00:01:23.810
topics that they wanted in the remaining lectures were Transformers and BERT,

00:01:23.810 --> 00:01:25.860
both of which are gonna be covered this week.

00:01:25.860 --> 00:01:29.780
Uh, question-answering which we talked about last week, um,

00:01:29.780 --> 00:01:33.230
and then text generation and summarization

00:01:33.230 --> 00:01:38.275
and you guys get Abby back next week to talk about that.

00:01:38.275 --> 00:01:42.380
Um, there are also a lot of people also answered this question

00:01:42.380 --> 00:01:46.400
a different way as to what kind of style of stuff,

00:01:46.400 --> 00:01:50.960
um, some people emphasized new research and the latest updates from the field.

00:01:50.960 --> 00:01:53.300
I guess we'll get some of that today as well,

00:01:53.300 --> 00:01:55.235
some people are more interested in

00:01:55.235 --> 00:02:00.000
successful applications in industry or trying to do a bit of that,

00:02:00.000 --> 00:02:02.840
um, cool new neural architectures.

00:02:02.840 --> 00:02:05.660
Um, the bottom answer wasn't the most popular one,

00:02:05.660 --> 00:02:08.390
I'll admit but at least a few people, um,

00:02:08.390 --> 00:02:11.810
wish that we were teaching more linguistic stuff.

00:02:11.810 --> 00:02:14.300
Um, I mean that is something that I actually feel

00:02:14.300 --> 00:02:18.580
a bit awkward about the way things were merged with CS224N,

00:02:18.580 --> 00:02:20.100
with this deep learning,

00:02:20.100 --> 00:02:22.370
I mean the truth of the matter is that sort of seems

00:02:22.370 --> 00:02:24.850
like in the early part of the course,

00:02:24.850 --> 00:02:27.210
there's so much to cover with,

00:02:27.210 --> 00:02:29.660
um, neural networks, backpropagation,

00:02:29.660 --> 00:02:34.250
different, um, neural net architectures and so on that the reality is that we

00:02:34.250 --> 00:02:39.890
teach rather less linguistic stuff than we used to in the class.

00:02:39.890 --> 00:02:43.160
I mean, for the last four weeks of the class we really do try and

00:02:43.160 --> 00:02:46.730
cover some more linguistic stuff topics.

00:02:46.730 --> 00:02:49.255
Um, so look forward to that.

00:02:49.255 --> 00:02:51.255
Um, announcements.

00:02:51.255 --> 00:02:54.365
Okay. So we've made a couple of deadline changes.

00:02:54.365 --> 00:02:57.410
Um, firstly, a number of people have

00:02:57.410 --> 00:03:00.920
mentioned that they think assignment five is a bit tough.

00:03:00.920 --> 00:03:04.160
And so, we're giving people one extra day,

00:03:04.160 --> 00:03:06.105
um, to do assignment five.

00:03:06.105 --> 00:03:09.830
Um, I'm realizing in one sense that one extra day is not a ton

00:03:09.830 --> 00:03:13.760
but you know there's sort of this complex balance here because on the other hand,

00:03:13.760 --> 00:03:18.695
we don't really want to undermine time that people have available for final projects.

00:03:18.695 --> 00:03:23.010
And if you're one of the people who hasn't yet started assignment five,

00:03:23.010 --> 00:03:26.075
um, we do really encourage you to get underway on it.

00:03:26.075 --> 00:03:29.955
Um, yeah, in the reverse direction

00:03:29.955 --> 00:03:34.160
we decided that the project milestone was really too late.

00:03:34.160 --> 00:03:38.060
If we are going to be able to give you feedback on it that you could usefully make use

00:03:38.060 --> 00:03:42.080
of, so we're moving the project milestone date two days earlier.

00:03:42.080 --> 00:03:45.935
And so, we've also gotten everyone's project proposals and our

00:03:45.935 --> 00:03:50.270
planned hope is to get them back to everybody on Friday.

00:03:50.270 --> 00:03:52.310
Yes, so, a lot of things moving.

00:03:52.310 --> 00:03:56.975
Um, and finally on other announcements I guess, um, on

00:03:56.975 --> 00:04:01.640
this Thursday is our first invited speaker, um, and so,

00:04:01.640 --> 00:04:05.165
if you're in person student you're meant to be here,

00:04:05.165 --> 00:04:08.930
um, and if you're not able to be here,

00:04:08.930 --> 00:04:12.350
you should know about our reaction paragraph policy and

00:04:12.350 --> 00:04:16.460
I actually stuck up on the Piazza pinned posts about, um,

00:04:16.460 --> 00:04:21.375
reaction pieces and attendance, an example of a reaction piece, um,

00:04:21.375 --> 00:04:27.320
from a past class to make it a little bit more concrete what's expected there.

00:04:27.320 --> 00:04:31.850
But, you know, the idea is what we're hoping for something that isn't a ton of work.

00:04:31.850 --> 00:04:35.855
You can just write 100, 150 words, a few sentences,

00:04:35.855 --> 00:04:40.040
but wanting you to pick out a specific thing that was

00:04:40.040 --> 00:04:42.410
interesting and write a couple of sentences

00:04:42.410 --> 00:04:45.140
about what it was and what your thoughts are about it.

00:04:45.140 --> 00:04:50.270
I, not just some very generic statement of this was a lecture about transformers.

00:04:50.270 --> 00:04:52.805
He talked about transformers and it was interesting,

00:04:52.805 --> 00:04:58.635
that is not what we want for the reaction piece. Um, okay.

00:04:58.635 --> 00:05:01.140
So, here's the plan for today.

00:05:01.140 --> 00:05:04.955
So, for today's, what I want to talk about is,

00:05:04.955 --> 00:05:09.785
um, the exciting recent work about contextual word representations.

00:05:09.785 --> 00:05:15.620
I mean I, I was thinking of what I was gonna say I was wanting to say, oh, this is

00:05:15.620 --> 00:05:18.770
the most exciting thing in deep learning for NLP in

00:05:18.770 --> 00:05:22.040
the last five years then something's just completely wrong,

00:05:22.040 --> 00:05:27.080
because really this is the most exciting thing in deep learning that happened in 2018.

00:05:27.080 --> 00:05:29.960
I mean, I guess things move very quickly, um,

00:05:29.960 --> 00:05:33.530
in deep learning at the moment and it's sort of I don't think it's

00:05:33.530 --> 00:05:38.130
really fair to say that you know it's got 5 years of life.

00:05:38.130 --> 00:05:40.490
But there's a very exciting thing that happened last year,

00:05:40.490 --> 00:05:42.655
and we'll talk about that.

00:05:42.655 --> 00:05:45.720
Okay. So, we'll talk about early stuff,

00:05:45.720 --> 00:05:47.455
the ELMo, ULMfit,

00:05:47.455 --> 00:05:50.630
transformer architectures briefly and then go on to

00:05:50.630 --> 00:05:55.320
talk about the BERT model that's being quite prominent lately.

00:05:55.670 --> 00:05:58.215
So, let's just recap,

00:05:58.215 --> 00:06:02.605
let's just go backwards a bit first to think about, um,

00:06:02.605 --> 00:06:08.075
where we've been and where we are now and why we might want something more.

00:06:08.075 --> 00:06:09.695
So, up until now,

00:06:09.695 --> 00:06:11.060
we've sort of just had,

00:06:11.060 --> 00:06:16.255
one representation for words which is what we learned at the beginning of class,

00:06:16.255 --> 00:06:22.085
there was a word, you trained a word vector for it and that's what you used in your model.

00:06:22.085 --> 00:06:24.774
Um, and you could do that, with algorithms like Word2vec,

00:06:24.774 --> 00:06:28.075
GloVe, or fastText that I mentioned last week.

00:06:28.075 --> 00:06:34.460
Um, so some on this sort of progression of ideas in deep learning,

00:06:34.460 --> 00:06:39.050
when deep learning for NLP or the general

00:06:39.050 --> 00:06:42.065
just the resurgence of neural networks for NLP

00:06:42.065 --> 00:06:45.620
came about sort of at the beginning of this decade.

00:06:45.620 --> 00:06:50.640
Um, these pre-trained word vectors.

00:06:50.640 --> 00:06:54.790
So, pre-trained unsupervised over a large amount of text.

00:06:54.790 --> 00:06:58.270
They were completely seen as the secret sauce,

00:06:58.270 --> 00:07:00.805
and they were the thing that transformed

00:07:00.805 --> 00:07:04.795
neural networks from NLP to something that didn't really work,

00:07:04.795 --> 00:07:06.650
to something that worked great.

00:07:06.650 --> 00:07:09.910
Um, so, this is actually an old slide of mine.

00:07:09.910 --> 00:07:12.670
So, this is a slide I guess I first made for

00:07:12.670 --> 00:07:18.490
2012 ACL tutorial and then sort of used in lectures.

00:07:18.490 --> 00:07:22.995
Sort of in 2013, 2014. Um-.

00:07:22.995 --> 00:07:26.460
And so this was sort of the picture in those years.

00:07:26.460 --> 00:07:28.000
So this was looking at two tasks,

00:07:28.000 --> 00:07:33.115
part of speech tagging and named entity recognition which I'll use quite a bit today.

00:07:33.115 --> 00:07:38.275
And, you know, the top line was showing a state of the art which was

00:07:38.275 --> 00:07:42.780
a traditional categorical feature based classifier of the kind

00:07:42.780 --> 00:07:47.445
that dominated NLP in the 2000s decade, in their performance.

00:07:47.445 --> 00:07:53.215
And what then the next line showed is that if you took the same data set

00:07:53.215 --> 00:07:59.515
and you trained a supervised neural network on it and said how good is your performance?

00:07:59.515 --> 00:08:01.845
Um, the story was, it wasn't great.

00:08:01.845 --> 00:08:06.720
Um, part-of-speech tagging has very high numbers always for various reasons.

00:08:06.720 --> 00:08:11.395
So perhaps the more indicative one to look at is these named entity recognition numbers.

00:08:11.395 --> 00:08:14.530
So, you know, this was sort of neural net sucked, right?

00:08:14.530 --> 00:08:18.310
The reason why last decade everybody used, um,

00:08:18.310 --> 00:08:20.790
categorical feature based, you know,

00:08:20.790 --> 00:08:23.340
CRF, SVM kind of classifiers.

00:08:23.340 --> 00:08:26.995
Well, if you look, it worked eight percent better than a neural network.

00:08:26.995 --> 00:08:28.330
Why wouldn't anybody?

00:08:28.330 --> 00:08:32.845
But then what had happened was people had come up with this idea that we could

00:08:32.845 --> 00:08:37.515
do unsupervised pre-training of word representations,

00:08:37.515 --> 00:08:40.770
um, to come up with word vectors for words.

00:08:40.770 --> 00:08:42.060
And, you know, in those days,

00:08:42.060 --> 00:08:45.620
this was very hard to do the alg- both because of

00:08:45.620 --> 00:08:49.500
the kind of algorithms and the kind of machines that were available, right?

00:08:49.500 --> 00:08:51.990
So Collobert and Weston, 2011,

00:08:51.990 --> 00:08:56.980
spent seven weeks training their unsupervised word representations.

00:08:56.980 --> 00:08:58.105
And at the end of the day,

00:08:58.105 --> 00:09:01.795
there are only 100 dimensional, um, word representations.

00:09:01.795 --> 00:09:03.865
But this was the miracle breakthrough, right?

00:09:03.865 --> 00:09:08.965
You've put in this miracle breakthrough of unsupervised word representations.

00:09:08.965 --> 00:09:12.215
And now, the neural net is getting to 88.87.

00:09:12.215 --> 00:09:15.380
So it's almost as good as the feature-based classifier,

00:09:15.380 --> 00:09:17.510
and then like any good engineers,

00:09:17.510 --> 00:09:19.500
they did some hacking with some extra features,

00:09:19.500 --> 00:09:21.175
because they had some stuff like that.

00:09:21.175 --> 00:09:26.720
And they got a system that was then slightly better than the feature based system.

00:09:26.720 --> 00:09:29.520
Okay. So that was sort of our picture that,

00:09:29.520 --> 00:09:33.180
um, having these pre-trained,

00:09:33.180 --> 00:09:36.560
unsuper- and unsupervised manner of word representations,

00:09:36.560 --> 00:09:39.000
that was sort of the big breakthrough and

00:09:39.000 --> 00:09:42.280
the secret sauce that gave all the oomph that made,

00:09:42.280 --> 00:09:44.765
um, neural networks competitive.

00:09:44.765 --> 00:09:46.200
Um, but, you know,

00:09:46.200 --> 00:09:51.565
it's a sort of a funny thing happened which was after people had sort of had

00:09:51.565 --> 00:09:54.430
some of these initial breakthroughs which were

00:09:54.430 --> 00:09:57.510
all about unsupervised methods for pre-training,

00:09:57.510 --> 00:09:59.030
it was the same in vision.

00:09:59.030 --> 00:10:00.625
This was the era in vision,

00:10:00.625 --> 00:10:03.380
where you were building restricted Boltzmann machines and doing

00:10:03.380 --> 00:10:07.410
complicated unsupervised pre-training techniques on them as well.

00:10:07.410 --> 00:10:13.425
Some- somehow, after people had kind of discovered that and started to get good on it,

00:10:13.425 --> 00:10:16.260
people sort of started to discover, well,

00:10:16.260 --> 00:10:20.280
actually we have some new technologies for non-linearities,

00:10:20.280 --> 00:10:22.665
regularization, and things like that.

00:10:22.665 --> 00:10:25.830
And if we keep using those same technologies,

00:10:25.830 --> 00:10:29.770
we can just go back to good old supervised learning.

00:10:29.770 --> 00:10:34.510
And shockingly, it works way better now inside neural networks.

00:10:34.510 --> 00:10:37.440
And so if you sort of go ahead to what I will call,

00:10:37.440 --> 00:10:43.615
sort of 2014 to 2018 picture,

00:10:43.615 --> 00:10:46.445
the, the picture is actually very different.

00:10:46.445 --> 00:10:48.270
So the picture is, so this,

00:10:48.270 --> 00:10:51.295
the results I'm actually gonna show you this is from the Chen and Manning,

00:10:51.295 --> 00:10:54.550
um, neural dependency parser that we talked about weeks ago.

00:10:54.550 --> 00:10:56.695
The picture there was, um,

00:10:56.695 --> 00:10:59.065
and you could- despite the fact that

00:10:59.065 --> 00:11:02.995
this dependency parser is being trained on a pretty small corpus,

00:11:02.995 --> 00:11:05.640
a million words of supervised data,

00:11:05.640 --> 00:11:09.250
you can just initialize it with random word vectors,

00:11:09.250 --> 00:11:11.830
um, and train a dependency parser.

00:11:11.830 --> 00:11:13.740
And to a first approximation,

00:11:13.740 --> 00:11:15.060
it just works fine.

00:11:15.060 --> 00:11:17.935
You get, get sort of a 90 percent accuracy,

00:11:17.935 --> 00:11:20.425
E- um, English dependency parser.

00:11:20.425 --> 00:11:23.740
Now, it is the case that instead,

00:11:23.740 --> 00:11:27.475
you could use pre-trained word embeddings and you do a bit better.

00:11:27.475 --> 00:11:29.230
You do about one percent better.

00:11:29.230 --> 00:11:31.550
And so this was sort of the,

00:11:31.550 --> 00:11:35.725
the new world order which was yeah, um,

00:11:35.725 --> 00:11:40.860
these pre-trained unsupervised word embeddings are useful because you can

00:11:40.860 --> 00:11:46.840
train them from a lot more data and they can know about a much larger vocabulary.

00:11:46.840 --> 00:11:48.025
That means they are useful.

00:11:48.025 --> 00:11:51.805
They help with rare words and things like that and they give you a percent,

00:11:51.805 --> 00:11:55.160
but they're definitely no longer the sort of night and day,

00:11:55.160 --> 00:11:59.910
uh, thing to make neural networks work that we used to believe.

00:11:59.910 --> 00:12:04.465
I'm, I'm just gonna deviate here to,

00:12:04.465 --> 00:12:08.455
from the main narrative to just sort of say, um,

00:12:08.455 --> 00:12:13.490
one more tip for dealing with unknown words with word vectors,

00:12:13.490 --> 00:12:16.290
um, just in case it's useful for some people,

00:12:16.290 --> 00:12:19.350
building question answering systems, right?

00:12:19.350 --> 00:12:24.450
So, um, so for sort of word vectors on unknown words, you know,

00:12:24.450 --> 00:12:29.695
the commonest thing historically is you've got your supervised training data,

00:12:29.695 --> 00:12:32.790
you define a vocab which might be words that occur

00:12:32.790 --> 00:12:36.255
five times or more in your supervised training data.

00:12:36.255 --> 00:12:39.040
And you treat everything else as an UNK.

00:12:39.040 --> 00:12:42.085
And so you also train one vector per UNK.

00:12:42.085 --> 00:12:46.140
Um, but that has some problems which you have no way to

00:12:46.140 --> 00:12:51.250
distinguish different UNK words either for identity or meaning.

00:12:51.250 --> 00:12:54.745
And that tends to be problematic for question answering systems.

00:12:54.745 --> 00:12:58.140
And so one way to fix that is what we talked about last week,

00:12:58.140 --> 00:13:00.630
you just say, "Oh, words are made out of characters.

00:13:00.630 --> 00:13:05.655
I can use character representations to learn word vectors for other words."

00:13:05.655 --> 00:13:06.960
And you can certainly do that.

00:13:06.960 --> 00:13:08.230
You might wanna try that.

00:13:08.230 --> 00:13:10.210
That adds some complexity.

00:13:10.210 --> 00:13:14.380
Um, but especially for things like question answering systems,

00:13:14.380 --> 00:13:16.380
there are a couple of other things that you can do

00:13:16.380 --> 00:13:18.720
that work considerably better and they've been

00:13:18.720 --> 00:13:23.365
explored in this paper by Dhingra et al., um, from 2017.

00:13:23.365 --> 00:13:26.695
Um, the first one is to say, well, um,

00:13:26.695 --> 00:13:34.255
when you at test-time encounter new words, probably your unsupervised word,

00:13:34.255 --> 00:13:39.335
pre-trained word embeddings have a much bigger vocabulary than your actual system does.

00:13:39.335 --> 00:13:42.090
So anytime you come across a word that isn't in

00:13:42.090 --> 00:13:44.955
your vocab but is in the pre-trained word embeddings,

00:13:44.955 --> 00:13:48.985
just use, get the word vector of that word and start using it.

00:13:48.985 --> 00:13:51.745
That'll be a much more useful thing to use.

00:13:51.745 --> 00:13:53.850
And then there's a second possible tip that if you

00:13:53.850 --> 00:13:56.305
see something that's still an unknown word,

00:13:56.305 --> 00:13:57.925
rather than treating it as UNK,

00:13:57.925 --> 00:14:00.030
you just assign it on the spot,

00:14:00.030 --> 00:14:01.750
a random word vector.

00:14:01.750 --> 00:14:06.810
And so this has the effect that each word does get a unique identity.

00:14:06.810 --> 00:14:09.360
Which means if you see the same word in the question,

00:14:09.360 --> 00:14:11.065
and a potential answer,

00:14:11.065 --> 00:14:14.940
they will match together beautifully in an accurate way which you're

00:14:14.940 --> 00:14:19.895
not getting with just UNK matching and those can be kind of useful ideas to try.

00:14:19.895 --> 00:14:25.275
Okay, end digression. Okay, so up until now,

00:14:25.275 --> 00:14:28.225
we just sort of had this representation of words,

00:14:28.225 --> 00:14:31.600
we ran Word2vec and we got a word vector,

00:14:31.600 --> 00:14:33.725
um, for each word.

00:14:33.725 --> 00:14:37.570
Um, so, um, that, that was useful.

00:14:37.570 --> 00:14:39.105
It's worked pretty well.

00:14:39.105 --> 00:14:41.545
Um, but it had, um,

00:14:41.545 --> 00:14:46.860
some big problems. So what were the big problems of doing that?

00:14:48.530 --> 00:14:50.785
The problems when we,

00:14:50.785 --> 00:14:53.475
of having a word vector in each word, yes.

00:14:53.475 --> 00:14:56.825
A lot of words have like one spelling, but a whole bunch of meanings.

00:14:56.825 --> 00:15:00.550
Right, so, a word can have- So, typically,

00:15:00.550 --> 00:15:05.620
you have one string of letters which has a whole bunch of meanings.

00:15:05.620 --> 00:15:09.220
So, words have a ton of senses.

00:15:09.220 --> 00:15:11.350
Um, and yeah, so that's

00:15:11.350 --> 00:15:13.405
the biggest and most obvious problem that we're

00:15:13.405 --> 00:15:15.550
collapsing together all the meanings of words.

00:15:15.550 --> 00:15:18.130
So, we talked about a bit where

00:15:18.130 --> 00:15:20.290
one solution to that was you could distinguish

00:15:20.290 --> 00:15:23.680
word senses and to have different word vectors for them.

00:15:23.680 --> 00:15:27.700
Um, and I then said something about also you could think of

00:15:27.700 --> 00:15:31.750
this word vector as a sort of a mixture of them and maybe your model could separate it.

00:15:31.750 --> 00:15:35.065
But it seems like we might want to take that more seriously.

00:15:35.065 --> 00:15:37.420
And one way, um,

00:15:37.420 --> 00:15:43.345
that we could take that more seriously is we could start to say, well,

00:15:43.345 --> 00:15:50.065
really, you know, traditional lists of word senses are themselves a crude approximation.

00:15:50.065 --> 00:15:57.925
What we actually want to know is the sense of the word inside a particular context of use.

00:15:57.925 --> 00:16:00.400
And sort of what I mean by that is, you know,

00:16:00.400 --> 00:16:04.570
we distinguish different senses of a word, right?

00:16:04.570 --> 00:16:08.380
Say for the word star there's the astronomical sense and

00:16:08.380 --> 00:16:12.415
there's the Hollywood sense and they're clearly different.

00:16:12.415 --> 00:16:16.270
But you know, if we then go to this what I'm calling the Hollywood sense,

00:16:16.270 --> 00:16:18.370
I could then say, well, wait a minute.

00:16:18.370 --> 00:16:21.520
There are movie stars and there are rock stars,

00:16:21.520 --> 00:16:24.070
and there, uh, are R&amp;B stars,

00:16:24.070 --> 00:16:25.825
and there are country stars.

00:16:25.825 --> 00:16:29.275
Now, all of those different senses, um,

00:16:29.275 --> 00:16:33.025
in certain contexts, though, one or other of them would be evoked.

00:16:33.025 --> 00:16:34.210
And so, you know,

00:16:34.210 --> 00:16:36.879
it's very hard if you're trying to actually enumerate

00:16:36.879 --> 00:16:40.825
senses of a word as to which ones count as different or the same.

00:16:40.825 --> 00:16:44.785
So, it's really you sort of wanna know what a word means in a context.

00:16:44.785 --> 00:16:50.515
There's a second limitation of these word vectors which is,

00:16:50.515 --> 00:16:53.710
we haven't really talked about and is less obvious,

00:16:53.710 --> 00:16:56.770
but it's also something that we might want to fix, and at least one of

00:16:56.770 --> 00:17:00.070
the models we discussed today takes some aim at that,

00:17:00.070 --> 00:17:04.045
and that is, we just sort of have one vector for a word.

00:17:04.045 --> 00:17:07.465
But there are sort of different dimensions of a word.

00:17:07.465 --> 00:17:10.390
So, words can have different meanings,

00:17:10.390 --> 00:17:14.605
some sort of real semantics or words can have

00:17:14.605 --> 00:17:19.765
different syntactic behavior like different parts of speech or grammatical behavior.

00:17:19.765 --> 00:17:23.065
So, in some sense, arrive and arrival,

00:17:23.065 --> 00:17:25.675
their semantics are almost the same,

00:17:25.675 --> 00:17:28.990
but they're different parts of speech.

00:17:28.990 --> 00:17:32.410
One is a, um, a verb and one is a noun,

00:17:32.410 --> 00:17:35.380
so they can kind of appear in quite different places.

00:17:35.380 --> 00:17:39.130
And you know, you'd wanna do different things with them in a dependency parser.

00:17:39.130 --> 00:17:41.290
And there are even other dimensions.

00:17:41.290 --> 00:17:47.200
So, words also have register and connotation differences.

00:17:47.200 --> 00:17:52.270
So, you can probably think of lots of different words for a bathroom,

00:17:52.270 --> 00:17:56.170
and a lot of those words all means semantically the same,

00:17:56.170 --> 00:17:58.330
but have rather different registers and

00:17:58.330 --> 00:18:01.225
connotations as to when they're appropriate to use.

00:18:01.225 --> 00:18:04.900
And so, we might want to distinguish words on that basis as well.

00:18:04.900 --> 00:18:08.350
And so these are the kinds of soluti- things we want to

00:18:08.350 --> 00:18:11.845
solve with our new contextual word embeddings.

00:18:11.845 --> 00:18:16.240
Um, so I've said up until now, you know,

00:18:16.240 --> 00:18:21.670
oh, we just had these word vectors that we use,

00:18:21.670 --> 00:18:24.205
words just had one vector.

00:18:24.205 --> 00:18:29.170
Um, but if you actually think about it, maybe that's wrong.

00:18:29.170 --> 00:18:34.270
I mean, maybe we never had a problem, or at any rate, we solved it six classes ago.

00:18:34.270 --> 00:18:36.205
Because if you remember back, [NOISE] um,

00:18:36.205 --> 00:18:39.460
to when we started talking about neural language models,

00:18:39.460 --> 00:18:41.950
well, what did a neural language model do?

00:18:41.950 --> 00:18:45.100
At the bottom, you fed into it the word vectors.

00:18:45.100 --> 00:18:49.599
But then you ran across that one or more recurrent layers,

00:18:49.599 --> 00:18:51.565
something like a LSTM layer,

00:18:51.565 --> 00:18:57.430
and it was calculating these representations that sit above each word and,

00:18:57.430 --> 00:19:00.760
you know, the role of those hidden states is a bit ambivalent.

00:19:00.760 --> 00:19:02.260
They are used for prediction.

00:19:02.260 --> 00:19:06.670
And they are used for next hidden state and output states and so on.

00:19:06.670 --> 00:19:09.205
But in many ways you can think huh,

00:19:09.205 --> 00:19:16.045
these representations are actually representations of a word in context.

00:19:16.045 --> 00:19:18.895
And if you think about what happened with, uh,

00:19:18.895 --> 00:19:21.310
the question answering systems,

00:19:21.310 --> 00:19:23.620
that's exactly how they were used, right?

00:19:23.620 --> 00:19:26.200
We ran LSTM's backwards and forwards,

00:19:26.200 --> 00:19:29.320
over a question in the passage, and then we say,

00:19:29.320 --> 00:19:33.085
okay those are a good representation of a word's meaning and context.

00:19:33.085 --> 00:19:36.205
Let's start matching them with attention functions et cetera.

00:19:36.205 --> 00:19:41.470
So, it sort of seemed like we'd already invented a way to have,

00:19:41.470 --> 00:19:47.185
um, context-specific representations of words.

00:19:47.185 --> 00:19:50.035
And effectively, you know,

00:19:50.035 --> 00:19:55.450
the rest of the content of this lecture is sort of basically no more complex than that.

00:19:55.450 --> 00:20:02.245
Um, that it took a while but sort of people woke up and started to notice, huh,

00:20:02.245 --> 00:20:04.600
really when you're running any language model,

00:20:04.600 --> 00:20:08.365
you generate a context-specific representation of words.

00:20:08.365 --> 00:20:11.364
Maybe, if we just took those context-specific

00:20:11.364 --> 00:20:16.810
representation of words, they'd be useful for doing other things with them.

00:20:16.810 --> 00:20:18.595
And that's sort of, you know,

00:20:18.595 --> 00:20:19.780
there are a few more details,

00:20:19.780 --> 00:20:23.980
but that's really the summary of the entire of this lecture.

00:20:23.980 --> 00:20:34.165
Um, so one of the first things to do that was a paper that Matt Peters wrote in 2017,

00:20:34.165 --> 00:20:36.265
um, the year before last.

00:20:36.265 --> 00:20:41.080
Um, and this was sort of a predecessor to the sort of modern, um,

00:20:41.080 --> 00:20:46.720
versions of, um, these context-sensitive word embeddings.

00:20:46.720 --> 00:20:49.840
So, um, together with co-authors,

00:20:49.840 --> 00:20:53.185
he came up with a paper called TagLM,

00:20:53.185 --> 00:20:56.920
but it essentially already had all the main ideas.

00:20:56.920 --> 00:21:01.255
So, what, um, was wanted was okay.

00:21:01.255 --> 00:21:05.620
We want to do better at tasks such as named-entity recognition.

00:21:05.620 --> 00:21:10.945
And what we'd like to do is know about the meaning of a word in context.

00:21:10.945 --> 00:21:14.500
Um, but you know, standardly if we're doing named-entity recognition,

00:21:14.500 --> 00:21:18.175
we just train it on half a million words of supervised data.

00:21:18.175 --> 00:21:20.230
And that's not much of a source of

00:21:20.230 --> 00:21:23.950
information to be learning about the meaning of words and context.

00:21:23.950 --> 00:21:28.810
So, why don't we adopt the semi-supervised approach and so that's what we do.

00:21:28.810 --> 00:21:32.740
So, we start off with a ton of unlabeled data.

00:21:32.740 --> 00:21:35.545
Um, and from that unlabeled data,

00:21:35.545 --> 00:21:39.850
we can train a conventional word embedding model like Word2vec.

00:21:39.850 --> 00:21:43.810
But we can also at the same time train a neural language model.

00:21:43.810 --> 00:21:47.590
So, something like a bi-LSTM language model.

00:21:47.590 --> 00:21:55.190
Okay. So, then for step two when we're using our supervised data,

00:21:55.740 --> 00:21:58.900
um, actually, I guess that's step three.

00:21:58.900 --> 00:22:05.965
Okay. Um, so for then when we want to learn our supervised part-of-speech tagger at the top,

00:22:05.965 --> 00:22:09.190
what we're gonna do is say, well,

00:22:09.190 --> 00:22:13.420
for the input words New what York is located,

00:22:13.420 --> 00:22:18.340
we can not only use the word embedding which is context independent,

00:22:18.340 --> 00:22:24.505
but we can use our trained recurrent language model and also run it over this import,

00:22:24.505 --> 00:22:31.180
and then we'll generate hidden states in our bi-LSTM language model and we can also

00:22:31.180 --> 00:22:38.380
feed those in as features into ou- our sequence tagging model,

00:22:38.380 --> 00:22:41.335
and those features will let it work better.

00:22:41.335 --> 00:22:47.095
Here's a second picture that runs this through in much greater detail.

00:22:47.095 --> 00:22:52.885
So, so, we're assuming that we have trained, uh,

00:22:52.885 --> 00:22:56.875
bi-LSTM language model, um,

00:22:56.875 --> 00:22:59.755
on a lot of unsupervised data.

00:22:59.755 --> 00:23:06.370
Then what we wanna do is we want to do named entity recognition for New York is located.

00:23:06.370 --> 00:23:09.160
So, the first thing we do is say,

00:23:09.160 --> 00:23:16.150
let's just run New York is located through our separately trained neural language model.

00:23:16.150 --> 00:23:18.925
So, we run it through a forward language model.

00:23:18.925 --> 00:23:21.490
We run it through a backward language model.

00:23:21.490 --> 00:23:23.830
We get from that, um,

00:23:23.830 --> 00:23:26.515
a hidden state representation,

00:23:26.515 --> 00:23:28.750
um, for each word,

00:23:28.750 --> 00:23:31.644
we concatenate the forward and backward ones,

00:23:31.644 --> 00:23:35.529
and that's going to give a set, a concatenated language model embedding

00:23:35.529 --> 00:23:40.090
which we'll use as features in our named entity recognizer.

00:23:40.090 --> 00:23:43.870
So, then for the named entity recognizer itself that we're gonna

00:23:43.870 --> 00:23:48.700
train supervised while we have the same sentence,

00:23:48.700 --> 00:23:55.390
so we can both look up a Word2vec-style token embedding for it.

00:23:55.390 --> 00:24:01.315
We can use what we learned about with character level CNNs and RNNs and we can build

00:24:01.315 --> 00:24:04.450
a character level representation for it which we also

00:24:04.450 --> 00:24:07.795
concatenate to have two representations.

00:24:07.795 --> 00:24:15.685
So, we feed these representations into a bi-LSTM layer.

00:24:15.685 --> 00:24:19.945
But then when we get the output of the, this bi-LSTM layer,

00:24:19.945 --> 00:24:22.180
as well as this normal output,

00:24:22.180 --> 00:24:28.285
we can concatenate with each output what was- what we get from our,

00:24:28.285 --> 00:24:30.730
um, neural language model.

00:24:30.730 --> 00:24:33.370
So, each of these things becomes a pair of states.

00:24:33.370 --> 00:24:36.490
One that's spit up from the first bi-LSTM layer and

00:24:36.490 --> 00:24:39.760
then it's concatenated with something from the neural language model.

00:24:39.760 --> 00:24:46.450
And so that concatenated representation is then fed into a second layer of bi-LSTM.

00:24:46.450 --> 00:24:48.265
And then from the output of that,

00:24:48.265 --> 00:24:51.310
we do the usual kind of softmax classification

00:24:51.310 --> 00:24:54.790
where we're then giving tags like beginning of location,

00:24:54.790 --> 00:24:59.380
end of location, say New York is a location and then is, we'll get

00:24:59.380 --> 00:25:05.540
another tag to say it's not a location. Does that makes sense?

00:25:07.860 --> 00:25:14.305
Yeah so, um, so the central thing is

00:25:14.305 --> 00:25:20.455
sort of having seen that these sort of representations that we get from Bi-LSTMs are useful.

00:25:20.455 --> 00:25:24.580
We're just going to feed them into supervised models as we train them,

00:25:24.580 --> 00:25:28.600
and the idea is that will give us better features of words.

00:25:28.600 --> 00:25:32.305
Some kind of representation of their meaning and context,

00:25:32.305 --> 00:25:39.610
which will allow us to learn better named entity recognizers or what it- whatever it is.

00:25:39.610 --> 00:25:42.580
Maybe I should put this slide earlier,

00:25:42.580 --> 00:25:45.955
but this slide was meant to remind you what a named entity recognizer is.

00:25:45.955 --> 00:25:47.305
I hope you remember that,

00:25:47.305 --> 00:25:50.605
something where are we going to find and label

00:25:50.605 --> 00:25:54.850
entities for things like person, location, date, organization.

00:25:54.850 --> 00:25:57.625
So anyway, doing this worked.

00:25:57.625 --> 00:25:59.905
So, here's a little bit of a history.

00:25:59.905 --> 00:26:07.285
So the most famous Named Entity Recognition dataset is this CoNLL 2003 dataset,

00:26:07.285 --> 00:26:10.180
which actually exists in multiple languages.

00:26:10.180 --> 00:26:14.770
But whenever people say CoNLL 2003 and don't mention a language,

00:26:14.770 --> 00:26:17.665
they mean the English version of it.

00:26:17.665 --> 00:26:20.410
That's the way the world works.

00:26:20.410 --> 00:26:24.430
Um, okay so on this dataset- yeah.

00:26:24.430 --> 00:26:29.035
So, it's sort of been around for whatever, 15 years roughly now.

00:26:29.035 --> 00:26:32.920
So, in the- so it was originally a competition, right?

00:26:32.920 --> 00:26:36.235
So, this is in 2003 was the original bake-off.

00:26:36.235 --> 00:26:38.455
My group actually took place in that.

00:26:38.455 --> 00:26:42.055
Took part in it. I think we got third or fourth place or something,

00:26:42.055 --> 00:26:46.720
and our F1 score was 86.

00:26:46.720 --> 00:26:52.810
The people who won were from IBM Research Labs,

00:26:52.810 --> 00:26:55.870
and they got 88 almost 89.

00:26:55.870 --> 00:27:00.490
But a difference between these two things is our system was

00:27:00.490 --> 00:27:05.290
a single clean machine-learning model categorical,

00:27:05.290 --> 00:27:08.800
whereas the IBM one was not only an ensemble

00:27:08.800 --> 00:27:13.600
of four different machine learning models, plus gazetteers.

00:27:13.600 --> 00:27:16.090
It also fit in the output of

00:27:16.090 --> 00:27:22.450
two other old NER systems that IBM people were trained years ago on different data.

00:27:22.450 --> 00:27:25.030
So it was- I guess it worked for them but,

00:27:25.030 --> 00:27:27.100
it was a fairly complex system.

00:27:27.100 --> 00:27:29.170
Here's another system from Stanford.

00:27:29.170 --> 00:27:33.910
So this was our classic Stanford NER system that is widely used.

00:27:33.910 --> 00:27:39.475
So, this was then using a conditional random field model which generally dominated

00:27:39.475 --> 00:27:46.935
sort of the second half of the 2000s and the first half of the 2010s for doing NER,

00:27:46.935 --> 00:27:52.800
and it was sort of, you know, a bit but not usually better than the 2003 system.

00:27:52.800 --> 00:27:59.910
This system here was sort of the best ever built categorical CRF system.

00:27:59.910 --> 00:28:06.105
But rather than only using the training data to build the model as this system did,

00:28:06.105 --> 00:28:11.065
it threw in Wikipedia and other stuff to make it work better,

00:28:11.065 --> 00:28:14.725
and that got you to about 90.8 F1.

00:28:14.725 --> 00:28:23.770
So, essentially, once sort of BiLSTM style models started to be known and used in NLP.

00:28:23.770 --> 00:28:28.060
That was when people were able to train, build training

00:28:28.060 --> 00:28:33.175
just on the training data systems that worked a lot better.

00:28:33.175 --> 00:28:38.440
Because essentially you're going from the same data from this system to that system.

00:28:38.440 --> 00:28:41.530
So, you're getting about 4 percent gain on it,

00:28:41.530 --> 00:28:45.835
because it's not- wasn't making use of Wikipedia and things like that;

00:28:45.835 --> 00:28:51.805
and so this Ma and Hovy system is pretty well-known getting about 91.21.

00:28:51.805 --> 00:28:56.140
Okay, but if we then go to this TagLM system, um,

00:28:56.140 --> 00:29:00.610
that Matt Peters and Co have a system that

00:29:00.610 --> 00:29:05.590
was sort of similar to the Ma and Hovy system that is a little bit worse.

00:29:05.590 --> 00:29:12.670
But the point is that this BiLSTM uses sorry- using the neural language model,

00:29:12.670 --> 00:29:17.080
is just a useful oomph giver which sort of takes the results up.

00:29:17.080 --> 00:29:18.610
Yeah, not night and day but,

00:29:18.610 --> 00:29:24.160
slightly over a percent and then gives them the best NER system that was then available.

00:29:24.160 --> 00:29:25.990
So that sort of proved these sort of

00:29:25.990 --> 00:29:33.380
contextual word representations really had some power and started to be useful,

00:29:33.660 --> 00:29:38.620
and then there's a white space at the top because we'll get back to more of this later.

00:29:38.620 --> 00:29:43.240
Um, there's some details on their language model.

00:29:43.240 --> 00:29:46.330
Some of their details are that it's useful to have

00:29:46.330 --> 00:29:49.285
a bidirectional language model, not unidirectional.

00:29:49.285 --> 00:29:51.640
It's useful to have a big um,

00:29:51.640 --> 00:29:55.510
language model to get much in the way of gains,

00:29:55.510 --> 00:30:01.960
um and, you need to train this language model over much more data.

00:30:01.960 --> 00:30:07.070
It doesn't work if you're just sort of training it over your supervised training data.

00:30:08.160 --> 00:30:11.140
Another model that was around was CoVe,

00:30:11.140 --> 00:30:12.610
but I think I'll skip that.

00:30:12.610 --> 00:30:15.895
Okay. So, then the next year, um,

00:30:15.895 --> 00:30:18.865
Matt Peters and a different set of colleagues

00:30:18.865 --> 00:30:23.410
then came up with an improved system called ELMo,

00:30:23.410 --> 00:30:27.610
and effectively this was the breakthrough system.

00:30:27.610 --> 00:30:30.960
That this was sort of just the system that everybody

00:30:30.960 --> 00:30:35.880
noticed and said "Wow these contextual word vectors are great.

00:30:35.880 --> 00:30:37.680
Everyone should be using them,

00:30:37.680 --> 00:30:41.620
not traditional word vectors." Yes?

00:30:41.790 --> 00:30:51.490
I have a simple question, imagine re-training a system, what exactly

00:30:59.330 --> 00:31:02.910
what measure [inaudible]

00:31:02.910 --> 00:31:06.250
It's pre-trained because this piece over here;

00:31:06.250 --> 00:31:11.035
a big neural language model is trained first,

00:31:11.035 --> 00:31:13.270
and there's an important thing I forgot to say.

00:31:13.270 --> 00:31:15.280
So, thank you for the question.

00:31:15.280 --> 00:31:20.020
The main reason why it's- in some sense pre-trained,

00:31:20.020 --> 00:31:21.670
is this was trained first.

00:31:21.670 --> 00:31:26.245
But the main reason why people think of this as pre-training

00:31:26.245 --> 00:31:30.985
is after you've trained this, it is frozen.

00:31:30.985 --> 00:31:35.680
So, this is just something that you can run with parameters which will give

00:31:35.680 --> 00:31:40.840
you a vector which is your contextual word representation each position,

00:31:40.840 --> 00:31:43.960
and then that's just going to be used in this system.

00:31:43.960 --> 00:31:46.420
So, when you're training this system,

00:31:46.420 --> 00:31:48.580
there's no gradient flowing back into

00:31:48.580 --> 00:31:52.885
this neural language model that's changing and updating it; it's just fixed.

00:31:52.885 --> 00:31:56.260
And so that's sort of the sense when people are talking about pre-training.

00:31:56.260 --> 00:31:59.185
It's sort of normally a model that you trained

00:31:59.185 --> 00:32:02.680
somewhere else and that you're using to give features,

00:32:02.680 --> 00:32:06.280
but isn't part of the model that you are now training. Yeah?

00:32:06.280 --> 00:32:12.060
[inaudible]

00:32:12.060 --> 00:32:16.650
Well, I guess that's, I wouldn't quite call it reconstruction.

00:32:16.650 --> 00:32:20.190
Yeah, it's unsupervised in the sense that this is a language model,

00:32:20.190 --> 00:32:22.470
you're training it to predict the next word.

00:32:22.470 --> 00:32:28.335
So here are words one to k. What is the k plus oneth word during a cross entropy loss,

00:32:28.335 --> 00:32:30.150
and repeat over for each position.

00:32:30.150 --> 00:32:37.530
[NOISE] Yes, so I mean,

00:32:37.530 --> 00:32:45.240
having gone through TagLM in some detail, I mean,

00:32:45.240 --> 00:32:52.350
in some sense, the difference between TagLM and ELMo is kind of small,

00:32:52.350 --> 00:32:54.090
it's sort of in the details.

00:32:54.090 --> 00:32:56.385
So I mean, to a first approximation,

00:32:56.385 --> 00:32:58.890
they're doing exactly the same again,

00:32:58.890 --> 00:33:00.675
but a little bit better.

00:33:00.675 --> 00:33:06.360
Um, so, um, I sort of hope it made sense the last time,

00:33:06.360 --> 00:33:09.015
I mean, what are the things that are different?

00:33:09.015 --> 00:33:13.710
Um, they do the bidirectional language model a bit differently,

00:33:13.710 --> 00:33:16.800
and actually one of their concerns was to try and come up with

00:33:16.800 --> 00:33:21.435
a compact language model that would be easy for people to use,

00:33:21.435 --> 00:33:27.390
um, in other tasks even if they don't have the beefiest computer hardware in the world.

00:33:27.390 --> 00:33:29.940
And so they decided to dispense with having

00:33:29.940 --> 00:33:34.185
word representations altogether and just use, um,

00:33:34.185 --> 00:33:38.610
character CNNs to build word representations,

00:33:38.610 --> 00:33:42.045
because that lessens the number of parameters you have to store,

00:33:42.045 --> 00:33:45.510
the big matrices you have to, um, use.

00:33:45.510 --> 00:33:50.280
Um, they expanded the hidden dimension to 4,096,

00:33:50.280 --> 00:33:52.020
but then they project it down to

00:33:52.020 --> 00:33:57.450
512 dimensions with a sort of feed-forward projection layer,

00:33:57.450 --> 00:34:00.300
and that's a fairly common technique to again reduce

00:34:00.300 --> 00:34:03.360
the parameterization of the model so that you have a lot of

00:34:03.360 --> 00:34:06.060
parameters going in their current direction but you

00:34:06.060 --> 00:34:09.315
need much smaller matrices for including,

00:34:09.315 --> 00:34:11.400
um, the input at the next level.

00:34:11.400 --> 00:34:13.530
Um, between the layers,

00:34:13.530 --> 00:34:18.300
they now use a residual connection and they do a bit of parameter tying.

00:34:18.300 --> 00:34:21.615
So it's sort of all in the little details there.

00:34:21.615 --> 00:34:25.200
Um, but there's another interesting thing

00:34:25.200 --> 00:34:28.890
that they did which was an important innovation of ELMo,

00:34:28.890 --> 00:34:30.405
so we should get this bit.

00:34:30.405 --> 00:34:32.400
So in TagLM,

00:34:32.400 --> 00:34:36.930
what was fed from the pre-trained LM into

00:34:36.930 --> 00:34:43.695
the main model was just the top level of the neural language model stack,

00:34:43.695 --> 00:34:47.040
and that was completely standard de rigueur in those days,

00:34:47.040 --> 00:34:49.800
that you might have had three layers of

00:34:49.800 --> 00:34:53.790
neural language model that you regard at the top-level as your sort

00:34:53.790 --> 00:34:57.120
of one that's really captured the meaning of

00:34:57.120 --> 00:35:01.185
the sentence and the lower layers for processing that led up to it.

00:35:01.185 --> 00:35:05.295
Um, and they had the idea that maybe

00:35:05.295 --> 00:35:09.780
it would be useful to actually use all layers of the,

00:35:09.780 --> 00:35:12.960
biLSTM of the neural language models.

00:35:12.960 --> 00:35:16.935
So maybe not just the top layer but all layers would be kind of useful.

00:35:16.935 --> 00:35:20.760
So, um, there are these kind of complex equations,

00:35:20.760 --> 00:35:24.480
uh, but essentially the point of it over here is,

00:35:24.480 --> 00:35:27.360
we going- for a particular position,

00:35:27.360 --> 00:35:29.505
word seven in the language model,

00:35:29.505 --> 00:35:33.930
we're going to take the hidden state at each level of our,

00:35:33.930 --> 00:35:36.599
our neural language model stack,

00:35:36.599 --> 00:35:40.545
we're going to give- learn a weight for that level,

00:35:40.545 --> 00:35:42.540
we go in to sort of sum them,

00:35:42.540 --> 00:35:47.190
so this is sort of a weighted average of the hidden layers at each position,

00:35:47.190 --> 00:35:51.225
and that will be used as our basic representation.

00:35:51.225 --> 00:35:55.785
Um, and so, they found that that gave quite a bit

00:35:55.785 --> 00:36:00.480
of extra usefulness for- and different tasks could prefer different layers.

00:36:00.480 --> 00:36:03.045
There's one other bit here which is,

00:36:03.045 --> 00:36:08.625
they learn a global scaling factor Gamma for a particular task.

00:36:08.625 --> 00:36:13.665
And this allows them to control that for some tasks, the, um,

00:36:13.665 --> 00:36:16.080
contextual word embeddings might be really

00:36:16.080 --> 00:36:19.515
useful and for other tasks they might not be so useful,

00:36:19.515 --> 00:36:21.449
so you're just sort of learning a specific,

00:36:21.449 --> 00:36:25.095
um, usefulness for the entire task.

00:36:25.095 --> 00:36:30.285
Okay. So, um, that's the sort of new version of language model.

00:36:30.285 --> 00:36:33.390
But this, this is allowing this idea of well,

00:36:33.390 --> 00:36:36.750
maybe there's sort of more syntactic meanings

00:36:36.750 --> 00:36:39.855
of a word and more semantic meanings of a word,

00:36:39.855 --> 00:36:43.380
possibly those could be represented at different layers of

00:36:43.380 --> 00:36:45.510
your neural language model and then for

00:36:45.510 --> 00:36:48.330
different tasks you can differentially weight them.

00:36:48.330 --> 00:36:51.330
Um, so that's the basic model.

00:36:51.330 --> 00:36:56.850
So you run your biLSTM before to g et representations of each word.

00:36:56.850 --> 00:36:59.610
And then the generic ELMo recipe was,

00:36:59.610 --> 00:37:03.215
well, with that frozen language model,

00:37:03.215 --> 00:37:08.540
you want to feed it into some supervised model depending on what the task was,

00:37:08.540 --> 00:37:10.070
and they sort of say in the paper, well,

00:37:10.070 --> 00:37:12.500
how you do this maybe depends on the task.

00:37:12.500 --> 00:37:15.965
You might want to kind of concatenate it to the intermediate layer,

00:37:15.965 --> 00:37:17.660
just as the TagLM did,

00:37:17.660 --> 00:37:19.085
that might be fine.

00:37:19.085 --> 00:37:22.220
But you know it might also be useful to make use of

00:37:22.220 --> 00:37:25.700
these ELMo representations when producing outputs,

00:37:25.700 --> 00:37:28.910
so if you're doing something like a

00:37:28.910 --> 00:37:35.210
generation system or you might just sort of feed in the ELMo representation again,

00:37:35.210 --> 00:37:38.630
be- before you sort of do the softmax to find the output,

00:37:38.630 --> 00:37:41.580
they sort of left it flexible as to how it was used,

00:37:41.580 --> 00:37:42.960
but the general picture,

00:37:42.960 --> 00:37:45.960
you know, was kinda like we saw before.

00:37:45.960 --> 00:37:49.590
Indeed I'm reusing the same picture that you've calculated

00:37:49.590 --> 00:37:54.105
an ELMo representation for each position as a weighted average,

00:37:54.105 --> 00:37:57.360
and then you're sort of concatenating that to the hidden state of

00:37:57.360 --> 00:38:01.125
your supervised system and generating your output.

00:38:01.125 --> 00:38:04.890
And anyway, um, one way or another,

00:38:04.890 --> 00:38:07.920
um, they were able to do this, uh,

00:38:07.920 --> 00:38:11.925
and that with the little improvements that gave them about an extra

00:38:11.925 --> 00:38:16.770
0.3 percent in Named Entity Recognition.

00:38:16.770 --> 00:38:21.165
Um, now, that sort of sounds like not very much.

00:38:21.165 --> 00:38:26.055
And you might conclude from this why the excitement [LAUGHTER] and,

00:38:26.055 --> 00:38:28.695
you know, in some sense, um,

00:38:28.695 --> 00:38:33.720
that's right because sort of to the extent that there was an interesting idea here really

00:38:33.720 --> 00:38:39.060
that come up with it for the TagLM paper which gave a much better gain.

00:38:39.060 --> 00:38:45.254
But, you know, why everyone got really excited was that in the ELMo paper,

00:38:45.254 --> 00:38:48.030
they then showed this isn't something that you can

00:38:48.030 --> 00:38:50.910
do one-off to improve a Named Entity Recognizer,

00:38:50.910 --> 00:38:58.035
you can take these ELMo representations and use them for pretty much any NLP task,

00:38:58.035 --> 00:39:01.695
and they can be very useful and give good gains.

00:39:01.695 --> 00:39:08.340
And so, essentially why people got excited was because of the data that's in this table.

00:39:08.340 --> 00:39:11.250
So here we're taking a whole bunch of very different tasks,

00:39:11.250 --> 00:39:13.620
so there's SQuAD question-answering, uh,

00:39:13.620 --> 00:39:16.380
there's natural language inference,

00:39:16.380 --> 00:39:18.345
there's semantic role labeling,

00:39:18.345 --> 00:39:23.760
there's co-reference, the Named Entity Recognition, doing sentiment analysis,

00:39:23.760 --> 00:39:26.730
so a wide range of different NLP tasks,

00:39:26.730 --> 00:39:30.315
and they have a previous state of the art system.

00:39:30.315 --> 00:39:34.860
They produced their own baseline um, which is,

00:39:34.860 --> 00:39:40.080
you know, commonly sort of similar to the previous state of the art,

00:39:40.080 --> 00:39:43.620
but usually actually a bit worse than

00:39:43.620 --> 00:39:45.360
the current state of the art because it's

00:39:45.360 --> 00:39:48.315
whatever simpler cleaner system that they came up with,

00:39:48.315 --> 00:39:51.345
but then they could say in each case,

00:39:51.345 --> 00:39:55.260
oh, just take this system and add

00:39:55.260 --> 00:39:59.985
ELMo vectors into the hidden representations in the middle,

00:39:59.985 --> 00:40:02.040
and have those help you predict.

00:40:02.040 --> 00:40:04.710
And in general, in all cases,

00:40:04.710 --> 00:40:08.970
that's giving you about a three percent or so gain absolute

00:40:08.970 --> 00:40:13.470
which was then producing this huge performance increase,

00:40:13.470 --> 00:40:18.450
which in all cases was moving the performance well above the previous,

00:40:18.450 --> 00:40:20.040
um, state of the art system.

00:40:20.040 --> 00:40:24.000
So you know, this sort of then made it seem like magic pixie dust,

00:40:24.000 --> 00:40:28.050
because, you know, in the stakes of NLP conference land, you know,

00:40:28.050 --> 00:40:30.960
a lot of people use to try and to come up

00:40:30.960 --> 00:40:34.500
with a paper for the next year that's one percent better

00:40:34.500 --> 00:40:37.080
on one task and writing it up and that's

00:40:37.080 --> 00:40:41.715
their big breakthrough for the year to get their new paper out.

00:40:41.715 --> 00:40:44.355
And the idea that there's just well this set of

00:40:44.355 --> 00:40:48.045
this way of creating context sensitive, um,

00:40:48.045 --> 00:40:51.660
word representations and you just use them in any task,

00:40:51.660 --> 00:40:55.245
and they'll give you around three percent and take you past the state of the art,

00:40:55.245 --> 00:40:58.395
this seemed like it was really great stuff.

00:40:58.395 --> 00:41:01.800
And so people got very excited about this and that won

00:41:01.800 --> 00:41:06.390
the Best Paper Award at the NAACL 2018 conference.

00:41:06.390 --> 00:41:10.590
Ah, and then, a- as I sort of vaguely mentioned,

00:41:10.590 --> 00:41:14.370
um, so the model that they actually used wasn't a deep stack,

00:41:14.370 --> 00:41:17.520
there were actually only two layers of biLSTMs,

00:41:17.520 --> 00:41:22.620
but they do show this interesting result that the lower level better captures

00:41:22.620 --> 00:41:26.790
low-level syntax word properties

00:41:26.790 --> 00:41:30.389
and its most useful things like part-of-speech tagging,  syntactic

00:41:30.389 --> 00:41:33.210
dependencies, NER, where the top layer of

00:41:33.210 --> 00:41:35.310
their language model is better for

00:41:35.310 --> 00:41:38.940
higher level semantics that is more useful for things like sentiments,

00:41:38.940 --> 00:41:42.495
semantic role labeling and question answering.

00:41:42.495 --> 00:41:45.150
Um, so that seemed interesting,

00:41:45.150 --> 00:41:47.940
though it'll actually be interesting to see how that panned

00:41:47.940 --> 00:41:51.820
out more if you had sort of more layers to play with.

00:41:52.100 --> 00:41:55.875
Okay. ELMo, done.

00:41:55.875 --> 00:41:58.590
Um, so I'm moving right ahead.

00:41:58.590 --> 00:42:05.550
Um, here's something else that I just thought I should mention a little bit about,

00:42:05.550 --> 00:42:09.270
another piece of work that came out around the same time,

00:42:09.270 --> 00:42:12.450
a few months later maybe or maybe not,

00:42:12.450 --> 00:42:14.430
came out around the same time, uh,

00:42:14.430 --> 00:42:18.420
in, in 2018, was this work on

00:42:18.420 --> 00:42:23.025
Universal Language Model Fine-tuning for text classification,

00:42:23.025 --> 00:42:25.995
um, or ULMfit, by Howard and Ruder.

00:42:25.995 --> 00:42:31.335
And essentially this had the same general idea of saying, Well,

00:42:31.335 --> 00:42:39.370
what we want to do is transfer learning where we could learn a big language model, um.

00:42:40.560 --> 00:42:43.075
A big language model,

00:42:43.075 --> 00:42:48.220
and then for our target task which might be named entity recognition.

00:42:48.220 --> 00:42:50.200
But here's text classification,

00:42:50.200 --> 00:42:55.690
we can transfer this language model information and help us to do better with the task.

00:42:55.690 --> 00:42:58.690
And so, they proposed an architecture to do that.

00:42:58.690 --> 00:43:00.640
And so, their architecture was,

00:43:00.640 --> 00:43:07.960
you have a big unsupervised corpus from which you train a neural language model.

00:43:07.960 --> 00:43:12.775
They used the deeper neural language model with three hidden layers.

00:43:12.775 --> 00:43:14.920
Um, you then fine tune

00:43:14.920 --> 00:43:19.660
your neural language model on the actual domain that you're interested in working in.

00:43:19.660 --> 00:43:22.255
So, this was sort of an extra stage that they did.

00:43:22.255 --> 00:43:24.730
And then finally, um,

00:43:24.730 --> 00:43:28.960
you now introduce your classification objectives.

00:43:28.960 --> 00:43:31.930
So, what they're going to be doing is making text classifiers.

00:43:31.930 --> 00:43:33.535
So, we're now wanting to,

00:43:33.535 --> 00:43:39.280
take this model and turn it from a language model into a text classifier.

00:43:39.280 --> 00:43:42.340
Um, but there's something that they did differently, um,

00:43:42.340 --> 00:43:43.720
which is in some sense,

00:43:43.720 --> 00:43:46.840
foreshadows the later work in transformers.

00:43:46.840 --> 00:43:52.210
So, rather than just feeding features from this into a completely different network,

00:43:52.210 --> 00:43:58.710
they keep using the same network but they introduce a different objective at the top.

00:43:58.710 --> 00:44:01.710
So, one thing you could do with this network is use

00:44:01.710 --> 00:44:05.015
it to predict the next word as a language model.

00:44:05.015 --> 00:44:06.460
And so at this point,

00:44:06.460 --> 00:44:09.820
they freeze the parameters of that softmax at the top,

00:44:09.820 --> 00:44:11.455
that's why it's shown in black.

00:44:11.455 --> 00:44:14.935
Um, but instead, they could stick on

00:44:14.935 --> 00:44:19.825
a different prediction unit where it's predicting stuff for a particular task.

00:44:19.825 --> 00:44:21.610
So, it might be predicting

00:44:21.610 --> 00:44:26.680
positive or negative sentiment in a text classification task or something like that.

00:44:26.680 --> 00:44:27.760
So, in their model,

00:44:27.760 --> 00:44:31.915
they're sort of reusing the same network but sticking on the top of that,

00:44:31.915 --> 00:44:36.205
a different layer, to do the new classification task.

00:44:36.205 --> 00:44:39.700
Um, they were also interested in something small,

00:44:39.700 --> 00:44:43.615
the sort of one GPU model of research, um,

00:44:43.615 --> 00:44:47.620
the paper has a lot of detail, the sort of tricks

00:44:47.620 --> 00:44:52.150
and care and feeding of your neural models to maximize performance.

00:44:52.150 --> 00:44:56.245
If you're interested in that, you could sort of look up some of the details about that.

00:44:56.245 --> 00:45:00.250
Um, but what they were able to show again,

00:45:00.250 --> 00:45:03.820
was making use of this language model pre-training was

00:45:03.820 --> 00:45:07.495
a very effective way to improve performance,

00:45:07.495 --> 00:45:09.865
this time for text classification.

00:45:09.865 --> 00:45:12.520
So, these are text classification datasets,

00:45:12.520 --> 00:45:14.260
IMDb is for sentiment,

00:45:14.260 --> 00:45:18.970
um, TREC is for topical text classification, and again,

00:45:18.970 --> 00:45:22.780
there are preceding systems that other people have developed and they

00:45:22.780 --> 00:45:26.620
are showing that by making use of this language model pre-training,

00:45:26.620 --> 00:45:31.390
they're able to significantly improve on the state of the art of these error rates,

00:45:31.390 --> 00:45:33.590
so that low is good.

00:45:33.900 --> 00:45:39.715
They also showed another interesting result which is kind of,

00:45:39.715 --> 00:45:44.395
um, what you would expect or hope from doing this kind of transfer learning,

00:45:44.395 --> 00:45:46.330
that what they were able to show is,

00:45:46.330 --> 00:45:51.205
if you can train this neural language model on a big amount of data,

00:45:51.205 --> 00:45:54.430
that that means you will then be able to do well on

00:45:54.430 --> 00:45:59.110
your supervised task even when trained on pretty little data.

00:45:59.110 --> 00:46:01.780
Um, so, here this is error rate,

00:46:01.780 --> 00:46:03.355
so low is good.

00:46:03.355 --> 00:46:05.170
So, what the- and here's the number of

00:46:05.170 --> 00:46:08.815
training examples which has being done on a log scale.

00:46:08.815 --> 00:46:11.710
And so the blue line is if you're just training

00:46:11.710 --> 00:46:15.730
a text classifier from scratch on supervised data.

00:46:15.730 --> 00:46:19.765
So, you need a lot of data to start to do pretty well.

00:46:19.765 --> 00:46:24.715
Um, but if you're making use of this transfer learning, um,

00:46:24.715 --> 00:46:27.894
from a pre-trained language model,

00:46:27.894 --> 00:46:30.310
you can get to that you're sort of doing pretty

00:46:30.310 --> 00:46:33.700
well with way less, um, training examples.

00:46:33.700 --> 00:46:35.889
Essentially, an order of magnitude,

00:46:35.889 --> 00:46:39.655
less training examples will give you the same amount of performance.

00:46:39.655 --> 00:46:44.020
And the difference between these two lines corresponds to the extra,

00:46:44.020 --> 00:46:48.670
um, phase that they had in the middle of theirs, um, which is,

00:46:48.670 --> 00:46:53.920
whether you're doing this sort of extra fine tuning on your target domain,

00:46:53.920 --> 00:46:58.690
um, it's part of your process and they found that to be pretty helpful.

00:46:58.690 --> 00:47:05.215
Okay. So, that, um, is another precursor.

00:47:05.215 --> 00:47:11.545
Um, and so, one big part of what has happened since then,

00:47:11.545 --> 00:47:15.820
is effectively people said this is a good idea, uh,

00:47:15.820 --> 00:47:21.910
maybe it'll become a really really good idea if we just make things way bigger.

00:47:21.910 --> 00:47:24.250
Um, so, ULMfit, um,

00:47:24.250 --> 00:47:28.045
was something that you could train in one GPU day,

00:47:28.045 --> 00:47:31.870
sounds appealing for CS224N final projects,

00:47:31.870 --> 00:47:34.930
remember that, um, and but well,

00:47:34.930 --> 00:47:39.115
then the people at OpenAI decided, well,

00:47:39.115 --> 00:47:43.300
we could build a pretrain language model and train it on

00:47:43.300 --> 00:47:47.590
a much larger amount of data on a much larger amount of compute,

00:47:47.590 --> 00:47:54.130
and use about 242 GPU days and that will get a lot better, and it did.

00:47:54.130 --> 00:47:57.190
Um, and then the people at Google said,

00:47:57.190 --> 00:48:00.445
well we could train a model, um,

00:48:00.445 --> 00:48:04.660
in to 256 TPU days,

00:48:04.660 --> 00:48:07.645
which means maybe about double the amount of computation.

00:48:07.645 --> 00:48:09.565
It's hard to figure out exactly,

00:48:09.565 --> 00:48:12.175
and that might be able to do exciting things,

00:48:12.175 --> 00:48:14.950
and that was the BERT model, and it did.

00:48:14.950 --> 00:48:18.370
Um, and then if you're following along these things, um,

00:48:18.370 --> 00:48:20.110
just last week, um,

00:48:20.110 --> 00:48:22.270
the OpenAI people said,

00:48:22.270 --> 00:48:26.845
well we can go much bigger again and we can train a model, um,

00:48:26.845 --> 00:48:32.830
for approximately 2,000 TPU version three days.

00:48:32.830 --> 00:48:36.340
Um, and it will be able to,

00:48:36.340 --> 00:48:39.294
um, do much bigger again,

00:48:39.294 --> 00:48:41.080
a bit much better again,

00:48:41.080 --> 00:48:44.410
um, and so, this is this GP2,

00:48:44.410 --> 00:48:47.800
GPT-2 language model, um,

00:48:47.800 --> 00:48:50.680
which OpenAI released last week.

00:48:50.680 --> 00:48:56.740
Um, and they're, they're actually very impressive results, um,

00:48:56.740 --> 00:49:00.730
when they're showing that if you're sort of building a really,

00:49:00.730 --> 00:49:05.155
really huge language model over a very large amount of data.

00:49:05.155 --> 00:49:09.745
And then you say language model go off and generate some text,

00:49:09.745 --> 00:49:11.800
on this particular topic,

00:49:11.800 --> 00:49:15.100
that it can actually just do a great job of producing text.

00:49:15.100 --> 00:49:17.125
So, the way this was being do- done,

00:49:17.125 --> 00:49:19.930
was a humanist writing a couple of sentences;

00:49:19.930 --> 00:49:21.190
in a shocking finding,

00:49:21.190 --> 00:49:23.515
scientists discovered a herd of unicorns,

00:49:23.515 --> 00:49:27.700
living in remote previously unexplored valley in the Andes Mountains.

00:49:27.700 --> 00:49:29.905
Um, and so, we then,

00:49:29.905 --> 00:49:33.700
using our neural language model and chugging through that,

00:49:33.700 --> 00:49:35.680
so that gives us context,

00:49:35.680 --> 00:49:37.765
and then say generate more text,

00:49:37.765 --> 00:49:39.760
and it starts to generate the scientist

00:49:39.760 --> 00:49:42.160
named the population after their distinctive horn,

00:49:42.160 --> 00:49:44.320
Ovid's Unicorn, these four-horned,

00:49:44.320 --> 00:49:47.815
silver-white Uni four corns were previously unknown to science.

00:49:47.815 --> 00:49:50.080
Um, it produces remarkably,

00:49:50.080 --> 00:49:52.735
um, good text or at least in the,

00:49:52.735 --> 00:49:57.220
in the hand-picked examples [LAUGHTER] that they showed in the tech news,

00:49:57.220 --> 00:49:59.920
um, it produces extremely good text.

00:49:59.920 --> 00:50:04.960
Um, yeah so, I think one should be a little bit cautious about, um,

00:50:04.960 --> 00:50:07.930
that and sort of some of its random outputs actually

00:50:07.930 --> 00:50:10.900
aren't nearly as good but nevertheless you know,

00:50:10.900 --> 00:50:12.895
I think is is actually dramatic

00:50:12.895 --> 00:50:16.540
how good language models are becoming once you are training

00:50:16.540 --> 00:50:23.210
them on long contexts as we can do with modern models on vast amounts of data, um-.

00:50:23.280 --> 00:50:27.430
So then, um, the OpenAI people decided

00:50:27.430 --> 00:50:31.720
this language model was so good that they weren't gonna release it to the world, um,

00:50:31.720 --> 00:50:34.480
which then got transformed into headlines of,

00:50:34.480 --> 00:50:39.265
Elon Musk's OpenAI builds artificial intelligence so powerful,

00:50:39.265 --> 00:50:41.980
it must be kept locked up for the good of humanity.

00:50:41.980 --> 00:50:46.660
[LAUGHTER] Um, with the suitable pictures that always turn off at

00:50:46.660 --> 00:50:52.075
these moments down the bottom of the screen, um, and,

00:50:52.075 --> 00:50:57.520
um, yeah I guess that was the leading even Elon Musk to be wanting to clarify and say

00:50:57.520 --> 00:51:03.020
that it's not actually really that he's directing what's happening at OpenAI anymore.

00:51:03.020 --> 00:51:06.355
Um, anyway, moving right along.

00:51:06.355 --> 00:51:09.760
Um, so, part of the story here is

00:51:09.760 --> 00:51:14.635
just a scaling thing that these things have been getting bigger and bigger,

00:51:14.635 --> 00:51:18.760
um, but the other part of the story is that all three of

00:51:18.760 --> 00:51:23.785
these are then systems that use the transformer architecture.

00:51:23.785 --> 00:51:27.700
And transformer architectures have not only being very powerful,

00:51:27.700 --> 00:51:32.575
but technically had allowed scaling to much bigger sizes.

00:51:32.575 --> 00:51:35.575
So to understand some of the rest of these, um,

00:51:35.575 --> 00:51:39.055
we should learn more about transformers.

00:51:39.055 --> 00:51:42.610
And so, I'm sort of gonna do that, um,

00:51:42.610 --> 00:51:46.495
but I mean, um, in mix of orders,

00:51:46.495 --> 00:51:50.200
um, our invited speaker coming Thursday uh, is, um,

00:51:50.200 --> 00:51:52.420
one of the authors of the transformer paper,

00:51:52.420 --> 00:51:54.490
and he's gonna talk about transformers.

00:51:54.490 --> 00:51:57.430
So I think what I'm gonna do is, um,

00:51:57.430 --> 00:52:01.000
say a little bit about transformers quickly,

00:52:01.000 --> 00:52:04.090
but not really dwell on all the details, um,

00:52:04.090 --> 00:52:06.265
but hope that it's a bit of an introduction,

00:52:06.265 --> 00:52:10.360
and you can find out more on Thursday about the details and

00:52:10.360 --> 00:52:15.190
then talk some more about the BERT model before finishing.

00:52:15.190 --> 00:52:19.450
So the motivation for transformers is essentially

00:52:19.450 --> 00:52:23.245
we want things to go faster so we can build bigger models,

00:52:23.245 --> 00:52:26.125
and the problem as we mentioned for these, um,

00:52:26.125 --> 00:52:31.060
LSTM or in general any of the recurrent models is the fact that they're recurrent.

00:52:31.060 --> 00:52:36.190
You have to generate sort of one to n status time chugging through,

00:52:36.190 --> 00:52:41.275
and that means you just can't do the same kind of parallel computation, um,

00:52:41.275 --> 00:52:46.970
that GPUs love that you can do in things like convolutional neural networks.

00:52:46.970 --> 00:52:48.855
But, you know, on the other hand,

00:52:48.855 --> 00:52:51.210
we discovered that even though, um,

00:52:51.210 --> 00:52:56.005
these gated recurrent units like LSTMs and GRUs are great,

00:52:56.005 --> 00:53:00.070
that to get really great performance out of these recurrent models,

00:53:00.070 --> 00:53:05.680
we found that we wanted to- we had a problem within these long sequence lengths,

00:53:05.680 --> 00:53:09.010
and we can improve things by adding attention mechanisms.

00:53:09.010 --> 00:53:12.070
And so that led to the idea of- well,

00:53:12.070 --> 00:53:14.425
since attention works so great,

00:53:14.425 --> 00:53:17.440
maybe we can just use attention,

00:53:17.440 --> 00:53:22.195
and we can actually get rid of the recurrent part of the model [NOISE] altogether.

00:53:22.195 --> 00:53:27.625
And so that actually then leads to the idea of these transformer architectures,

00:53:27.625 --> 00:53:32.545
and the original paper on this is actually called attention is all you need,

00:53:32.545 --> 00:53:36.700
which reflects this idea of we're gonna keep the attention part,

00:53:36.700 --> 00:53:40.000
and we're getting- going to get rid of the, um,

00:53:40.000 --> 00:53:43.960
recurrent part, and we'll be able to build a great model.

00:53:43.960 --> 00:53:45.310
So in the initial work,

00:53:45.310 --> 00:53:48.790
what they're doing is machine translation kind of like

00:53:48.790 --> 00:53:52.720
the Neural Machine Translation with attention we described,

00:53:52.720 --> 00:53:56.185
but what they're wanting to do is build

00:53:56.185 --> 00:54:03.625
a complex encoder and a complex decoder that works non-recurrently,

00:54:03.625 --> 00:54:07.659
and, um, nevertheless is able to translate sentences

00:54:07.659 --> 00:54:13.075
well by making use of lots of attention distributions.

00:54:13.075 --> 00:54:18.070
And so, I wanted to say a little bit more quickly about that,

00:54:18.070 --> 00:54:20.965
and hopefully we'll get more of this on Thursday.

00:54:20.965 --> 00:54:24.685
Um, first as a- as a recommended resource,

00:54:24.685 --> 00:54:26.545
if you wanna look at, um,

00:54:26.545 --> 00:54:29.695
home and learn more about, um,

00:54:29.695 --> 00:54:34.000
the transformer architecture, there's this really great, um,

00:54:34.000 --> 00:54:39.100
bit of work by Sasha Rush called The Annotated Transformer that goes through

00:54:39.100 --> 00:54:45.025
the entire transformer paper accompanied by PyTorch code in a Jupyter Notebook,

00:54:45.025 --> 00:54:48.220
and so that can actually be a really useful thing,

00:54:48.220 --> 00:54:54.235
but I'll go through a little bit of the basics now of how we do things.

00:54:54.235 --> 00:54:57.460
So the basic idea, um,

00:54:57.460 --> 00:55:03.385
is that they're going to use attention everywhere to calculate things.

00:55:03.385 --> 00:55:07.540
And, um, we talked before about the different kinds of

00:55:07.540 --> 00:55:12.520
attention of the sort of multiplicative by linear attention and the little,

00:55:12.520 --> 00:55:15.490
um, feed-forward network additive attention.

00:55:15.490 --> 00:55:18.670
They kind of go for the simplest kind of attention,

00:55:18.670 --> 00:55:23.035
where the attention is just dot-products between two things.

00:55:23.035 --> 00:55:26.860
Um, but they sort of do the more comp- for various purposes,

00:55:26.860 --> 00:55:32.830
they do the more complicated version of dot-product between two things where they have,

00:55:32.830 --> 00:55:36.280
um, when the- the things that they're looking up are

00:55:36.280 --> 00:55:40.375
assumed to be key-value pairs, keys and values,

00:55:40.375 --> 00:55:46.765
and so you're calculating the similarity as a dot-product between a query and the key,

00:55:46.765 --> 00:55:48.415
and then based on that,

00:55:48.415 --> 00:55:52.060
you're going to be using the vector for the corresponding value.

00:55:52.060 --> 00:55:55.795
So our equation here for what we're calculating is where you are

00:55:55.795 --> 00:56:00.130
looking using the softmax over query, um,

00:56:00.130 --> 00:56:03.610
key similarities and using that to give

00:56:03.610 --> 00:56:08.680
the weightings as an attention based weighting over the corresponding values.

00:56:08.680 --> 00:56:12.220
Um, so that's the basic attention model.

00:56:12.220 --> 00:56:15.985
Um, so that add- saying it that way, um,

00:56:15.985 --> 00:56:18.100
adds a little bit of complexity,

00:56:18.100 --> 00:56:21.145
but sort of for the simplest part for their encoder.

00:56:21.145 --> 00:56:26.065
Actually, all of the query keys and values are exactly the same.

00:56:26.065 --> 00:56:28.225
They are the words, um,

00:56:28.225 --> 00:56:32.620
that they're using as their source language, um, things.

00:56:32.620 --> 00:56:38.000
So, it sort of adds some complexity that isn't really there.

00:56:38.340 --> 00:56:42.280
Um, okay. Um, I'll skip that.

00:56:42.280 --> 00:56:48.175
Um, so, there are a couple of other things that they do.

00:56:48.175 --> 00:56:52.165
One thing that they note is that, um,

00:56:52.165 --> 00:56:57.745
the- the values you get from, um, QTK, um,

00:56:57.745 --> 00:57:03.280
very, in variances the dimension gets large

00:57:03.280 --> 00:57:08.230
so that they sort of do some normalization by the size of the hidden state dimension,

00:57:08.230 --> 00:57:12.280
but I'll leave that out as well for details, right.

00:57:12.280 --> 00:57:13.945
So in the encoder, um,

00:57:13.945 --> 00:57:17.020
everything is just our word vectors,

00:57:17.020 --> 00:57:20.380
there are the queries, the keys, and the values.

00:57:20.380 --> 00:57:23.785
Um, and we're gonna use attention everywhere in the system.

00:57:23.785 --> 00:57:29.860
Oops. Okay. So the second new idea is, well,

00:57:29.860 --> 00:57:36.115
attention is great but maybe it's bad if you only have one attention distribution,

00:57:36.115 --> 00:57:39.190
because you're gonna only attend to things one way.

00:57:39.190 --> 00:57:42.415
Maybe for various users it would be great

00:57:42.415 --> 00:57:45.760
if you could attend from one position to various things.

00:57:45.760 --> 00:57:51.190
So, if you're thinking about syntax and what we did with dependency parsers.

00:57:51.190 --> 00:57:54.970
If you're a word, you might want to attend to your headword,

00:57:54.970 --> 00:57:59.155
but you might also wanna attend- attend to your dependent words.

00:57:59.155 --> 00:58:01.689
And if you happen to be a pronoun,

00:58:01.689 --> 00:58:06.010
you might want to attend to what the pronoun refers to you.

00:58:06.010 --> 00:58:07.855
You might want to have lots of attention.

00:58:07.855 --> 00:58:12.010
So they introduced this idea of multi-head attention.

00:58:12.010 --> 00:58:16.360
And so what you're doing with multi-head attention is you have,

00:58:16.360 --> 00:58:18.130
um, your hidden states,

00:58:18.130 --> 00:58:20.170
um, in your system,

00:58:20.170 --> 00:58:23.800
and you map them via projection layers, um,

00:58:23.800 --> 00:58:27.670
which are just multiplications by different W matrices as

00:58:27.670 --> 00:58:32.350
linear projections into sort of different lower dimensional spaces,

00:58:32.350 --> 00:58:37.030
and then you use each of those to calculate dot-product attention,

00:58:37.030 --> 00:58:40.270
and so you can attend to different things at the same time.

00:58:40.270 --> 00:58:42.670
And this multi-head attention was one of

00:58:42.670 --> 00:58:48.655
the very successful ideas of transformers that made them a more powerful architecture.

00:58:48.655 --> 00:58:54.715
Okay. Um, so, then for our complete transformer block,

00:58:54.715 --> 00:59:00.505
it's sort of then starting to build complex architectures like we sort of started seeing,

00:59:00.505 --> 00:59:02.200
um, the other week.

00:59:02.200 --> 00:59:05.320
Um, so- okay.

00:59:05.320 --> 00:59:06.969
Yeah. So, starting,

00:59:06.969 --> 00:59:10.060
um, from our word vectors,

00:59:10.060 --> 00:59:16.915
we're kind of going to do attention to multiple different things,

00:59:16.915 --> 00:59:19.900
um, and we're simultaneously gonna have

00:59:19.900 --> 00:59:23.530
a residual connection that short-circuits around them.

00:59:23.530 --> 00:59:28.045
Um, we're then going to sort of sum the two of these,

00:59:28.045 --> 00:59:33.115
and then they're going to do a normalization at that point.

00:59:33.115 --> 00:59:36.400
Um, I talked previously about batch normalization,

00:59:36.400 --> 00:59:38.020
they don't do batch normalization,

00:59:38.020 --> 00:59:41.200
they do another variant which is layer normalization,

00:59:41.200 --> 00:59:43.855
which is a different way of doing normalization,

00:59:43.855 --> 00:59:45.625
but I'll skip that for now.

00:59:45.625 --> 00:59:49.000
And then they sort of for one transformer block,

00:59:49.000 --> 00:59:52.045
you then go after the multi-head attention,

00:59:52.045 --> 00:59:56.755
you put things through a feed-forward layer which also has a residual connection,

00:59:56.755 --> 00:59:58.810
you sum the output of those,

00:59:58.810 --> 01:00:03.790
and you then again do another, um, layer normalization.

01:00:03.790 --> 01:00:08.965
So this is the basic transformer block that they're gonna use everywhere.

01:00:08.965 --> 01:00:11.320
And to make their complete architectures,

01:00:11.320 --> 01:00:13.210
they're then gonna sort of start stacking

01:00:13.210 --> 01:00:17.050
these transformer blocks to produce a very deep network.

01:00:17.050 --> 01:00:18.160
And in some sense,

01:00:18.160 --> 01:00:22.780
what has been found is that transformers performed very well.

01:00:22.780 --> 01:00:25.000
But, you know, there's no free lunch,

01:00:25.000 --> 01:00:26.440
um, you kind of can't.

01:00:26.440 --> 01:00:28.150
You're- now, no longer getting

01:00:28.150 --> 01:00:31.450
recurrent information actually being carried along a sequence.

01:00:31.450 --> 01:00:36.280
You've got a word at some position which can be casting attention,

01:00:36.280 --> 01:00:38.035
uh, on other words.

01:00:38.035 --> 01:00:41.560
So if you'd like to have information carried along in a chain,

01:00:41.560 --> 01:00:44.980
you've sort of first of all gotta walk the first step of the chain,

01:00:44.980 --> 01:00:46.690
and then you need to have another layer

01:00:46.690 --> 01:00:49.690
vertically which can walk the next step of the chain,

01:00:49.690 --> 01:00:53.800
and then you need to have another layer vertically that walks the next step of the chain.

01:00:53.800 --> 01:00:57.520
So, you're getting rid of the recurrence along the sequence,

01:00:57.520 --> 01:01:03.220
but you're substituting some depth to allow things to walk along multiple hops.

01:01:03.220 --> 01:01:07.885
But nevertheless, that's highly advantageous in GPU architectures

01:01:07.885 --> 01:01:13.300
because it allows you to use parallelization to calculate everything at each,

01:01:13.300 --> 01:01:16.400
um, depth at the same time. Um.

01:01:19.290 --> 01:01:22.900
Maybe I'll go light on explaining this as well.

01:01:22.900 --> 01:01:25.420
Um, so they use byte-pair encodings.

01:01:25.420 --> 01:01:27.490
But if you do nothing else,

01:01:27.490 --> 01:01:30.850
you just have words fed in this word vectors and you have

01:01:30.850 --> 01:01:34.765
no idea whether you're at the beginning of the sentence or at the end of the sentence.

01:01:34.765 --> 01:01:38.680
Though, they have a message of- method of doing positional encoding which gives

01:01:38.680 --> 01:01:42.865
you some ideas to pro- position your word has in the sentence.

01:01:42.865 --> 01:01:47.950
Okay. Um, so that's sort of the, um, encoder system.

01:01:47.950 --> 01:01:49.540
So from the words,

01:01:49.540 --> 01:01:51.550
they have an initial word embedding,

01:01:51.550 --> 01:01:54.085
you add in their positional encoding,

01:01:54.085 --> 01:01:58.105
you go into one of these transformer blocks,

01:01:58.105 --> 01:02:01.030
and you then repeat it n times.

01:02:01.030 --> 01:02:03.835
So you'll have a stack of these transformer blocks.

01:02:03.835 --> 01:02:06.775
So you're multiple times doing, um,

01:02:06.775 --> 01:02:11.590
multi-head attention to other parts of the sentence, calculating values,

01:02:11.590 --> 01:02:12.940
feeding forward a value,

01:02:12.940 --> 01:02:14.860
putting it through a fully-connected layer,

01:02:14.860 --> 01:02:19.735
and then you just sort of repeat, do attention to different places in the sentence.

01:02:19.735 --> 01:02:21.310
Get all your information,

01:02:21.310 --> 01:02:23.275
put it through a fully connected layer,

01:02:23.275 --> 01:02:26.755
and go up, um, proceeding up deeply.

01:02:26.755 --> 01:02:31.000
And and that sounds a little mysterious,

01:02:31.000 --> 01:02:34.215
but it turns out to work just great.

01:02:34.215 --> 01:02:36.600
And the way to think about,

01:02:36.600 --> 01:02:39.900
I think is that at each stage,

01:02:39.900 --> 01:02:44.760
you can look with your multi-headed attention and various other places in the sentence,

01:02:44.760 --> 01:02:48.210
accumulate information, push it up to the next layer.

01:02:48.210 --> 01:02:51.255
And if you do that sort of half a dozen times,

01:02:51.255 --> 01:02:55.530
you can be starting to progressively push information along

01:02:55.530 --> 01:03:01.455
the sequence in either direction to calculate values that are of interest.

01:03:01.455 --> 01:03:08.605
Um, and the interesting thing is that these models turn out to work

01:03:08.605 --> 01:03:15.970
really well at sort of learning to attend the interesting things in linguistic structure.

01:03:15.970 --> 01:03:19.810
Um, so these are just sort of suggestive diagrams,

01:03:19.810 --> 01:03:24.190
but this is looking at layer five of the transformer stack and

01:03:24.190 --> 01:03:28.945
seeing what words are being attended to by different attention heads.

01:03:28.945 --> 01:03:33.010
So these different colors correspond to different attention heads.

01:03:33.010 --> 01:03:35.050
And so the sentence is,

01:03:35.050 --> 01:03:39.010
um, it is, "In this spirit,

01:03:39.010 --> 01:03:42.310
that a majority of American governments have passed new laws since

01:03:42.310 --> 01:03:47.064
2009 making the registration or voting process more difficult."

01:03:47.064 --> 01:03:53.275
And so what we see is sort of most of the attention heads,

01:03:53.275 --> 01:03:58.840
uh, looking from making to making more difficult and that seems to be useful.

01:03:58.840 --> 01:04:03.700
One of the attention heads seems to be looking at the word itself might be okay.

01:04:03.700 --> 01:04:10.570
Um, then the other ones are sort of looking a bit at laws and at 2009.

01:04:10.570 --> 01:04:14.530
So it's sort of picking out the arguments, um,

01:04:14.530 --> 01:04:18.910
and modifiers and making in a syntax kind of like way.

01:04:18.910 --> 01:04:21.880
Um, interestingly, for pronouns,

01:04:21.880 --> 01:04:26.770
attention heads appear to learn to be able to look back to reference.

01:04:26.770 --> 01:04:28.795
So the law will never be perfect,

01:04:28.795 --> 01:04:35.185
but its application should be just that one attention head it for its,

01:04:35.185 --> 01:04:39.055
is looking at what its is modifying in the application.

01:04:39.055 --> 01:04:40.930
But another attention head,

01:04:40.930 --> 01:04:45.640
the its is looking strongly at what its refers back to as the law.

01:04:45.640 --> 01:04:47.740
So that seems kind of cool.

01:04:47.740 --> 01:04:49.810
Um, yeah.

01:04:49.810 --> 01:04:52.870
Um, okay.

01:04:52.870 --> 01:04:56.035
And so then, for the rest of the model, um,

01:04:56.035 --> 01:04:58.990
there's then some more complexity for how to use

01:04:58.990 --> 01:05:05.020
the transformers decoder to give you a full neural machine translation system.

01:05:05.020 --> 01:05:08.770
But I think maybe I will skip that and go

01:05:08.770 --> 01:05:13.750
on and say a bit about BERT in my remaining minutes.

01:05:13.750 --> 01:05:18.490
Okay. So, um, the latest and greatest contextual

01:05:18.490 --> 01:05:23.590
word representations to help you flow your tasks have been these BERT vectors,

01:05:23.590 --> 01:05:29.965
where BERT is Bidirectional Encoder Representations from Transformers.

01:05:29.965 --> 01:05:35.095
And so essentially, it's using the encoder from a transformer network.

01:05:35.095 --> 01:05:40.195
Uh, this deep multi-headed attention stack to calculate, um,

01:05:40.195 --> 01:05:43.615
a representation of a sentence and saying,

01:05:43.615 --> 01:05:49.750
"That's a great all-purpose representation of a sentence that you can use for tasks.

01:05:49.750 --> 01:05:54.054
Be it named entity recognition or SQuAD question answering."

01:05:54.054 --> 01:05:59.320
And so there's actually an interesting new idea that these people had.

01:05:59.320 --> 01:06:04.990
And that well, their idea was well standard language models are

01:06:04.990 --> 01:06:08.230
unidirectional and that's useful

01:06:08.230 --> 01:06:11.755
because it gives you a probability distribution of a language model.

01:06:11.755 --> 01:06:16.210
But it's bad because you'd like to be able to do

01:06:16.210 --> 01:06:21.190
prediction from both sides to understand word meaning and context.

01:06:21.190 --> 01:06:23.725
There's a second choice, um,

01:06:23.725 --> 01:06:29.185
which is you can kind of do bidirectional models when you incorporate,

01:06:29.185 --> 01:06:31.705
um, information in both ways.

01:06:31.705 --> 01:06:35.050
But that sort of has problems as well,

01:06:35.050 --> 01:06:37.480
because then you get crosstalk.

01:06:37.480 --> 01:06:40.615
Um, and so if you run a BiLSTM,

01:06:40.615 --> 01:06:43.090
and then you merge the representations by

01:06:43.090 --> 01:06:46.765
concatenation and then feed them into the next layer.

01:06:46.765 --> 01:06:48.655
When you're running the next layer,

01:06:48.655 --> 01:06:51.430
the forward LSTM will have already gotten

01:06:51.430 --> 01:06:54.385
information about the future from the first layer.

01:06:54.385 --> 01:06:56.545
Um, so it sort of, um,

01:06:56.545 --> 01:07:00.490
ends up with words that have already seen the future themselves.

01:07:00.490 --> 01:07:03.685
So you have this sort of complex non-generative model.

01:07:03.685 --> 01:07:08.005
Um, so somehow, they wanted to do things a bit differently,

01:07:08.005 --> 01:07:13.600
so they can have bidirectional context without words being able to see themselves.

01:07:13.600 --> 01:07:16.915
And the idea that they came up with is well,

01:07:16.915 --> 01:07:21.430
we're gonna train things with a transformer encoder.

01:07:21.430 --> 01:07:26.515
But what we're gonna do is mask out some of the words in the sentence,

01:07:26.515 --> 01:07:30.160
like, maybe we'll mask here store and gallon.

01:07:30.160 --> 01:07:34.180
And then, so our language mod- our language modelling like

01:07:34.180 --> 01:07:36.130
objective will no longer be

01:07:36.130 --> 01:07:40.090
a true language model that's sort of generating a probability of a sentence,

01:07:40.090 --> 01:07:43.705
um, which is standardly done by working from left to right,

01:07:43.705 --> 01:07:49.390
but it will instead be a Mad Libs style fill in the blank objective.

01:07:49.390 --> 01:07:52.120
So you'll see this context,

01:07:52.120 --> 01:07:53.800
which will be literally,

01:07:53.800 --> 01:07:56.965
"The man went to the mask to buy a mask of milk."

01:07:56.965 --> 01:08:00.790
And your, what's your training objective is to say,

01:08:00.790 --> 01:08:03.430
try and predict what this word is,

01:08:03.430 --> 01:08:08.035
which you can do with a cross entropy loss to the extent that you don't guess store.

01:08:08.035 --> 01:08:12.880
And then, it will be trying to guess what this word is and you want to let guess gallon.

01:08:12.880 --> 01:08:14.995
So you're training a model,

01:08:14.995 --> 01:08:17.920
um, to fill in these blanks.

01:08:17.920 --> 01:08:22.840
Um, and the rate at which they blank words is essentially one word in seven,

01:08:22.840 --> 01:08:25.225
and they discuss how this is a trade-off.

01:08:25.225 --> 01:08:28.540
Because if you blank too few words,

01:08:28.540 --> 01:08:30.700
it gets very expensive to train.

01:08:30.700 --> 01:08:32.590
And if you blank many words,

01:08:32.590 --> 01:08:35.545
well you've blanked out most of the context of a word,

01:08:35.545 --> 01:08:38.064
and that means it's not very useful for training,

01:08:38.064 --> 01:08:42.325
and they found about sort of one in seven seemed to work pretty well for them.

01:08:42.325 --> 01:08:46.585
But what they want to argue is, um,

01:08:46.585 --> 01:08:51.220
that for the OpenAI's GPT,

01:08:51.220 --> 01:08:53.470
which is also a transformer model.

01:08:53.470 --> 01:08:56.845
It's a sort of a classic language model working from

01:08:56.845 --> 01:09:00.700
left to right and so you only get left context.

01:09:00.700 --> 01:09:03.805
Um, for the BERT language model,

01:09:03.805 --> 01:09:07.285
sorry, the ELMo language model that's shown up at the top.

01:09:07.285 --> 01:09:11.680
Um, well, they're running a left to right language model and they're running,

01:09:11.680 --> 01:09:13.990
um, right to left language models.

01:09:13.990 --> 01:09:16.030
So in some sense, um,

01:09:16.030 --> 01:09:18.295
they have context from both sides.

01:09:18.295 --> 01:09:22.690
But these two language models are trained completely independently

01:09:22.690 --> 01:09:27.265
and then you're just sort of concatenating their representations, um, together.

01:09:27.265 --> 01:09:32.170
So there's no sense in which we're actually kind of having a model that's jointly

01:09:32.170 --> 01:09:37.930
using context from both sides at the time though that the pre-trained,

01:09:37.930 --> 01:09:40.930
um, contextual word representations are built.

01:09:40.930 --> 01:09:45.940
So their hope is using inside a transformer model

01:09:45.940 --> 01:09:47.980
this trick of blanking out words,

01:09:47.980 --> 01:09:53.290
and predicting it using the entire context will allow them to use two-sided context,

01:09:53.290 --> 01:09:55.540
and be much more effective.

01:09:55.540 --> 01:10:00.025
And that's what they seem to show, um.

01:10:00.025 --> 01:10:03.835
There's one other complication and,

01:10:03.835 --> 01:10:05.485
I mean, I'll show later.

01:10:05.485 --> 01:10:09.835
Um, this last complication is a bit useful,

01:10:09.835 --> 01:10:12.999
but it's sort of not really essential to their main idea,

01:10:12.999 --> 01:10:14.845
was that they thought,

01:10:14.845 --> 01:10:18.550
one of the, one of the goals in their head was clearly to be able to

01:10:18.550 --> 01:10:22.660
have this be useful for things like question answering,

01:10:22.660 --> 01:10:25.080
um, tasks, or, um,

01:10:25.080 --> 01:10:26.770
natural language inference tasks,

01:10:26.770 --> 01:10:30.640
and their relationships between, um, two sentences.

01:10:30.640 --> 01:10:32.260
So, their idea was, well,

01:10:32.260 --> 01:10:36.430
one good objective is this fill in the blank word objective which is,

01:10:36.430 --> 01:10:39.085
sort of, like language modeling objective.

01:10:39.085 --> 01:10:42.310
But they thought it would be useful to have a second objective

01:10:42.310 --> 01:10:45.925
where you're predicting relationships between sentences.

01:10:45.925 --> 01:10:51.415
So, they secondly have a loss function which is, um,

01:10:51.415 --> 01:10:54.670
let's have two sentences where

01:10:54.670 --> 01:10:58.359
the sentences might be two successive sentences in the text,

01:10:58.359 --> 01:11:02.650
or a sentence followed by a random sentence from somewhere else.

01:11:02.650 --> 01:11:06.475
And we want to train the system to predict when you've,

01:11:06.475 --> 01:11:10.930
seeing an- a correct next sentence versus a random sentence.

01:11:10.930 --> 01:11:16.330
And so you're also training a loss based on this next sentence prediction task.

01:11:16.330 --> 01:11:19.660
And so it'll be something like: The man went to the store.

01:11:19.660 --> 01:11:21.430
He bought a gallon of milk.

01:11:21.430 --> 01:11:24.610
You're meant to predict true is the next sentence,

01:11:24.610 --> 01:11:26.740
um: The man went to the store.

01:11:26.740 --> 01:11:28.090
Penguins are flightless.

01:11:28.090 --> 01:11:29.515
You're meant to say false.

01:11:29.515 --> 01:11:31.285
This isn't the next sentence.

01:11:31.285 --> 01:11:33.580
And so they're simultaneously also,

01:11:33.580 --> 01:11:36.325
um, training with this representation.

01:11:36.325 --> 01:11:40.345
So, what they end up looks, looks like this.

01:11:40.345 --> 01:11:44.245
Um, so, they have,

01:11:44.245 --> 01:11:45.490
um, for the input,

01:11:45.490 --> 01:11:47.170
they'll have a pair of sentences.

01:11:47.170 --> 01:11:48.700
My dog is cute.

01:11:48.700 --> 01:11:50.095
Um, separator.

01:11:50.095 --> 01:11:51.925
He likes playing.

01:11:51.925 --> 01:11:57.955
Um, the words are represented as word pieces like we talked about last week.

01:11:57.955 --> 01:12:01.570
Um, so there's a token embedding for each word piece.

01:12:01.570 --> 01:12:05.350
Um, then there's a positional embedding for

01:12:05.350 --> 01:12:09.535
each word piece which is gonna be summed with the token embedding.

01:12:09.535 --> 01:12:14.470
And then finally, there's a segment embedding for each word piece which is simply

01:12:14.470 --> 01:12:17.050
whether it comes from the first sentence or

01:12:17.050 --> 01:12:19.915
the second sentence before or after the separator.

01:12:19.915 --> 01:12:24.940
So, you're summing those three things together to get the token representations.

01:12:24.940 --> 01:12:28.914
And then you're going to use those in a transformer model

01:12:28.914 --> 01:12:33.835
where you will have losses to the extent that you can't predict the masked words.

01:12:33.835 --> 01:12:38.410
And then your binary prediction function as to whether there's

01:12:38.410 --> 01:12:43.525
a correct next sentence or not which is the training architecture.

01:12:43.525 --> 01:12:47.485
Okay. So, it's a transformer as before,

01:12:47.485 --> 01:12:50.740
it's trained on Wikipedia plus the BookCorpus.

01:12:50.740 --> 01:12:52.720
And they built two models.

01:12:52.720 --> 01:12:57.175
Um, the Base-BERT model was a twelve layer transformer.

01:12:57.175 --> 01:13:02.470
And so this corresponded to what the previous transformer paper had used, right?

01:13:02.470 --> 01:13:09.190
Those two layer transformer blocks repeated six times gave you 12 layers with 768 hidden,

01:13:09.190 --> 01:13:14.665
um, dimension hidden states and 12 heads for the multi-head attention.

01:13:14.665 --> 01:13:16.479
And then they went bigger,

01:13:16.479 --> 01:13:18.610
um, and trained BERT-Large which is,

01:13:18.610 --> 01:13:20.620
sort of, double the number of layers,

01:13:20.620 --> 01:13:23.485
bigger hidden states, even more attention heads.

01:13:23.485 --> 01:13:26.410
Um, and training these on,

01:13:26.410 --> 01:13:29.185
um, pods of TPUs.

01:13:29.185 --> 01:13:33.850
Um, so, first of all, you're training, um,

01:13:33.850 --> 01:13:38.260
on this basis for masked words and,

01:13:38.260 --> 01:13:40.375
um, next sentence or not.

01:13:40.375 --> 01:13:45.940
Um, so then what they wanted to say was this pre-trained model,

01:13:45.940 --> 01:13:51.685
um, evaluated on these losses and masked language model and next sentence prediction.

01:13:51.685 --> 01:13:54.925
Um, we could then take this model,

01:13:54.925 --> 01:13:59.050
fr- freeze most of its what weak. No, sorry, that's wrong.

01:13:59.050 --> 01:14:01.270
We could take this model, um,

01:14:01.270 --> 01:14:06.610
pre-trained and it would be incredibly useful for various different tasks.

01:14:06.610 --> 01:14:08.800
We could use it for named entity recognition,

01:14:08.800 --> 01:14:12.310
question answering, natural language inference et cetera.

01:14:12.310 --> 01:14:14.890
And the way we're going to do it, is kind of,

01:14:14.890 --> 01:14:18.550
doing the same thing as the ULMFit model did.

01:14:18.550 --> 01:14:20.755
We're not just going to say here's our,

01:14:20.755 --> 01:14:25.240
here's a contextual word representation like ELMo did.

01:14:25.240 --> 01:14:29.560
Instead, what we're gonna say is just keep on using this,

01:14:29.560 --> 01:14:32.230
keep on using this um,

01:14:32.230 --> 01:14:36.880
transformer network that we trained as a, sort of,

01:14:36.880 --> 01:14:42.535
language model, but fine tune it for a particular task.

01:14:42.535 --> 01:14:45.190
So, you're now going to run this transformer

01:14:45.190 --> 01:14:49.180
calculating representations for a particular task.

01:14:49.180 --> 01:14:55.990
And what we're going to change is we're going to remove the very top-level prediction.

01:14:55.990 --> 01:15:00.415
The bits that predict the mass language model and next sentence prediction.

01:15:00.415 --> 01:15:02.770
And we're going to substitute on it,

01:15:02.770 --> 01:15:08.080
on top, um, a final prediction layer that's appropriate for the task.

01:15:08.080 --> 01:15:11.005
So, if our task is SQuAD question answering,

01:15:11.005 --> 01:15:16.344
our final prediction layer will be predicting start of span and end of span,

01:15:16.344 --> 01:15:20.740
kind of, like when we saw DrQA a couple of weeks ago.

01:15:20.740 --> 01:15:23.979
If what we're doing is the NER task,

01:15:23.979 --> 01:15:26.889
our final prediction layer will be predicting

01:15:26.889 --> 01:15:33.895
the net- named entity recognition class of each token just like a standard NER system.

01:15:33.895 --> 01:15:42.775
Okay, um, and so they built this system and tested it on a whole bunch of data sets.

01:15:42.775 --> 01:15:45.610
Um, one of the main things they tested on was

01:15:45.610 --> 01:15:48.625
this GLUE data set which has a whole bunch of tasks.

01:15:48.625 --> 01:15:50.170
A lot of the tasks, they're,

01:15:50.170 --> 01:15:53.530
uh, natural language inference tasks.

01:15:53.530 --> 01:15:57.205
And I've kept saying that phrase all of this lecture but I haven't really defined it.

01:15:57.205 --> 01:16:00.820
So, with a natural language inference you're given two sentences

01:16:00.820 --> 01:16:05.935
like: Hills and mountains are especially sanctified in Jainism.

01:16:05.935 --> 01:16:09.550
And then you can write a hypothesis on: Jainism hates nature.

01:16:09.550 --> 01:16:11.530
And what you're meant to say is,

01:16:11.530 --> 01:16:13.570
whether the hypothesis, um,

01:16:13.570 --> 01:16:15.505
follows from the premise,

01:16:15.505 --> 01:16:19.240
contradicts the premise, or has no relation to the premise.

01:16:19.240 --> 01:16:21.265
So, that's a three-way classification.

01:16:21.265 --> 01:16:23.845
And so here it contradicts the premise.

01:16:23.845 --> 01:16:30.115
Um, there are various other tasks such as this linguistic acceptability task.

01:16:30.115 --> 01:16:33.550
Um, but if we look at these, um, GLUE tasks.

01:16:33.550 --> 01:16:37.735
Um, these are showing the Pre-OpenAI State Of The Art.

01:16:37.735 --> 01:16:40.735
How well, um, ELMo works.

01:16:40.735 --> 01:16:43.900
How well OpenAI GPT works,

01:16:43.900 --> 01:16:48.415
and then how well do small and large BERT models work.

01:16:48.415 --> 01:16:53.290
And effectively, what you're finding is,

01:16:53.290 --> 01:16:57.370
um, that the OpenAI GPT was so,

01:16:57.370 --> 01:16:58.495
you know, pretty good.

01:16:58.495 --> 01:17:02.455
It showed actually good advances on most of these tasks.

01:17:02.455 --> 01:17:05.890
For many, but not all of them that broke the previous state of the art,

01:17:05.890 --> 01:17:08.995
showing the power of these contextual language models.

01:17:08.995 --> 01:17:15.205
But the bidirectional form of BERT's prediction just seemed much better again.

01:17:15.205 --> 01:17:19.180
So, going from this line to this line you're getting depending on

01:17:19.180 --> 01:17:23.185
the task about two percent better performance.

01:17:23.185 --> 01:17:27.010
And so the BERT people actually did their experiments carefully.

01:17:27.010 --> 01:17:30.430
So, these models are pretty comparable in terms of size,

01:17:30.430 --> 01:17:33.775
but the bidirectional context seems to really help.

01:17:33.775 --> 01:17:35.470
And then what they found was,

01:17:35.470 --> 01:17:37.570
well, by going to just a bigger model,

01:17:37.570 --> 01:17:41.545
again, you could get another big lift in performance.

01:17:41.545 --> 01:17:44.740
And so you're getting for many of the tasks about

01:17:44.740 --> 01:17:48.145
another two percent lift in performance going into the bigger model.

01:17:48.145 --> 01:17:51.010
So, this really produced super-strong results.

01:17:51.010 --> 01:17:54.085
And in general, um, people have found,

01:17:54.085 --> 01:17:57.400
um, that BERT continues to give super strong results.

01:17:57.400 --> 01:18:01.480
So, if I return back to my ConLL NER task,

01:18:01.480 --> 01:18:05.260
we had ELMo giving you 92.2,

01:18:05.260 --> 01:18:06.640
um, and you, sort of,

01:18:06.640 --> 01:18:08.050
continue to get gains.

01:18:08.050 --> 01:18:13.900
So, BERT Base gets you to 92.4 and BERT Large takes you to 92.8.

01:18:13.900 --> 01:18:17.650
Though in, um, truth in, truth in description,

01:18:17.650 --> 01:18:23.125
there is now a system of beats BERT Large on NER which is actually a character-level,

01:18:23.125 --> 01:18:25.990
um, transformer language model from Flair.

01:18:25.990 --> 01:18:27.835
Um, but, you know,

01:18:27.835 --> 01:18:30.790
this continued over to a lot of other things.

01:18:30.790 --> 01:18:33.865
So, on SQuAD 1.1, um,

01:18:33.865 --> 01:18:36.370
BERT immediately just outperformed

01:18:36.370 --> 01:18:39.745
everything else that people have been working on for SQuAD for ages.

01:18:39.745 --> 01:18:42.610
In particular, what was especially dramatic, um,

01:18:42.610 --> 01:18:45.985
was the sing- a single BERT model, um,

01:18:45.985 --> 01:18:50.770
beat everything else that had been done previously on SQuAD version 1.1,

01:18:50.770 --> 01:18:53.575
even though they could also show that an

01:18:53.575 --> 01:18:59.815
ensemble of BERT models could give further good, um, performance gains.

01:18:59.815 --> 01:19:03.055
Um, and as I've mentioned before,

01:19:03.055 --> 01:19:05.980
essentially if you look at the SQuAD 2.0, um,

01:19:05.980 --> 01:19:08.935
leaderboard, all of the top ranked systems,

01:19:08.935 --> 01:19:12.280
um, are using BERT one place or another.

01:19:12.280 --> 01:19:14.590
Um, and so that,

01:19:14.590 --> 01:19:16.060
sort of, led into this,

01:19:16.060 --> 01:19:19.570
sort of, new world order, um, that, okay,

01:19:19.570 --> 01:19:22.735
it seems like the state of NLP now is to,

01:19:22.735 --> 01:19:25.240
if you want to have the best performance,

01:19:25.240 --> 01:19:26.410
you want to be using

01:19:26.410 --> 01:19:31.855
these deep pre-trained transformer stacks to get the best performance.

01:19:31.855 --> 01:19:33.220
And so this is, sort of, making,

01:19:33.220 --> 01:19:35.410
um, NLP more like vision.

01:19:35.410 --> 01:19:38.560
Because really vision for five years has had

01:19:38.560 --> 01:19:42.730
these deep pre-trained neural network stacks, um, like ResNets.

01:19:42.730 --> 01:19:47.124
Where for most vision tasks what you do is you take a pre-trained ResNet,

01:19:47.124 --> 01:19:49.870
and then you fine tune a layer at the top to

01:19:49.870 --> 01:19:52.870
do some classification tasks you're interested in.

01:19:52.870 --> 01:19:54.970
And this is, sort of, now, um,

01:19:54.970 --> 01:19:57.520
starting to be what's happening in NLP as well.

01:19:57.520 --> 01:20:00.280
That you can do the same thing by downloading

01:20:00.280 --> 01:20:05.875
your pre-trained BERT and fine tuning it to do some particular performance task.

01:20:05.875 --> 01:20:09.400
Okay, um, that's it for today and more on

01:20:09.400 --> 01:20:18.330
transformers on Thursday [NOISE].

