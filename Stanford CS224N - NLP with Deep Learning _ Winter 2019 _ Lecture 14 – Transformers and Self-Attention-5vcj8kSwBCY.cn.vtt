WEBVTT
Kind: captions
Language: en

00:00:04.520 --> 00:00:08.670
好。我很高兴向大家介绍，00:00:08.670 --> 00:00:11.355
我们的第一位受邀演讲者。00:00:11.355 --> 00:00:14.460
今天我们邀请了两位演讲者。00:00:14.460 --> 00:00:16.260
所以从，嗯，00:00:16.260 --> 00:00:18.960
我们去找Ashish Vaswani，她会是00:00:18.960 --> 00:00:23.115
关于生成性模型的自我注意，00:00:23.115 --> 00:00:25.530
我们会介绍一些工作00:00:25.530 --> 00:00:29.190
他和他的同事因《变形金刚》而闻名。00:00:29.190 --> 00:00:32.220
然后作为一种，00:00:32.220 --> 00:00:35.235
我们还会有一个特别的版本00:00:35.235 --> 00:00:39.440
黄安娜谈论了这项工作的一些应用。00:00:39.440 --> 00:00:41.540
实际上班上至少有几个人是这样的00:00:41.540 --> 00:00:43.790
实际上对音乐应用很感兴趣。00:00:43.790 --> 00:00:48.740
所以这将是你们在这门课上看到音乐在深度学习中的应用的一个机会。00:00:48.740 --> 00:00:51.890
好吧，我把它交给Ashish。00:00:51.890 --> 00:00:53.360
谢谢克里斯，谢谢埃维。00:00:53.360 --> 00:00:55.940
安娜来这是为了让课堂不那么无聊。00:00:55.940 --> 00:00:57.830
所以[笑声]她是这部电影的亮点。00:00:57.830 --> 00:01:00.845
大家好。00:01:00.845 --> 00:01:03.395
很高兴来到这里。00:01:03.395 --> 00:01:06.205
这是一个很大的班级。00:01:06.205 --> 00:01:07.920
第一位受邀演讲者00:01:07.920 --> 00:01:09.920
没有压力，希望一切顺利。00:01:09.920 --> 00:01:14.975
是的，所以今天的演讲将会是关于自我关注的。00:01:14.975 --> 00:01:18.345
目的是，00:01:18.345 --> 00:01:22.685
不是只讨论一个特定的模型，而是，00:01:22.685 --> 00:01:25.850
as, as, as经验主义者00:01:25.850 --> 00:01:27.830
我是一个经验主义者00:01:27.830 --> 00:01:30.380
使用机器学习将其应用于各种任务。00:01:30.380 --> 00:01:35.210
然后，然后，开始的时候总是要问这个问题，00:01:35.210 --> 00:01:36.800
它的结构是什么00:01:36.800 --> 00:01:38.720
我的数据集中有哪些对称性，00:01:38.720 --> 00:01:41.880
是否存在一个很好的模型，00:01:41.880 --> 00:01:46.010
这就对我的数据集中存在的这些属性进行了建模。00:01:46.010 --> 00:01:48.455
希望在这段时间里，00:01:48.455 --> 00:01:51.680
这次讲座安娜和我要说服你00:01:51.680 --> 00:01:54.410
自我关注确实有一些00:01:54.410 --> 00:01:56.165
能力模型和归纳偏见00:01:56.165 --> 00:01:58.685
可能对你关心的问题有用。00:01:58.685 --> 00:02:04.790
这次演讲主要是关于我们的学习表征，00:02:04.790 --> 00:02:07.445
可变长度数据，我们有图像，00:02:07.445 --> 00:02:10.190
大部分都是变长数据。00:02:10.190 --> 00:02:11.960
还有，还有，还有，00:02:11.960 --> 00:02:14.975
我们都关心这个问题，因为我们- in00:02:14.975 --> 00:02:17.990
深度学习，而深度学习就是表征性学习。00:02:17.990 --> 00:02:22.655
建立正确的工具来学习表征，00:02:22.655 --> 00:02:24.830
as是一个重要的因素，00:02:24.830 --> 00:02:26.600
取得经验上的成功。00:02:26.600 --> 00:02:29.810
现在，选择的模型，00:02:29.810 --> 00:02:32.165
主要的工作00:02:32.165 --> 00:02:35.600
也许直到现在，或者直到这一点，都是递归神经网络。00:02:35.600 --> 00:02:40.730
这里有多少人熟悉RNNs?00:02:40.730 --> 00:02:43.055
(笑)好。00:02:43.055 --> 00:02:45.260
所以到这一点，00:02:45.260 --> 00:02:47.690
最主要的工作是递归神经网络，00:02:47.690 --> 00:02:50.465
还有一些，一些，00:02:50.465 --> 00:02:54.540
一些门控变体显式地添加乘法交互，比如LSTMs，00:02:54.540 --> 00:02:58.175
它们也有更好的梯度转移机制。00:02:58.175 --> 00:03:00.620
最近的一些变种，比如门控，00:03:00.620 --> 00:03:02.300
简化的递归单位，00:03:02.300 --> 00:03:06.860
它们主宰着这片反复出现的景观。00:03:06.860 --> 00:03:09.605
一般来说，递归神经网络，00:03:09.605 --> 00:03:12.675
学习还是，嗯，创造表象?00:03:12.675 --> 00:03:15.765
它们消耗一个字符串或一个句子，00:03:15.765 --> 00:03:17.625
甚至是一个图像，想象一下，00:03:17.625 --> 00:03:21.135
在一个特定的顺序和，呃，在每一个，00:03:21.135 --> 00:03:22.530
在每个位置00:03:22.530 --> 00:03:24.900
每一步它们都会产生，00:03:24.900 --> 00:03:26.899
一个连续的表示00:03:26.899 --> 00:03:30.845
总结一下，他们所经历的一切。00:03:30.845 --> 00:03:36.960
现在，在，在，在，在，00:03:36.960 --> 00:03:39.480
在大数据领域，00:03:39.480 --> 00:03:41.310
平行模型是，00:03:41.310 --> 00:03:42.890
非常，非常有益。00:03:42.890 --> 00:03:45.140
事实上，我在读奥利弗・塞尔弗里奇的书。00:03:45.140 --> 00:03:46.370
他是个…00:03:46.370 --> 00:03:48.770
他是麻省理工学院的教授，00:03:48.770 --> 00:03:53.360
抱歉，他写了深网的前身叫做Pandemoniums。00:03:53.360 --> 00:03:54.590
我建议大家都读一读。00:03:54.590 --> 00:03:56.450
他有一个有趣的笔记，你知道，00:03:56.450 --> 00:03:57.960
如果你给我更多的并行计算，00:03:57.960 --> 00:03:59.900
我将添加更多的数据，让它变慢。00:03:59.900 --> 00:04:02.180
这样就可以消耗更多的数据。00:04:02.180 --> 00:04:07.260
还有递归，递归就是通过构造，00:04:07.260 --> 00:04:08.915
限制并行化，因为你必须，00:04:08.915 --> 00:04:11.105
你必须等待，直到你的等待un-00:04:11.105 --> 00:04:14.030
对于特定的时间点生成表示。00:04:14.030 --> 00:04:16.275
但是如果有任何问题，00:04:16.275 --> 00:04:17.325
请举手，我来00:04:17.325 --> 00:04:18.900
希望能四处看看，00:04:18.900 --> 00:04:21.230
我能回答你的问题。00:04:21.230 --> 00:04:24.920
再一次，现在因为我们实际上在生成这些表示，00:04:24.920 --> 00:04:26.150
我们在总结，00:04:26.150 --> 00:04:27.740
如果你想传递信息，00:04:27.740 --> 00:04:29.765
如果你想传递共同参考信息，00:04:29.765 --> 00:04:32.350
然后我们要把这些都塞进去00:04:32.350 --> 00:04:36.110
这个固定大小的向量，可能很难建模。00:04:36.110 --> 00:04:39.635
虽然他们在语言上很成功00:04:39.635 --> 00:04:42.530
他们没有明确的架构00:04:42.530 --> 00:04:45.890
并没有一个非常清晰明确的方法来为层次结构建模，00:04:45.890 --> 00:04:48.155
这在语言中非常重要。00:04:48.155 --> 00:04:54.390
嗯，现在，嗯，所以他们一直在，这是非常出色的工作，00:04:54.390 --> 00:04:58.640
一种自我注意的前兆，实际上克服了这些困难。00:04:58.640 --> 00:05:01.550
这些困难主要是卷积序列模型00:05:01.550 --> 00:05:05.180
这些接收域卷积是有限的，00:05:05.180 --> 00:05:07.220
再说一遍，这句话现在没有了，00:05:07.220 --> 00:05:09.590
不是顺序而是深度。00:05:09.590 --> 00:05:12.130
它们代表每一个00:05:12.130 --> 00:05:14.720
它们表示可变长度序列。00:05:14.720 --> 00:05:17.720
呃，呃，它们对00:05:17.720 --> 00:05:21.110
并行化，因为你可以在每个位置同时应用这些卷积。00:05:21.110 --> 00:05:23.030
每一层并行化都很简单。00:05:23.030 --> 00:05:26.165
串行依赖关系只存在于层数中。00:05:26.165 --> 00:05:28.235
你可以00:05:28.235 --> 00:05:29.960
你可以――你可以得到00:05:29.960 --> 00:05:32.750
这些局部依赖关系之所以有效，是因为单个应用程序00:05:32.750 --> 00:05:37.475
卷积可以消耗其局部接受域内的所有信息。00:05:37.475 --> 00:05:39.320
如果你想00:05:39.320 --> 00:05:42.170
这些远距离的互动00:05:42.170 --> 00:05:45.020
不需要经过线性的步骤，00:05:45.020 --> 00:05:46.060
你还是因为这些，00:05:46.060 --> 00:05:49.670
因为这些接受域是局部的你可能需要线性的东西00:05:49.670 --> 00:05:53.525
深度或者对数如果你做的是膨胀卷积。00:05:53.525 --> 00:05:56.030
所以仍然需要-需要的层数是00:05:56.030 --> 00:05:59.215
它仍然是一个关于字符串长度的函数。00:05:59.215 --> 00:06:01.070
但这是一个很大的进步00:06:01.070 --> 00:06:03.320
实际上推动了很多研究，比如WaveRNN，00:06:03.320 --> 00:06:05.225
是一种经典的成功故事吗00:06:05.225 --> 00:06:08.825
卷积-卷积序列模型甚至可以通过网络实现。00:06:08.825 --> 00:06:15.085
目前为止注意力是最重要的组成部分之一00:06:15.085 --> 00:06:16.810
基于内容的，00:06:16.810 --> 00:06:19.060
记忆检索机制。00:06:19.060 --> 00:06:23.560
它是基于内容的，因为你有你的解码器来处理所有这些内容，00:06:23.560 --> 00:06:26.625
那是你的编码器然后决定什么，00:06:26.625 --> 00:06:28.580
吸收什么样的信息基于多么相似00:06:28.580 --> 00:06:30.980
这个内容对应于内存中的每个位置。00:06:30.980 --> 00:06:33.440
所以这是一个非常关键的机制，00:06:33.440 --> 00:06:34.955
神经机器翻译。00:06:34.955 --> 00:06:36.950
所以现在我们的问题是，为什么，00:06:36.950 --> 00:06:40.460
为什么不直接用注意力来表现，00:06:40.460 --> 00:06:43.790
这是一个大致的框架，00:06:43.790 --> 00:06:46.445
这个表示机制看起来像，00:06:46.445 --> 00:06:49.635
只是重复一下注意力的本质。00:06:49.635 --> 00:06:52.360
现在想象你有――你想表示这个单词，00:06:52.360 --> 00:06:55.730
重新表示表示的单词，您希望构造它的新表示。00:06:55.730 --> 00:06:58.610
然后首先，你，你参加或者你，00:06:58.610 --> 00:07:00.710
你比较你自己，你比较你的内容，00:07:00.710 --> 00:07:02.765
一开始它可能只是一个单词的嵌入。00:07:02.765 --> 00:07:05.540
你用你所有的话来比较内容，00:07:05.540 --> 00:07:07.340
基于所有的嵌入，00:07:07.340 --> 00:07:09.905
基于这些兼容性或比较，00:07:09.905 --> 00:07:14.180
你产生，呃，你产生整个社区的加权组合，00:07:14.180 --> 00:07:16.180
基于这个加权组合，00:07:16.180 --> 00:07:17.870
你总结所有的信息。00:07:17.870 --> 00:07:20.150
所以这就像是，你在用特定的方式重新表达你自己00:07:20.150 --> 00:07:22.730
整个社区的加权组合。00:07:22.730 --> 00:07:23.930
这就是注意力的作用，00:07:23.930 --> 00:07:28.955
你可以添加前馈层来为你计算新特性。00:07:28.955 --> 00:07:34.700
第一部分是关于，00:07:34.700 --> 00:07:37.760
自我注意的一些特性实际上帮助我们生成文本，比如，00:07:37.760 --> 00:07:39.320
归纳偏见实际上是有用的，00:07:39.320 --> 00:07:40.950
我们的经验表明，00:07:40.950 --> 00:07:43.055
它们在文本生成中发挥着重要作用。00:07:43.055 --> 00:07:44.990
这是关于机器翻译的，00:07:44.990 --> 00:07:47.420
还有其他的工作我们稍后会讲到。00:07:47.420 --> 00:07:49.880
所以[噪音]现在有了这个，呃，00:07:49.880 --> 00:07:51.885
用这种，呃，00:07:51.885 --> 00:07:55.470
通过这种注意力机制，我们得到了一个恒定的路径长度。00:07:55.470 --> 00:07:58.000
所以所有的对或一个单词都可以00:07:58.000 --> 00:08:01.100
位置可以同时与任何位置、任何位置交互。00:08:01.100 --> 00:08:04.250
希望职位的数量不要太多。00:08:04.250 --> 00:08:06.410
注意只是因为，比如，00:08:06.410 --> 00:08:08.060
这是一个结构，你有一个softmax，00:08:08.060 --> 00:08:10.205
你有这些门控和乘法的相互作用。00:08:10.205 --> 00:08:12.680
再说一遍，我无法解释为什么，00:08:12.680 --> 00:08:14.195
但很有趣的是，00:08:14.195 --> 00:08:15.290
你们见过这些模型，00:08:15.290 --> 00:08:16.400
甚至，甚至，呃，00:08:16.400 --> 00:08:19.980
甚至Pixel PixelCNN00:08:19.980 --> 00:08:21.660
当它在建模图像时，00:08:21.660 --> 00:08:24.965
他们必须明确地在模型中添加这些乘法交互，00:08:24.965 --> 00:08:26.885
为了打败RNNs，00:08:26.885 --> 00:08:29.390
通过构造得到的注意，因为你，00:08:29.390 --> 00:08:33.035
你把注意力概率和你的活动相乘。00:08:33.035 --> 00:08:34.580
并行化很简单，为什么?00:08:34.580 --> 00:08:39.440
因为你只需要注意matmuls，尤其是我们论文中使用的变量，00:08:39.440 --> 00:08:40.865
在我们的工作中。00:08:40.865 --> 00:08:43.895
现在的问题是00:08:43.895 --> 00:08:49.160
卷积序列到卷积序列模型在，00:08:49.160 --> 00:08:52.325
在，在，在通用生成任务的文本。00:08:52.325 --> 00:08:54.830
我们真的能做同样的事吗?00:08:54.830 --> 00:08:58.580
注意力是我们学习表征的主要工具。00:08:58.580 --> 00:09:03.485
为了增加一些背景，00:09:03.485 --> 00:09:07.430
有一些最先进的变压器有很多00:09:07.430 --> 00:09:12.025
伟大的工作，使用自我注意主要为分类内。00:09:12.025 --> 00:09:15.290
有一些关于自我注意的研究，00:09:15.290 --> 00:09:16.610
比如递归神经网络。00:09:16.610 --> 00:09:19.370
也许离我们最近的是00:09:19.370 --> 00:09:20.915
是内存网络，00:09:20.915 --> 00:09:22.820
苏赫巴托市韦斯顿00:09:22.820 --> 00:09:25.715
他们实际上有一种复发性注意力，00:09:25.715 --> 00:09:27.290
但他们没有00:09:27.290 --> 00:09:30.705
但实际上，从经验上讲，00:09:30.705 --> 00:09:33.500
他们没有把它展示给条件模型，00:09:33.500 --> 00:09:37.370
翻译的机制是，00:09:37.370 --> 00:09:41.555
他们在每一步都使用固定的查询。00:09:41.555 --> 00:09:43.495
所以，它留下了一些值得期待的东西。00:09:43.495 --> 00:09:47.060
他们仍然有这个问题，它真的会起作用吗，00:09:47.060 --> 00:09:50.870
大规模机器翻译系统或大规模文本生成系统。00:09:50.870 --> 00:09:54.100
所以这是，嗯，00:09:54.100 --> 00:09:57.435
自我关注，我们的自我关注工作。00:09:57.435 --> 00:10:00.495
这是tran- the-，我们把它放在变压器模型中。00:10:00.495 --> 00:10:03.195
这看起来怎么样?00:10:03.195 --> 00:10:05.975
所以我们要用注意力优先级00:10:05.975 --> 00:10:09.395
主要注意计算输入的表示形式。00:10:09.395 --> 00:10:11.480
想象你在做英语到德语的翻译。00:10:11.480 --> 00:10:14.030
你有你自己的话，注意到，00:10:14.030 --> 00:10:16.610
注意是排列不变的。00:10:16.610 --> 00:10:19.220
所以你只需要改变位置的顺序。00:10:19.220 --> 00:10:20.910
你改变你的语序，00:10:20.910 --> 00:10:23.315
它不会影响实际的输出。00:10:23.315 --> 00:10:25.340
为了保持顺序，00:10:25.340 --> 00:10:26.985
我们添加位置表示。00:10:26.985 --> 00:10:29.710
我们在论文中尝试了两种，00:10:29.710 --> 00:10:33.185
这些奇妙的正弦曲线没有熵。00:10:33.185 --> 00:10:35.625
我们也用习得的表示00:10:35.625 --> 00:10:38.090
非常普通的香草味这两种都很好。00:10:38.090 --> 00:10:40.420
嗯，所以，00:10:40.420 --> 00:10:42.895
首先，编码器是这样的?00:10:42.895 --> 00:10:46.970
所以我们有一个自我注意层它只是重新计算表示，00:10:46.970 --> 00:10:50.090
对于每个同时使用注意力的位置，00:10:50.090 --> 00:10:51.545
然后我们有一个前馈层。00:10:51.545 --> 00:10:52.820
还有余量，00:10:52.820 --> 00:10:54.380
剩余连接，00:10:54.380 --> 00:10:56.600
我会让你们看一下这些残留的联系00:10:56.600 --> 00:10:59.090
可能是在每个人之间00:10:59.090 --> 00:11:02.990
每一层，和输入我们有一个跳跃连接，只添加激活。00:11:02.990 --> 00:11:05.330
然后这个元组，00:11:05.330 --> 00:11:08.135
自我注意和前馈层基本上是重复的。00:11:08.135 --> 00:11:10.215
现在，在解码器这边，00:11:10.215 --> 00:11:13.925
我们有一种标准的编码解码架构。00:11:13.925 --> 00:11:17.500
在解码器方面，我们使用自我注意模仿语言模型，00:11:17.500 --> 00:11:20.300
用自我注意来模仿语言模型的方法是强加00:11:20.300 --> 00:11:23.540
因果关系只是掩盖了你能看到的位置。00:11:23.540 --> 00:11:25.660
所以基本上,呃,00:11:25.660 --> 00:11:29.285
第一个立场是――不能向前看，向前看是违法的。00:11:29.285 --> 00:11:32.075
它可以看着自己因为我们移动了输入。00:11:32.075 --> 00:11:34.750
所以它不是复制。00:11:34.750 --> 00:11:37.240
令人惊讶的是，有了这些模型，00:11:37.240 --> 00:11:38.790
很容易复制，00:11:38.790 --> 00:11:41.585
早期通用电气更难，00:11:41.585 --> 00:11:43.355
使用循环模型进行复制。00:11:43.355 --> 00:11:44.860
但是现在，至少，你可以很好地复制，00:11:44.860 --> 00:11:46.850
总的来说，我认为这是一个积极的信号。00:11:46.850 --> 00:11:49.830
但现在在解码器那边00:11:49.830 --> 00:11:51.150
我们有，我们有00:11:51.150 --> 00:11:54.380
这个因果自我注意层之后是编码-解码注意，00:11:54.380 --> 00:11:56.180
我们真正关注的是00:11:56.180 --> 00:11:59.450
编码器的最后一层和前馈层，这是三倍的，00:11:59.450 --> 00:12:00.670
重复mul-几次，00:12:00.670 --> 00:12:02.945
最后我们得到了标准的交叉熵损失。00:12:02.945 --> 00:12:08.465
嗯，嗯，所以，嗯，有点，00:12:08.465 --> 00:12:10.655
凝视着，00:12:10.655 --> 00:12:12.740
在我们的部分，在自我的特殊变体00:12:12.740 --> 00:12:15.205
我们使用的注意力机制，00:12:15.205 --> 00:12:17.965
我们同时追求简单和快速。00:12:17.965 --> 00:12:21.635
那么，如何计算注意力呢?00:12:21.635 --> 00:12:24.470
假设你想重新表示e2的位置。00:12:24.470 --> 00:12:26.930
首先是线性的，00:12:26.930 --> 00:12:30.220
线性变换成，呃，一个查询，00:12:30.220 --> 00:12:32.150
然后进行线性变换00:12:32.150 --> 00:12:34.520
你周围的每一个位置00:12:34.520 --> 00:12:36.510
或者说输入点的每个位置因为这是，00:12:36.510 --> 00:12:37.805
编码器那边，00:12:37.805 --> 00:12:39.175
去，呃，钥匙。00:12:39.175 --> 00:12:41.680
这些线性变换可以被认为是特征，00:12:41.680 --> 00:12:43.100
稍后我会详细讲。00:12:43.100 --> 00:12:45.500
它基本上是一个双线性形式。00:12:45.500 --> 00:12:48.355
你把这些向量投影到一个点积所在的空间00:12:48.355 --> 00:12:51.655
一个好的点积是相似性的一个很好的代表。00:12:51.655 --> 00:12:53.200
好吧?现在，你有了你的logit，00:12:53.200 --> 00:12:55.810
你只要做一个so- softmax的计算机凸组合。00:12:55.810 --> 00:12:57.930
现在根据这个凸组合，00:12:57.930 --> 00:13:01.475
然后重新表示e2或in00:13:01.475 --> 00:13:05.380
所有这些位置的所有向量的凸组合的项。00:13:05.380 --> 00:13:08.105
在做凸组合之前，00:13:08.105 --> 00:13:10.505
我们再做一次线性变换来产生值。00:13:10.505 --> 00:13:13.940
然后我们做第二个线性变换00:13:13.940 --> 00:13:17.620
混合这些信息，并通过一个-通过一个前馈层传递它。00:13:17.620 --> 00:13:19.075
这是-嗯，00:13:19.075 --> 00:13:21.915
这些基本都可以表示出来00:13:21.915 --> 00:13:24.900
在二中二中二矩阵乘法中，00:13:24.900 --> 00:13:27.620
平方根因子是为了确保，00:13:27.620 --> 00:13:29.080
这些点积不会爆炸。00:13:29.080 --> 00:13:30.425
它只是一个比例因子。00:13:30.425 --> 00:13:32.145
还有，还有，还有，00:13:32.145 --> 00:13:33.605
为什么这个特别，为什么00:13:33.605 --> 00:13:35.735
这种机制有吸引力吗?嗯，就是太快了。00:13:35.735 --> 00:13:37.340
你可以在GPU上做得很快，00:13:37.340 --> 00:13:39.010
simul-你可以同时做00:13:39.010 --> 00:13:43.045
所有的位置只有两个matmuls和一个softmax。00:13:43.045 --> 00:13:45.320
在解码器方面，00:13:45.320 --> 00:13:46.640
完全一样，00:13:46.640 --> 00:13:54.585
除了我们通过在logits上加上10e - (- 10e9)来确定因果关系。00:13:54.585 --> 00:13:58.135
所以你得到这些位置的概率是零。00:13:58.135 --> 00:14:00.820
所以我们只是把因果关系，加上这些，00:14:00.820 --> 00:14:04.410
高度负的注意力值，在注意力日志上。00:14:04.410 --> 00:14:06.745
嗯，就是，就是一切00:14:06.745 --> 00:14:07.410
(笑声)00:14:07.410 --> 00:14:13.600
我觉得这是个问题。00:14:13.600 --> 00:14:18.455
所以，嗯，[笑声]好吧，所以注意力真的，嗯，注意力是廉价的。00:14:18.455 --> 00:14:20.890
因为这个变量00:14:20.890 --> 00:14:23.905
注意只涉及到两个矩阵的乘法，00:14:23.905 --> 00:14:26.510
它是序列长度的二次函数。00:14:26.510 --> 00:14:30.815
那么RNNs或卷积的计算轮廓是什么呢?00:14:30.815 --> 00:14:32.370
它们在维度上是二次的。00:14:32.370 --> 00:14:35.045
因为，基本上，你可以把卷积看成是平坦的00:14:35.045 --> 00:14:38.170
你的输入或者只是在它上面应用一个线性变换，对吧?00:14:38.170 --> 00:14:40.835
那么，这个什么时候会变得非常有吸引力呢?00:14:40.835 --> 00:14:44.140
当你的维度是，00:14:44.140 --> 00:14:46.635
比你的身长长得多。00:14:46.635 --> 00:14:48.455
这就是机器翻译的情况。00:14:48.455 --> 00:14:51.335
现在，我们将讨论当这不是真的时候，00:14:51.335 --> 00:14:54.280
我们必须-我们必须做-我们必须做其他的模型发展。00:14:54.280 --> 00:14:56.440
但是，但是00:14:56.440 --> 00:14:58.020
短序列或其中的序列00:14:58.020 --> 00:15:00.020
长度是这样的，当维数大于长度时，00:15:00.020 --> 00:15:02.890
注意是一个非常-具有非常好的计算剖面。00:15:02.890 --> 00:15:06.355
正如你所看到的，它比RNN快四倍。00:15:06.355 --> 00:15:08.920
嗯，嗯，而且，比00:15:08.920 --> 00:15:12.995
卷积模型中有一个类似滤波器的核，有三个。00:15:12.995 --> 00:15:19.320
所以，还有一个问题。00:15:19.320 --> 00:15:21.455
在语言方面，00:15:21.455 --> 00:15:22.595
通常，我们想知道，00:15:22.595 --> 00:15:23.900
谁对谁做了什么?00:15:23.900 --> 00:15:25.770
现在，假设你应用卷积滤波器。00:15:25.770 --> 00:15:26.825
因为实际上00:15:26.825 --> 00:15:30.445
基于相对距离的不同线性变换，00:15:30.445 --> 00:15:31.770
像这样，这个，这个，这个，00:15:31.770 --> 00:15:35.320
关于who的线性变换，关于概念，00:15:35.320 --> 00:15:37.690
我们可以学习这个概念，00:15:37.690 --> 00:15:40.355
从这个单词I的嵌入中找出不同的信息。00:15:40.355 --> 00:15:41.930
这个线性变换，00:15:41.930 --> 00:15:44.530
红色的线性变换可以提取不同的信息00:15:44.530 --> 00:15:47.765
从踢和蓝色线性变换可以看出不同，00:15:47.765 --> 00:15:49.445
不同于ball的信息。00:15:49.445 --> 00:15:53.230
现在，当你只有一个注意力层时，这很难做到。00:15:53.230 --> 00:15:55.330
因为它们只是一个凸组合00:15:55.330 --> 00:15:57.320
到处都有相同的线性变换。00:15:57.320 --> 00:16:00.215
你能得到的只是a-只是混合比例。00:16:00.215 --> 00:16:03.670
所以你不能从不同的地方找出不同的信息。00:16:03.670 --> 00:16:10.270
如果我们有一个关注层给谁呢?00:16:10.270 --> 00:16:13.595
你可以把注意力层想象成一个特征检测器，00:16:13.595 --> 00:16:15.040
比如，因为一个特殊的，00:16:15.040 --> 00:16:18.340
它可能尝试――它可能――因为它带有一个线性变换，00:16:18.340 --> 00:16:21.700
所以它把它们投射到一个空间里，这个空间开始关注语法，00:16:21.700 --> 00:16:24.485
或者是在这个空间中投射开始关心谁或什么。00:16:24.485 --> 00:16:28.940
然后我们可以有另一个注意力层或者注意力头，00:16:28.940 --> 00:16:31.490
做了什么，另一个-另一个注意力，00:16:31.490 --> 00:16:34.115
为了谁，为了谁，为了谁。00:16:34.115 --> 00:16:37.300
所有这些都可以并行完成，00:16:37.300 --> 00:16:39.210
这就是我们要做的。00:16:39.210 --> 00:16:41.255
为了效率，而不是实际00:16:41.255 --> 00:16:44.230
这些维度在一个大空间中运作，00:16:44.230 --> 00:16:46.990
我们只是，我们只是降低所有这些正面的维数00:16:46.990 --> 00:16:50.225
我们同时操作这些注意力层，在某种程度上填补了空白。00:16:50.225 --> 00:16:51.670
这是，呃，00:16:51.670 --> 00:16:53.660
也许，这里有个小测验。00:16:53.660 --> 00:16:56.400
我的意思是，你真的能00:16:56.400 --> 00:17:01.105
是正面的组合还是有一种构型，00:17:01.105 --> 00:17:04.260
实际上，准确地模拟卷积可能有更多的参数?00:17:04.260 --> 00:17:06.390
我想应该有一个简单的方法来证明00:17:06.390 --> 00:17:09.650
有更多的正面或者正面是位置的函数，00:17:09.650 --> 00:17:11.875
你可以模拟一个卷积，00:17:11.875 --> 00:17:13.375
但是，尽管有很多参数。00:17:13.375 --> 00:17:15.150
所以它可以-进去，进去，00:17:15.150 --> 00:17:17.140
在极限情况下，它可以模拟卷积。00:17:17.140 --> 00:17:21.280
我们还可以继续享受并行的好处，00:17:21.280 --> 00:17:23.045
但是我们确实增加了softmaxes的数量00:17:23.045 --> 00:17:24.820
因为每个头都有一个软最大值。00:17:24.820 --> 00:17:27.190
但是失败的次数没有改变，因为我们00:17:27.190 --> 00:17:30.010
实际上这些头不是在很大的维度上运作的，00:17:30.010 --> 00:17:32.220
他们在非常小的空间里工作。00:17:32.220 --> 00:17:35.105
当我们把这个应用到，00:17:35.105 --> 00:17:37.535
关于机器翻译，00:17:37.535 --> 00:17:40.295
我们能够戏剧性地，呃，戏剧性地超越，00:17:40.295 --> 00:17:43.640
之前的英德英法翻译成绩。00:17:43.640 --> 00:17:47.075
所以我们有一个非常标准的设置:32000个单词的词汇表，00:17:47.075 --> 00:17:50.315
文字编码，WMT14-，00:17:50.315 --> 00:17:52.540
WMT 2014是我们的测试集00:17:52.540 --> 00:17:53.970
2013年进行了开发设置。00:17:53.970 --> 00:17:59.120
而且，呃，其中一些结果甚至比我们之前的整体模型更强。00:17:59.120 --> 00:18:02.690
在英法两种语言中，00:18:02.690 --> 00:18:05.400
我们有一些――我们有一些非常有利的结果。00:18:05.400 --> 00:18:06.630
我们，我们是，00:18:06.630 --> 00:18:08.425
我们，我们，我们达到了最先进的水平。00:18:08.425 --> 00:18:11.260
现在，退一步说，00:18:11.260 --> 00:18:13.905
我不是说我们00:18:13.905 --> 00:18:17.105
我们得到的架构比LSTM具有更好的表达性。00:18:17.105 --> 00:18:18.600
我是说，有，有，有，00:18:18.600 --> 00:18:22.430
有一些定理说LSTMs可以对任何函数建模。00:18:22.880 --> 00:18:27.870
也许，我们所做的只是建立一个对SGD有利的架构。00:18:27.870 --> 00:18:30.895
因为随机梯度下降可以很好地训练这个结构，00:18:30.895 --> 00:18:33.400
因为梯度动力学和注意是非常简单的注意，00:18:33.400 --> 00:18:34.615
只是一个线性组合。00:18:34.615 --> 00:18:37.820
我想那是我00:18:37.820 --> 00:18:39.560
我认为这是有利的。00:18:39.560 --> 00:18:42.235
但希望随着我们继续00:18:42.235 --> 00:18:43.480
但是，嗯，我想，00:18:43.480 --> 00:18:44.860
我还想指出，00:18:44.860 --> 00:18:47.770
我们显式地建模，00:18:47.770 --> 00:18:49.440
所有路径连接，所有，所有，00:18:49.440 --> 00:18:54.065
所有的成对连接都有它的优点――一个非常清晰的模型，00:18:54.065 --> 00:18:56.575
非常明确的关系直接之间，任何两个词之间。00:18:56.575 --> 00:19:00.640
希望我们也能00:19:00.640 --> 00:19:02.260
表明还有其他的归纳偏见。00:19:02.260 --> 00:19:05.605
这不仅仅是建造更多的建筑，00:19:05.605 --> 00:19:08.720
这对SGD有很好的归纳偏差。00:19:08.720 --> 00:19:13.000
所以框架，我们的很多工作最初是用张量张量推出的。00:19:13.000 --> 00:19:15.985
也许将来随着JAX的到来，这种情况会有所改变。00:19:15.985 --> 00:19:18.790
还有一个来自亚马逊的叫做Sockeye的框架。00:19:18.790 --> 00:19:20.810
还有Fairseq，呃，se- the00:19:20.810 --> 00:19:23.640
卷积序列到序列工具箱，00:19:23.640 --> 00:19:26.290
我不确定它是否有转换器实现，00:19:26.290 --> 00:19:29.480
但是他们也有一些很好的序列对序列模型。00:19:29.480 --> 00:19:31.700
嗯,好吧。00:19:31.700 --> 00:19:32.845
残差的重要性。00:19:32.845 --> 00:19:37.875
所以，我们有这些残差连接，00:19:37.875 --> 00:19:41.795
所以我们有从这里到这里的剩余连接，00:19:41.795 --> 00:19:45.240
从这里到这里，就像在每一层之间，这很有趣。00:19:45.240 --> 00:19:47.895
我们要做的就是00:19:47.895 --> 00:19:51.025
将输入处的位置信息添加到模型中。00:19:51.025 --> 00:19:53.525
我们不注入，我们不注入00:19:53.525 --> 00:19:56.150
或者我们不会在每一层都注入位置信息。00:19:56.150 --> 00:20:02.185
所以，当我们切断这些残留的联系，盯着这些，00:20:02.185 --> 00:20:04.805
盯着这些注意力分布，这是中心，00:20:04.805 --> 00:20:07.545
中间的图是注意力的分布。00:20:07.545 --> 00:20:10.750
实际上，基本上，它不能选择这个对角线。00:20:10.750 --> 00:20:13.370
它应该有一个非常强的对角焦点。00:20:13.370 --> 00:20:15.330
结果是这些残差00:20:15.330 --> 00:20:18.155
我们把这个位置信息传送到每一层。00:20:18.155 --> 00:20:20.820
因为这些后续的层没有位置的概念，00:20:20.820 --> 00:20:22.900
他们发现很难真正参加。00:20:22.900 --> 00:20:25.875
这是编码器-解码器的注意力，通常是对角的。00:20:25.875 --> 00:20:27.380
然后我们说好吧00:20:27.380 --> 00:20:30.700
然后我们继续-继续切断残差，00:20:30.700 --> 00:20:32.950
但是我们在每一层都添加了位置信息。00:20:32.950 --> 00:20:34.835
我们输入了位置信息。00:20:34.835 --> 00:20:36.390
我们没有恢复精确度，00:20:36.390 --> 00:20:37.790
但我们确实得到了一些，00:20:37.790 --> 00:20:39.300
对角线上的焦点。00:20:39.300 --> 00:20:41.400
残差的作用更大，00:20:41.400 --> 00:20:44.160
将这个位置信息移动到模型中。00:20:44.160 --> 00:20:46.805
他们把这个位置信息输入模型。00:20:46.805 --> 00:20:49.070
嗯,好吧。00:20:49.070 --> 00:20:51.365
所以，现在我们看到，00:20:51.365 --> 00:20:52.445
你知道，能够，某种程度上，00:20:52.445 --> 00:20:53.895
模型既长又短，00:20:53.895 --> 00:20:56.510
短期的关系，呃，嘘，呃，长期的，00:20:56.510 --> 00:20:58.435
长期和短期的关系，00:20:58.435 --> 00:21:01.745
用注意力是有益的，对于文本的生成。00:21:01.745 --> 00:21:03.525
什么样的归纳，00:21:03.525 --> 00:21:06.780
归纳偏见实际上，呃，出现了，00:21:06.780 --> 00:21:10.860
什么样的现象出现在图像和一些东西，我们不断看到-不断00:21:10.860 --> 00:21:12.745
在图像和音乐中看到的就是这个概念00:21:12.745 --> 00:21:15.180
重复的结构彼此非常相似?00:21:15.180 --> 00:21:18.055
这些图案以不同的比例重复出现。00:21:18.055 --> 00:21:21.420
举个例子，这是另一个人造的但很漂亮的例子00:21:21.420 --> 00:21:24.900
自相似性就是梵高的这幅画的纹理，00:21:24.900 --> 00:21:26.410
这些小东西只是重复。00:21:26.410 --> 00:21:30.280
这些图像――这些不同的图像片段非常相似，00:21:30.280 --> 00:21:31.595
但是它们可能有不同的尺度。00:21:31.595 --> 00:21:32.950
在音乐方面，00:21:32.950 --> 00:21:34.615
这是一个重复的主题，00:21:34.615 --> 00:21:36.605
它可能有，它可能有，00:21:36.605 --> 00:21:40.115
di- different, like, span time between in, in, between it。00:21:40.115 --> 00:21:43.250
所以，嗯，所以，所以这个，00:21:43.250 --> 00:21:44.440
所以我们，我们，我们，00:21:44.440 --> 00:21:45.775
在这之后，我们尝试着，00:21:45.775 --> 00:21:49.720
问这个问题:自我关注能帮助我们对其他物体(比如图像)建模吗?00:21:49.720 --> 00:21:51.715
所以我们选择的道路是，00:21:51.715 --> 00:21:57.450
标准的自回归图像建模-或概率图像建模，而不是甘斯。00:21:57.450 --> 00:21:58.910
因为，嗯，第一，这很简单。00:21:58.910 --> 00:22:00.085
我们几乎有一个语言模型。00:22:00.085 --> 00:22:02.245
这就像图像上的语言建模。00:22:02.245 --> 00:22:03.910
还有最大限度的训练00:22:03.910 --> 00:22:04.930
很可能，它让你，00:22:04.930 --> 00:22:06.720
衡量，衡量你做得有多好，00:22:06.720 --> 00:22:08.780
呃，在你订满的电视机上。00:22:08.780 --> 00:22:10.840
它也给了你多样性，00:22:10.840 --> 00:22:12.785
所以你希望涵盖了所有可能的，00:22:12.785 --> 00:22:15.900
不同类型的图片，00:22:15.900 --> 00:22:17.290
这里是al-我们有00:22:17.290 --> 00:22:18.880
这也是一个优势00:22:18.880 --> 00:22:22.425
在使用PixelRNN和PixelCNN等递归模型方面做得很好，00:22:22.425 --> 00:22:26.330
我们得到了很好的压缩率。嗯- - - - - -00:22:26.330 --> 00:22:31.805
这里，00:22:31.805 --> 00:22:35.610
最初的论点是，00:22:35.610 --> 00:22:37.920
在图像中，因为你想要对称，00:22:37.920 --> 00:22:39.300
因为如果你有一张脸，00:22:39.300 --> 00:22:41.580
你想要一只耳朵和另一只耳朵相匹配。00:22:41.580 --> 00:22:43.575
如果你有一个很大的接受域，00:22:43.575 --> 00:22:47.130
你可以用更低的计算成本，00:22:47.130 --> 00:22:50.820
然后它应该是有益的-然后它应该是非常有益的，对于图像，00:22:50.820 --> 00:22:53.640
对于图像，你不需要像在里面那样有很多层00:22:53.640 --> 00:22:57.955
卷积来得到这些遥远像素之间的依赖关系。00:22:57.955 --> 00:23:00.670
所以看起来自我关注应该是，00:23:00.670 --> 00:23:03.745
什么是好的计算机制?00:23:03.745 --> 00:23:06.580
但这是一种，但实际上很有趣00:23:06.580 --> 00:23:09.700
它是如何建模的――自然地建模自相似性，00:23:09.700 --> 00:23:12.460
人们在图像生成中使用了自相似性，00:23:12.460 --> 00:23:15.760
Efros做了一个很酷的实验，00:23:15.760 --> 00:23:18.670
在训练集中，那些补丁是什么，00:23:18.670 --> 00:23:19.810
和我很像?00:23:19.810 --> 00:23:21.640
基于和我非常相似的补丁，00:23:21.640 --> 00:23:23.035
我要把这些信息填满。00:23:23.035 --> 00:23:25.495
这就像图像生成。00:23:25.495 --> 00:23:27.430
有一部经典的作品叫做00:23:27.430 --> 00:23:29.980
非局部的意思是图像去噪，00:23:29.980 --> 00:23:31.915
他们想要去噪，00:23:31.915 --> 00:23:34.450
这个补丁p，他们说，00:23:34.450 --> 00:23:38.290
基于我的图像中所有其他补丁之间的相似性，00:23:38.290 --> 00:23:41.125
我要计算一些基于内容相似性的函数，00:23:41.125 --> 00:23:43.525
基于相似性，我要提取信息。00:23:43.525 --> 00:23:46.645
利用图像非常自我相似这一事实。00:23:46.645 --> 00:23:50.440
而且，呃，呃，这也有点，00:23:50.440 --> 00:23:52.390
申请了最近的工作。00:23:52.390 --> 00:23:55.030
如果你用这个编码器的自我注意机制00:23:55.030 --> 00:23:57.175
把这些词的嵌入替换成补丁，00:23:57.175 --> 00:23:58.765
这就是它所做的。00:23:58.765 --> 00:24:01.330
计算基于内容的相似度00:24:01.330 --> 00:24:04.120
在这些元素之间然后基于基于内容的相似性，00:24:04.120 --> 00:24:07.510
它构造了一个凸组合，本质上把这些东西组合在一起。00:24:07.510 --> 00:24:09.385
所以它是，它是一个非常ni-它是，00:24:09.385 --> 00:24:11.545
很高兴看到，00:24:11.545 --> 00:24:13.975
这是非局部均值的一种可微方法。00:24:13.975 --> 00:24:22.030
然后，呃，我们采用了transformer架构并用像素替换单词。00:24:22.030 --> 00:24:26.005
嗯，有一些――有一些架构调整要做。00:24:26.005 --> 00:24:28.300
所以这是但是这是00:24:28.300 --> 00:24:31.090
基本上，它和原作非常相似，00:24:31.090 --> 00:24:34.360
这里的位置表示不是，00:24:34.360 --> 00:24:36.760
一维的，因为我们不是在处理序列，00:24:36.760 --> 00:24:38.350
我们有二维的位置表示。00:24:38.350 --> 00:24:40.365
嗯,好吧。00:24:40.365 --> 00:24:42.070
我之前说过，00:24:42.070 --> 00:24:45.775
注意是一个非常非常有利的计算剖面00:24:45.775 --> 00:24:49.270
如果长度-如果维数大于长度，00:24:49.270 --> 00:24:51.250
这绝对不是真的，00:24:51.250 --> 00:24:52.540
绝对不真实的图像。00:24:52.540 --> 00:24:56.170
因为即使是32×32的图像，00:24:56.170 --> 00:24:59.260
当你把它们压平，你-你把它们压平，你得到30-你得到30，00:24:59.260 --> 00:25:02.960
72个位置，这是你的标准CFIR图像。00:25:02.960 --> 00:25:06.400
简单的解决方案，00:25:06.400 --> 00:25:09.220
因为就像卷积，00:25:09.220 --> 00:25:11.140
得到卷积00:25:11.140 --> 00:25:13.345
在局部窗口得到平移等方差。00:25:13.345 --> 00:25:16.660
我们说,“好吧。让我们采用同样的策略。”00:25:16.660 --> 00:25:19.225
还有很多空间位置和图像。00:25:19.225 --> 00:25:24.265
但是现在，我们仍然有一个更好的计算概要。00:25:24.265 --> 00:25:27.340
如果你的接受域仍然小于你的维度，00:25:27.340 --> 00:25:29.485
你可以负担得起――实际上你仍然可以00:25:29.485 --> 00:25:34.600
远比标准卷积计算长得多因为，00:25:34.600 --> 00:25:37.615
因为长度是二次的。00:25:37.615 --> 00:25:40.390
所以只要我们不增加维数以外的长度，00:25:40.390 --> 00:25:42.415
我们仍然有一个良好的计算剖面。00:25:42.415 --> 00:25:44.440
所以我们的做法是，00:25:44.440 --> 00:25:46.195
实际上我们有，00:25:46.195 --> 00:25:47.950
两种栅格化。00:25:47.950 --> 00:25:52.210
我们有一个一维的栅格化你有一个单一的查询块，00:25:52.210 --> 00:25:54.790
呃，那是，呃，00:25:54.790 --> 00:25:58.660
然后它会进入一个更大的记忆块，00:25:58.660 --> 00:26:02.680
用这种栅格化的方式沿着-沿着行。00:26:02.680 --> 00:26:05.320
然后我们尝试了另一种栅格化，00:26:05.320 --> 00:26:07.780
降标准二维局域性，00:26:07.780 --> 00:26:10.360
我们实际制作图像的地方，00:26:10.360 --> 00:26:13.300
在每一个块中我们都有一个栅格化方案。00:26:13.300 --> 00:26:18.640
再一次，这些-图像变压器层非常相似。00:26:18.640 --> 00:26:21.280
我们有二维的位置表示00:26:21.280 --> 00:26:24.670
用查询――用相同的――用非常相似的注意机制。00:26:24.670 --> 00:26:27.220
我们试过了00:26:27.220 --> 00:26:30.550
超分辨率和无条件和条件图像生成。00:26:30.550 --> 00:26:34.074
这是-这是妮基・帕玛尔00:26:34.074 --> 00:26:37.255
我和一个合作者以及Brain的其他几个作者，00:26:37.255 --> 00:26:39.580
我们在ICML上展示过。00:26:39.580 --> 00:26:44.815
而且，我们能够比现有的模型获得更好的困惑。00:26:44.815 --> 00:26:47.650
pixel蜗牛实际上是另一个混合模型00:26:47.650 --> 00:26:50.695
复杂和自我关注都比我们做得好，00:26:50.695 --> 00:26:52.675
开，开，开，开，每维位。00:26:52.675 --> 00:26:54.310
所以我们测量困惑度，因为这些00:26:54.310 --> 00:26:56.560
概率-这些是概率模型。00:26:56.560 --> 00:26:58.675
这基本上是一个图像的语言模型，00:26:58.675 --> 00:27:01.120
它只是――还有你的――还有因式分解00:27:01.120 --> 00:27:03.580
你的语言模型取决于你如何栅格化。00:27:03.580 --> 00:27:05.620
在一维栅格化中，00:27:05.620 --> 00:27:07.000
首先是行，然后是列。00:27:07.000 --> 00:27:08.185
在二维栅格化中，00:27:08.185 --> 00:27:11.155
我们按区块划分，在每个区块内进行栅格化。00:27:11.155 --> 00:27:14.650
在ImageNet上，我们获得了更好的困惑，00:27:14.650 --> 00:27:18.775
是啊，我是说我们在GAN水平，对吧?00:27:18.775 --> 00:27:23.694
我的意思是这个奇怪的-这是-我认为概率自回归图像生成，00:27:23.694 --> 00:27:26.725
这个时候还没到甘斯。00:27:26.725 --> 00:27:31.090
在ICLR 2019年，Nal的一篇论文实际上使用了自我关注，00:27:31.090 --> 00:27:32.575
非常高质量的图像。00:27:32.575 --> 00:27:34.705
但是我们观察到，00:27:34.705 --> 00:27:36.670
我们得到的结构化对象相当好。00:27:36.670 --> 00:27:39.930
比如人们能认出第二行是什么吗?00:27:39.930 --> 00:27:43.770
汽车(重叠)00:27:43.770 --> 00:27:46.050
我听到――我说――几乎――几乎每个人都说汽车。00:27:46.050 --> 00:27:48.720
我不打算问谁说了别的，但是是的，它们是汽车。00:27:48.720 --> 00:27:53.350
是的。最后一行是另一种交通工具，00:27:53.350 --> 00:27:58.330
本质上讲，当结构化的jo- structured对象很容易捕获时。00:27:58.330 --> 00:28:01.165
就像青蛙，00:28:01.165 --> 00:28:04.150
你知道，那些伪装过的东西就变成了这种糊状物。00:28:04.150 --> 00:28:07.090
但是在超分辨率下，00:28:07.090 --> 00:28:08.650
超分辨率很有趣，因为00:28:08.650 --> 00:28:10.375
有很多条件反射的信息，对吧?00:28:10.375 --> 00:28:13.525
当你有很多条件反射的信息时，00:28:13.525 --> 00:28:15.520
那种可能的――你打破了――你，00:28:15.520 --> 00:28:17.905
你实际上锁定了很多模式。00:28:17.905 --> 00:28:20.020
所以在输出端只有几个选项。00:28:20.020 --> 00:28:22.390
我们的超分辨率结果更好。00:28:22.390 --> 00:28:26.770
我们能够得到比以前更好的面部定位和结构。00:28:26.770 --> 00:28:31.390
这些是不同温度下的样品00:28:31.390 --> 00:28:34.690
当我们用实际的人类评估者来量化它时，00:28:34.690 --> 00:28:36.160
我们-就像我们闪过一张图片然后说，00:28:36.160 --> 00:28:37.345
这是真的吗?这是假的吗?00:28:37.345 --> 00:28:38.630
我们能够，呃，00:28:38.630 --> 00:28:40.750
我们愚弄了四个人00:28:40.750 --> 00:28:43.285
时间比以前的结果更好的超分辨率。00:28:43.285 --> 00:28:46.990
这些结果就像，我想，00:28:46.990 --> 00:28:50.470
英伟达最新的GAN测试结果让我们看起来像个笑话。00:28:50.470 --> 00:28:51.715
但是，我的意思是，00:28:51.715 --> 00:28:53.050
我是说，我们比甘晚开始。00:28:53.050 --> 00:28:54.130
希望我们能赶上来。00:28:54.130 --> 00:28:57.250
但是，这里的重点是这对于图像来说是一个有趣的归纳偏见，00:28:57.250 --> 00:28:59.500
所以对图像有很自然的归纳偏见。00:28:59.500 --> 00:29:01.375
嗯，还有，还有，00:29:01.375 --> 00:29:05.650
我们有希望把它应用到分类和其他类似的任务中去。00:29:05.650 --> 00:29:07.450
有趣的是，00:29:07.450 --> 00:29:09.640
只是出于好奇和00:29:09.640 --> 00:29:12.745
问最大可能性有多好或最大可能性有多大。00:29:12.745 --> 00:29:16.180
首先，这个模型真的捕捉到了角色中一些有趣的结构吗?00:29:16.180 --> 00:29:17.650
第二，你有多样性吗?00:29:17.650 --> 00:29:19.540
最大可能性应该是多样性，00:29:19.540 --> 00:29:21.955
通过，通过，通过它所做的。00:29:21.955 --> 00:29:23.860
然后我们做了图像补全。00:29:23.860 --> 00:29:25.870
为什么是-为什么是图像补全，因为只要你00:29:25.870 --> 00:29:28.000
锁定目标真理的一半图像，00:29:28.000 --> 00:29:30.610
你实际上是在剔除很多可能的模式。00:29:30.610 --> 00:29:32.230
这样抽样就简单多了。00:29:32.230 --> 00:29:34.090
首先，00:29:34.090 --> 00:29:35.935
首先是我们提供给模型的东西。00:29:35.935 --> 00:29:38.785
右边的行，最右边的列，00:29:38.785 --> 00:29:41.155
是金子，我们可以得到不同的样本。00:29:41.155 --> 00:29:43.270
但真正有趣的是第三行。00:29:43.270 --> 00:29:46.180
所以最右边的一列是-最右边的一列是金子。00:29:46.180 --> 00:29:48.655
如果你看第三排，这匹马。00:29:48.655 --> 00:29:52.135
所以实际上有这样的一瞥或暗示，00:29:52.135 --> 00:29:55.045
但是这个模型在其中一些实验中产生了幻觉，00:29:55.045 --> 00:29:56.170
在这些照片中，00:29:56.170 --> 00:29:58.450
有趣的是，它至少捕捉到了00:29:58.450 --> 00:30:02.230
数据教会它捕捉世界的一些结构。00:30:02.230 --> 00:30:06.190
这只狗很可爱，我想这也表明，00:30:06.190 --> 00:30:07.480
这是整个物体，00:30:07.480 --> 00:30:10.660
这把椅子，模型完全拒绝想象。00:30:10.660 --> 00:30:12.835
所以有很多困难。00:30:12.835 --> 00:30:15.070
我想Anna会讲到00:30:15.070 --> 00:30:19.465
[噪音]另一种利用自我相似性的方法。00:30:19.465 --> 00:30:20.080
谢谢你！00:30:20.080 --> 00:30:31.600
(掌声)00:30:31.600 --> 00:30:34.060
谢谢Ashish的介绍。00:30:34.060 --> 00:30:37.105
图像中有很多自相似性。00:30:37.105 --> 00:30:39.460
音乐中也有很多自我相似性。00:30:39.460 --> 00:30:43.180
所以我们可以想象，transformer是一个很好的模型。00:30:43.180 --> 00:30:46.060
我们要展示一下00:30:46.060 --> 00:30:48.100
我们可以再加一点00:30:48.100 --> 00:30:50.350
对自我的关注，要多想想00:30:50.350 --> 00:30:54.220
关系信息以及它如何帮助音乐的产生。00:30:54.220 --> 00:30:57.160
[噪音]首先我想00:30:57.160 --> 00:31:01.225
澄清什么是我们现在正在处理的原始表示。00:31:01.225 --> 00:31:03.280
就像语言一样，00:31:03.280 --> 00:31:07.060
你可以想象有一段文字，有人在朗读一段文字，00:31:07.060 --> 00:31:09.430
所以他们加入了自己的语调，00:31:09.430 --> 00:31:12.385
然后你会听到从那个演讲中传出的声波。00:31:12.385 --> 00:31:16.100
所以对于音乐有一个va-非常类似的，00:31:16.100 --> 00:31:21.270
你说作曲家有想法，00:31:21.270 --> 00:31:23.355
记下分数然后00:31:23.355 --> 00:31:25.575
表演者表演，然后你就有了声音。00:31:25.575 --> 00:31:29.535
所以我们今天主要关注的是，00:31:29.535 --> 00:31:31.410
你可以想想比分，00:31:31.410 --> 00:31:34.075
呃，一场表演，00:31:34.075 --> 00:31:41.545
因为这是一种符号表示MIDI钢琴被使用，00:31:41.545 --> 00:31:44.065
呃，呃，专业的业余爱好者，00:31:44.065 --> 00:31:46.645
音乐家们正在用钢琴演奏。00:31:46.645 --> 00:31:47.890
我们有录音，00:31:47.890 --> 00:31:49.660
他们演奏的信息。00:31:49.660 --> 00:31:51.295
特别地，00:31:51.295 --> 00:31:55.810
在每一次se- step建模音乐作为这个序列，嗯，00:31:55.810 --> 00:31:58.720
过程，输出是什么，00:31:58.720 --> 00:32:00.145
把这张纸条打开，00:32:00.145 --> 00:32:01.960
把时钟拨快这么多，00:32:01.960 --> 00:32:03.220
然后把这张纸条关掉。00:32:03.220 --> 00:32:05.965
还有，呃，动力学信息，00:32:05.965 --> 00:32:07.660
所以当你打开纸条的时候，你首先要说，00:32:07.660 --> 00:32:09.985
有多吵啊。00:32:09.985 --> 00:32:13.090
所以传统上，模特00:32:13.090 --> 00:32:15.085
音乐是一种语言，00:32:15.085 --> 00:32:18.130
我们一直在用，呃，递归神经网络。00:32:18.130 --> 00:32:23.350
因为正如Ashish所介绍的，00:32:23.350 --> 00:32:25.510
需要进行很多压缩，00:32:25.510 --> 00:32:29.830
一个长序列必须嵌入到一个固定长度的向量中。00:32:29.830 --> 00:32:32.200
当，呃，00:32:32.200 --> 00:32:35.200
在音乐中，有重复，00:32:35.200 --> 00:32:37.150
嗯，在远处。00:32:37.150 --> 00:32:39.490
我先给你们看，00:32:39.490 --> 00:32:43.270
来自RNNs的样本，00:32:43.270 --> 00:32:46.450
从一个变压器，然后从一个音乐变压器00:32:46.450 --> 00:32:48.580
相对的关注和某种程度上让你听到00:32:48.580 --> 00:32:51.970
差异，然后我会讲到，00:32:51.970 --> 00:32:54.580
呃，什么是，什么是，呃，00:32:54.580 --> 00:32:58.660
我们需要在变压器模型的基础上做一些修改。00:32:58.660 --> 00:33:00.745
这里，00:33:00.745 --> 00:33:03.295
这个任务是一种图像完成任务。00:33:03.295 --> 00:33:08.335
所以我们给它一个初始的主题然后我们让模型继续。00:33:08.335 --> 00:33:10.660
这就是我们的主题。00:33:10.660 --> 00:33:16.225
(音乐)有多少人认识到这一点?00:33:16.225 --> 00:33:19.090
太棒了。好。(笑)是的,00:33:19.090 --> 00:33:20.380
这是，呃，00:33:20.380 --> 00:33:22.900
这是肖邦练习曲的片段。00:33:22.900 --> 00:33:24.910
我们会问，00:33:24.910 --> 00:33:26.680
RNN做一个延拓。00:33:26.680 --> 00:33:34.990
(噪音)00:33:34.990 --> 00:33:48.325
(音乐)00:33:48.325 --> 00:33:50.950
所以在这里，就像一开始，它试图重复它。00:33:50.950 --> 00:33:52.330
但是很快，00:33:52.330 --> 00:33:55.870
误入另一个不同的世界。00:33:55.870 --> 00:33:58.120
这是一个挑战，因为，00:33:58.120 --> 00:34:01.705
不能直接回顾过去发生的事00:34:01.705 --> 00:34:04.060
可以看到一个模糊的版本，00:34:04.060 --> 00:34:06.400
这个模糊的版本变得越来越模糊。00:34:06.400 --> 00:34:08.455
这就是变压器的作用。00:34:08.455 --> 00:34:10.990
所以细节是00:34:10.990 --> 00:34:14.455
这些模型的训练长度是你听到的长度的一半。00:34:14.455 --> 00:34:18.760
所以我们要求模型推广到它所训练的长度之外。00:34:18.760 --> 00:34:20.170
你可以看到对于这个变压器，00:34:20.170 --> 00:34:22.285
它还会恶化。00:34:22.285 --> 00:34:25.150
但它可以保持主题相当一致。00:34:25.150 --> 00:34:34.690
(音乐)好。你,你,00:34:34.690 --> 00:34:35.770
你懂的。00:34:35.770 --> 00:34:40.690
[笑声]最初，它能很好地重复这个动作。00:34:40.690 --> 00:34:42.400
所以它能很好地复制。00:34:42.400 --> 00:34:44.170
但在训练的长度之外，00:34:44.170 --> 00:34:47.440
它不知道如何处理较长的上下文。00:34:47.440 --> 00:34:48.880
你看到的是00:34:48.880 --> 00:34:51.325
最后一个来自音乐转换器。00:34:51.325 --> 00:34:53.350
我认为是这样的[噪音]关系信息。00:34:53.350 --> 00:34:56.470
你可以直观地看到它是多么的一致00:34:56.470 --> 00:34:59.940
重复这些[噪音]这些更大的，呃，弧。00:34:59.940 --> 00:35:21.190
(音乐)00:35:21.190 --> 00:35:23.815
是的。这就是音乐变形金刚。00:35:23.815 --> 00:35:27.070
所以在音乐方面，00:35:27.070 --> 00:35:30.415
我们说过的自我相似性，00:35:30.415 --> 00:35:31.765
所以我们看到，00:35:31.765 --> 00:35:32.950
这里的主旨是，00:35:32.950 --> 00:35:35.005
所以我们在模型中加入了一个主题，00:35:35.005 --> 00:35:36.445
这是一个样本，00:35:36.445 --> 00:35:37.870
模型中的无条件样本。00:35:37.870 --> 00:35:40.690
所以没有，呃，没有启动，00:35:40.690 --> 00:35:42.880
模型需要创造自己的主题，00:35:42.880 --> 00:35:45.115
从这里继续。00:35:45.115 --> 00:35:49.210
在这里，如果我们仔细观察并分析一下，00:35:49.210 --> 00:35:51.805
重复了很多遍00:35:51.805 --> 00:35:54.040
中间有空隙。00:35:54.040 --> 00:35:56.635
如果你观察自我注意结构，00:35:56.635 --> 00:35:58.870
我们确实看到了这个模型，00:35:58.870 --> 00:36:00.625
看一下相关部分。00:36:00.625 --> 00:36:04.075
即使不是在它之前。00:36:04.075 --> 00:36:05.500
所以，所以这里，呃，00:36:05.500 --> 00:36:09.970
我涂上阴影的是主题出现的地方。00:36:09.970 --> 00:36:11.830
你可以看到不同的颜色00:36:11.830 --> 00:36:14.710
有一个不同的注意力头，它们有点专注，00:36:14.710 --> 00:36:16.810
在那些灰色的部分。00:36:16.810 --> 00:36:19.750
[噪音]我会播放样本，我们也有00:36:19.750 --> 00:36:23.695
一种形象化的方式，让你看到音乐是pa，00:36:23.695 --> 00:36:28.930
是被演奏的，或者是它所关注的音符，因为它预测了那个音符。00:36:28.930 --> 00:36:31.150
这是白手起家的。00:36:31.150 --> 00:36:33.880
自我注意是，00:36:33.880 --> 00:36:37.270
从一个音符到另一个音符，或者从一个事件到另一个事件。00:36:37.270 --> 00:36:39.325
所以，这是相当低的水平。00:36:39.325 --> 00:36:40.975
所以当你看它的时候，它是，00:36:40.975 --> 00:36:42.655
这是ki-有点势不可挡。00:36:42.655 --> 00:36:44.350
它有多个头，00:36:44.350 --> 00:36:45.925
很多东西在动。00:36:45.925 --> 00:36:47.950
但也有一些结构性的时刻00:36:47.950 --> 00:36:50.275
在那里你会看到更多，00:36:50.275 --> 00:36:52.795
干净，有点，00:36:52.795 --> 00:36:55.270
呃，它要处理的部分。00:36:55.270 --> 00:37:52.390
(音乐)00:37:52.390 --> 00:37:53.710
VOkay。所以,嗯,00:37:53.710 --> 00:37:55.690
我们是怎么做到的?00:37:55.690 --> 00:37:59.440
所以从常规注意力机制开始，00:37:59.440 --> 00:38:02.695
我们知道它是过去历史的加权平均值。00:38:02.695 --> 00:38:04.690
好的是，00:38:04.690 --> 00:38:07.165
不管有多远，我们都可以直接进入。00:38:07.165 --> 00:38:08.845
如果我们知道，00:38:08.845 --> 00:38:10.870
出现了一些主题，00:38:10.870 --> 00:38:13.000
在这首歌的开头，00:38:13.000 --> 00:38:15.385
我们仍然能够基于，00:38:15.385 --> 00:38:17.080
事实是相似的东西，00:38:17.080 --> 00:38:19.240
能找回那些东西00:38:19.240 --> 00:38:22.915
但是，它也变成了，00:38:22.915 --> 00:38:25.030
所有的过去都变成了一袋文字，00:38:25.030 --> 00:38:27.310
就像没有结构，00:38:27.310 --> 00:38:28.570
呃，之前还是之后。00:38:28.570 --> 00:38:31.195
这就是Ashish提到的位置正弦曲线。00:38:31.195 --> 00:38:33.595
基本上是这样的00:38:33.595 --> 00:38:38.395
指数是指以不同速度运动的正弦曲线。00:38:38.395 --> 00:38:40.645
所以近处的位置，00:38:40.645 --> 00:38:42.160
非常相似的一种，00:38:42.160 --> 00:38:45.680
横截面成多个正弦曲线。00:38:46.320 --> 00:38:48.805
相比之下，00:38:48.805 --> 00:38:50.920
对于卷积，你有这个，00:38:50.920 --> 00:38:54.940
固定的过滤器，它四处移动，捕捉相对距离。00:38:54.940 --> 00:38:56.875
像1 b4 2 b4。00:38:56.875 --> 00:38:59.185
这些是，呃，00:38:59.185 --> 00:39:02.935
在某种程度上，就像一个刚性结构，00:39:02.935 --> 00:39:04.925
一种，呃，带进来，00:39:04.925 --> 00:39:07.440
距离信息非常明确。00:39:07.440 --> 00:39:10.770
你可以想象相对注意力，00:39:10.770 --> 00:39:13.080
有好几个脑袋在玩00:39:13.080 --> 00:39:15.395
是这些的组合。00:39:15.395 --> 00:39:17.170
一方面，00:39:17.170 --> 00:39:18.580
你可以进入00:39:18.580 --> 00:39:20.485
历史非常直接。00:39:20.485 --> 00:39:22.510
另一方面，你也知道，00:39:22.510 --> 00:39:25.210
你如何看待这段历史?00:39:25.210 --> 00:39:26.860
比如说捕捉，00:39:26.860 --> 00:39:29.575
比如平移不变性，00:39:29.575 --> 00:39:32.440
比如说，00:39:32.440 --> 00:39:35.455
我们认为其中一个原因是，00:39:35.455 --> 00:39:38.830
启动你听说的样本00:39:38.830 --> 00:39:40.945
音乐变压器能够产生00:39:40.945 --> 00:39:43.735
超出了以非常连贯的方式训练的长度，00:39:43.735 --> 00:39:47.830
它能够依赖于平移不变性，00:39:47.830 --> 00:39:50.785
关系信息转发。00:39:50.785 --> 00:39:55.000
所以，如果我们仔细看看，00:39:55.000 --> 00:39:56.545
这是怎么回事00:39:56.545 --> 00:39:58.540
普通的变压器，00:39:58.540 --> 00:40:00.250
你比较所有的查询和键，00:40:00.250 --> 00:40:02.260
就得到了这个方阵。00:40:02.260 --> 00:40:04.390
你可以把它看作是自我相似性，00:40:04.390 --> 00:40:06.010
矩阵，所以它是一个正方形。00:40:06.010 --> 00:40:08.890
相对注意力的作用是，00:40:08.890 --> 00:40:12.355
加上一个额外的术语，00:40:12.355 --> 00:40:14.530
每当你比较两件事的时候，00:40:14.530 --> 00:40:16.210
你们相距多远?00:40:16.210 --> 00:40:18.820
同样基于内容，我，00:40:18.820 --> 00:40:21.340
我关心的是两步之外的东西吗00:40:21.340 --> 00:40:24.175
三步之遥或者我关心的是不断重复的事情，00:40:24.175 --> 00:40:26.275
以一种周期性的距离。00:40:26.275 --> 00:40:29.305
有了这些信息00:40:29.305 --> 00:40:33.835
这影响了，位置之间的相似性。00:40:33.835 --> 00:40:35.815
尤其是，呃，00:40:35.815 --> 00:40:39.460
这个额外的项是基于距离的。00:40:39.460 --> 00:40:40.510
所以你想00:40:40.510 --> 00:40:41.950
收集嵌入物，00:40:41.950 --> 00:40:44.500
这和00:40:44.500 --> 00:40:46.180
查询键距离，00:40:46.180 --> 00:40:49.285
呃，关于日志上的[噪音]。00:40:49.285 --> 00:40:51.715
所以，在翻译中，00:40:51.715 --> 00:40:53.230
已经证明了00:40:53.230 --> 00:40:55.015
有很多改进，00:40:55.015 --> 00:40:57.730
比如英语到德语的翻译。00:40:57.730 --> 00:41:00.010
但是在翻译中，00:41:00.010 --> 00:41:01.765
序列通常很短。00:41:01.765 --> 00:41:03.415
这只是一句接着一句。00:41:03.415 --> 00:41:05.110
比如说翻译，00:41:05.110 --> 00:41:07.210
大概50到100个单词。00:41:07.210 --> 00:41:12.010
但是你听到的音乐，呃，样本是在2000个时间步长的范围内。00:41:12.010 --> 00:41:16.015
所以大概需要2000个令牌才能放入内存。00:41:16.015 --> 00:41:17.500
所以这是个问题00:41:17.500 --> 00:41:23.350
因为最初的公式依赖于建立这个三维张量，00:41:23.350 --> 00:41:25.795
内存很大。00:41:25.795 --> 00:41:27.715
为什么会这样呢?00:41:27.715 --> 00:41:30.055
因为每一双鞋，00:41:30.055 --> 00:41:32.710
你查一下00:41:32.710 --> 00:41:35.200
你可以计算相对距离是多少，00:41:35.200 --> 00:41:38.320
然后你查找一个对应于这个距离的嵌入。00:41:38.320 --> 00:41:43.540
就像这样，有一个长度乘长度的矩阵，比如L乘L矩阵。00:41:43.540 --> 00:41:44.815
你需要，呃，00:41:44.815 --> 00:41:47.635
收集每个位置的嵌入值，00:41:47.635 --> 00:41:51.070
深度d，得到3D。00:41:51.070 --> 00:41:52.900
我们意识到，00:41:52.900 --> 00:41:58.480
你可以直接将查询和嵌入距离相乘。00:41:58.480 --> 00:42:00.670
[噪音]他们，呃，00:42:00.670 --> 00:42:02.080
以不同的顺序出现，00:42:02.080 --> 00:42:04.630
因为现在查询按相对距离排序，00:42:04.630 --> 00:42:07.930
但你需要按键排序的查询00:42:07.930 --> 00:42:11.440
这是一个绝对对绝对的构型。00:42:11.440 --> 00:42:13.360
所以我们能做的就是00:42:13.360 --> 00:42:16.705
做一系列的倾斜，00:42:16.705 --> 00:42:20.515
把它变成正确的构型。00:42:20.515 --> 00:42:23.875
这是，嗯，是的。00:42:23.875 --> 00:42:25.570
只是一个简单的对比，00:42:25.570 --> 00:42:28.480
来展示记忆需求的不同。00:42:28.480 --> 00:42:31.705
所以很多时候挑战在于00:42:31.705 --> 00:42:33.820
能够伸缩，呃，你知道，00:42:33.820 --> 00:42:37.660
能够提高内存效率，以便[噪音]您可以建模更长的序列。00:42:37.660 --> 00:42:40.420
有了这些，呃，这是，00:42:40.420 --> 00:42:42.850
如果我们有时间，我可以再给你们举一个例子。00:42:42.850 --> 00:42:45.125
但是如果我们没有时间，我们可以，继续。00:42:45.125 --> 00:42:46.175
我们会看到更多。00:42:46.175 --> 00:42:47.980
好。[笑声]这是，00:42:47.980 --> 00:42:49.930
这可能是一个00:42:49.930 --> 00:42:54.480
大约一分钟的样品，希望你们喜欢。00:42:54.480 --> 00:44:06.385
谢谢。(音乐)00:44:06.385 --> 00:44:07.720
谢谢你的聆听。00:44:07.720 --> 00:44:18.610
(掌声)。00:44:18.610 --> 00:44:23.830
(笑声)谢谢你,安娜。嗯,嗯,很好。00:44:23.830 --> 00:44:26.930
所以为了分类，00:44:26.970 --> 00:44:31.615
所以相对注意力是一种强大的机制，00:44:31.615 --> 00:44:35.185
这是一种非常强大的音乐机制。00:44:35.185 --> 00:44:37.285
它也有助于机器翻译。00:44:37.285 --> 00:44:39.355
一个非常有趣的例子，00:44:39.355 --> 00:44:41.545
结果是，呃，呃，00:44:41.545 --> 00:44:44.395
相对注意力的一个有趣结果是，00:44:44.395 --> 00:44:46.030
呃，图像，是这样的吗，00:44:46.030 --> 00:44:48.370
就像卷积一样，00:44:48.370 --> 00:44:50.740
卷积实现平移等方差。00:44:50.740 --> 00:44:51.970
如果你有，00:44:51.970 --> 00:44:54.640
假设，你有这个，00:44:54.640 --> 00:44:58.345
这个红点或者说这个特征点，00:44:58.345 --> 00:45:01.465
这并不取决于狗的图像在图像中的位置，00:45:01.465 --> 00:45:04.720
在大图上。它不依赖于它的绝对位置。00:45:04.720 --> 00:45:07.000
它会产生相同的激活。00:45:07.000 --> 00:45:10.915
卷积有平移等方差。00:45:10.915 --> 00:45:13.135
现在，和亲戚们，00:45:13.135 --> 00:45:15.220
位置或相对注意力00:45:15.220 --> 00:45:18.550
你得到完全一样的效果，因为你没有任何-一旦你只是00:45:18.550 --> 00:45:22.495
去除绝对位置的概念，你是在给模型注入[噪音]，00:45:22.495 --> 00:45:24.280
一旦你把它拿掉00:45:24.280 --> 00:45:26.470
然后你的注意力计算，00:45:26.470 --> 00:45:28.840
因为它实际上包括，我的意思是，00:45:28.840 --> 00:45:32.215
妮琪和我还有其他人00:45:32.215 --> 00:45:34.870
安娜实际上是在处理图像，而且似乎00:45:34.870 --> 00:45:37.480
而且看起来效果更好。00:45:37.480 --> 00:45:42.040
这个动作，这个满足这个，00:45:42.040 --> 00:45:44.440
呃，呃，我是说，00:45:44.440 --> 00:45:47.470
它可以实现平移等方差，这是图像的一个重要特性。00:45:47.470 --> 00:45:49.300
所以有很多-看起来可能是这样00:45:49.300 --> 00:45:51.250
如果你想推动一个有趣的方向，00:45:51.250 --> 00:45:55.090
自我监督学习的图像中的自我注意。00:45:55.090 --> 00:45:59.785
我想，关于自我监督学习基因生成模型的工作，00:45:59.785 --> 00:46:01.450
我之前讲过，00:46:01.450 --> 00:46:05.320
仅仅有图像的概率模型，我的意思是，00:46:05.320 --> 00:46:06.895
我想最好的图像模型是I，00:46:06.895 --> 00:46:09.580
我去谷歌搜索，我拿起一张图片给你，00:46:09.580 --> 00:46:12.325
但我想图像的生成模型是有用的，00:46:12.325 --> 00:46:14.470
如果你想做一些像半决赛这样的比赛，00:46:14.470 --> 00:46:16.810
在自我监督学习中，你只需要预先训练一个模型00:46:16.810 --> 00:46:19.375
很多未标记的数据，然后你传输它。00:46:19.375 --> 00:46:22.765
所以希望这能有所帮助这将成为这个机制的一部分。00:46:22.765 --> 00:46:26.890
另一个有趣的是，00:46:26.890 --> 00:46:30.520
另一个有趣的行业结构，相对注意力可以让你建模，00:46:30.520 --> 00:46:31.960
是一种图表。00:46:31.960 --> 00:46:33.520
想象一下，00:46:33.520 --> 00:46:36.265
你有这个相似度图，这些红色的边，00:46:36.265 --> 00:46:37.600
公司的概念，00:46:37.600 --> 00:46:40.180
蓝色的边是水果的概念，00:46:40.180 --> 00:46:44.500
苹果有这两种形式。00:46:44.500 --> 00:46:47.140
你可以想象一下00:46:47.140 --> 00:46:50.650
相对注意力只是建模，只是能够建模，00:46:50.650 --> 00:46:52.285
或者能够――你，你，00:46:52.285 --> 00:46:56.170
你自己能够强加这些不同的相似性概念，00:46:56.170 --> 00:46:58.375
在不同的元素之间。00:46:58.375 --> 00:47:00.715
如果你有图形问题，00:47:00.715 --> 00:47:03.925
那么相对的自我关注可能很适合你。00:47:03.925 --> 00:47:08.530
还有，还有一份simi，这是巴塔利亚等人的观点论文00:47:08.530 --> 00:47:13.930
《心灵深处》讲的是相对注意力以及如何在图表中使用。00:47:13.930 --> 00:47:15.580
所以当我们在图表上的时候，00:47:15.580 --> 00:47:18.685
我只是想――也许联系起来会很有趣，00:47:18.685 --> 00:47:21.490
呃，呃，一些，呃，00:47:21.490 --> 00:47:22.810
做得很好00:47:22.810 --> 00:47:25.030
在图上称为消息传递神经网络。00:47:25.030 --> 00:47:27.265
这很有趣，如果你看，00:47:27.265 --> 00:47:30.730
如果你看消息传递函数，00:47:30.730 --> 00:47:34.480
它的意思是你实际上只是在节点对之间传递消息。00:47:34.480 --> 00:47:37.090
所以你可以把自我关注看作是一种完全的连接00:47:37.090 --> 00:47:39.970
就像一个bipe，一个完整的二分图，00:47:39.970 --> 00:47:42.250
你在传递信息00:47:42.250 --> 00:47:43.750
您正在节点之间传递消息。00:47:43.750 --> 00:47:46.540
信息传递，信息传递神经网络就是这样做的。00:47:46.540 --> 00:47:49.420
它们还在节点之间传递消息。它们有什么不同呢?00:47:49.420 --> 00:47:51.580
从数学上讲，00:47:51.580 --> 00:47:53.965
他们只是在传递信息时有所不同，00:47:53.965 --> 00:47:57.370
强迫信息位于成对节点之间00:47:57.370 --> 00:48:00.790
但是因为Softmax函数，你可以得到所有节点之间的相互作用，00:48:00.790 --> 00:48:03.175
自我关注就像一种信息传递机制，00:48:03.175 --> 00:48:05.470
所有节点之间的交互。00:48:05.470 --> 00:48:07.315
所以，他们，他们说，00:48:07.315 --> 00:48:08.800
它们在数学上并不遥远，00:48:08.800 --> 00:48:11.320
同时也介绍了me- the Message Passing Paper00:48:11.320 --> 00:48:14.620
一个有趣的概念叫做多塔，类似于多头注意力，00:48:14.620 --> 00:48:16.585
那是诺曼发明的00:48:16.585 --> 00:48:21.160
就像你并行运行k个传递神经网络信息的副本。00:48:21.160 --> 00:48:23.590
所以现有的有很多相似之处，00:48:23.590 --> 00:48:27.805
这些连接到之前存在的工作，但这些连接是后来才出现的。00:48:27.805 --> 00:48:31.930
我们有一个图库，我们把它们连接起来，00:48:31.930 --> 00:48:34.150
这两股信息都在传递00:48:34.150 --> 00:48:37.495
我们把它写成张量张量。00:48:37.495 --> 00:48:40.705
总结一下，00:48:40.705 --> 00:48:43.510
自我关注能够帮助我们00:48:43.510 --> 00:48:46.240
美国模型是任意两点之间的恒定路径长度，00:48:46.240 --> 00:48:47.870
任何两个位置，00:48:47.870 --> 00:48:49.600
它被证明是非常有用的，00:48:49.600 --> 00:48:52.165
在序列建模中。00:48:52.165 --> 00:48:56.200
拥有无界内存的优势在于无需将信息压缩到有限的空间中，00:48:56.200 --> 00:48:58.360
在有限的a中，00:48:58.360 --> 00:48:59.575
在固定的空间里，00:48:59.575 --> 00:49:03.625
在我们的例子中，我们的记忆基本上是随着序列增长的，00:49:03.625 --> 00:49:07.180
它可以帮助你计算，并行化很简单。00:49:07.180 --> 00:49:09.110
你可以，你可以处理很多数据，00:49:09.110 --> 00:49:12.040
如果你想要大数据集，这很有用。00:49:12.040 --> 00:49:14.275
我们发现它可以对自相似性进行建模。00:49:14.275 --> 00:49:16.330
这似乎是件很自然的事00:49:16.330 --> 00:49:20.350
这是一个非常自然的现象，如果你在处理图像或音乐。00:49:20.350 --> 00:49:23.200
同时，相对注意力让你，给你这个额外的维度00:49:23.200 --> 00:49:26.080
能够模仿有表现力的时间和音乐，00:49:26.080 --> 00:49:27.925
这个平移等方差，00:49:27.925 --> 00:49:30.475
它很自然地延伸到图表。00:49:30.475 --> 00:49:37.030
到目前为止，我讲的这部分或所有内容都是关于并行训练的。00:49:37.030 --> 00:49:41.905
现在有一个非常活跃的研究领域使用自我注意模型，00:49:41.905 --> 00:49:43.975
对于较少的自回归生成。00:49:43.975 --> 00:49:45.790
注意a-代入时，00:49:45.790 --> 00:49:47.575
注意解码器掩码是因果关系，00:49:47.575 --> 00:49:48.670
我们无法展望未来。00:49:48.670 --> 00:49:51.190
所以当我们生成的时候00:49:51.190 --> 00:49:54.250
在目标端按顺序从左到右生成。00:49:54.250 --> 00:49:56.845
嗯，所以，嗯，还有，还有，00:49:56.845 --> 00:49:59.170
而且，为什么，为什么一代人很难?00:49:59.170 --> 00:50:00.670
因为你的输出是多模态的。00:50:00.670 --> 00:50:02.845
如果你想把英语翻译成德语，00:50:02.845 --> 00:50:04.285
有很多方法，00:50:04.285 --> 00:50:08.410
你要翻译的第二个单词取决于第一个单词。00:50:08.410 --> 00:50:11.605
例如，如果你，如果你预测的第一个词是danke，00:50:11.605 --> 00:50:13.675
这将改变你预测的第二个单词。00:50:13.675 --> 00:50:15.670
如果你只是独立地预测它们，00:50:15.670 --> 00:50:17.620
然后你可以想象你可以有各种各样的00:50:17.620 --> 00:50:20.185
这些排列将是不正确的。00:50:20.185 --> 00:50:22.690
我们打破模式的方法是00:50:22.690 --> 00:50:24.940
我们所做的决定只是连续的生成。00:50:24.940 --> 00:50:27.700
一旦我们对一个词做出决定，00:50:27.700 --> 00:50:30.490
这就确定了下一个你要预测的单词。00:50:30.490 --> 00:50:34.210
这是一个活跃的研究领域，00:50:34.210 --> 00:50:36.700
你可以对这些论文进行分类00:50:36.700 --> 00:50:41.740
非自走变压器的快速-第三篇论文，快速解码。00:50:41.740 --> 00:50:43.870
第四篇论文是为了更好的理解00:50:43.870 --> 00:50:46.000
在所有矢量量化的自动编码器中，00:50:46.000 --> 00:50:49.255
他们实际上是在一个潜在的空间里做决定，00:50:49.255 --> 00:50:53.470
这是，呃，这是e-要么是通过单词对齐来学习，00:50:53.470 --> 00:50:56.860
受精，或者是用自动编码器来学习。00:50:56.860 --> 00:50:59.680
所以你在潜在空间里做决定，00:50:59.680 --> 00:51:02.275
一旦你在潜在空间里做了决定，00:51:02.275 --> 00:51:04.030
假设所有的输出，00:51:04.030 --> 00:51:05.725
实际上是条件独立的，00:51:05.725 --> 00:51:07.180
既然你已经做了这些决定。00:51:07.180 --> 00:51:08.485
这就是它们加速的方式。00:51:08.485 --> 00:51:10.600
还有，还有一篇论文。00:51:10.600 --> 00:51:11.860
第二个是a00:51:11.860 --> 00:51:14.020
迭代求精的论文。00:51:14.020 --> 00:51:17.665
还有一篇由米切尔・斯特恩(Mitchell Stern)撰写的分块并行解码论文，00:51:17.665 --> 00:51:20.215
Noam Shazeer和Jakob Uszkoreit00:51:20.215 --> 00:51:23.440
他们实际上只是运行多个模型，比如，00:51:23.440 --> 00:51:29.440
然后用一个更快的模型和分数重新编码，00:51:29.440 --> 00:51:31.405
使用更贵的型号。00:51:31.405 --> 00:51:33.580
这就是它加速的方式。00:51:33.580 --> 00:51:38.350
嗯，[噪音]转移学习已经有-自我关注在转移中是有益的00:51:38.350 --> 00:51:42.820
OpenAI中的GPT和BERT中的GPT是两个经典的例子。00:51:42.820 --> 00:51:44.995
实际上已经有一些工作在做，扩大规模，00:51:44.995 --> 00:51:48.085
比如添加一个因子作为高效的优化器。00:51:48.085 --> 00:51:52.120
Rohan Anil和Yoram Singer最近发表了一篇论文。00:51:52.120 --> 00:51:54.445
还有网状张力流，00:51:54.445 --> 00:51:57.850
他们到底培养了哪些模特00:51:57.850 --> 00:52:02.530
只比最初的模型大几个数量级。00:52:02.530 --> 00:52:05.710
所以，我的意思是，当你在处理这个大数据系统时，你可能会想要00:52:05.710 --> 00:52:07.720
记住很多――你想记住的00:52:07.720 --> 00:52:10.270
你的参数里面有很多东西用来训练一个更大的模型。00:52:10.270 --> 00:52:12.415
Mesh-Tensorflow可以让你这么做。00:52:12.415 --> 00:52:16.300
有很多有趣的工作，通用变压器，00:52:16.300 --> 00:52:19.240
递归神经网络可以很好地计数。00:52:19.240 --> 00:52:21.910
这是施米德胡伯的一些可爱的论文他展示了这些00:52:21.910 --> 00:52:25.480
循环的神经，计数-细胞机制只是学会了一个很好的计数器，00:52:25.480 --> 00:52:27.340
你可以学习a ^ n，00:52:27.340 --> 00:52:29.230
b的n次方，和LSTM。00:52:29.230 --> 00:52:31.735
那么，呃，通用变压器00:52:31.735 --> 00:52:34.660
带回到变压器内部深处的递归式。00:52:34.660 --> 00:52:37.195
有一篇很酷的维基百科论文，00:52:37.195 --> 00:52:40.900
同时用图像变压器的纸张，也使用局部注意事项。00:52:40.900 --> 00:52:46.060
Transformer-XL论文将递归和自我关注结合起来，00:52:46.060 --> 00:52:47.290
所以他们会大块大块地集中注意力，00:52:47.290 --> 00:52:50.275
但是他们用递归来总结历史，这有点可爱。00:52:50.275 --> 00:52:52.135
它曾在演讲中使用过，但我不知道是否有过00:52:52.135 --> 00:52:55.315
演讲中自我关注的成功案例。00:52:55.315 --> 00:52:58.350
同样，类似的问题也有很大的，00:52:58.350 --> 00:53:00.900
作为位置，00:53:00.900 --> 00:53:03.165
重新集中注意力。00:53:03.165 --> 00:53:08.050
自我监督是一种如果有效的话00:53:08.050 --> 00:53:09.640
这将是非常有益的。00:53:09.640 --> 00:53:12.910
我们不需要大标签数据集，理解传输，00:53:12.910 --> 00:53:15.490
转会正变得非常成功――正变得越来越成功00:53:15.490 --> 00:53:18.960
伯特和其他一些模型在NLP中的应用。00:53:18.960 --> 00:53:21.630
所以理解这些，实际发生的是a-00:53:21.630 --> 00:53:24.555
对我和一对夫妇来说，这是一个正在进行的有趣研究领域。00:53:24.555 --> 00:53:29.505
我的一些合作者，还有多任务学习和克服这些，00:53:29.505 --> 00:53:32.860
这个关于自我关注的二次问题是00:53:32.860 --> 00:53:38.150
an interesting area of research that I- that I'd like to pursue. Thank you.

