WEBVTT
Kind: captions
Language: en

00:00:05.480 --> 00:00:07.890
可以。大家好。

00:00:07.890 --> 00:00:10.365
让我们重新开始吧。

00:00:10.365 --> 00:00:17.100
好吧，首先让我说说作业5。

00:00:17.100 --> 00:00:20.715
任务5今天就要出来了。

00:00:20.715 --> 00:00:22.580
嗯，这是一个全新的任务，

00:00:22.580 --> 00:00:25.080
所以你们这些家伙就是为了这个。

00:00:25.080 --> 00:00:31.365
嗯，那么它将是什么，它基本上建立在作业4上。

00:00:31.365 --> 00:00:34.560
嗯，所以如果你在作业4上做得不好没关系，

00:00:34.560 --> 00:00:36.405
但我认为事实上大多数人都这样做了。

00:00:36.405 --> 00:00:39.990
嗯，我们要做的是增加，嗯，

00:00:39.990 --> 00:00:42.290
卷积神经网络与子字

00:00:42.290 --> 00:00:45.590
神经机器翻译系统的建模

00:00:45.590 --> 00:00:47.305
努力让它变得更好。

00:00:47.305 --> 00:00:52.850
嗯，所以这项任务是编码重，书面问题轻。

00:00:52.850 --> 00:00:56.360
嗯，我是说你的编码

00:00:56.360 --> 00:01:00.410
实际上，要做到这一点并不比作业4难，

00:01:00.410 --> 00:01:02.585
这有点像作业4。

00:01:02.585 --> 00:01:08.710
但我们希望这次你能自己完成。

00:01:08.710 --> 00:01:10.725
我的意思是，

00:01:10.725 --> 00:01:12.480
嗯，是作业4。

00:01:12.480 --> 00:01:16.640
好吧，有很多脚手架告诉你应该做什么，

00:01:16.640 --> 00:01:20.270
所有这些自动分级机检查，你可以继续工作

00:01:20.270 --> 00:01:24.980
你的密码，直到他们通过了所有的签名支票，所有人都通过了。

00:01:24.980 --> 00:01:29.270
嗯，我们可以说，这是一种溺爱。

00:01:29.270 --> 00:01:31.625
嗯，但我是说，

00:01:31.625 --> 00:01:37.160
我想我们真正想要的是有一个更对不起的问题。

00:01:37.160 --> 00:01:38.390
[听不见]？

00:01:38.390 --> 00:01:43.510
对.所以我们希望这是，呃，有用的。

00:01:43.510 --> 00:01:48.380
嗯，这会是短期的痛苦，但有助于

00:01:48.380 --> 00:01:51.350
更有效的斜坡

00:01:51.350 --> 00:01:55.400
最后一个项目，甚至你的余生，对吧。

00:01:55.400 --> 00:01:58.220
事实是在你的余生里，

00:01:58.220 --> 00:02:01.820
如果你想做一些有深度学习的事情，

00:02:01.820 --> 00:02:03.920
你得弄清楚

00:02:03.920 --> 00:02:06.740
要建立的模型和要缝合的片段，

00:02:06.740 --> 00:02:11.090
以及如何编写一些测试来看看它是否在做一些明智的事情。

00:02:11.090 --> 00:02:13.775
如果它没有做一些明智的事情，嗯，

00:02:13.775 --> 00:02:17.509
为了弄清楚你如何改变事情并尝试不同的事情，

00:02:17.509 --> 00:02:19.355
让它理智地工作。

00:02:19.355 --> 00:02:21.155
所以这就是我们希望的，

00:02:21.155 --> 00:02:22.730
嗯，那些人，嗯，

00:02:22.730 --> 00:02:24.650
可以在任务5中完成，

00:02:24.650 --> 00:02:25.850
所以你必须，

00:02:25.850 --> 00:02:28.090
嗯，想想办法。

00:02:28.090 --> 00:02:31.790
嗯，应该编写自己的测试代码。

00:02:31.790 --> 00:02:34.490
嗯，我们没有公开签名，

00:02:34.490 --> 00:02:38.510
所以你应该-这是你自己做健康检查的一部分，

00:02:38.510 --> 00:02:44.150
试着做我上周说的事情

00:02:44.150 --> 00:02:47.420
简单的钻头工作，确认它们在工作

00:02:47.420 --> 00:02:51.620
记录大量的测试数据等，并更加明智地进行操作。

00:02:51.620 --> 00:02:53.999
尤其是，

00:02:55.490 --> 00:02:59.100
嗯，其中一个特别的部分，

00:02:59.100 --> 00:03:02.640
我们计划要做的，嗯，为了，

00:03:02.640 --> 00:03:06.015
嗯，这次任务，我在找，

00:03:06.015 --> 00:03:07.920
嗯，但在下一张幻灯片上。

00:03:07.920 --> 00:03:11.620
嗯，所以，嗯，为了这次任务以及以后，嗯，

00:03:11.620 --> 00:03:16.260
我们将像在CS107中那样执行规则，

00:03:16.260 --> 00:03:18.130
对于你们这些本科生来说，

00:03:18.130 --> 00:03:22.835
这意味着助教不会帮你检查和调试代码。

00:03:22.835 --> 00:03:24.860
嗯，所以，你知道，

00:03:24.860 --> 00:03:29.930
当然，我们还是希望助教能帮上忙，带着你的问题来找他们，嗯，

00:03:29.930 --> 00:03:33.514
谈谈你应该如何使用不同的东西，

00:03:33.514 --> 00:03:35.420
嗯，在Pythoch图书馆，

00:03:35.420 --> 00:03:41.300
嗯，但你不应该把它当作助教的工作，这里有一个大的python文件。

00:03:41.300 --> 00:03:43.580
嗯，你能告诉我怎么了吗？

00:03:43.580 --> 00:03:45.830
帮你修好。

00:03:45.830 --> 00:03:49.485
可以。嗯，确切的政策是，

00:03:49.485 --> 00:03:51.870
嗯，写在广场上。

00:03:51.870 --> 00:03:58.130
可以。所以在那之后-有什么问题吗？还是我直接进去？

00:03:59.580 --> 00:04:05.145
可以。嗯，是的，今天的讲座，

00:04:05.145 --> 00:04:10.650
嗯，从某种意义上说，今天的讲座很简单。

00:04:10.650 --> 00:04:13.350
嗯，上次的讲座，

00:04:13.350 --> 00:04:16.420
真的有很多新东西

00:04:16.420 --> 00:04:19.975
神经网络上你以前没见过的东西，

00:04:19.975 --> 00:04:23.410
我们做了convnets和pooling层，

00:04:23.410 --> 00:04:26.260
我们做了高速公路和残余连接，

00:04:26.260 --> 00:04:29.970
批处理规范，我不知道，不管我们做了什么。

00:04:29.970 --> 00:04:32.525
嗯，我猜是一个卷积。

00:04:32.525 --> 00:04:36.110
所以有很多新东西真的

00:04:36.110 --> 00:04:39.580
在这堂课中，我们讲的是神经网络机器，

00:04:39.580 --> 00:04:41.300
根本没有什么新东西。

00:04:41.300 --> 00:04:42.500
所以这很容易。

00:04:42.500 --> 00:04:48.875
嗯，这确实是一个新的讲座，但它是有原因的。

00:04:48.875 --> 00:04:53.570
这和我说的一句话有关

00:04:53.570 --> 00:04:58.700
上一次关于神经网络领域中有多少东西在不断变化。

00:04:58.700 --> 00:05:01.850
所以在我们第一次设计这门课的时候，

00:05:01.850 --> 00:05:05.090
它的许多结构仍然是这样的。

00:05:05.090 --> 00:05:10.830
嗯，在2014-2015年我们设计这个课程的时候，

00:05:10.830 --> 00:05:13.175
基本上是公理化的

00:05:13.175 --> 00:05:18.185
所有自然语言处理的深度学习模型都是基于单词。

00:05:18.185 --> 00:05:22.400
因此，我们从单词向量开始是完全有意义的，

00:05:22.400 --> 00:05:26.170
然后我们开始研究像重复模型之类的东西。

00:05:26.170 --> 00:05:31.580
鉴于这件事是在过去大约三年内发生的，

00:05:31.580 --> 00:05:36.455
有很多新的工作，包括一些最有影响力的新工作。

00:05:36.455 --> 00:05:40.960
有一些语言模型不是-不是-不是-不是，嗯，

00:05:40.960 --> 00:05:46.895
建立在他们正在建立的单词之上，建立在单词或字符的片段之上。

00:05:46.895 --> 00:05:49.640
所以这场演讲是为了给你

00:05:49.640 --> 00:05:52.985
对这些其他方式的理解，

00:05:52.985 --> 00:05:56.900
嗯，对正在发生的事情有一些定位。

00:05:56.900 --> 00:05:59.930
但是我们正在看的实际型号，呃，

00:05:59.930 --> 00:06:03.920
有点像使用我们已经看到的所有构建块，

00:06:03.920 --> 00:06:08.030
嗯，RNN和Convnets之类的。

00:06:08.030 --> 00:06:09.950
所以让我们来讨论这个问题。

00:06:09.950 --> 00:06:13.130
嗯，所以我要从一点点开始

00:06:13.130 --> 00:06:17.105
学习语言结构的语言学，嗯，

00:06:17.105 --> 00:06:22.520
首先是较低层次的语言单位，然后我们会看到它是如何发展的，

00:06:22.520 --> 00:06:25.345
嗯，对于角色级别的模型。

00:06:25.345 --> 00:06:27.440
所以在语言学方面，

00:06:27.440 --> 00:06:30.980
如果你从图腾柱的底部开始，

00:06:30.980 --> 00:06:34.099
语言学的第一个层次是语音学，

00:06:34.099 --> 00:06:38.420
这就是理解人类语言的声音和生理学。

00:06:38.420 --> 00:06:40.860
这有点像物理学或生理学，

00:06:40.860 --> 00:06:44.355
或者什么东西，对吧，有嘴的部分在动，

00:06:44.355 --> 00:06:46.529
耳部起过滤作用，

00:06:46.529 --> 00:06:50.030
还有，嗯，在它们之间有音频波。

00:06:50.030 --> 00:06:53.270
所以在某种意义上这是不争的。

00:06:53.270 --> 00:06:55.420
嗯，但高于这个水平，

00:06:55.420 --> 00:07:00.460
人们分析人类语言的标准做法是，

00:07:00.460 --> 00:07:06.230
人类语言似乎使用了相对较小的

00:07:06.230 --> 00:07:12.320
独特的单位，通常称为音素，实际上是分类的。

00:07:12.320 --> 00:07:16.480
这里的想法是，

00:07:16.480 --> 00:07:19.890
呃，我们的嘴是连续的，对吧。

00:07:19.890 --> 00:07:23.030
他们嘴巴里有各种各样的东西，你知道，

00:07:23.030 --> 00:07:24.890
舌头和咽喉等，

00:07:24.890 --> 00:07:27.020
但这是一个连续的空间。

00:07:27.020 --> 00:07:31.610
所以实际上，嗯，我们可以制造出无限多种声音，对吧。

00:07:31.610 --> 00:07:37.830
所以，如果我张开嘴，发出声音，然后摆动舌头，我就可以走了。

00:07:37.830 --> 00:07:41.570
我能发出各种各样的声音。

00:07:41.570 --> 00:07:46.990
但事实是人类的语言不是这样的，

00:07:46.990 --> 00:07:49.915
从各种各样的声音中，

00:07:49.915 --> 00:07:53.230
我们能分辨出一小部分声音。

00:07:53.230 --> 00:07:58.570
嗯，当语言改变时发生的事情是，

00:07:58.570 --> 00:08:01.535
声音的空间

00:08:01.535 --> 00:08:05.365
语言变化中的重要和显著。

00:08:05.365 --> 00:08:10.570
这种情况甚至发生在一种语言中，如英语。

00:08:10.570 --> 00:08:13.255
我将举一个例子。

00:08:13.255 --> 00:08:20.630
嗯，所以在认知心理学中人们谈论范畴知觉的现象。

00:08:20.630 --> 00:08:24.535
这意味着确实有一些连续的东西，

00:08:24.535 --> 00:08:30.410
但人类认为它属于相当尖锐的范畴。

00:08:30.410 --> 00:08:31.720
嗯，而且，你知道，

00:08:31.720 --> 00:08:34.345
你可以用它来，你知道，

00:08:34.345 --> 00:08:38.350
衣着风格或某人是否算胖。

00:08:38.350 --> 00:08:43.745
嗯，但是最著名的分类知觉的例子是语言，

00:08:43.745 --> 00:08:46.930
在那里我们可以发出各种各样的声音，但是

00:08:46.930 --> 00:08:50.435
每个人都将他们视为类别。

00:08:50.435 --> 00:08:55.775
有效地说，这意味着当你有明确的认知，

00:08:55.775 --> 00:09:00.650
一个类别内的差异有点缩小了。

00:09:00.650 --> 00:09:03.700
你几乎没有注意到它们的差异

00:09:03.700 --> 00:09:07.615
跨类别扩展并非常清晰。

00:09:07.615 --> 00:09:09.850
所以其中一个案例

00:09:09.850 --> 00:09:12.770
很多是所谓的-被称为某种，

00:09:12.770 --> 00:09:14.770
嗯，发声时间。

00:09:14.770 --> 00:09:19.930
很多语言，包括英语，都有P和B两种声音，呃，

00:09:19.930 --> 00:09:25.150
PAH和BAH，它们根据发声开始的时间而不同。

00:09:25.150 --> 00:09:29.165
但是，它的声音听起来像一个带R的元音。

00:09:29.165 --> 00:09:31.510
这是一个连续参数，

00:09:31.510 --> 00:09:36.835
你可以在一个P和一个B之间的光谱上做任何点，但是，嗯，

00:09:36.835 --> 00:09:39.710
说英语的人，

00:09:39.710 --> 00:09:43.010
嗯，只感知光谱上的两点。

00:09:43.010 --> 00:09:47.320
你不会真正注意到它们之间的细微差别。

00:09:47.320 --> 00:09:50.380
嗯，有些语言在频谱上区分更多的点。

00:09:50.380 --> 00:09:53.980
所以泰语区分了三种不同的辅音，

00:09:53.980 --> 00:09:56.810
嗯，这取决于发声时间。

00:09:56.810 --> 00:09:58.840
嗯，可能是，嗯，

00:09:58.840 --> 00:10:01.400
更容易接近的是，

00:10:01.400 --> 00:10:03.920
嗯，这是语言变化的一个例子。

00:10:03.920 --> 00:10:06.375
所以像我这样的演讲者，嗯，

00:10:06.375 --> 00:10:10.480
有“抓到”和“cot”，这些是不同的元音，

00:10:10.480 --> 00:10:12.815
我听到的是不同的元音。

00:10:12.815 --> 00:10:17.455
但如果你是在美国西南部长大的，

00:10:17.455 --> 00:10:21.820
嗯，那么这些是完全相同的元音，你不能区分它们。

00:10:21.820 --> 00:10:26.390
你以为我说了两次同样的话，尽管我说的是两个不同的元音。

00:10:26.390 --> 00:10:34.120
因此，即使在辩证法层面上，人们也会发展到这种程度。

00:10:34.120 --> 00:10:36.420
对

00:10:36.420 --> 00:10:43.400
它们对哪些区别和声音敏感或不敏感。

00:10:43.950 --> 00:10:46.535
可以。总结-和，

00:10:46.535 --> 00:10:51.370
我的意思是，我为什么要提到这在某种意义上是这些声音的区别

00:10:51.370 --> 00:10:54.460
分类的声音区别是什么很多

00:10:54.460 --> 00:10:58.975
我们的语言书写系统，我们将在一分钟内记录下来。

00:10:58.975 --> 00:11:03.500
可以。嗯，在传统语言学中，

00:11:03.500 --> 00:11:08.195
你有声音，但声音在语言中没有任何意义。

00:11:08.195 --> 00:11:10.805
所以，pah和bah没有意义，

00:11:10.805 --> 00:11:12.935
A和E没有意义。

00:11:12.935 --> 00:11:15.875
所以人们通常把

00:11:15.875 --> 00:11:20.150
下一个层次的形态学是单词的一部分。

00:11:20.150 --> 00:11:23.540
这被看作是有意义的最小层次。

00:11:23.540 --> 00:11:27.620
所以这个想法是很多词都是复杂的，可以做出来的

00:11:27.620 --> 00:11:31.660
u-由碎片组成，但这些碎片确实有意义。

00:11:31.660 --> 00:11:36.050
所以幸运有一个意义，嗯，幸运，

00:11:36.050 --> 00:11:38.805
你以这个结尾结尾，

00:11:38.805 --> 00:11:42.510
嗯，哪种方式给人带来了财富。

00:11:42.510 --> 00:11:45.405
这意味着你知道，拥有财富，嗯，

00:11:45.405 --> 00:11:46.610
这是有意义的，

00:11:46.610 --> 00:11:49.360
联合国有一个意思，意思是要推翻这一点。

00:11:49.360 --> 00:11:54.195
不幸的是，你没有财富，而且，

00:11:54.195 --> 00:11:57.265
这就意味着要把这一切变成副词，

00:11:57.265 --> 00:11:59.825
你可以说不幸的是没有，

00:11:59.825 --> 00:12:02.290
嗯，发了财，出了点事。

00:12:02.290 --> 00:12:04.400
所以这些碎片，嗯，

00:12:04.400 --> 00:12:07.910
单词是最基本的有意义的东西。

00:12:07.910 --> 00:12:10.775
嗯，在深度学习方面几乎没有工作尝试过

00:12:10.775 --> 00:12:13.480
利用这种语素层次的结构。

00:12:13.480 --> 00:12:17.785
事实上，六年前我和几个学生

00:12:17.785 --> 00:12:22.515
试着建立一个系统，在那里建立这些树结构的神经网络，

00:12:22.515 --> 00:12:25.710
把单词的意思拼凑在一起。

00:12:25.710 --> 00:12:29.875
嗯，但这不是一个被广泛接受的想法。

00:12:29.875 --> 00:12:33.660
这有点不太普遍的原因，

00:12:33.660 --> 00:12:38.490
这就是做这件事，找出有语义意义的单词，

00:12:38.490 --> 00:12:44.925
在NLP中，人们发现了很多困难和时间

00:12:44.925 --> 00:12:48.215
你能得到同样的

00:12:48.215 --> 00:12:52.090
结果，如果您只使用字符n-grams。

00:12:52.090 --> 00:12:55.560
你放入卷积神经网络的那种单位。

00:12:55.560 --> 00:12:59.105
因为如果你有一个模型

00:12:59.105 --> 00:13:05.045
字符三角图，你有一些单词开头，un和nfo，等等。

00:13:05.045 --> 00:13:08.770
为了通过最后一句话，

00:13:08.770 --> 00:13:10.840
那些不同的单位。

00:13:10.840 --> 00:13:13.085
有不同的角色三角图，

00:13:13.085 --> 00:13:15.280
以分布式方式

00:13:15.280 --> 00:13:19.765
这个词的所有重要意义成分都很好，

00:13:19.765 --> 00:13:21.545
这就足够了。

00:13:21.545 --> 00:13:27.035
事实上，这是一个非常经典的想法，已经复兴了。

00:13:27.035 --> 00:13:30.220
嗯，那么回到第二次

00:13:30.220 --> 00:13:35.045
神经网络在80年代中期到90年代早期，嗯，

00:13:35.045 --> 00:13:37.255
有曲恩，

00:13:37.255 --> 00:13:41.440
在这方面有很多有争议的工作

00:13:41.440 --> 00:13:46.030
语言的结构，特别是戴夫·鲁梅尔哈特和杰伊·麦克莱兰。

00:13:46.030 --> 00:13:48.500
所以Jay McClelland还在精神科，

00:13:48.500 --> 00:13:50.725
如果你想在业余时间找他。

00:13:50.725 --> 00:13:57.460
嗯，他们提出了一个如何在英语中生成过去时形式的模型。

00:13:57.460 --> 00:14:00.610
所以这是一个我们能做的心理实验

00:14:00.610 --> 00:14:04.045
一个可以学习英语动词过去时态的系统？

00:14:04.045 --> 00:14:07.780
困难的部分有一些，很多动词是规则的，

00:14:07.780 --> 00:14:09.740
你加上类似的结尾，

00:14:09.740 --> 00:14:14.015
但是有些单词是不规则的，你必须学习一些不规则的模式。

00:14:14.015 --> 00:14:17.015
嗯，但他们是这样做的。

00:14:17.015 --> 00:14:23.530
我的意思是部分是因为这是早期的，关于，嗯，序列模型，

00:14:23.530 --> 00:14:27.094
他们用的是代表

00:14:27.094 --> 00:14:31.255
字词正好有这些字符三角。

00:14:31.255 --> 00:14:35.780
这就是他们在模型中使用和提供的单词的表示。

00:14:35.780 --> 00:14:40.220
这个想法在

00:14:40.220 --> 00:14:43.535
语言学家、哲学家和其他人

00:14:43.535 --> 00:14:47.075
语言，所以在那时候有很多争论。

00:14:47.075 --> 00:14:50.500
但作为一个纯粹的工程解决方案，

00:14:50.500 --> 00:14:54.070
事实证明，这是一种很好的做事方式。

00:14:54.070 --> 00:14:59.245
所以这十年还有其他的工作，包括模型，嗯，

00:14:59.245 --> 00:15:02.995
在微软开发的一种深度，嗯，

00:15:02.995 --> 00:15:05.860
语义模型，他们使用的是这些类型的

00:15:05.860 --> 00:15:09.420
把意思放在单词上的字符n-grams。

00:15:09.420 --> 00:15:17.195
好吧，那么，嗯，现在我们可能有兴趣建立一个不会言过其实的模型。

00:15:17.195 --> 00:15:21.065
所以我们要把一个词写成字符

00:15:21.065 --> 00:15:25.355
我们将用它来做一些事情，比如构建字符n-grams。

00:15:25.355 --> 00:15:28.775
所以一些有用的东西，嗯，

00:15:28.775 --> 00:15:31.010
知道有没有

00:15:31.010 --> 00:15:33.980
当你这样做的时候，语言之间有相当大的差异。

00:15:33.980 --> 00:15:36.200
所以不是所有的东西都是一样的，对吧？

00:15:36.200 --> 00:15:38.465
所以第一个问题是，嗯，

00:15:38.465 --> 00:15:42.455
有些语言在单词之间不加空格。

00:15:42.455 --> 00:15:45.125
最著名的例子是中国。

00:15:45.125 --> 00:15:51.545
嗯，但对于那些有欧洲血统的人来说，这是一个有趣的事实，

00:15:51.545 --> 00:15:56.630
你知道古希腊人写古希腊语的时候，

00:15:56.630 --> 00:15:59.525
嗯，他们也没有在单词之间加空格。

00:15:59.525 --> 00:16:02.390
实际上是后来发明的

00:16:02.390 --> 00:16:06.170
中世纪的学者们正在重新复制他们的手稿，

00:16:06.170 --> 00:16:08.690
他们决定谁[噪音]如果我们

00:16:08.690 --> 00:16:11.720
把空间放进去，然后他们开始做。

00:16:11.720 --> 00:16:20.000
嗯，[噪音]现在大多数语言都在词与词之间加了空格，但甚至，

00:16:20.000 --> 00:16:22.580
还有很多很好的案子。

00:16:22.580 --> 00:16:26.930
因此，特别是，很多语言都有一些小片段

00:16:26.930 --> 00:16:31.550
可能是代词或介词的东西，

00:16:31.550 --> 00:16:36.380
或者各种各样的连接词，比如and，等等，

00:16:36.380 --> 00:16:42.050
有时他们一起写，有时分开写。

00:16:42.050 --> 00:16:44.750
所以在法语里，嗯，

00:16:44.750 --> 00:16:49.865
你有这种介词，对不起，

00:16:49.865 --> 00:16:55.865
代词，嗯，给你的标记，我，你，带来了。

00:16:55.865 --> 00:16:58.640
嗯，你知道，这些小字眼和

00:16:58.640 --> 00:17:01.940
发音就像我说的一样，

00:17:01.940 --> 00:17:05.060
可以说这几乎是一个词，

00:17:05.060 --> 00:17:07.325
但它是分开写的。

00:17:07.325 --> 00:17:11.480
嗯，还有其他语言可以把东西粘在一起，

00:17:11.480 --> 00:17:13.580
可以说，它们是分开的词。

00:17:13.580 --> 00:17:19.550
所以在阿拉伯语中，你会听到发音上的clitics和一些类似的连词，

00:17:19.550 --> 00:17:23.960
而且，它们是作为一个词写在一起的，

00:17:23.960 --> 00:17:27.485
可以说，它们应该是四个字。

00:17:27.485 --> 00:17:31.790
另一个著名的例子是复合名词。

00:17:31.790 --> 00:17:34.025
嗯，所以在英语里，

00:17:34.025 --> 00:17:37.160
我们写复合名词，中间有空格，

00:17:37.160 --> 00:17:38.690
所以你可以看到每个名词。

00:17:38.690 --> 00:17:42.290
嗯，尽管在很多方面复合名词

00:17:42.290 --> 00:17:46.460
像白板这样的东西表现得就像是一个词，或者说是高中。

00:17:46.460 --> 00:17:48.770
嗯，而其他语言，

00:17:48.770 --> 00:17:50.480
德语是最著名的案例，

00:17:50.480 --> 00:17:52.580
还有其他日耳曼语言，

00:17:52.580 --> 00:17:57.470
把它们都写成一个单词，你就会得到很长的单词。

00:17:57.470 --> 00:18:03.680
所以，如果我们只使用空格而不做其他事情，我们就可以得到不同的单词。嗯，很好。

00:18:03.680 --> 00:18:07.310
可以。对.所以对于文字的处理，

00:18:07.310 --> 00:18:10.175
有这些实际问题。

00:18:10.175 --> 00:18:13.070
嗯，我们已经开始

00:18:13.070 --> 00:18:16.339
如果你想建立基于单词的模型，

00:18:16.339 --> 00:18:18.785
有这么大的话语空间，

00:18:18.785 --> 00:18:21.740
严格地说，单词的空间是无限的

00:18:21.740 --> 00:18:24.845
因为一旦你允许数字之类的东西，

00:18:24.845 --> 00:18:28.985
更不用说联邦快递的路线号码了，或者，嗯，

00:18:28.985 --> 00:18:31.520
或者如果你只允许形态学，

00:18:31.520 --> 00:18:34.175
当你能让那些像不幸一样。

00:18:34.175 --> 00:18:37.190
是的，有点，你可以扩大单词的空间，

00:18:37.190 --> 00:18:40.265
所以你得到了大量的开放词汇。

00:18:40.265 --> 00:18:43.340
嗯，你知道，英语有点问题。

00:18:43.340 --> 00:18:46.400
它比许多其他语言更容易产生问题。

00:18:46.400 --> 00:18:49.160
这里有一个可爱的捷克语单词，嗯，

00:18:49.160 --> 00:18:51.635
最差的农场，嗯，

00:18:51.635 --> 00:18:56.420
在那里你可以用许多其他的语言造出更复杂的单词。

00:18:56.420 --> 00:18:59.150
嗯，很多美洲土著语言，

00:18:59.150 --> 00:19:03.170
其他欧洲语言，如芬兰语，都有这些非常复杂的单词，

00:19:03.170 --> 00:19:05.570
土耳其语的单词很复杂。

00:19:05.570 --> 00:19:07.895
嗯，那是个坏消息。

00:19:07.895 --> 00:19:10.190
嗯，还有其他的原因我们想

00:19:10.190 --> 00:19:12.410
看看单词级别以下的单词，

00:19:12.410 --> 00:19:13.970
了解他们的情况。

00:19:13.970 --> 00:19:16.115
所以当你翻译的时候，

00:19:16.115 --> 00:19:18.110
有很多东西，

00:19:18.110 --> 00:19:23.405
尤其是名字，翻译本质上是音译，

00:19:23.405 --> 00:19:29.015
你要把某人名字的发音改写成粗略的，你知道，

00:19:29.015 --> 00:19:31.790
也许不完全正确，但大致正确

00:19:31.790 --> 00:19:34.730
根据不同语言的声音系统。

00:19:34.730 --> 00:19:36.020
如果我们想这样做，

00:19:36.020 --> 00:19:38.900
我们基本上想在信的层面上工作，

00:19:38.900 --> 00:19:40.310
不是字面意思。

00:19:40.310 --> 00:19:46.340
但另一个现代的原因是，我们想开始在单词级别以下建模，

00:19:46.340 --> 00:19:51.350
我们生活在这个社交媒体时代，如果你在社交媒体领域，

00:19:51.350 --> 00:19:53.810
有很多东西不是写的

00:19:53.810 --> 00:19:56.975
使用字典中的规范词，

00:19:56.975 --> 00:19:58.730
不知怎的，我们想开始，

00:19:58.730 --> 00:20:00.005
嗯，做个模型。

00:20:00.005 --> 00:20:02.180
所以在某种意义上，这是，嗯，

00:20:02.180 --> 00:20:03.545
简单的情况。

00:20:03.545 --> 00:20:06.260
嗯，感觉不错。

00:20:06.260 --> 00:20:08.720
嗯，但不管怎么说，这是用一个拼写的，

00:20:08.720 --> 00:20:10.370
二，三，四，五，六，

00:20:10.370 --> 00:20:13.010
七个O，一个，两个，

00:20:13.010 --> 00:20:15.875
三，四，五，还有七，他们匹配。

00:20:15.875 --> 00:20:20.270
我不知道这是不是故意的[笑声]。嗯，好吧。

00:20:20.270 --> 00:20:24.905
所以这种写作方式很常见，嗯，而且，

00:20:24.905 --> 00:20:27.830
你知道，如果我们

00:20:27.830 --> 00:20:31.100
在文字层面上对待事物，我们正在努力树立正确的模式。

00:20:31.100 --> 00:20:34.730
很明显这不是人类在做的，我们在看

00:20:34.730 --> 00:20:38.675
角色和识别发生的事情。

00:20:38.675 --> 00:20:45.290
嗯，从某种意义上说，这是一种很容易的情况，你可以想象预处理出来。

00:20:45.290 --> 00:20:49.100
嗯，有很多比较难的东西会出现。

00:20:49.100 --> 00:20:53.645
嗯，我想有种缩写的说法，好像我不在乎。

00:20:53.645 --> 00:20:58.640
嗯，但是你会得到很多有创意的拼写，嗯，

00:20:58.640 --> 00:21:04.565
这是因为我的发音变小了，萨姆。

00:21:04.565 --> 00:21:08.450
嗯，看来我们需要的不是

00:21:08.450 --> 00:21:13.265
如果我们要开始更好地处理大量的文本，就要使用规范词。

00:21:13.265 --> 00:21:22.070
哎呀。可以。所以这表明我们有点想从我们的模型开始。

00:21:22.070 --> 00:21:28.055
因此，这导致了人们对使用字符级模型的极大兴趣。

00:21:28.055 --> 00:21:35.300
嗯，我的意思是有两种程度你可以做到，

00:21:35.300 --> 00:21:38.045
我们会看他们两个。

00:21:38.045 --> 00:21:40.940
嗯，一个层次是说，

00:21:40.940 --> 00:21:43.820
看，我们的系统里还有单词。

00:21:43.820 --> 00:21:47.345
基本上，我们要建立一个能克服文字的系统，

00:21:47.345 --> 00:21:49.670
但我们希望能够创造

00:21:49.670 --> 00:21:54.950
任何字符序列的单词表示，我们希望

00:21:54.950 --> 00:21:58.910
以一种能够利用

00:21:58.910 --> 00:22:03.050
识别字符序列中看起来熟悉的部分，

00:22:03.050 --> 00:22:07.790
这样我们就可以猜测Vibes是什么意思了。

00:22:07.790 --> 00:22:10.670
嗯，这样就解决了问题

00:22:10.670 --> 00:22:13.489
用不知道的词，我们得到相似的词，

00:22:13.489 --> 00:22:18.125
词条、拼写等类似的单词的类似嵌入。

00:22:18.125 --> 00:22:20.810
但另一种选择是，

00:22:20.810 --> 00:22:23.820
哦，不，把这些话都忘了，谁需要呢？

00:22:23.820 --> 00:22:28.880
我们为什么不把所有的语言处理都放在字符序列上呢？

00:22:28.880 --> 00:22:30.410
一切都会好起来的。

00:22:30.410 --> 00:22:35.090
嗯，这两种方法都被证明是非常成功的。

00:22:35.090 --> 00:22:38.810
嗯，我只想在这一点上停留一下，

00:22:38.810 --> 00:22:41.480
回到我的，

00:22:41.480 --> 00:22:44.630
嗯，形态在这里滑动。

00:22:44.630 --> 00:22:47.840
当人们第一次提出他们要去的时候

00:22:47.840 --> 00:22:51.245
在角色上建立深度学习模型。

00:22:51.245 --> 00:22:54.650
我是说，我的第一感觉是，哦，那永远不会

00:22:54.650 --> 00:22:57.965
工作是因为看起来，

00:22:57.965 --> 00:23:01.310
好吧，单词有意义，它是有意义的，

00:23:01.310 --> 00:23:03.890
嗯，你可以做一些类似于建造的事情

00:23:03.890 --> 00:23:07.160
一个word2vec模型，它将真正能够

00:23:07.160 --> 00:23:09.650
有点看单词及其分布和学习

00:23:09.650 --> 00:23:12.905
单词的意思是因为单词有意义。

00:23:12.905 --> 00:23:15.680
你能说的话，

00:23:15.680 --> 00:23:19.310
我将提出一个h的向量表示，

00:23:19.310 --> 00:23:22.880
以及不同的向量表示，

00:23:22.880 --> 00:23:26.404
以及T的不同矢量表示，

00:23:26.404 --> 00:23:29.330
不知怎的，这对于代表什么帽子是有用的。

00:23:29.330 --> 00:23:32.495
意味着一旦我把它穿过足够多的神经网络层，

00:23:32.495 --> 00:23:36.335
嗯，坦白说，我觉得这听起来很不可信。

00:23:36.335 --> 00:23:40.315
嗯，但是，嗯，我想，你知道-

00:23:40.315 --> 00:23:44.595
但是，它，它，完全有效，所以我现在相信，经验证明。

00:23:44.595 --> 00:23:48.885
我认为我们需要意识到的是，

00:23:48.885 --> 00:23:52.350
就这样走了-是的，

00:23:52.350 --> 00:23:55.755
在某种程度上，我们只是拥有这些并不重要的角色。

00:23:55.755 --> 00:24:02.940
但是我们有这些非常强大的组合模型，其中有很多参数，

00:24:02.940 --> 00:24:04.950
像是循环神经网络和

00:24:04.950 --> 00:24:10.305
卷积神经网络和它们各自能够，某种程度上，建立，

00:24:10.305 --> 00:24:15.330
存储和构建多字母组的意义表示，

00:24:15.330 --> 00:24:18.720
这样他们就可以模拟

00:24:18.720 --> 00:24:22.590
语素和更大的单位，因此把单词的意思放在一起。

00:24:22.590 --> 00:24:24.420
嗯，是的。所以，嗯，

00:24:24.420 --> 00:24:27.930
关于使用字符的更多细节，

00:24:27.930 --> 00:24:30.195
嗯，从书写系统。

00:24:30.195 --> 00:24:33.660
因此，如果你是一个语言学家，你倾向于认为声音是主要的。

00:24:33.660 --> 00:24:36.900
那些是我们之前提到的电话。

00:24:36.900 --> 00:24:40.170
你知道，嗯，基本上，

00:24:40.170 --> 00:24:43.250
嗯，深度学习根本没有尝试使用音素。

00:24:43.250 --> 00:24:46.425
传统的语音识别器经常使用音素，

00:24:46.425 --> 00:24:48.135
但在深造之地，

00:24:48.135 --> 00:24:53.580
你想要大量的数据，而你获取大量数据的方式就是你只需要使用，嗯，

00:24:53.580 --> 00:24:55.340
写东西是因为，你知道，

00:24:55.340 --> 00:25:00.920
这是很容易找到的数据，在那里你可以得到数以百万计的文字资料。

00:25:00.920 --> 00:25:04.220
嗯，从数据的角度来看，这是有意义的。

00:25:04.220 --> 00:25:07.805
但结果有点奇怪，

00:25:07.805 --> 00:25:11.005
当你建立一个角色级别的模型时，

00:25:11.005 --> 00:25:13.714
你的角色等级模型是什么？

00:25:13.714 --> 00:25:17.325
实际上根据语言的书写系统而有所不同。

00:25:17.325 --> 00:25:18.875
所以你，有点，

00:25:18.875 --> 00:25:21.990
有这些完全不同的书写系统。

00:25:21.990 --> 00:25:27.405
所以你有一些完全是音素的书写系统，

00:25:27.405 --> 00:25:32.130
有一些字母有特定的声音，你说这个声音。

00:25:32.130 --> 00:25:34.575
类似西班牙语的东西几乎是音位学。

00:25:34.575 --> 00:25:36.810
有时候有点复杂。

00:25:36.810 --> 00:25:38.580
所以你可能有一个有向图。

00:25:38.580 --> 00:25:41.700
所以这个有向图，Ngabulu，有点像，

00:25:41.700 --> 00:25:46.470
英语中的n-g，用于“ng”的发音，听起来像是在看的末尾，

00:25:46.470 --> 00:25:49.080
但是，你知道，基本上这只是“Jiyawu”，

00:25:49.080 --> 00:25:50.939
每一个字母都是一个声音，

00:25:50.939 --> 00:25:52.860
你可以读，嗯，

00:25:52.860 --> 00:25:54.960
只是，嗯，音位。

00:25:54.960 --> 00:25:58.140
嗯，那和英语的比较

00:25:58.140 --> 00:26:01.890
所有非母语人士都知道拼写很糟糕。

00:26:01.890 --> 00:26:04.515
它有点像高度石化的，

00:26:04.515 --> 00:26:05.700
从前，

00:26:05.700 --> 00:26:08.885
十世纪的音位系统。

00:26:08.885 --> 00:26:12.030
嗯，但是现在我们有了一个系统

00:26:12.030 --> 00:26:17.700
相当随意的拼写，实际上并不代表声音，嗯，非常清楚。

00:26:17.700 --> 00:26:20.490
但这是一种音位系统。

00:26:20.490 --> 00:26:23.430
但是还有一些语言使用更大的单位。

00:26:23.430 --> 00:26:25.235
嗯，这是，嗯，

00:26:25.235 --> 00:26:29.715
加拿大人和因纽特人，我把他们放进去是因为这是一个很好的写作系统。

00:26:29.715 --> 00:26:35.820
嗯，但是有很多语言用它们的字符来表示音节。

00:26:35.820 --> 00:26:38.880
嗯，比如说你会用韩文写这样的东西，

00:26:38.880 --> 00:26:41.190
朝鲜文，每个，嗯，

00:26:41.190 --> 00:26:46.860
字母则是一种辅音-元音组合的音节，如小节。

00:26:46.860 --> 00:26:52.455
嗯，如果你能从这个水平上升，如果我们再回到中国，

00:26:52.455 --> 00:26:57.465
嗯，嗯，你可以说，这也是一种音节系统。

00:26:57.465 --> 00:27:02.130
但实际上，汉字不仅仅是声音。

00:27:02.130 --> 00:27:03.515
它们也有意义。

00:27:03.515 --> 00:27:06.435
这真的是一个表意系统

00:27:06.435 --> 00:27:09.570
有特殊含义的字符。

00:27:09.570 --> 00:27:10.860
所以他们，有点，呃，

00:27:10.860 --> 00:27:14.025
整个语素写成一个字母。

00:27:14.025 --> 00:27:16.964
还有，你知道，这种语言的另一个例子，

00:27:16.964 --> 00:27:19.500
嗯，是埃及象形文字，如果你看过的话。

00:27:19.500 --> 00:27:23.700
它们是，某种程度上，表意系统，在那里你有意义的字母。

00:27:23.700 --> 00:27:27.420
嗯，然后你有语言系统，混合了其中的几个。

00:27:27.420 --> 00:27:31.875
所以日本人是部分冰碛的混合物，

00:27:31.875 --> 00:27:34.875
部分表意系统混合在一起。

00:27:34.875 --> 00:27:36.100
所以如果你只是，

00:27:36.100 --> 00:27:37.695
有点，开始说，

00:27:37.695 --> 00:27:41.805
“好吧，我要建立一个基于角色的系统。”很好。

00:27:41.805 --> 00:27:45.495
但实际上，你的角色单位喜欢

00:27:45.495 --> 00:27:50.610
字母三角在汉语中是非常不同的，

00:27:50.610 --> 00:27:54.030
通常情况下，字母三元符号是，

00:27:54.030 --> 00:27:55.140
一个半字，

00:27:55.140 --> 00:27:57.915
有意义的三个语素。

00:27:57.915 --> 00:27:59.820
但是如果你用的是英语，

00:27:59.820 --> 00:28:03.105
你的角色三角学就像T-H-O

00:28:03.105 --> 00:28:06.980
它仍然是一个小得多的单位，没有任何意义。

00:28:06.980 --> 00:28:08.810
所以往前走。

00:28:08.810 --> 00:28:11.550
所以这两种方法，嗯，

00:28:11.550 --> 00:28:15.875
一个是做一个完整的角色级模型，另一个是，

00:28:15.875 --> 00:28:18.120
有点，你可以利用字符

00:28:18.120 --> 00:28:20.640
建造更大的东西，然后你会放东西，

00:28:20.640 --> 00:28:22.860
比如，进入一个更文字层次的模型。

00:28:22.860 --> 00:28:25.455
所以我先做这个，再做另一个。

00:28:25.455 --> 00:28:27.810
所以对于纯粹的字符级模型，

00:28:27.810 --> 00:28:30.785
最后一次我展示了一个例子。你还记得吗？

00:28:30.785 --> 00:28:33.820
所以有一个非常深的卷积网络

00:28:33.820 --> 00:28:37.710
Connau等人在结尾的文字分类词，嗯，

00:28:37.710 --> 00:28:39.660
刚开始的时候，

00:28:39.660 --> 00:28:43.620
并在此基础上构建了这些卷积层，

00:28:43.620 --> 00:28:47.565
在视觉上喜欢网络和分类文件。

00:28:47.565 --> 00:28:51.450
嗯，那就是，某种程度上，一个完全的字符级模型。

00:28:51.450 --> 00:28:54.405
嗯，不过这方面还有点工作要做。

00:28:54.405 --> 00:28:59.325
所以机器翻译的人已经建立了，嗯，

00:28:59.325 --> 00:29:05.250
只读字符和写字符的机器翻译系统。

00:29:05.250 --> 00:29:08.615
当人们第一次尝试这样做的时候，

00:29:08.615 --> 00:29:11.940
嗯，有点不管用，对吧？

00:29:11.940 --> 00:29:13.740
人们认为这可能有助于建设

00:29:13.740 --> 00:29:17.775
字符级模型，特别是对于汉语这样的语言。

00:29:17.775 --> 00:29:21.240
但是人们不能建立起

00:29:21.240 --> 00:29:25.365
以及基于单词的模型和前神经系统，

00:29:25.365 --> 00:29:28.095
非神经或神经世界。

00:29:28.095 --> 00:29:30.930
但渐渐地，情况开始改变。

00:29:30.930 --> 00:29:36.480
所以人们开始有成功的字符级解码器，然后，

00:29:36.480 --> 00:29:40.065
大概在2015年和16年左右，

00:29:40.065 --> 00:29:41.790
嗯，人们开始展示，

00:29:41.790 --> 00:29:45.840
看，你可以-实际上可以做机器翻译非常

00:29:45.840 --> 00:29:50.055
好吧，在只有几个星号的字符级别。

00:29:50.055 --> 00:29:53.910
所以，嗯，这是我们做的一些工作。

00:29:53.910 --> 00:29:56.400
嗯，罗和曼宁一号，从，嗯，

00:29:56.400 --> 00:29:59.145
最后一张幻灯片上的2015年。

00:29:59.145 --> 00:30:03.570
这是从英语到捷克语的翻译和捷克语的翻译

00:30:03.570 --> 00:30:07.800
如果你想激发你在角色层面上做事的积极性，

00:30:07.800 --> 00:30:10.650
因为它有很多可怕的词

00:30:10.650 --> 00:30:15.555
形态学就像我之前给你看的例子，稍后我再给你看一些。

00:30:15.555 --> 00:30:20.145
所以人们为捷克语建立了单词级的模型。

00:30:20.145 --> 00:30:22.650
嗯，而且，你知道，他们工作不太好，

00:30:22.650 --> 00:30:25.320
部分原因是这些声音问题。

00:30:25.320 --> 00:30:26.880
所以，嗯，有点，

00:30:26.880 --> 00:30:31.905
当时的世界水平是15.7布鲁，

00:30:31.905 --> 00:30:37.680
你知道，这比我们在你的家庭作业中接受的满分要少得多。

00:30:37.680 --> 00:30:39.555
[笑声]嗯，但是，你知道，

00:30:39.555 --> 00:30:43.740
什么是一个好的布鲁分数取决于语言对有多困难。

00:30:43.740 --> 00:30:46.320
呃，嗯，所以你不学捷克语。

00:30:46.320 --> 00:30:48.875
嗯，但是，嗯，这是，某种程度上，

00:30:48.875 --> 00:30:51.690
我们讨论过的那种神经模型。

00:30:51.690 --> 00:30:53.400
所以它是一个seq2seq模型，

00:30:53.400 --> 00:31:00.375
注意到了，然后它有额外的东西来代替UNK，

00:31:00.375 --> 00:31:05.790
嗯，单字翻译或者从原文中复制东西。

00:31:05.790 --> 00:31:07.200
基本上是，

00:31:07.200 --> 00:31:12.825
2015年最先进的神经mt，得到15.7 bleu。

00:31:12.825 --> 00:31:14.970
差别不大，

00:31:14.970 --> 00:31:16.380
嗯，但我们可以展示，

00:31:16.380 --> 00:31:18.875
看，我们可以完全建立这个，嗯，

00:31:18.875 --> 00:31:22.530
字符级模型，然后实际上，它做得更好。

00:31:22.530 --> 00:31:24.300
嗯，所以这个，有点，

00:31:24.300 --> 00:31:28.260
从翻译质量来看

00:31:28.260 --> 00:31:32.760
完全基于字符的模型在

00:31:32.760 --> 00:31:37.380
捕获文本和基于单词的模型的含义。

00:31:37.380 --> 00:31:40.395
嗯，这是个好结果吗？

00:31:40.395 --> 00:31:42.630
嗯，在很多方面，在某些方面，

00:31:42.630 --> 00:31:45.105
是的，从另一方面来说，不是。

00:31:45.105 --> 00:31:48.810
我的意思是，这个模型真的很难训练，对吧？

00:31:48.810 --> 00:31:52.590
所以我们花了大约三周的时间来训练这个模型，在运行时，

00:31:52.590 --> 00:31:54.855
它也工作得很慢。

00:31:54.855 --> 00:31:57.240
所以字符级模型的问题，

00:31:57.240 --> 00:32:00.135
如果你把它们放进类似LSTM的地方，

00:32:00.135 --> 00:32:02.580
你的序列会变长吗？

00:32:02.580 --> 00:32:07.110
所以你有大约7倍的序列，你以前有。

00:32:07.110 --> 00:32:10.440
因为没有太多的信息，人物，

00:32:10.440 --> 00:32:15.840
你必须在更远的时间里进行反向传播。

00:32:15.840 --> 00:32:19.040
所以我们把传播时间倒流

00:32:19.040 --> 00:32:22.590
600步之后，我们就开始截断它了。

00:32:22.590 --> 00:32:23.990
所以这个，有点，制造，

00:32:23.990 --> 00:32:25.210
也许这太过分了，

00:32:25.210 --> 00:32:27.770
但它让模特们，嗯，动作很慢。

00:32:27.770 --> 00:32:32.520
但是我们能够证明它能够取得一些好的效果，对吧。

00:32:32.520 --> 00:32:33.705
这是捷克语，

00:32:33.705 --> 00:32:35.230
嗯，翻译成捷克语，

00:32:35.230 --> 00:32:36.540
她11岁的女儿，

00:32:36.540 --> 00:32:39.645
谢尼·巴特说感觉有点奇怪。

00:32:39.645 --> 00:32:42.885
而且，嗯，我可能不知道。

00:32:42.885 --> 00:32:45.480
有人说捷克语吗？有人说捷克语吗？

00:32:45.480 --> 00:32:48.615
嗯，没有说捷克语的人？

00:32:48.615 --> 00:32:51.195
好吧，嗯，我也不会说捷克语，嗯，

00:32:51.195 --> 00:32:57.390
但我们可以看到（笑声），我们可以看到这是有趣的事情，对吧。

00:32:57.390 --> 00:33:00.045
第二行是人工翻译

00:33:00.045 --> 00:33:03.010
我们可以用捷克语做一些指导。

00:33:03.010 --> 00:33:04.590
尤其是，嗯，

00:33:04.590 --> 00:33:07.815
在捷克，有一个词代表11岁，

00:33:07.815 --> 00:33:11.295
嗯，你可以看到第二行的那个蓝色单词。

00:33:11.295 --> 00:33:15.645
你可以看到，尽管11岁，嗯，

00:33:15.645 --> 00:33:19.425
对于11岁的孩子来说，它能完美的完成，嗯，

00:33:19.425 --> 00:33:21.720
一个字母一个字母地写，嗯，

00:33:21.720 --> 00:33:25.830
捷克语11岁的单词，效果很好。

00:33:25.830 --> 00:33:29.160
相反，对于单词级模型，嗯，

00:33:29.160 --> 00:33:33.585
11岁是个不知名的词，因为它不在词汇表中。

00:33:33.585 --> 00:33:37.920
然后它有两种机制来尝试和处理未知的单词。

00:33:37.920 --> 00:33:39.435
也可以，呃，

00:33:39.435 --> 00:33:43.485
它们的单字翻译，否则它只能复制它们。

00:33:43.485 --> 00:33:46.230
不管出于什么原因，它决定了最好的策略

00:33:46.230 --> 00:33:49.580
是抄袭，所以完全失败了。

00:33:49.580 --> 00:33:53.115
嗯，如果我们继续讨论角色级别的模型，

00:33:53.115 --> 00:33:55.890
还有一件事是对的，那真的很酷，

00:33:55.890 --> 00:33:57.995
嗯，是Shani Bart的名字。

00:33:57.995 --> 00:34:02.370
它能够完成我刚才提到的这个音译任务。

00:34:02.370 --> 00:34:04.755
它变成了沙尼·巴托娃

00:34:04.755 --> 00:34:07.760
这正是人类翻译所做的。

00:34:07.760 --> 00:34:11.440
所以，你知道，它实际上做了一些非常好的事情，嗯，

00:34:11.440 --> 00:34:15.635
人类翻译，嗯，喜欢这样的东西。

00:34:15.635 --> 00:34:17.480
我的意思是，事实上，

00:34:17.480 --> 00:34:20.605
我能从花点时间在谷歌翻译上看出，

00:34:20.605 --> 00:34:23.195
在这句话中，句号实际上做得很好。

00:34:23.195 --> 00:34:25.880
好吧，这里的部分开始不同了，

00:34:25.880 --> 00:34:27.815
嗯，来自人类翻译。

00:34:27.815 --> 00:34:29.480
但实际上并不坏。

00:34:29.480 --> 00:34:31.460
这是一种更直白的翻译。

00:34:31.460 --> 00:34:33.210
所以这个城市，

00:34:33.210 --> 00:34:37.340
事实上，在英语课本中，翻译的感觉很像。

00:34:37.340 --> 00:34:38.875
而人类，有点，

00:34:38.875 --> 00:34:43.230
他们刚去的捷克语版本中没有使用feel这个词，

00:34:43.230 --> 00:34:45.550
嗯，有点，

00:34:45.550 --> 00:34:49.360
嗯，奇怪还是奇怪。很酷。

00:34:49.360 --> 00:34:54.965
可以。所以这里有更多的结果。

00:34:54.965 --> 00:34:58.535
这是第二年建造的另一个系统。

00:34:58.535 --> 00:35:00.830
这些人贾森·李，

00:35:00.830 --> 00:35:03.425
赵庆贤和霍夫曼。

00:35:03.425 --> 00:35:09.515
所以他们想做些我不知道的事，

00:35:09.515 --> 00:35:12.980
更复杂的神经系统和

00:35:12.980 --> 00:35:16.385
理解原文的意思。

00:35:16.385 --> 00:35:20.555
所以他们更多地使用我们上次看到的技术。

00:35:20.555 --> 00:35:29.960
所以在编码器方面，您从字符嵌入的字母序列开始。

00:35:29.960 --> 00:35:34.850
然后你用了四个卷积，

00:35:34.850 --> 00:35:39.710
三个和五个字符在这里得到表示。

00:35:39.710 --> 00:35:44.270
然后你将以5步的步幅进行最大限度的游泳池。

00:35:44.270 --> 00:35:49.370
所以你得到了三个文本的最大集合表示，

00:35:49.370 --> 00:35:51.680
四个和五个卷积。

00:35:51.680 --> 00:35:54.590
然后你就可以通过多层的

00:35:54.590 --> 00:35:57.815
公路网和通过

00:35:57.815 --> 00:36:05.570
双向门控循环单元，它为您提供了源代码表示。

00:36:05.570 --> 00:36:07.519
在解码器方面，

00:36:07.519 --> 00:36:09.440
它和我们的解码器差不多，

00:36:09.440 --> 00:36:13.680
它只是运行一个字符级序列模型。

00:36:14.110 --> 00:36:21.590
总的来说，他们做的是相反的任务。

00:36:21.590 --> 00:36:24.330
这是捷克语对英语。

00:36:25.120 --> 00:36:28.325
但是，所以他们开始得到更好的分数。

00:36:28.325 --> 00:36:31.610
但我的意思是，如果你看这些不同的数字，

00:36:31.610 --> 00:36:34.475
在这里我将在一分钟内详细解释这个系统，

00:36:34.475 --> 00:36:41.630
我的意思是，似乎他们得到很多价值的地方是使用

00:36:41.630 --> 00:36:46.700
字符级解码器通过

00:36:46.700 --> 00:36:53.550
源端的这个非常复杂的模型几乎没有给它们任何价值。

00:36:54.760 --> 00:36:58.085
最近的一篇论文，

00:36:58.085 --> 00:37:03.425
这是ColinCherry和谷歌的研究员。

00:37:03.425 --> 00:37:09.560
所以去年他们又对

00:37:09.560 --> 00:37:11.570
按顺序执行LSTM序列

00:37:11.570 --> 00:37:16.505
比较基于单词和字符的模型的样式模型。

00:37:16.505 --> 00:37:19.220
这是英语到法语还有这个

00:37:19.220 --> 00:37:23.090
从捷克语到英语，这正是我们所做的。

00:37:23.090 --> 00:37:27.440
所以在这两种情况下，当你有一个大模型的时候，

00:37:27.440 --> 00:37:30.005
人物模型为他们赢得了胜利。

00:37:30.005 --> 00:37:34.670
蓝色的型号在上面，但是像你这样有趣的东西

00:37:34.670 --> 00:37:36.830
看这些不同的效果取决于

00:37:36.830 --> 00:37:39.140
语言的形态复杂性。

00:37:39.140 --> 00:37:41.555
所以像捷克语这样的语言，

00:37:41.555 --> 00:37:43.850
这真是个好主意，

00:37:43.850 --> 00:37:45.995
如果你想建立一个好的模型，

00:37:45.995 --> 00:37:49.760
要使用角色级别，他们会在那里找到一个Bleu的不同点，

00:37:49.760 --> 00:37:54.710
而对于一个不加法语或英语的模特来说

00:37:54.710 --> 00:38:01.295
实际上，使用字符级模型只会获得很小的收益。

00:38:01.295 --> 00:38:05.300
好吧，我来解释一下这些模型，

00:38:05.300 --> 00:38:08.405
所以这些模型是不同尺寸的模型。

00:38:08.405 --> 00:38:17.720
因此，这些模型使用双向LSTM编码器和单向LSTM解码器。

00:38:17.720 --> 00:38:20.405
所以最简单的模型

00:38:20.405 --> 00:38:28.430
一个浅双向LSTM编码器和一个两层LSTM解码器。

00:38:28.430 --> 00:38:33.080
中间的模型有三层

00:38:33.080 --> 00:38:39.935
双向LSTM编码器和一个四层的LSTM解码器。

00:38:39.935 --> 00:38:43.940
最复杂的模型有六层

00:38:43.940 --> 00:38:51.530
双向LSTM编码器和八个深度的LSTM解码器堆栈。

00:38:51.530 --> 00:38:53.660
这就是在谷歌工作有帮助的地方。

00:38:53.660 --> 00:38:55.310
可能是你的项目，

00:38:55.310 --> 00:38:59.750
你不想超过三四岁。呆在这儿。

00:38:59.750 --> 00:39:04.310
好吧，是的，所以这些就是结果。

00:39:04.310 --> 00:39:07.010
所以基本上你发现的是如果你

00:39:07.010 --> 00:39:10.100
有点小的模特，你用词比较好，

00:39:10.100 --> 00:39:15.680
但是当你去看大模型的时候，尤其是当你用一种形态丰富的语言的时候，

00:39:15.680 --> 00:39:18.425
你显然开始从角色中取胜。

00:39:18.425 --> 00:39:21.410
但是仍然有一个损失

00:39:21.410 --> 00:39:27.635
和我们在2015年遭受的损失完全一样，对吧？

00:39:27.635 --> 00:39:35.510
这是时间图，所以这三个模型和这里相同，

00:39:35.510 --> 00:39:40.610
它只是轴被改变为LSTM层总数的总和。

00:39:40.610 --> 00:39:45.050
所以本质上，如果你是在单词层面，

00:39:45.050 --> 00:39:51.800
你可以运行这三个模型中的任何一个，而且它们速度很快，你可以翻译

00:39:51.800 --> 00:39:59.000
虽然时间不多，但对于角色级别的模型，您的坡度要高得多。

00:39:59.000 --> 00:40:04.770
因此，运行深度角色级别的模型开始变得相当昂贵。

00:40:05.860 --> 00:40:09.035
好吧，那就是那个部分。

00:40:09.035 --> 00:40:12.810
所以就一直往前走。

00:40:12.910 --> 00:40:18.395
然后我想看看其他的做事方式。

00:40:18.395 --> 00:40:23.930
所以这些模型在某种意义上仍然有文字，但是在哪里

00:40:23.930 --> 00:40:29.675
我们将要用片段构建单词表示。

00:40:29.675 --> 00:40:35.465
人们已经探索了两种方法。

00:40:35.465 --> 00:40:39.830
一种方法是说，看，我们只是想用

00:40:39.830 --> 00:40:44.495
与我们用于单词模型的体系结构完全相同

00:40:44.495 --> 00:40:48.710
只是我们的话不太可能

00:40:48.710 --> 00:40:53.390
做一个词，至少有时候他们会成为一个词的碎片。

00:40:53.390 --> 00:40:56.750
所以这些通常被称为单字模型。

00:40:56.750 --> 00:40:59.450
尤其是，有一种最常见的方法。

00:40:59.450 --> 00:41:03.740
它叫BPE，我将详细介绍一下。

00:41:03.740 --> 00:41:06.695
另一种选择是说，

00:41:06.695 --> 00:41:09.710
好吧，我们要做一种混合物或者混合。

00:41:09.710 --> 00:41:14.420
所以我们的主要模式是用词来表达，但是我们

00:41:14.420 --> 00:41:19.415
会有一些设施，我们可以在那里构建一个表示，

00:41:19.415 --> 00:41:21.530
对于其他未知词，

00:41:21.530 --> 00:41:24.380
通过在一个角色或一个较低层次上做的事情。

00:41:24.380 --> 00:41:26.750
我也给你看一点。

00:41:26.750 --> 00:41:31.370
好的，这是BPE。

00:41:31.370 --> 00:41:36.080
BPE实际上是一个非常简单的概念，它与

00:41:36.080 --> 00:41:41.150
与深度学习有关，但是BPE的使用已经成为

00:41:41.150 --> 00:41:49.340
相当标准并且成功地表示了一些单词

00:41:49.340 --> 00:41:52.685
有无限的词汇

00:41:52.685 --> 00:41:58.490
而一个无限有效的词汇表却同时使用了有限的词汇表。

00:41:58.490 --> 00:42:01.670
因此，字节对编码的起源和

00:42:01.670 --> 00:42:07.760
名称字节对与自然语言处理或神经网络无关，

00:42:07.760 --> 00:42:10.115
我们只是在写一个压缩算法。

00:42:10.115 --> 00:42:14.375
所以这类似于用gzip压缩文档。

00:42:14.375 --> 00:42:18.830
所以基本的字节对编码是什么，

00:42:18.830 --> 00:42:23.090
你有一组字节的东西，你是

00:42:23.090 --> 00:42:28.820
寻找两个字节最频繁的序列，你说，

00:42:28.820 --> 00:42:32.675
好的，我要把两个字节的序列

00:42:32.675 --> 00:42:37.700
我的可能值字典中的一个新元素。

00:42:37.700 --> 00:42:42.830
这意味着我可以有257个不同的字节值，所以

00:42:42.830 --> 00:42:45.110
说我可以缩短

00:42:45.110 --> 00:42:49.370
我的顺序和我可以重复一遍，然后再做一次。

00:42:49.370 --> 00:42:53.900
因此，从本质上讲，这项工作表明，

00:42:53.900 --> 00:42:58.700
我们可以应用这种压缩算法

00:42:58.700 --> 00:43:03.890
用一种方法写出

00:43:03.890 --> 00:43:08.480
很有用，但不要严格使用字节

00:43:08.480 --> 00:43:13.400
名称，但用字符和字符n-grams代替。

00:43:13.400 --> 00:43:16.190
所以最常见的方法是

00:43:16.190 --> 00:43:19.850
字符和字符n-grams，如果你适应了现代，

00:43:19.850 --> 00:43:22.790
你知道这意味着有Unicode，你可以表示

00:43:22.790 --> 00:43:26.060
所有这些可爱的字母像加拿大因纽特的

00:43:26.060 --> 00:43:28.445
印第安方言领音之类的。

00:43:28.445 --> 00:43:31.340
但实际上Unicode有问题，

00:43:31.340 --> 00:43:34.595
实际上有很多Unicode字符。

00:43:34.595 --> 00:43:36.815
理论上我忘了这个数字。

00:43:36.815 --> 00:43:40.520
我想大概有20万个Unicode字符。

00:43:40.520 --> 00:43:44.900
但是无论如何，如果你想处理包括东亚语言在内的一系列语言，

00:43:44.900 --> 00:43:49.205
也许你需要20000个字符，这有点太多了。

00:43:49.205 --> 00:43:54.920
实际上有些人已经回到字节，说，

00:43:54.920 --> 00:43:57.650
“你知道20万，这是一个非常大的词汇。

00:43:57.650 --> 00:43:59.270
我不想处理任何事情。”

00:43:59.270 --> 00:44:01.700
对不起，20万是一个非常大的词汇。

00:44:01.700 --> 00:44:03.845
我甚至不想处理这么大的事情。

00:44:03.845 --> 00:44:10.010
那我为什么不直接通过字节来做这些算法呢？

00:44:10.010 --> 00:44:14.495
这意味着在UTF-8编码中，

00:44:14.495 --> 00:44:17.870
汉字各占三个字节。

00:44:17.870 --> 00:44:21.890
所以实际上你必须-只有当你

00:44:21.890 --> 00:44:27.100
实际上合并了几个通用序列器的字节。

00:44:27.100 --> 00:44:30.600
可以。更具体地说，

00:44:30.600 --> 00:44:32.040
嗯，这是怎么回事？

00:44:32.040 --> 00:44:37.110
所以我们可以从下到上对短序列进行聚类。

00:44:37.110 --> 00:44:40.245
所以我们从一个统一的词汇开始，

00:44:40.245 --> 00:44:44.120
这是所有的Unicode字符和一些数据。

00:44:44.120 --> 00:44:49.310
然后我们会问，这里最频繁的NGRAM是什么？

00:44:49.310 --> 00:44:53.985
嗯，最初它将是一个双内存对，我们将其添加到词汇表中。

00:44:53.985 --> 00:44:56.610
所以如果我们开始，你知道，

00:44:56.610 --> 00:44:59.670
我们可以记下我们的课文，嗯，

00:44:59.670 --> 00:45:00.810
我马上就回来。

00:45:00.810 --> 00:45:05.895
假设我们有一个被划分为单词的文本，所以我们确实有单词标记。

00:45:05.895 --> 00:45:11.670
所以我们可以用字典来表示，这里有一些单词，它们的频率。

00:45:11.670 --> 00:45:17.985
嗯，现在我们找一个普通的字母序列，然后我们说，“哦，是的。”

00:45:17.985 --> 00:45:20.610
发生了九次，嗯，

00:45:20.610 --> 00:45:25.250
在这个数据中，因为我们有左边单词的计数。

00:45:25.250 --> 00:45:30.600
所以，嗯，我们从词汇表开始，所有的字母都是独立的。

00:45:30.600 --> 00:45:34.515
我们发现了一个最常见的字母序列，如es，

00:45:34.515 --> 00:45:40.245
所以我们说，“让我们把它聚集在一起，在我们的词汇表中创造一个新的东西。”

00:45:40.245 --> 00:45:43.185
所以现在我们的词汇表中有一个额外的东西。

00:45:43.185 --> 00:45:47.340
现在最常见的聚集物的NGRAM序列是什么？

00:45:47.340 --> 00:45:51.030
好吧，实际上所有这些e后面都是t，

00:45:51.030 --> 00:45:52.650
所以我们也有ES，

00:45:52.650 --> 00:45:55.065
t频率为9，

00:45:55.065 --> 00:45:58.050
所以我们可以把它添加到我们的词汇表中。

00:45:58.050 --> 00:45:59.670
然后我们再问一次，

00:45:59.670 --> 00:46:03.030
另一个常见的字母序列是什么？

00:46:03.030 --> 00:46:06.825
让我们看看，有七箱O型双孔，

00:46:06.825 --> 00:46:10.170
我想有7例是左眼还是左眼，

00:46:10.170 --> 00:46:13.380
所以我们可以先把它们包起来然后再包起来

00:46:13.380 --> 00:46:17.795
再说一次，如果我们运行这个，

00:46:17.795 --> 00:46:22.410
我们开始构建这些公共字母序列的簇，

00:46:22.410 --> 00:46:26.205
像EST这样的普通数据，

00:46:26.205 --> 00:46:28.450
但也只是普通的词语，

00:46:28.450 --> 00:46:31.470
用英语写的那种东西很快就会

00:46:31.470 --> 00:46:35.130
聚集在一起成为我们词汇的一个单位。

00:46:35.130 --> 00:46:38.300
嗯，所以我们做了一段时间。

00:46:38.300 --> 00:46:40.650
所以通常我们做的是决定

00:46:40.650 --> 00:46:44.250
我们要使用的词汇大小。我们说，“好吧。

00:46:44.250 --> 00:46:47.400
我想使用8000个单词的词汇。”

00:46:47.400 --> 00:46:49.905
那就意味着我的模型会很快，

00:46:49.905 --> 00:46:54.255
我们只是一直这样做，直到我们有8000个词汇。

00:46:54.255 --> 00:46:56.900
这意味着我们的词汇将包含在其中

00:46:56.900 --> 00:47:00.650
所有的单字母，因为我们从它们开始，它会

00:47:00.650 --> 00:47:07.285
拥有我们词汇表中常见的词序，如es和est，

00:47:07.285 --> 00:47:11.420
但也要有完整的单词，只要有共同的单词，比如，你知道，

00:47:11.420 --> 00:47:13.280
还有，还有，

00:47:13.280 --> 00:47:16.835
等等，将成为我们词汇表的一部分。

00:47:16.835 --> 00:47:21.000
嗯，所以当我们有一段文字的时候，我们可以做

00:47:21.000 --> 00:47:25.125
确定的最长分词法，

00:47:25.125 --> 00:47:28.310
我们会说这是我们的单词片段集。

00:47:28.310 --> 00:47:30.900
所以对于输入的文本，

00:47:30.900 --> 00:47:32.700
我们变成文字碎片，

00:47:32.700 --> 00:47:37.695
然后我们就在机器翻译系统中运行它，就好像我们在用词一样，

00:47:37.695 --> 00:47:39.900
但事实上，这只是一句话，

00:47:39.900 --> 00:47:42.000
然后在输出端，

00:47:42.000 --> 00:47:45.845
我们只是根据需要将它们重新连接在一起。

00:47:45.845 --> 00:47:49.875
可以。所以我们得到了这种基于单词的自动系统。

00:47:49.875 --> 00:47:53.525
事实证明这是一个非常成功的系统。

00:47:53.525 --> 00:47:57.525
因此，使用字节对编码的想法实际上是

00:47:57.525 --> 00:48:02.115
出现在2015年，然后在2016年，

00:48:02.115 --> 00:48:06.480
机器翻译讲习班一直是每年的主要竞争对手

00:48:06.480 --> 00:48:12.300
多个顶级系统都是使用字节对编码构建的。

00:48:12.300 --> 00:48:14.700
如果你看看去年的比赛，

00:48:14.700 --> 00:48:16.350
有更多的种类，

00:48:16.350 --> 00:48:21.044
但实际上许多顶级系统仍在使用字节对编码，

00:48:21.044 --> 00:48:24.345
这是一个很好的方法。

00:48:24.345 --> 00:48:29.130
因此，对于谷歌的神经机器翻译，

00:48:29.130 --> 00:48:33.450
它们有效地使用了一种字节对编码的变体。

00:48:33.450 --> 00:48:37.160
所以他们不使用完全相同的算法，嗯，

00:48:37.160 --> 00:48:40.260
他们使用稍微不同的算法

00:48:40.260 --> 00:48:44.010
使用语言模型，他们说，

00:48:44.010 --> 00:48:47.285
什么-什么-而不仅仅是使用纯计数，

00:48:47.285 --> 00:48:52.575
他们说，“聚集在一起能最大限度地减少

00:48:52.575 --> 00:48:58.410
我的语言模式很困惑，把这些东西一窝蜂地重复一遍？”

00:48:58.410 --> 00:49:02.280
所以他们做了-他们做了两个版本的模型。

00:49:02.280 --> 00:49:07.170
所以第一个版本，写字板模型有点像，嗯，

00:49:07.170 --> 00:49:11.585
字节对编码假定您具有初始标记化技术

00:49:11.585 --> 00:49:16.035
对单词，然后你就有了一些单词，

00:49:16.035 --> 00:49:17.880
嗯，用这个算法。

00:49:17.880 --> 00:49:20.790
然后他们做了第二个版本，嗯，

00:49:20.790 --> 00:49:25.050
你可以在这个Github网站上找到的sentencepiece模型，它说，“好吧，

00:49:25.050 --> 00:49:28.560
如果我们需要先把它标记成单词，那就有问题了。

00:49:28.560 --> 00:49:30.208
因为我们需要一个标记器

00:49:30.208 --> 00:49:32.550
每一种语言，这是一项很大的工作。”

00:49:32.550 --> 00:49:34.545
嗯，也许不是那样，

00:49:34.545 --> 00:49:36.825
我们可以好好享受一下，

00:49:36.825 --> 00:49:39.080
从字符序列开始，

00:49:39.080 --> 00:49:44.855
保留空白并将其视为聚集过程的一部分，

00:49:44.855 --> 00:49:47.045
所以，嗯，

00:49:47.045 --> 00:49:50.015
你只需要把你的文字拼凑起来

00:49:50.015 --> 00:49:54.485
通常一边或另一边有空间，嗯，

00:49:54.485 --> 00:49:57.710
因为一个词里面的东西通常是

00:49:57.710 --> 00:50:01.680
更常见的-更常见的团块，你建立起来，

00:50:01.680 --> 00:50:04.755
这被证明是相当成功的。

00:50:04.755 --> 00:50:10.530
嗯，尤其是你们中的一些人可能看到这个的地方，

00:50:10.530 --> 00:50:14.375
嗯，是的，嗯，我们还没有在课堂上描述它，真的，

00:50:14.375 --> 00:50:19.950
但是我们下个星期在课堂上讨论的最近的工作是

00:50:19.950 --> 00:50:22.980
尤其是建立这些变压器模型，

00:50:22.980 --> 00:50:26.160
谷歌发布了这个伯特模型

00:50:26.160 --> 00:50:29.460
很好，嗯，单词表示法。

00:50:29.460 --> 00:50:32.835
如果你下载伯特并尝试使用它，

00:50:32.835 --> 00:50:36.615
你会发现它不会在文字上起作用，

00:50:36.615 --> 00:50:40.050
它对单词片段进行操作。

00:50:40.050 --> 00:50:43.260
嗯，所以它的词汇量很大。

00:50:43.260 --> 00:50:46.470
它不是一个8000字的词汇。

00:50:46.470 --> 00:50:47.685
我忘了号码，

00:50:47.685 --> 00:50:50.640
但是这些模型有大量的词汇，

00:50:50.640 --> 00:50:55.380
但他们仍然不是一个庞大的词汇表，它使用的是单词片段。

00:50:55.380 --> 00:50:57.450
所以词汇表中有很多单词。

00:50:57.450 --> 00:50:59.070
所以如果你看看英国模特，

00:50:59.070 --> 00:51:01.140
它不仅有F这样的词，

00:51:01.140 --> 00:51:04.470
但它甚至还有费尔法克斯和1910年代这样的词，

00:51:04.470 --> 00:51:06.045
这不常见。

00:51:06.045 --> 00:51:10.245
嗯，但它仍然包括所有的文字，

00:51:10.245 --> 00:51:12.660
它再次使用了这个写字板的想法。

00:51:12.660 --> 00:51:16.075
所以如果我想用hypatia这个词来表示，嗯，

00:51:16.075 --> 00:51:17.895
这在词汇表中没有，

00:51:17.895 --> 00:51:19.800
所以我把它拼成了碎片。

00:51:19.800 --> 00:51:21.795
有一个H代表，

00:51:21.795 --> 00:51:24.110
在伯特版本中，

00:51:24.110 --> 00:51:26.880
与Google NMT版本不同，

00:51:26.880 --> 00:51:32.625
非初始单词片段在开头用两个哈希表示，

00:51:32.625 --> 00:51:36.944
so I can put that together with h##yp etc.,

00:51:36.944 --> 00:51:40.200
这就是我对催眠的描述。

00:51:40.200 --> 00:51:43.245
有效地说，我有词向量，嗯，

00:51:43.245 --> 00:51:45.435
对于四个单词的片段，

00:51:45.435 --> 00:51:47.880
然后我要想办法处理它们。

00:51:47.880 --> 00:51:51.600
最简单也很常见的方法是平均四个。

00:51:51.600 --> 00:51:53.340
很明显还有其他事情你可以做。

00:51:53.340 --> 00:51:56.100
你可以用convnet和maxpool或者运行

00:51:56.100 --> 00:52:00.640
一个小的lstm或一些东西来组成一个表示。

00:52:00.840 --> 00:52:07.220
可以。是啊。所以-那些模型，嗯，

00:52:07.220 --> 00:52:10.530
有点像是，用文字来表达

00:52:10.530 --> 00:52:14.835
无限的词汇表，并通过正常的系统运行它们。

00:52:14.835 --> 00:52:18.335
另一种可能是说，“好吧，

00:52:18.335 --> 00:52:22.530
我们想用字符来处理无限的词汇，

00:52:22.530 --> 00:52:27.945
但我们会把它们整合到一个更大的系统中。”

00:52:27.945 --> 00:52:30.540
一大堆工作已经完成了

00:52:30.540 --> 00:52:33.990
从某种意义上说，这是一件相当明显的事情。

00:52:33.990 --> 00:52:38.730
嗯，所以2014年的这项工作是早期的工作之一。

00:52:38.730 --> 00:52:40.005
所以他们说，“好吧，

00:52:40.005 --> 00:52:42.165
我们可以从角色开始。

00:52:42.165 --> 00:52:47.640
我们可以对字符进行卷积以生成单词嵌入，

00:52:47.640 --> 00:52:53.250
然后我们可以在更高层次的模型中使用这些单词嵌入。”

00:52:53.250 --> 00:52:58.215
嗯，这实际上是一种固定窗口模型，用于做部分语音标记。

00:52:58.215 --> 00:53:00.240
嗯，这是有道理的。

00:53:00.240 --> 00:53:01.935
不是卷积，

00:53:01.935 --> 00:53:03.690
你可以用LSTM。

00:53:03.690 --> 00:53:06.720
所以这是一年后的工作，

00:53:06.720 --> 00:53:07.835
他们说，“好吧，

00:53:07.835 --> 00:53:09.795
我们也会建立，嗯，

00:53:09.795 --> 00:53:12.245
字符的单词表示。

00:53:12.245 --> 00:53:17.520
我们要做的是运行字符级的bi lstms，

00:53:17.520 --> 00:53:20.044
连接两个最终状态，

00:53:20.044 --> 00:53:23.430
我们称之为对外代表，

00:53:23.430 --> 00:53:27.750
然后我们要把这个词的表示法

00:53:27.750 --> 00:53:34.890
语言模型，它是一个更高级别的LSTM，沿单词序列工作。”

00:53:34.890 --> 00:53:37.785
我以为我会-哦，是的。

00:53:37.785 --> 00:53:41.400
语言，是不是在训练，呃，像性格-

00:53:41.400 --> 00:53:44.130
是啊。哦，是的，这很重要。

00:53:44.130 --> 00:53:50.550
是的，所以是的，所以如果你在学习-你会学习-我的意思是这是隐藏层。

00:53:50.550 --> 00:53:54.150
我想我不是在显示输入层，而是在显示输入层

00:53:54.150 --> 00:53:58.260
你在为每个角色学习向量。

00:53:58.260 --> 00:54:01.800
所以实际上你做的和我们看到的一样

00:54:01.800 --> 00:54:07.545
在此之前，您将从每个字符的随机表示开始。

00:54:07.545 --> 00:54:13.590
你把它嵌入了一个单词序列lstm中，

00:54:13.590 --> 00:54:20.160
您的目标是将更高级别的LSTM的困惑降至最低，

00:54:20.160 --> 00:54:26.100
嗯，作为一种语言模型，它过滤掉了它的梯度。

00:54:26.100 --> 00:54:30.450
所以它想提出字符向量，如果它

00:54:30.450 --> 00:54:35.860
产生好的词向量，产生低，嗯，困惑。

00:54:35.900 --> 00:54:41.220
好问题。嗯，这是，嗯，

00:54:41.220 --> 00:54:44.820
稍微复杂一点的尝试

00:54:44.820 --> 00:54:48.720
做这件事，这是最近的事了，我们能再建立一个想法吗？

00:54:48.720 --> 00:54:53.505
一个好的语言模型，从字符开始

00:54:53.505 --> 00:54:59.010
希望挖掘出相关子词和稀有词的种类。

00:54:59.010 --> 00:55:02.760
所以他们建造了这种

00:55:02.760 --> 00:55:07.815
这个更为复杂的模型，我们将经历几个阶段

00:55:07.815 --> 00:55:11.805
我们从一个用字符表示的词开始。

00:55:11.805 --> 00:55:14.700
我们有自己构建的角色嵌入

00:55:14.700 --> 00:55:17.910
进入一个卷积网络，然后我们向上走。

00:55:17.910 --> 00:55:20.354
所以如果我们一次拿一块，

00:55:20.354 --> 00:55:24.420
嗯，每个字符都嵌入了一个字符。

00:55:24.420 --> 00:55:28.680
嗯，你会得到一个卷积层

00:55:28.680 --> 00:55:33.540
然后是代表，有不同的过滤器来处理这些问题，

00:55:33.540 --> 00:55:37.800
嗯，两、三和四克字符的字符序列。

00:55:37.800 --> 00:55:41.340
所以你得到了部分单词的表示。

00:55:41.340 --> 00:55:48.600
嗯，然后从这些卷积网络中，你将在一段时间内做最大的池

00:55:48.600 --> 00:55:51.510
实际上有点像选择

00:55:51.510 --> 00:55:56.155
这些n克最能代表一个词的意思。

00:55:56.155 --> 00:56:00.350
嗯，那他们在那之后做的就是

00:56:00.350 --> 00:56:05.030
它们有一个字符n-gram的输出表示，

00:56:05.030 --> 00:56:12.095
然后他们把它输入公路网，就像我们上次讨论的那样。

00:56:12.095 --> 00:56:16.985
然后在字级输出，

00:56:16.985 --> 00:56:20.405
嗯，进入一个LSTM网络，

00:56:20.405 --> 00:56:24.365
这个lstm网络现在是单词级lstm网络，

00:56:24.365 --> 00:56:27.710
你在尝试最大化

00:56:27.710 --> 00:56:32.365
困惑就像我们之前看到的神经语言模型。

00:56:32.365 --> 00:56:35.385
嗯，他们能用这个展示什么？

00:56:35.385 --> 00:56:38.910
好吧，他们能展示的第一件事就是

00:56:38.910 --> 00:56:43.950
事实上，尽管存在这种怀疑，但作为一种语言模型，它仍然可以很好地工作。

00:56:43.950 --> 00:56:46.800
我没有告诉你

00:56:46.800 --> 00:56:50.040
问题是你可以建立这些角色级别的模型

00:56:50.040 --> 00:56:56.535
训练他们，他们工作到一级近似和单词级语言模型。

00:56:56.535 --> 00:56:59.790
但是他们的观察之一是你可以

00:56:59.790 --> 00:57:03.840
取得了同样好的效果，但模型要小得多。

00:57:03.840 --> 00:57:05.235
所以在上面这是

00:57:05.235 --> 00:57:10.695
字符级的lstm模型和它们所构建的单词模型。

00:57:10.695 --> 00:57:14.460
这里有一大堆模型在这个数据集上。

00:57:14.460 --> 00:57:19.410
嗯，随着时间的推移，困惑也在减少，

00:57:19.410 --> 00:57:23.790
到了78.4，他们的观点很好，我们可以

00:57:23.790 --> 00:57:29.100
建立一个相当好的角色模型，78.9令人困惑，但

00:57:29.100 --> 00:57:32.730
我们的模型实际上小得多，这里的模型

00:57:32.730 --> 00:57:35.760
5200万个参数，而我们的模型

00:57:35.760 --> 00:57:39.240
在字符级别上工作的参数只有1900万。

00:57:39.240 --> 00:57:41.685
所以大约是40%的大小。

00:57:41.685 --> 00:57:45.210
这看起来有点有趣。

00:57:45.210 --> 00:57:51.510
但也许更有趣的是从里面偷看

00:57:51.510 --> 00:57:55.500
出现在单词的表示中

00:57:55.500 --> 00:57:59.745
这部分是由角色组成的，实际上有点酷。

00:57:59.745 --> 00:58:06.064
嗯，所以这是为了显示那些最高级的单词，

00:58:06.064 --> 00:58:08.235
他的，你，理查德，交易。

00:58:08.235 --> 00:58:11.520
它是在问什么是最重要的词

00:58:11.520 --> 00:58:15.510
根据计算出的单词表示，与之类似。

00:58:15.510 --> 00:58:21.480
最上面的部分是单词级lstm模型的输出，这有点好。

00:58:21.480 --> 00:58:23.550
理查德和乔纳森长得很像，

00:58:23.550 --> 00:58:25.515
罗伯特、尼尔和南希等。

00:58:25.515 --> 00:58:30.810
虽然分秒必争，但基本上还行。

00:58:30.810 --> 00:58:35.730
但有趣的是，他们的角色级别模型发生了什么，嗯，

00:58:35.730 --> 00:58:37.605
尤其是，嗯，

00:58:37.605 --> 00:58:41.910
有意思的是，首先你记得的是他们

00:58:41.910 --> 00:58:46.845
字符嵌入通过卷积层和最大池。

00:58:46.845 --> 00:58:51.150
如果那时你问什么是最重要的

00:58:51.150 --> 00:58:55.830
基本上，它还记得关于人物的事情。

00:58:55.830 --> 00:58:58.350
所以最相似的词是while，

00:58:58.350 --> 00:59:01.740
智利，整个，同时和白人。

00:59:01.740 --> 00:59:06.600
所以，至少在前一种情况下，它们都以le结尾。

00:59:06.600 --> 00:59:10.560
你在理查德附近的其他地方看到了这种模式，

00:59:10.560 --> 00:59:15.630
硬的，富的，富的，富的，这硬的结束在ARD，富的。

00:59:15.630 --> 00:59:18.390
所以你只是得到了字符序列的相似性，

00:59:18.390 --> 00:59:20.595
这根本没有意义。

00:59:20.595 --> 00:59:25.500
但有趣的是，当他们穿越公路时，

00:59:25.500 --> 00:59:29.775
公路层正在成功地学习如何

00:59:29.775 --> 00:59:33.285
转换这些字符序列表示

00:59:33.285 --> 00:59:35.685
去捕捉意义。

00:59:35.685 --> 00:59:38.850
所以如果你说在输出

00:59:38.850 --> 00:59:44.295
公路分层了哪些单词最相似，然后似乎工作得很好，

00:59:44.295 --> 00:59:49.245
理查德和爱德华、杰拉德、爱德华和卡尔很相似。

00:59:49.245 --> 00:59:51.750
他们现在的工作方式更像

00:59:51.750 --> 00:59:55.755
用于捕获语义相似性的词级模型。

00:59:55.755 --> 00:59:57.900
所以这看起来有点酷。

00:59:57.900 --> 01:00:02.070
嗯，那么他们会说如果我们问

01:00:02.070 --> 01:00:06.555
不在模型词汇表中的单词。

01:00:06.555 --> 01:00:09.150
如果它们不在模型的词汇表中，

01:00:09.150 --> 01:00:13.890
单词级别的模型不能做任何事情，所以这就是为什么你会有这些破折号。

01:00:13.890 --> 01:00:16.650
他们想展示的是

01:00:16.650 --> 01:00:19.470
字符级模型仍然工作得很好。

01:00:19.470 --> 01:00:23.310
所以如果你看的时候中间有7个O

01:00:23.310 --> 01:00:27.300
它正确地决定了你的表情，

01:00:27.300 --> 01:00:29.370
看，看，实际上看

01:00:29.370 --> 01:00:33.150
最相似的词实际上是非常好用的。

01:00:33.150 --> 01:00:36.375
其他一些例子类似，计算机辅助，

01:00:36.375 --> 01:00:38.865
被视为最类似于计算机引导，

01:00:38.865 --> 01:00:44.685
计算机驱动，计算机化，计算机，你得到了类似的合理结果。

01:00:44.685 --> 01:00:47.010
然后上面的小图片，

01:00:47.010 --> 01:00:49.755
嗯，是的，嗯，

01:00:49.755 --> 01:00:56.235
显示，嗯，这些二维可视化的单位已经学习。

01:00:56.235 --> 01:00:58.770
所以红色，

01:00:58.770 --> 01:01:02.760
红色的东西是单词前缀，

01:01:02.760 --> 01:01:05.550
蓝色的东西是字符后缀，

01:01:05.550 --> 01:01:09.180
橙色的东西是连字符的东西

01:01:09.180 --> 01:01:13.050
就像在电脑的引导下，灰色是一切。

01:01:13.050 --> 01:01:14.790
所以有某种感觉，

01:01:14.790 --> 01:01:19.050
用它来挑选单词的不同重要部分。

01:01:19.050 --> 01:01:26.880
可以。嗯，这就是为什么我也猜这只是另一个很好的例子

01:01:26.880 --> 01:01:30.870
组合不同类型的构建基块

01:01:30.870 --> 01:01:32.670
制作更强大的模型

01:01:32.670 --> 01:01:35.415
还想为你的最终项目考虑。

01:01:35.415 --> 01:01:37.450
可以。

01:01:45.200 --> 01:01:46.770
嗯，这是

01:01:46.770 --> 01:01:51.135
回到神经机器翻译系统的另一个例子

01:01:51.135 --> 01:01:55.995
这是一个混合体系结构，具有字级和字符级。

01:01:55.995 --> 01:01:59.400
我之前给你看了一个纯粹的字符级模型。

01:01:59.400 --> 01:02:02.580
我是说我们是为了看

01:02:02.580 --> 01:02:05.940
它做得多好，但我们真的有点想建造

01:02:05.940 --> 01:02:09.810
一个混合模型，因为它看起来更实用

01:02:09.810 --> 01:02:14.265
建立一个翻译相对快速和良好的东西。

01:02:14.265 --> 01:02:16.860
嗯，所以我们的想法是主要建造

01:02:16.860 --> 01:02:20.550
一种词级神经机器翻译系统

01:02:20.550 --> 01:02:26.970
当我们有罕见或看不见的单词时，能够处理字符级的东西。

01:02:26.970 --> 01:02:30.090
嗯，结果很好，

01:02:30.090 --> 01:02:33.000
嗯，成功地提高了性能。

01:02:33.000 --> 01:02:35.355
所以这个模型的想法是这样的。

01:02:35.355 --> 01:02:40.575
嗯，我们会有一个很好的标准，嗯，

01:02:40.575 --> 01:02:47.100
注意顺序LSTM神经机器翻译系统。

01:02:47.100 --> 01:02:51.900
在我的照片中-我的意思是，它实际上是一个四级深度的系统，但在我的照片中

01:02:51.900 --> 01:02:56.490
显示了不到四个层次的堆叠，使它更容易看到的东西。

01:02:56.490 --> 01:03:01.920
我们将用16000个词的合理词汇来运行这个程序。

01:03:01.920 --> 01:03:07.995
因此，对于普通单词，我们只需要输入单词表示

01:03:07.995 --> 01:03:13.320
我们的神经机器翻译模型，但是对于那些不在词汇表中的单词

01:03:13.320 --> 01:03:19.530
通过使用字符级lstm来为它们计算单词表示，

01:03:19.530 --> 01:03:23.130
相反，当我们开始在

01:03:23.130 --> 01:03:28.710
另一方面，我们有一个词汇量为16000的软最大值。

01:03:28.710 --> 01:03:34.275
它可以生成类似[噪音]的单词，但其中一个单词是unk符号。

01:03:34.275 --> 01:03:38.610
如果它生成UNK符号，我们就运行

01:03:38.610 --> 01:03:42.060
这个隐藏的表示并将其作为

01:03:42.060 --> 01:03:47.370
最初输入到字符级别lstm，然后我们得到字符级别

01:03:47.370 --> 01:03:51.390
lstm生成一个字符序列，直到它生成

01:03:51.390 --> 01:03:56.650
一个停止符号，我们用它来生成单词。嗯

01:03:57.240 --> 01:04:02.080
可以。所以我们最终会变成这样

01:04:02.080 --> 01:04:08.170
由八个LSTM层组成的混合叠层。嗯，是的。

01:04:08.170 --> 01:04:12.690
[听不见]你总能得到一些未知符号的概率。

01:04:12.690 --> 01:04:15.180
如果你想得到合适的梯度，

01:04:15.180 --> 01:04:19.230
你-你总是要逐字逐句，但你-你做什么？

01:04:19.230 --> 01:04:22.870
我常说，你只在训练时跑步，

01:04:22.870 --> 01:04:28.990
只有当UNK符号收到最大可能性时，才能运行字符级LSTM。

01:04:28.990 --> 01:04:29.905
所以我们-

01:04:29.905 --> 01:04:30.685
那是什么？

01:04:30.685 --> 01:04:34.335
所以在训练的时候，在训练的时候，

01:04:34.335 --> 01:04:36.510
有一个决定技术的因素，对吧？

01:04:36.510 --> 01:04:39.705
你知道源头，也知道目标，

01:04:39.705 --> 01:04:43.740
所以在训练的时候，

01:04:43.740 --> 01:04:46.230
我们已经决定了词汇量，对吧？

01:04:46.230 --> 01:04:51.400
我们刚刚决定了15999个最常见的词是什么，

01:04:51.400 --> 01:04:53.950
那些和unk是我们的词汇。

01:04:53.950 --> 01:04:57.115
所以对于输入端和输出端，

01:04:57.115 --> 01:05:00.895
我们知道哪些单词不在我们的词汇表中。

01:05:00.895 --> 01:05:03.400
所以如果不是我们的词汇，

01:05:03.400 --> 01:05:04.840
我们负责这个。

01:05:04.840 --> 01:05:07.450
如果输出的内容不在我们的词汇表中，

01:05:07.450 --> 01:05:11.950
我们正在运行那个，否则我们根本就不运行它，是的。

01:05:11.950 --> 01:05:18.130
所以我没有解释但实际上很重要的一点也许

01:05:18.130 --> 01:05:23.920
相关的，比如当我们计算一个我们可以收回的损失时

01:05:23.920 --> 01:05:26.200
传播，在这上面，

01:05:26.200 --> 01:05:28.120
有两种损失。

01:05:28.120 --> 01:05:32.920
你知道在这个职位上你会有一个损失，

01:05:32.920 --> 01:05:36.085
给概率1生成UNK，但实际上，

01:05:36.085 --> 01:05:41.470
这个模型，我们会说，unk，你知道概率是0.2或其他什么。

01:05:41.470 --> 01:05:44.515
所以会有损失，其次，

01:05:44.515 --> 01:05:48.400
有一个特定的字符序列，你想生成，你也有

01:05:48.400 --> 01:05:53.410
因为你已经达到了你对角色的概率而造成的损失。

01:05:53.410 --> 01:05:55.825
嗯，那么，嗯，

01:05:55.825 --> 01:05:58.570
我想我们-我想艾比有点简单地提到过这个。

01:05:58.570 --> 01:06:01.810
通常，解码器会

01:06:01.810 --> 01:06:06.310
在决定前考虑不同可能性的波束搜索，

01:06:06.310 --> 01:06:10.795
嗯，一个单词序列中概率最高的一个。

01:06:10.795 --> 01:06:13.930
所以这是一个稍微复杂一点的版本。

01:06:13.930 --> 01:06:17.830
所以在运行时会有一个字级的波束搜索，然后

01:06:17.830 --> 01:06:23.335
同时进行字符级波束搜索以考虑不同的可能性。

01:06:23.335 --> 01:06:27.235
所以如果你想把这两个整合在一起。

01:06:27.235 --> 01:06:29.785
嗯，但本质上，嗯，

01:06:29.785 --> 01:06:32.980
这很管用。

01:06:32.980 --> 01:06:40.510
嗯，所以，嗯，这是WMT 2015的制胜系统，

01:06:40.510 --> 01:06:43.540
使用了30倍的数据并整合在一起

01:06:43.540 --> 01:06:48.115
与为任务提供的数据相比，还有三个系统。

01:06:48.115 --> 01:06:52.675
这是我之前展示的系统，他们得到了18.3分。

01:06:52.675 --> 01:06:54.940
嗯，如果你还记得我们的性格，

01:06:54.940 --> 01:06:58.375
纯字符级系统得到18.5。

01:06:58.375 --> 01:07:02.500
嗯，然后通过构建这个混合系统，

01:07:02.500 --> 01:07:07.600
我们能够建立一个更好的系统，比2.5分更好，

01:07:07.600 --> 01:07:12.805
嗯，比这个词级或字符级系统之后。

01:07:12.805 --> 01:07:15.070
所以这很好，嗯，

01:07:15.070 --> 01:07:17.725
尤其是那是当时最先进的技术。

01:07:17.725 --> 01:07:21.355
当然，如果你密切关注，

01:07:21.355 --> 01:07:24.430
现在这还远不是最先进的。

01:07:24.430 --> 01:07:30.370
因为当我向你展示谷歌系统之前的幻灯片时，

01:07:30.370 --> 01:07:33.310
你会发现他们

01:07:33.310 --> 01:07:37.600
20年代的数字要高得多，但随着时间的推移，情况就这样了。

01:07:37.600 --> 01:07:39.490
嗯，好吧。

01:07:39.490 --> 01:07:41.800
但这里有一个例子

01:07:41.800 --> 01:07:46.300
这些不同的系统在工作，他们犯了一些错误。

01:07:46.300 --> 01:07:48.775
嗯，这里有一个樱桃采摘的例子，嗯，

01:07:48.775 --> 01:07:51.940
我们的系统，混合动力系统，

01:07:51.940 --> 01:07:54.985
工作得很好，因为-这就是你希望看到的。

01:07:54.985 --> 01:07:57.685
嗯，所以，你知道，

01:07:57.685 --> 01:08:02.260
你可以看到一些可能出错的地方。

01:08:02.260 --> 01:08:05.155
嗯，在这种情况下，

01:08:05.155 --> 01:08:09.550
你知道字符级系统在这里不起作用，因为它只是排序

01:08:09.550 --> 01:08:15.250
从史蒂芬开始，这似乎是自由联想，

01:08:15.250 --> 01:08:19.690
嗯，一个完全虚构的名字，与消息来源没有任何关系。

01:08:19.690 --> 01:08:22.570
所以那个不太好。

01:08:22.570 --> 01:08:27.595
嗯，词级系统在这里爆炸了，

01:08:27.595 --> 01:08:30.340
所以你记得当它产生一个UNK时，

01:08:30.340 --> 01:08:36.400
这个词级系统在生成时会有，它使用的是注意力。

01:08:36.400 --> 01:08:38.890
所以当它想产生，嗯，

01:08:38.890 --> 01:08:42.010
它把注意力放回单词和词源上。

01:08:42.010 --> 01:08:45.070
当它产生时，UNK有两种策略。

01:08:45.070 --> 01:08:50.410
它可以对它最大限度地

01:08:50.410 --> 01:08:56.140
注意，或者它可以复制它最大限度地注意的词。

01:08:56.140 --> 01:08:57.790
嗯，在这种情况下，

01:08:57.790 --> 01:09:02.890
它选择翻译它最大限度地引起注意的那个词，而不是它的那个词。

01:09:02.890 --> 01:09:08.080
最大限度的关注是诊断后而不是诊断后。

01:09:08.080 --> 01:09:11.290
所以你只要把这个订单从后面拿出来，

01:09:11.290 --> 01:09:14.140
之后，我们完全失去了这个词。

01:09:14.140 --> 01:09:17.965
在这个例子中，在这个例子中，

01:09:17.965 --> 01:09:20.065
混合动力系统，嗯，

01:09:20.065 --> 01:09:24.370
最后工作得很漂亮，给你准确的翻译。

01:09:24.370 --> 01:09:26.740
是啊。嗯，当然，

01:09:26.740 --> 01:09:29.035
现实世界并不总是那么好。

01:09:29.035 --> 01:09:31.000
嗯，这里有一个不同的例子。

01:09:31.000 --> 01:09:35.455
这就是我之前展示给11岁女儿的例子。

01:09:35.455 --> 01:09:40.420
在这个例子中，

01:09:40.420 --> 01:09:44.350
混合模型与字符模型具有相同的强度。

01:09:44.350 --> 01:09:50.065
在翻译过程中，它在一个字符级别上正确地产生了11岁的年龄，

01:09:50.065 --> 01:09:52.300
但你知道这一次，无论出于什么原因，

01:09:52.300 --> 01:09:56.170
这是一个混合动力模型。

01:09:56.170 --> 01:10:01.165
生成名称，并将shani bart翻译为graham bart。

01:10:01.165 --> 01:10:04.015
嗯，而角色级别的模型可以做到这一点。

01:10:04.015 --> 01:10:06.700
嗯，实际上，我认为这是

01:10:06.700 --> 01:10:10.120
该混合模型与字符级模型进行了比较。

01:10:10.120 --> 01:10:13.060
因为字符级生成器

01:10:13.060 --> 01:10:16.670
类似于第二层次。

01:10:17.190 --> 01:10:20.605
对于纯字符级模型，

01:10:20.605 --> 01:10:27.010
它能够非常有效地使用字符序列作为条件上下文。

01:10:27.010 --> 01:10:28.870
而我们的混合动力车型，

01:10:28.870 --> 01:10:32.410
尽管我们提供了

01:10:32.410 --> 01:10:34.450
AS中的字级模型

01:10:34.450 --> 01:10:37.855
字符级模型的起始隐藏表示，

01:10:37.855 --> 01:10:40.570
它没有进一步的代表性

01:10:40.570 --> 01:10:43.615
比单词级模型中的要早。

01:10:43.615 --> 01:10:47.695
因此，它往往不总是表现得很好，

01:10:47.695 --> 01:10:53.030
捕捉上下文，使其能够进行名称等事物的翻译。

01:10:53.100 --> 01:10:58.510
可以。嗯，差不多完成了，但是

01:10:58.510 --> 01:11:01.120
只是我之前想提的一件事

01:11:01.120 --> 01:11:03.985
最后，这几乎是一个实际的事情。

01:11:03.985 --> 01:11:07.465
嗯，我们从单词嵌入开始，

01:11:07.465 --> 01:11:10.510
但现在我们讨论了很多角色级别的模型。

01:11:10.510 --> 01:11:15.760
所以，当然，仅仅为了嵌入文字，你应该能够用它们做有用的事情，

01:11:15.760 --> 01:11:18.880
用文字或词条。

01:11:18.880 --> 01:11:21.340
这是人们开始玩的东西。

01:11:21.340 --> 01:11:24.790
所以在曹操和赖报上他们说得很好

01:11:24.790 --> 01:11:30.280
让我们用完全相同的方法训练word2vec模型，

01:11:30.280 --> 01:11:34.495
损失正如word2vec使用的，但让我们，

01:11:34.495 --> 01:11:37.795
嗯，不是用词来表达，

01:11:37.795 --> 01:11:42.160
让我们从字符序列开始，然后运行

01:11:42.160 --> 01:11:47.425
一个双向的lstm来计算单词表示，

01:11:47.425 --> 01:11:49.720
然后我们将有效地

01:11:49.720 --> 01:11:52.780
在我们学习的地方训练这个更复杂的模型

01:11:52.780 --> 01:12:00.550
字符嵌入和lstm参数，这将给我们提供单词表示。

01:12:00.550 --> 01:12:04.480
这是人们一直在玩的一个想法，

01:12:04.480 --> 01:12:08.845
所以我特别想提到这些FastText嵌入。

01:12:08.845 --> 01:12:10.900
嗯，几年前，

01:12:10.900 --> 01:12:12.820
嗯，现在在Facebook上的人，

01:12:12.820 --> 01:12:16.015
原来是托马斯·米科洛夫写的第二个词，

01:12:16.015 --> 01:12:18.100
推出了一套新的嵌入式系统，

01:12:18.100 --> 01:12:21.340
FastText嵌入，他们的目标是

01:12:21.340 --> 01:12:24.880
有一个新一代的word2vec，嗯，

01:12:24.880 --> 01:12:27.940
这是一种高效的快速，嗯，

01:12:27.940 --> 01:12:30.535
词汇向量学习库，嗯，

01:12:30.535 --> 01:12:35.215
但对于形态丰富的稀有词汇和语言来说，这是更好的选择。

01:12:35.215 --> 01:12:39.010
他们这样做的基本上是把单词2vec跳过

01:12:39.010 --> 01:12:43.585
但他们将其扩充为n-gram字符。

01:12:43.585 --> 01:12:46.450
更准确地说，这就是他们所做的。

01:12:46.450 --> 01:12:49.720
所以，当你有话要说的时候，

01:12:49.720 --> 01:12:52.000
我的例子是，

01:12:52.000 --> 01:12:57.610
对于一些N克大小，您将其表示为一组N克。

01:12:57.610 --> 01:13:01.180
所以这和我提到的电话差不多

01:13:01.180 --> 01:13:05.080
就在你开始有一种边界符号的时候，

01:13:05.080 --> 01:13:06.820
所以你知道单词的开头。

01:13:06.820 --> 01:13:11.965
所以如果长度是3，你就有了wh，whe，her这个词的开头。

01:13:11.965 --> 01:13:14.500
在此，重新结束，

01:13:14.500 --> 01:13:17.695
作为代表性的一部分。

01:13:17.695 --> 01:13:20.740
然后你就有了一个完整的单词。

01:13:20.740 --> 01:13:24.355
所以在这个模型中仍然有完整的单词表示。

01:13:24.355 --> 01:13:28.980
那么哪里有六个东西代表呢？

01:13:28.980 --> 01:13:34.350
然后你将在你的计算中用到这六件事。

01:13:34.350 --> 01:13:37.260
嗯，如果你还记得

01:13:37.260 --> 01:13:40.590
word2vec你所做的就是你所做的

01:13:40.590 --> 01:13:45.585
这些矢量点积在上下文表示之间

01:13:45.585 --> 01:13:48.240
以及你的中心词表达。

01:13:48.240 --> 01:13:52.080
所以他们会做同样的事情，除了

01:13:52.080 --> 01:13:56.870
中心词，他们会用到这六个向量。

01:13:56.870 --> 01:14:00.400
所有向量都对应于

01:14:00.400 --> 01:14:03.685
这些表示，它们将对它们求和。

01:14:03.685 --> 01:14:06.910
所以你只需要做一个简单的求和运算，

01:14:06.910 --> 01:14:10.765
这就是你相似性的表现。

01:14:10.765 --> 01:14:13.180
嗯，非常准确地说，他们不太会那样做。

01:14:13.180 --> 01:14:15.550
因为有一个散列技巧，但我不提。

01:14:15.550 --> 01:14:21.370
但他们能证明的是，这个模型实际上运行得相当成功。

01:14:21.370 --> 01:14:24.340
所以这些是单词相似度分数，

01:14:24.340 --> 01:14:28.480
跳过克，他们都是cbow，

01:14:28.480 --> 01:14:31.554
这是一种新的模式，

01:14:31.554 --> 01:14:36.925
嗯，也就是说，嗯，使用这种n克。

01:14:36.925 --> 01:14:38.755
在这里，嗯，

01:14:38.755 --> 01:14:40.960
你至少知道其中一个英文数据集，

01:14:40.960 --> 01:14:42.235
情况没有好转。

01:14:42.235 --> 01:14:47.830
嗯，但是他们特别注意到的是，对于语言来说，

01:14:47.830 --> 01:14:52.690
Morp-更多的形态，你得到了一些相当明确的收获。

01:14:52.690 --> 01:14:55.375
70，69到75，

01:14:55.375 --> 01:14:58.765
右栏59、60到66，

01:14:58.765 --> 01:15:02.440
所以这些写字板模型确实给了他们一个更好的

01:15:02.440 --> 01:15:07.330
文字和快速文本，嗯，

01:15:07.330 --> 01:15:12.940
图书馆现在有大约60或70种不同语言的单词嵌入，

01:15:12.940 --> 01:15:16.930
因此，它是多语言应用程序的单词嵌入的一个很好的来源。

01:15:16.930 --> 01:15:19.735
好吧，我想我完了。

01:15:19.735 --> 01:15:23.000
So thanks a lot and see you again next week.

