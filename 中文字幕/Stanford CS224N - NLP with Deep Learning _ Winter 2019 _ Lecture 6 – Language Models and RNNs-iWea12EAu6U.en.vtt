WEBVTT
Kind: captions
Language: en

00:00:05.210 --> 00:00:07.950
大家好。我是艾比，

00:00:07.950 --> 00:00:09.544
我是这个班的助教组长

00:00:09.544 --> 00:00:12.510
我也是斯坦福大学NLP小组的博士生。

00:00:12.510 --> 00:00:14.670
今天我要告诉你

00:00:14.670 --> 00:00:17.040
语言模型和循环神经网络。

00:00:17.040 --> 00:00:19.980
下面是我们今天要做的事情的概述。

00:00:19.980 --> 00:00:24.885
今天，首先，我们将介绍一个新的NLP任务，即语言建模，

00:00:24.885 --> 00:00:29.504
这将激励我们学习新的神经网络家族，

00:00:29.504 --> 00:00:32.730
这就是反复出现的神经网络或RNN。

00:00:32.730 --> 00:00:34.440
所以，我想说这是

00:00:34.440 --> 00:00:37.415
剩下的课程你将学习的最重要的想法。

00:00:37.415 --> 00:00:41.070
所以，我们今天要介绍一些相当酷的材料。

00:00:41.070 --> 00:00:44.305
那么，让我们从语言建模开始。

00:00:44.305 --> 00:00:48.775
语言建模是预测下一个单词的任务。

00:00:48.775 --> 00:00:52.230
所以，根据这段课文，学生们打开了空白处，

00:00:52.230 --> 00:00:56.200
有人能喊出一个你认为下一个可能会出现的词吗？

00:00:58.370 --> 00:00:59.550
目的。[噪音]。

00:00:59.550 --> 00:01:03.840
[重叠]注意，还有什么？我听不太清楚，

00:01:03.840 --> 00:01:06.720
但是，呃，是的，这些都是可能的，对吧？

00:01:06.720 --> 00:01:08.150
所以，这些是我想的，

00:01:08.150 --> 00:01:09.490
学生们可能会开学，呃，

00:01:09.490 --> 00:01:11.200
学生们打开书本，似乎很有可能。

00:01:11.200 --> 00:01:13.280
学生们打开笔记本电脑，

00:01:13.280 --> 00:01:15.040
学生们开始考试，

00:01:15.040 --> 00:01:16.580
学生们敞开心扉，难以置信，

00:01:16.580 --> 00:01:18.700
有人想出了一个，刚才那个，

00:01:18.700 --> 00:01:20.330
嗯，这是开场白的隐喻意义。

00:01:20.330 --> 00:01:23.675
所以，你们现在都在执行语言建模。

00:01:23.675 --> 00:01:25.475
想想下一个词是什么，

00:01:25.475 --> 00:01:27.250
你是一个语言模型。

00:01:27.250 --> 00:01:31.385
这里有一个更正式的语言模型定义。

00:01:31.385 --> 00:01:34.430
给定一个从x1到xt的单词序列，

00:01:34.430 --> 00:01:37.340
语言模型是一种计算

00:01:37.340 --> 00:01:41.200
下一个词的概率分布，xt加1。

00:01:41.200 --> 00:01:44.194
所以，语言模型给出了概率分布，

00:01:44.194 --> 00:01:49.067
条件概率，即x t加1得到的单词。

00:01:49.067 --> 00:01:50.810
这里我们假设，xt加1

00:01:50.810 --> 00:01:53.960
可以是固定词汇V中的任何单词w。

00:01:53.960 --> 00:01:55.520
所以我们假设

00:01:55.520 --> 00:01:58.205
我们正在考虑的预先定义的单词列表。

00:01:58.205 --> 00:02:00.140
通过这种方式，您可以查看语言建模

00:02:00.140 --> 00:02:01.850
作为一种分类任务，

00:02:01.850 --> 00:02:04.580
因为有很多种可能。

00:02:04.580 --> 00:02:09.360
我们称这个系统为语言模型。

00:02:09.850 --> 00:02:12.050
有另一种思维方式

00:02:12.050 --> 00:02:13.715
关于语言模型。

00:02:13.715 --> 00:02:15.200
你可以想到一个语言模型

00:02:15.200 --> 00:02:19.060
把概率分配给一段文字的系统。

00:02:19.060 --> 00:02:21.470
例如，如果我们有一段文字，

00:02:21.470 --> 00:02:23.180
x到x大写t，

00:02:23.180 --> 00:02:25.040
那么，这篇文章的概率

00:02:25.040 --> 00:02:27.830
根据语言模型可以分解。

00:02:27.830 --> 00:02:29.250
所以，根据定义，

00:02:29.250 --> 00:02:31.250
你可以说概率等于，

00:02:31.250 --> 00:02:34.530
所有这些条件概率的乘积。

00:02:34.530 --> 00:02:37.375
还有，呃，里面的表格，

00:02:37.375 --> 00:02:40.480
产品正是语言模型所提供的。

00:02:40.480 --> 00:02:42.465
所以，你可以认为这些东西是等价的。

00:02:42.465 --> 00:02:44.720
预测下一个单词，给你一个系统，

00:02:44.720 --> 00:02:49.110
这可以给出给定文本的概率。

00:02:49.270 --> 00:02:52.745
所以，事实上，你每天都使用语言模型。

00:02:52.745 --> 00:02:56.120
例如，当你在手机上发短信，并且在写信息时，

00:02:56.120 --> 00:02:57.610
如果你有智能手机，

00:02:57.610 --> 00:03:00.080
它将预测你将要说什么词。

00:03:00.080 --> 00:03:01.790
所以，如果你说，嗯，我会在-

00:03:01.790 --> 00:03:04.250
你的电话可能暗示你是指机场或咖啡馆，

00:03:04.250 --> 00:03:06.025
例如，办公室。

00:03:06.025 --> 00:03:08.905
每天使用语言模型的另一种情况

00:03:08.905 --> 00:03:11.495
就是当你在互联网上搜索一些东西时，比如谷歌，

00:03:11.495 --> 00:03:12.830
然后你开始输入你的查询，

00:03:12.830 --> 00:03:15.952
然后谷歌会尝试为你完成查询，这就是语言建模。

00:03:15.952 --> 00:03:19.020
它在预测下一个词或词会是什么。

00:03:20.480 --> 00:03:23.715
所以，这就是语言模型，

00:03:23.715 --> 00:03:26.675
问题是，你将如何学习语言模型？

00:03:26.675 --> 00:03:29.917
所以，如果我在学前时代问这个问题，

00:03:29.917 --> 00:03:31.730
那是几年前的事了，

00:03:31.730 --> 00:03:35.005
答案是，你将学习一个n-gram语言模型。

00:03:35.005 --> 00:03:38.570
所以，今天我们首先要学习n-gram语言模型。

00:03:38.570 --> 00:03:41.330
所以，在我告诉你N-gram语言模型是什么之前，

00:03:41.330 --> 00:03:43.160
你需要知道N-克是什么。

00:03:43.160 --> 00:03:47.905
因此，根据定义，n-gram是n个连续单词的一块。

00:03:47.905 --> 00:03:50.400
例如，一克或一克，

00:03:50.400 --> 00:03:52.050
只是所有的个别词

00:03:52.050 --> 00:03:55.020
按照“学生打开-

00:03:55.020 --> 00:03:58.810
一个两克或两克的单词都是连续的成对单词块，

00:03:58.810 --> 00:04:00.980
“学生”，“学生打开”，“打开”

00:04:00.980 --> 00:04:04.560
对于三角函数和四克函数等等。

00:04:05.050 --> 00:04:08.575
因此，n-gram语言模型的核心思想

00:04:08.575 --> 00:04:11.035
是为了预测下一个词是什么，

00:04:11.035 --> 00:04:12.815
你要收集一些统计数据，

00:04:12.815 --> 00:04:14.930
关于不同n-gram的频率，

00:04:14.930 --> 00:04:16.490
从某种训练数据中，

00:04:16.490 --> 00:04:18.110
然后你可以用这些统计数据

00:04:18.110 --> 00:04:21.120
预测下一个词可能是什么。

00:04:21.830 --> 00:04:23.640
这里有更多的细节。

00:04:23.640 --> 00:04:26.325
所以，要建立一个n-gram语言模型，

00:04:26.325 --> 00:04:28.490
首先你需要做一个简单的假设，

00:04:28.490 --> 00:04:30.305
这是你的假设。

00:04:30.305 --> 00:04:33.350
你说下一个词xt加1

00:04:33.350 --> 00:04:37.535
只取决于前面的n-1个词。

00:04:37.535 --> 00:04:39.900
所以，我们假设，

00:04:39.900 --> 00:04:41.650
是概率分布，

00:04:41.650 --> 00:04:45.020
xt加1的条件概率给出了它们后面的所有单词，

00:04:45.020 --> 00:04:46.160
我们只是简化一下，

00:04:46.160 --> 00:04:50.485
说它只取决于最后的n-1个词，这就是我们的假设。

00:04:50.485 --> 00:04:53.950
所以，根据条件概率的定义，

00:04:53.950 --> 00:04:55.600
我们可以说这个概率，

00:04:55.600 --> 00:04:58.385
只是两个不同概率的比值。

00:04:58.385 --> 00:05:01.180
所以，在顶部，你有可能

00:05:01.180 --> 00:05:03.220
一个特定的N克，在底部我们有

00:05:03.220 --> 00:05:06.192
得到一个n-1克的概率

00:05:06.192 --> 00:05:08.020
这有点难读，因为所有的上标

00:05:08.020 --> 00:05:11.015
但我要举一个例子，在下一张幻灯片上写下单词。

00:05:11.015 --> 00:05:15.055
可以。这就是下一个词概率的定义，

00:05:15.055 --> 00:05:17.140
但问题仍然存在，我们如何获得

00:05:17.140 --> 00:05:19.980
这些n-gram和n-1 gram的概率？

00:05:19.980 --> 00:05:22.300
所以，答案是，我们要让他们通过

00:05:22.300 --> 00:05:25.050
在一些大的文本文集中计算它们。

00:05:25.050 --> 00:05:26.510
所以，我们要估计一下，

00:05:26.510 --> 00:05:29.560
这些概率只是根据

00:05:29.560 --> 00:05:34.190
这些特定的n-grams和n-1 grams出现在我们的训练语料库中。

00:05:34.410 --> 00:05:37.370
可以。下面是一个有一些单词的例子。

00:05:37.370 --> 00:05:40.565
假设我们试图学习一个4-gram语言模型，

00:05:40.565 --> 00:05:42.830
假设我们有一段文字，上面写着，

00:05:42.830 --> 00:05:44.540
“当监察员开始计时时，

00:05:44.540 --> 00:05:46.100
学生们打开了空白处“，

00:05:46.100 --> 00:05:48.895
我们试图预测下一个词是什么。

00:05:48.895 --> 00:05:51.740
所以，因为我们正在学习一个4克的语言模型，

00:05:51.740 --> 00:05:55.910
一个简单的假设是，下一个词只取决于最后三个词，

00:05:55.910 --> 00:05:57.605
最后n-1个单词。

00:05:57.605 --> 00:06:01.520
所以，除了最后几句话，我们将抛弃所有的上下文，

00:06:01.520 --> 00:06:03.780
也就是说，“学生们打开了他们的。”

00:06:03.800 --> 00:06:07.620
因此，作为提醒，n-gram语言模型说，

00:06:07.620 --> 00:06:08.940
下一个词的概率是，

00:06:08.940 --> 00:06:13.230
词汇表中的某个特定单词w等于我们看到的次数

00:06:13.230 --> 00:06:15.510
学生打开W除以

00:06:15.510 --> 00:06:18.655
有时我们看到学生们在训练语料库中打开了他们的。

00:06:18.655 --> 00:06:21.440
那么，假设在我们的训练语料库中，

00:06:21.440 --> 00:06:24.215
我们看到“学生打开”这个短语1000次。

00:06:24.215 --> 00:06:28.340
假设这一点，我们看到“学生打开书本”400次。

00:06:28.340 --> 00:06:32.220
这意味着下一个单词成为书的概率是0.4。

00:06:32.220 --> 00:06:36.810
同样的，假设我们看到学生100次开学，

00:06:36.810 --> 00:06:39.260
这意味着给学生的考试概率

00:06:39.260 --> 00:06:41.930
打开它们的是0.1。有问题吗？

00:06:41.930 --> 00:06:44.900
[听不见]。

00:06:44.900 --> 00:06:47.010
问题是，单词的顺序有关系吗？

00:06:47.010 --> 00:06:50.340
答案是肯定的，学生开放的顺序很重要。

00:06:50.340 --> 00:06:53.190
这和“学生开放”不同。

00:06:53.190 --> 00:06:56.985
所以，我现在想提出的问题是，

00:06:56.985 --> 00:07:00.805
我们放弃普罗克特的背景是个好主意吗？

00:07:00.805 --> 00:07:03.115
如果你看看我们的实际例子，

00:07:03.115 --> 00:07:06.070
例如，当程序启动时钟时，

00:07:06.070 --> 00:07:07.850
学生们打开了空白处。

00:07:07.850 --> 00:07:12.360
那么，我们是否认为，考虑到实际情况，书籍或考试更有可能，

00:07:12.360 --> 00:07:14.550
完整的背景？是的。

00:07:14.550 --> 00:07:15.450
考试。

00:07:15.450 --> 00:07:17.795
正确的。考试更有可能是因为学监和

00:07:17.795 --> 00:07:20.260
时钟意味着这是一个考试场景，所以

00:07:20.260 --> 00:07:22.625
他们比书本更可能打开考试，

00:07:22.625 --> 00:07:24.400
除非是开卷考试。

00:07:24.400 --> 00:07:26.830
嗯，但我认为，总的来说，应该是考试。

00:07:26.830 --> 00:07:29.890
所以，我们在这里看到的问题是，在训练语料库中，

00:07:29.890 --> 00:07:31.240
学生们正在开放的事实

00:07:31.240 --> 00:07:33.990
这意味着它更可能是书而不是考试。

00:07:33.990 --> 00:07:36.305
因为总的来说，书本比考试更常见。

00:07:36.305 --> 00:07:38.565
但是如果我们知道上下文是，

00:07:38.565 --> 00:07:41.078
学监和时钟，然后应该是考试。

00:07:41.078 --> 00:07:44.240
所以，我在这里强调的是我们简化假设的一个问题。

00:07:44.240 --> 00:07:45.860
如果我们抛弃太多的上下文，

00:07:45.860 --> 00:07:50.455
那么，如果我们保持上下文的话，我们就不如预测单词那么好了。

00:07:50.455 --> 00:07:54.690
可以。所以，这是n-gram语言模型的一个问题。

00:07:54.690 --> 00:07:56.810
嗯，还有其他问题。

00:07:56.810 --> 00:08:00.470
那么，呃，这里又是你以前看到的方程。

00:08:00.470 --> 00:08:01.880
有一个问题我们会打电话给你

00:08:01.880 --> 00:08:05.465
稀疏性问题是如果上面的数字，

00:08:05.465 --> 00:08:08.380
分子，如果计数等于零怎么办？

00:08:08.380 --> 00:08:11.210
那么，如果对于某个特定的单词w，

00:08:11.210 --> 00:08:14.450
学生们打开的“W”一词在数据中从未出现过。

00:08:14.450 --> 00:08:17.240
例如，假设学生打开培养皿，

00:08:17.240 --> 00:08:19.880
非常罕见，而且从未出现在数据中，

00:08:19.880 --> 00:08:24.355
这意味着我们下一个单词被培养皿的概率是零。

00:08:24.355 --> 00:08:27.390
这很糟糕，因为这可能很少见，但事实上，

00:08:27.390 --> 00:08:29.385
一个有效的场景，对吗？

00:08:29.385 --> 00:08:31.090
例如，如果你是一个生物系的学生。

00:08:31.090 --> 00:08:34.085
所以，这是个问题，我们称之为稀疏性问题，

00:08:34.085 --> 00:08:37.790
因为问题是如果我们从未在培训数据中看到过事件发生，

00:08:37.790 --> 00:08:41.485
然后我们的模型将零概率分配给那个事件。

00:08:41.485 --> 00:08:46.415
所以，这个问题的一个部分解决方案是，也许我们应该增加一个小的delta，

00:08:46.415 --> 00:08:48.290
小数字增量到计数，

00:08:48.290 --> 00:08:50.420
对于词汇表中的每一个词。

00:08:50.420 --> 00:08:53.920
然后这样，接下来的每一个可能的词，

00:08:53.920 --> 00:08:56.250
至少有一些小概率。

00:08:56.250 --> 00:08:59.089
所以，培养皿的概率很小，

00:08:59.089 --> 00:09:02.410
但是，其他所有可能是坏选择的词也会是这样。

00:09:02.410 --> 00:09:05.580
所以，这个技术叫做平滑，因为这个想法是，

00:09:05.580 --> 00:09:06.945
你从一个非常，呃，

00:09:06.945 --> 00:09:10.050
稀疏概率分布，几乎处处为零，

00:09:10.050 --> 00:09:11.550
有几个尖峰，

00:09:11.550 --> 00:09:13.445
呃，作为我们见过的N克，

00:09:13.445 --> 00:09:16.100
从这一点到更平滑的概率分布

00:09:16.100 --> 00:09:19.615
每件事都有一个小概率。

00:09:19.615 --> 00:09:24.270
因此，第二个稀疏性问题可能比第一个问题更严重，

00:09:24.270 --> 00:09:28.130
如果分母中的数字是零，会发生什么？

00:09:28.130 --> 00:09:30.200
在我们的例子中，这意味着，

00:09:30.200 --> 00:09:34.655
如果我们从来没有在训练数据中看到过“学生打开他们的”三角图呢？

00:09:34.655 --> 00:09:38.480
如果发生这种情况，那么我们甚至无法计算

00:09:38.480 --> 00:09:42.820
因为我们以前从来没有见过这样的上下文。

00:09:42.820 --> 00:09:45.825
所以，一个可能的解决办法是

00:09:45.825 --> 00:09:48.450
如果在语料库中找不到“学生打开他们的”

00:09:48.450 --> 00:09:51.940
然后你就应该对最后两个词进行调整，

00:09:51.940 --> 00:09:53.545
而不是最后三个字。

00:09:53.545 --> 00:09:55.900
所以，现在你看到的时候，

00:09:55.900 --> 00:09:58.460
嗯，“打开他们的”看看接下来会发生什么。

00:09:58.460 --> 00:10:01.350
所以，这被取消，因为在这个失败的案例中，

00:10:01.350 --> 00:10:04.025
因为当你没有4-gram语言模型的数据时，

00:10:04.025 --> 00:10:06.020
你要退回到一个三角语言模型。

00:10:06.020 --> 00:10:09.510
现在有什么问题吗？

00:10:12.310 --> 00:10:17.570
可以。还有一件事要注意的是这些稀疏性问题

00:10:17.570 --> 00:10:22.100
如果你增加n，情况会更糟。如果你在n-gram语言模型中使n变大，

00:10:22.100 --> 00:10:23.870
你可能想这样做，例如，

00:10:23.870 --> 00:10:26.390
你可能会想，呃，我想有一个更大的背景，

00:10:26.390 --> 00:10:28.565
所以我可以注意到

00:10:28.565 --> 00:10:30.890
很久以前的事了，这会让它成为更好的预测器。

00:10:30.890 --> 00:10:33.275
所以，你可能认为把N变大是个好主意。

00:10:33.275 --> 00:10:36.410
但问题是，如果你这样做，那么稀疏性问题会变得更糟。

00:10:36.410 --> 00:10:37.700
因为，假设你说，

00:10:37.700 --> 00:10:39.215
我想要一个10克的语言模型。

00:10:39.215 --> 00:10:40.910
问题是你要数数，

00:10:40.910 --> 00:10:43.480
你经常看到9克和10克的过程。

00:10:43.480 --> 00:10:45.485
但是9克和10克，有这么多，

00:10:45.485 --> 00:10:47.615
你感兴趣的那个可能从未发生过，

00:10:47.615 --> 00:10:51.155
在你的训练数据中，这意味着整个事情变得不正常。

00:10:51.155 --> 00:10:55.680
所以，在实践中，我们通常不能有大于五的。

00:10:56.170 --> 00:10:58.490
可以。所以，那是，呃，

00:10:58.490 --> 00:11:00.875
n-gram语言模型的两个稀疏性问题。

00:11:00.875 --> 00:11:02.770
存储有问题。

00:11:02.770 --> 00:11:04.710
如果我们看这个方程，

00:11:04.710 --> 00:11:06.780
你得想想你需要什么

00:11:06.780 --> 00:11:09.365
存储以便使用N-gram语言模型。

00:11:09.365 --> 00:11:12.020
你需要储存这个号码，

00:11:12.020 --> 00:11:14.090
对于你观察到的所有n克

00:11:14.090 --> 00:11:17.215
当你通过训练语料库计算它们的时候。

00:11:17.215 --> 00:11:19.440
问题是，随着n的增加，

00:11:19.440 --> 00:11:23.480
然后你必须储存和计数的N克数就会增加。

00:11:23.480 --> 00:11:27.515
因此，增加n的另一个问题是，模型的大小，

00:11:27.515 --> 00:11:30.750
或者你的N克模型，呃，变大了。

00:11:31.490 --> 00:11:37.215
好的，那么N-gram语言模型在实践中。让我们来看一个例子。

00:11:37.215 --> 00:11:42.540
实际上，您可以在170万字的语料库上构建一个简单的三角语言模型，

00:11:42.540 --> 00:11:44.325
呃，几秒钟后，在你的笔记本上。

00:11:44.325 --> 00:11:46.140
事实上，我做这个的语料库

00:11:46.140 --> 00:11:47.970
是你在第一次任务中遇到的那个。

00:11:47.970 --> 00:11:49.605
这是路透社的语料库，

00:11:49.605 --> 00:11:51.180
嗯，商业和金融新闻。

00:11:51.180 --> 00:11:52.380
所以，如果你想自己动手，

00:11:52.380 --> 00:11:55.005
稍后您可以按照幻灯片底部的链接进行操作。

00:11:55.005 --> 00:11:57.000
所以，呃，这是，呃，

00:11:57.000 --> 00:11:59.280
我在几秒钟内在笔记本电脑上运行的东西。

00:11:59.280 --> 00:12:02.790
所以我今天给了它一个大人物的背景，

00:12:02.790 --> 00:12:06.480
然后我问三角语言模型下一个单词可能是什么。

00:12:06.480 --> 00:12:09.855
所以，语言模型说，最可能出现的单词是

00:12:09.855 --> 00:12:13.455
公司、银行、普莱斯、意大利、阿联酋等。

00:12:13.455 --> 00:12:17.640
所以，只要看看这些不同单词的概率，

00:12:17.640 --> 00:12:19.590
嗯，你可以看到有一个稀疏的问题。

00:12:19.590 --> 00:12:21.840
例如，前两个最可能的单词

00:12:21.840 --> 00:12:24.720
完全相同的概率和原因是，

00:12:24.720 --> 00:12:26.760
这个数字是4比26。

00:12:26.760 --> 00:12:28.800
所以这些是很小的整数，呃，

00:12:28.800 --> 00:12:30.270
意思是我们只看到，呃，

00:12:30.270 --> 00:12:33.000
今天公司和银行各四次。

00:12:33.000 --> 00:12:34.560
这是一个例子

00:12:34.560 --> 00:12:37.290
稀疏性问题，因为总的来说，这些都是非常低的计数，

00:12:37.290 --> 00:12:39.165
我们没见过这么多不同的，呃，

00:12:39.165 --> 00:12:40.500
此活动的版本，

00:12:40.500 --> 00:12:43.885
所以我们没有一个非常粒度的概率分布。

00:12:43.885 --> 00:12:46.385
但不管怎样，忽略了稀疏性问题，

00:12:46.385 --> 00:12:47.765
总的来说，

00:12:47.765 --> 00:12:50.640
这些建议看起来很合理。

00:12:52.600 --> 00:12:55.670
所以你可以使用语言模型

00:12:55.670 --> 00:12:58.305
生成文本，这就是您要做的。

00:12:58.305 --> 00:13:00.735
我们假设你已经有了前两个词，呃，

00:13:00.735 --> 00:13:04.560
你在这个条件下，问你的语言模型下一步会发生什么。

00:13:04.560 --> 00:13:07.305
所以考虑到这些词的概率分布，

00:13:07.305 --> 00:13:08.850
你可以从中取样，也就是说，

00:13:08.850 --> 00:13:11.865
选择一些有关联概率的词。

00:13:11.865 --> 00:13:14.235
所以让我们假设这给了我们“价格”这个词。

00:13:14.235 --> 00:13:17.730
那么价格就是你的下一个词，然后你只需要在最后两个词上加条件，

00:13:17.730 --> 00:13:20.385
在这个例子中，现在是价格。

00:13:20.385 --> 00:13:23.790
所以现在你得到了一个新的概率分布，你可以继续这个过程，

00:13:23.790 --> 00:13:27.960
嗯，取样，然后再次调节，然后取样。

00:13:27.960 --> 00:13:30.150
所以如果你做的足够久，

00:13:30.150 --> 00:13:31.350
你会得到一段文字，

00:13:31.350 --> 00:13:33.690
所以这是我在

00:13:33.690 --> 00:13:37.005
我用这个三角语言模型运行这个生成过程。

00:13:37.005 --> 00:13:40.260
所以它说，“今天每吨黄金的价格，

00:13:40.260 --> 00:13:43.260
鞋楦和鞋业的生产，

00:13:43.260 --> 00:13:46.230
银行在考虑和拒绝之后进行了干预。

00:13:46.230 --> 00:13:49.365
国际货币基金组织要求重建枯竭的欧洲股市，

00:13:49.365 --> 00:13:52.810
9月30日结束的初选76股算一股。

00:13:52.810 --> 00:13:55.250
可以。那么，呃，我们对这篇文章怎么看？

00:13:55.250 --> 00:13:59.195
我们觉得很好？我们，呃，惊讶吗？

00:13:59.195 --> 00:14:02.370
嗯，我想说在某些方面是好的，

00:14:02.370 --> 00:14:04.620
你知道，这有点奇怪的语法，

00:14:04.620 --> 00:14:07.860
主要是，呃，有点停顿，

00:14:07.860 --> 00:14:09.150
但是你肯定会这么说，

00:14:09.150 --> 00:14:10.500
这真的没有任何意义。

00:14:10.500 --> 00:14:12.180
很不连贯。

00:14:12.180 --> 00:14:14.580
我们不应该惊讶它是不连贯的我

00:14:14.580 --> 00:14:17.715
想想，如果你记得这是一个三角语言模型，

00:14:17.715 --> 00:14:20.265
它只记得最后一口井，

00:14:20.265 --> 00:14:22.635
三个或两个词取决于你如何看待它。

00:14:22.635 --> 00:14:24.510
很明显我们需要考虑

00:14:24.510 --> 00:14:27.990
如果我们想很好地模拟语言，一次超过三个单词。

00:14:27.990 --> 00:14:32.265
但是我们已经知道，增加n会使稀疏性问题变得更糟，

00:14:32.265 --> 00:14:38.370
n-gram语言模型，它还增加了模型大小。这是个问题吗？

00:14:38.370 --> 00:14:40.320
怎么[听不见][噪音]

00:14:40.320 --> 00:14:43.380
所以问题是，n-gram语言模型如何知道何时使用逗号。

00:14:43.380 --> 00:14:45.150
呃，所以你可以，

00:14:45.150 --> 00:14:50.400
[杂音]决定逗号和其他标点符号只是另一种单词，

00:14:50.400 --> 00:14:51.705
那是井还是记号？

00:14:51.705 --> 00:14:54.510
然后，对于语言模型来说，这并没有什么区别。

00:14:54.510 --> 00:14:57.705
它只是用来作为另一个可能的世界，嗯，可以预测，

00:14:57.705 --> 00:14:59.445
这就是为什么我们有奇怪的间隔，

00:14:59.445 --> 00:15:01.770
逗号是因为它本质上被视为一个单独的词。

00:15:01.770 --> 00:15:06.135
[噪音]好的。

00:15:06.135 --> 00:15:09.195
所以这门课叫做NLP，有深度的学习。

00:15:09.195 --> 00:15:12.765
所以你可能在想我们如何建立一个神经语言模型？

00:15:12.765 --> 00:15:15.450
那么，让我们简单回顾一下，呃，万一你忘了。

00:15:15.450 --> 00:15:17.940
记住，语言模型需要

00:15:17.940 --> 00:15:20.760
输入是从x1到xt的一系列单词，

00:15:20.760 --> 00:15:26.290
然后它输出下一个单词可能是xt加1的概率分布。

00:15:27.470 --> 00:15:32.070
好吧，当我们考虑到我们在这门课上遇到的神经模型是什么时候。

00:15:32.070 --> 00:15:34.545
呃，我们已经见过基于窗口的神经模型。

00:15:34.545 --> 00:15:36.780
在第三课，我们看到了你如何申请

00:15:36.780 --> 00:15:40.035
一种基于窗口的神经模型，用于命名实体识别。

00:15:40.035 --> 00:15:43.050
因此，在这种情况下，你需要借助某种窗口，

00:15:43.050 --> 00:15:46.125
不管这个例子中哪个是巴黎，然后，呃，

00:15:46.125 --> 00:15:48.780
你得到了这些的单词嵌入，将它们连接起来

00:15:48.780 --> 00:15:52.890
一些层次，然后你就可以决定巴黎是一个位置，而不是，

00:15:52.890 --> 00:15:55.425
你知道，一个人或一个组织。

00:15:55.425 --> 00:15:57.900
这是我们在第三课中所看到的。

00:15:57.900 --> 00:16:03.795
我们如何将这样的模型应用于语言建模？所以你可以这样做。

00:16:03.795 --> 00:16:06.930
下面是一个固定窗口神经语言模型的例子。

00:16:06.930 --> 00:16:09.420
所以，再一次，我们有某种背景

00:16:09.420 --> 00:16:12.060
也就是说，当学监开始计时时，学生们打开了他们的钟，

00:16:12.060 --> 00:16:15.225
嗯，我们想知道接下来会有什么消息。

00:16:15.225 --> 00:16:18.450
所以我们必须做一个和以前类似的简化假设。

00:16:18.450 --> 00:16:21.255
因为这是一个固定大小的窗户，

00:16:21.255 --> 00:16:25.500
我们必须放弃上下文，除了我们要调节的窗口。

00:16:25.500 --> 00:16:29.070
所以假设我们的固定窗户是四号的。

00:16:29.070 --> 00:16:34.390
所以我们要做的是类似于，啊，纳模型。

00:16:34.390 --> 00:16:38.400
我们将用一个热向量来表示这些词，

00:16:38.400 --> 00:16:42.745
然后我们会用它们来查找这些单词的嵌入，

00:16:42.745 --> 00:16:44.895
呃，嵌入查找矩阵。

00:16:44.895 --> 00:16:48.075
然后我们得到所有嵌入的单词e，1，2，3，4，

00:16:48.075 --> 00:16:51.270
然后我们把它们连接在一起得到e。

00:16:51.270 --> 00:16:55.215
一个线性层和一个非线性函数f得到某种隐藏层，

00:16:55.215 --> 00:16:57.720
然后我们把它穿过另一个线性层

00:16:57.720 --> 00:17:01.860
SoftMax函数，现在我们有一个输出概率分布y hat。

00:17:01.860 --> 00:17:05.925
在我们的例子中，因为我们试图预测下一个词是什么，啊，啊，

00:17:05.925 --> 00:17:08.430
向量y的长度为v，其中v是

00:17:08.430 --> 00:17:10.020
词汇表和它将包含

00:17:10.020 --> 00:17:12.555
词汇表中所有不同单词的概率。

00:17:12.555 --> 00:17:15.600
所以在这里，我把它表示为一个条形图，如果你认为

00:17:15.600 --> 00:17:18.690
你已经按字母顺序从A到Z列出了所有单词，

00:17:18.690 --> 00:17:21.300
然后是不同概率的单词。

00:17:21.300 --> 00:17:22.845
如果一切顺利，

00:17:22.845 --> 00:17:24.480
那么这个语言模型应该告诉我们

00:17:24.480 --> 00:17:27.930
例如，接下来的一些词可能是书籍和笔记本电脑。

00:17:27.930 --> 00:17:29.940
所以这些都不应该是，嗯，

00:17:29.940 --> 00:17:31.770
对你来说不熟悉，因为你上周看到了这一切。

00:17:31.770 --> 00:17:36.100
我们只是将基于窗口的模型应用于不同的任务，例如语言建模。

00:17:36.470 --> 00:17:38.940
好吧，那是什么？

00:17:38.940 --> 00:17:42.240
与n-gram语言模型相比，这个模型有什么好处？

00:17:42.240 --> 00:17:46.305
所以，我想说的一个优点是没有稀疏性问题。

00:17:46.305 --> 00:17:49.695
如果你记得一个n-gram语言模型有一个稀疏性问题

00:17:49.695 --> 00:17:53.205
也就是说，如果你在训练中从未见过特定的N-gram，

00:17:53.205 --> 00:17:55.005
你不能给它分配任何概率。

00:17:55.005 --> 00:17:56.445
你没有任何数据。

00:17:56.445 --> 00:17:59.340
但是至少在这里你可以拿任何东西，比如，

00:17:59.340 --> 00:18:02.115
4克你想要的，你可以把它喂进，啊，

00:18:02.115 --> 00:18:03.795
神经网络会给你

00:18:03.795 --> 00:18:06.150
它认为下一个词是什么的输出分布。

00:18:06.150 --> 00:18:10.245
这可能不是一个好的预测，但至少它会，它会运行。

00:18:10.245 --> 00:18:12.930
另一个好处是你不需要储存

00:18:12.930 --> 00:18:15.090
所有观察到的n克数。

00:18:15.090 --> 00:18:17.280
所以，呃，这是一个优势，呃，

00:18:17.280 --> 00:18:19.230
比较你只需要存储

00:18:19.230 --> 00:18:22.155
词汇表中所有单词的所有向量。

00:18:22.155 --> 00:18:26.085
但是这个固定窗口语言模型有很多问题。

00:18:26.085 --> 00:18:29.160
这里还有一些问题：呃，

00:18:29.160 --> 00:18:31.470
一是你的固定窗口可能太小了。

00:18:31.470 --> 00:18:33.885
不管你的固定窗户有多大，呃，

00:18:33.885 --> 00:18:35.640
你可能会失去某种

00:18:35.640 --> 00:18:38.490
有时你会用到的有用的上下文。

00:18:38.490 --> 00:18:41.745
实际上，如果你想放大窗户的尺寸，

00:18:41.745 --> 00:18:44.175
然后你还要放大你的尺寸，

00:18:44.175 --> 00:18:45.480
呃，体重因素，对不起，

00:18:45.480 --> 00:18:47.580
你的体重表

00:18:47.580 --> 00:18:49.590
所以w的宽度，因为你要乘以它。

00:18:49.590 --> 00:18:52.110
其中e是单词嵌入的串联。

00:18:52.110 --> 00:18:56.230
W的宽度随着窗口大小的增加而增大。

00:18:56.390 --> 00:19:01.210
所以在包含真的你的窗口永远不会足够大。

00:19:01.280 --> 00:19:05.460
这个模型的另一个更微妙的问题是

00:19:05.460 --> 00:19:08.820
x1和x2，实际上窗口中的所有单词都是，

00:19:08.820 --> 00:19:11.100
乘以完全不同的租金权重

00:19:11.100 --> 00:19:14.565
为了证明这一点，你可以画张图。

00:19:14.565 --> 00:19:17.610
所以问题是如果你有

00:19:17.610 --> 00:19:21.720
你的重量矩阵，然后你有

00:19:21.720 --> 00:19:26.910
嵌入E的串联，我们有，呃，四个嵌入。

00:19:26.910 --> 00:19:30.390
所以我们有E-1，E-2，E-3，

00:19:30.390 --> 00:19:33.135
E_4，你乘以，呃，

00:19:33.135 --> 00:19:36.615
按权重矩阵连接的嵌入。

00:19:36.615 --> 00:19:39.120
所以你真的可以看到

00:19:39.120 --> 00:19:42.449
重量矩阵的四个部分，

00:19:42.449 --> 00:19:45.570
第一个嵌入e_1的单词是

00:19:45.570 --> 00:19:48.825
乘以本节中的权重，

00:19:48.825 --> 00:19:53.025
它完全独立于乘以e_2等的权重。

00:19:53.025 --> 00:19:56.700
所以问题是你

00:19:56.700 --> 00:20:00.060
在一个部分的权重矩阵中学习不会与其他部分共享。

00:20:00.060 --> 00:20:03.985
你已经四次学习了很多类似的函数。

00:20:03.985 --> 00:20:07.910
所以我们认为这是个问题的原因是

00:20:07.910 --> 00:20:12.130
处理输入词嵌入的共性。

00:20:12.130 --> 00:20:14.880
所以你学到了如何处理的东西，

00:20:14.880 --> 00:20:18.375
第三个嵌入，至少其中一些应该与所有嵌入共享。

00:20:18.375 --> 00:20:21.960
所以我要说的是，我们学习的效率有点低，

00:20:21.960 --> 00:20:24.300
所有这些不同的词都有不同的权重

00:20:24.300 --> 00:20:27.970
当他们之间有很多共同点的时候。有问题吗？

00:20:29.840 --> 00:20:31.180
这就是为什么[听不见][噪音]。

00:20:31.180 --> 00:20:31.965
好的-

00:20:31.965 --> 00:20:36.560
是的，希望-希望口头描述已经开始。

00:20:38.280 --> 00:20:42.310
所以，最后，我要说的是，我们最大的问题是

00:20:42.310 --> 00:20:45.280
这个固定大小的神经模型显然是

00:20:45.280 --> 00:20:48.355
需要某种能处理任何长度输入的神经结构，

00:20:48.355 --> 00:20:51.070
因为这里的大部分问题都是因为我们必须

00:20:51.070 --> 00:20:54.920
这简化了存在固定窗口的假设。

00:20:56.670 --> 00:21:00.040
可以。所以这激发了，呃，

00:21:00.040 --> 00:21:02.590
我们来介绍这个新的神经架构家族，

00:21:02.590 --> 00:21:05.515
它被称为循环神经网络或RNN。

00:21:05.515 --> 00:21:09.100
所以，这是一个简单的图表，它向你展示了最重要的，

00:21:09.100 --> 00:21:11.320
嗯，RNN的特点。

00:21:11.320 --> 00:21:15.070
我们还有一个输入序列x1，x2，

00:21:15.070 --> 00:21:20.245
等等，但是你可以假设这个序列有你喜欢的任意长度。

00:21:20.245 --> 00:21:24.460
我们的想法是，你有一系列隐藏的状态，而不仅仅是，

00:21:24.460 --> 00:21:27.175
例如，我们在前一个模型中所做的一个隐藏状态。

00:21:27.175 --> 00:21:30.940
我们有一系列隐藏的状态，它们的数量和我们输入的一样多。

00:21:30.940 --> 00:21:35.440
重要的是，每个隐藏状态ht都是基于

00:21:35.440 --> 00:21:40.315
上一个隐藏状态以及该步骤的输入。

00:21:40.315 --> 00:21:44.050
所以它们被称为隐藏状态的原因是因为你可以想到

00:21:44.050 --> 00:21:47.425
这是一种随时间变化的单一状态。

00:21:47.425 --> 00:21:50.260
它有点像同一事物的几个版本。

00:21:50.260 --> 00:21:53.830
出于这个原因，我们经常称之为时间步骤，对吗？

00:21:53.830 --> 00:21:55.540
所以这些从左到右的步骤，

00:21:55.540 --> 00:21:57.860
我们经常称之为时间步。

00:21:58.950 --> 00:22:01.870
所以最重要的是

00:22:01.870 --> 00:22:07.210
在该RNN的每个时间步上应用相同的权重矩阵w。

00:22:07.210 --> 00:22:11.365
这就是我们能够处理任何长度输入的原因。

00:22:11.365 --> 00:22:13.930
因为我们不必在每一步都有不同的重量，

00:22:13.930 --> 00:22:17.990
因为我们只是在每一步上应用完全相同的转换。

00:22:18.870 --> 00:22:22.690
因此，您还可以从RNN获得一些输出。

00:22:22.690 --> 00:22:23.995
所以这些帽子，

00:22:23.995 --> 00:22:26.155
这些是每个步骤的输出。

00:22:26.155 --> 00:22:28.735
它们是可选的，因为你不需要计算它们

00:22:28.735 --> 00:22:31.210
或者，您可以只在某些步骤上计算它们，而不在其他步骤上计算。

00:22:31.210 --> 00:22:34.160
这取决于你想用RNN做什么。

00:22:34.920 --> 00:22:38.260
可以。这是一个简单的RNN图。

00:22:38.260 --> 00:22:39.850
呃，我来给你详细介绍一下。

00:22:39.850 --> 00:22:43.630
下面介绍如何应用RNN进行语言建模。

00:22:43.630 --> 00:22:48.175
所以，呃，再一次，假设我们到目前为止有一些文本。

00:22:48.175 --> 00:22:50.860
我的课文只有四个字，

00:22:50.860 --> 00:22:53.320
但是你可以假设它可以是任何长度，对吗？

00:22:53.320 --> 00:22:55.420
它很短，因为我们无法在幻灯片上容纳更多内容。

00:22:55.420 --> 00:22:58.390
所以你有一些标签序列，可能有点长。

00:22:58.390 --> 00:23:02.020
再一次，我们将用一种热向量来表示这些

00:23:02.020 --> 00:23:06.460
使用它们从嵌入矩阵中查找单词embeddings。

00:23:06.460 --> 00:23:10.370
然后计算第一个隐藏状态h1，

00:23:10.370 --> 00:23:14.300
我们需要根据以前的隐藏状态和当前输入来计算它。

00:23:14.300 --> 00:23:16.615
我们已经有了电流输入，即e1，

00:23:16.615 --> 00:23:19.570
但问题是，我们从哪里得到这个第一个隐藏的状态？

00:23:19.570 --> 00:23:21.160
好吧，h1之前是什么？

00:23:21.160 --> 00:23:24.670
所以我们经常称初始隐藏状态为h0，呃，是的，

00:23:24.670 --> 00:23:28.015
我们称之为初始隐藏状态，它可以是你学到的东西，

00:23:28.015 --> 00:23:32.065
就像它是网络的一个参数，你要学会如何初始化它，

00:23:32.065 --> 00:23:35.395
或者你可以假设它是零向量。

00:23:35.395 --> 00:23:40.495
所以我们用这个公式来计算新的隐藏状态，

00:23:40.495 --> 00:23:43.195
电流输入也写在左边。

00:23:43.195 --> 00:23:46.690
所以你对前一个隐藏状态做一个线性变换

00:23:46.690 --> 00:23:48.640
当前输入，然后添加

00:23:48.640 --> 00:23:50.919
偏压，然后把它放进一个非线性，

00:23:50.919 --> 00:23:52.990
例如，sigmoid函数。

00:23:52.990 --> 00:23:55.700
这给了你一个新的隐藏状态。

00:23:56.670 --> 00:23:59.470
可以。所以，一旦你做到了，

00:23:59.470 --> 00:24:01.480
然后你可以计算下一个隐藏状态，

00:24:01.480 --> 00:24:03.850
可以继续这样展开网络。

00:24:03.850 --> 00:24:06.025
那是，嗯，是的，

00:24:06.025 --> 00:24:07.450
这叫做展开，因为你

00:24:07.450 --> 00:24:10.270
计算前一步的每一步。

00:24:10.270 --> 00:24:12.160
好吧。最后，如果你还记得，

00:24:12.160 --> 00:24:13.330
我们正在尝试进行语言建模。

00:24:13.330 --> 00:24:17.530
因此，我们试图预测学生们打开后，接下来应该出现哪些单词。

00:24:17.530 --> 00:24:19.870
所以在这里的第四步，

00:24:19.870 --> 00:24:21.205
我们可以用，呃，

00:24:21.205 --> 00:24:22.825
当前隐藏状态，h4，

00:24:22.825 --> 00:24:27.430
把它放进一个线性层，然后把它放进一个SoftMax函数，然后我们得到

00:24:27.430 --> 00:24:32.800
我们的输出分布y-hat 4是在词汇表上的分布。

00:24:32.800 --> 00:24:34.720
再次，希望我们能得到一些

00:24:34.720 --> 00:24:38.080
对下一个词可能是什么的合理估计。

00:24:38.080 --> 00:24:43.210
有什么问题吗？是的？

00:24:43.210 --> 00:24:47.650
是隐藏状态的个数，还是输入的字数？

00:24:47.650 --> 00:24:50.845
问题是，隐藏状态的数量是您输入的单词数量吗？

00:24:50.845 --> 00:24:53.485
是的，在这里，呃，是的，

00:24:53.485 --> 00:24:58.405
或者你可以更一般地说，隐藏状态的数量就是输入的数量。是的。

00:24:58.405 --> 00:24:59.950
就像N-gram模型一样，

00:24:59.950 --> 00:25:05.590
我们可以使用输出作为转换模型中任务突变的输入？

00:25:05.590 --> 00:25:07.000
是的，所以问题是，

00:25:07.000 --> 00:25:08.650
就像N-gram语言模型一样，

00:25:08.650 --> 00:25:10.570
我们可以使用输出作为下一步的输入吗？

00:25:10.570 --> 00:25:12.715
答案是肯定的，我马上给你看。

00:25:12.715 --> 00:25:15.700
还有其他问题吗？是啊。

00:25:15.700 --> 00:25:17.995
你在学习嵌入吗？

00:25:17.995 --> 00:25:20.560
问题是，你在学习嵌入技术吗？

00:25:20.560 --> 00:25:21.925
嗯，那是个选择。

00:25:21.925 --> 00:25:23.770
例如，你可以用嵌入的方式，

00:25:23.770 --> 00:25:27.370
预生成的嵌入，你下载并使用它们，它们被冻结，

00:25:27.370 --> 00:25:28.750
或者你可以下载它们，

00:25:28.750 --> 00:25:30.190
但是你可以对它们进行微调。

00:25:30.190 --> 00:25:32.200
也就是说，允许将它们作为

00:25:32.200 --> 00:25:35.170
或者你可以初始化它们，

00:25:35.170 --> 00:25:38.560
你知道，小的，呃，随机值，从零开始学习。

00:25:38.560 --> 00:25:40.570
还有其他问题吗？是啊。

00:25:40.570 --> 00:25:43.690
你说你用的是同样的三角矩阵，

00:25:43.690 --> 00:25:45.490
就像你做反向传播一样，

00:25:45.490 --> 00:25:48.030
你只会像我们一样更新吗？

00:25:48.030 --> 00:25:51.080
或者你会更新wh和we吗？

00:25:51.080 --> 00:25:56.085
所以问题是，你说我们重用矩阵，是更新我们和wh，还是只更新一个？

00:25:56.085 --> 00:25:58.980
所以你突然学会了我们和什么。

00:25:58.980 --> 00:26:01.410
我想我是在强调什么，但是是的，

00:26:01.410 --> 00:26:04.090
它们都是重复应用的矩阵。

00:26:04.090 --> 00:26:05.500
还有一个关于后支撑的问题，

00:26:05.500 --> 00:26:07.675
但我们稍后会在这节课中讨论这个问题。

00:26:07.675 --> 00:26:12.250
好的，现在继续。嗯，所以，

00:26:12.250 --> 00:26:17.530
这种RNN语言模型有哪些优点和缺点？

00:26:17.530 --> 00:26:23.005
因此，与固定窗口相比，我们可以看到一些优势。

00:26:23.005 --> 00:26:28.210
所以一个明显的优点是这个RNN可以处理任何长度的输入。

00:26:28.210 --> 00:26:31.180
另一个优点是

00:26:31.180 --> 00:26:35.050
理论上，步骤t可以使用许多步骤返回的信息。

00:26:35.050 --> 00:26:36.730
所以在我们的动机例子中，

00:26:36.730 --> 00:26:38.650
就在节目主持人开始计时的时候，

00:26:38.650 --> 00:26:39.970
学生们打开了门。

00:26:39.970 --> 00:26:42.250
我们认为Proctor和Clock是

00:26:42.250 --> 00:26:45.340
这两个很重要的提示下一步可能会发生什么。

00:26:45.340 --> 00:26:47.275
所以，至少在理论上，

00:26:47.275 --> 00:26:49.390
最后的隐藏状态

00:26:49.390 --> 00:26:54.950
可以访问许多步骤前输入的信息。

00:26:55.350 --> 00:26:59.785
另一个优点是，模型大小不会随着输入时间的延长而增大。

00:26:59.785 --> 00:27:02.485
所以，呃，模型的大小是固定的。

00:27:02.485 --> 00:27:05.005
这只是wh和we，s

00:27:05.005 --> 00:27:09.400
如果你计算的话，还有偏差和嵌入矩阵。

00:27:09.400 --> 00:27:13.000
如果你想把它应用到更多的地方，这些都不会变大，

00:27:13.000 --> 00:27:17.300
嗯，输入的时间更长，因为你只需要重复使用相同的权重。

00:27:18.030 --> 00:27:23.995
另一个优点是你在每一个时间步上都有相同的权重。

00:27:23.995 --> 00:27:29.425
我之前说过，固定大小的窗神经模型，

00:27:29.425 --> 00:27:31.720
因为它正在应用，所以效率较低

00:27:31.720 --> 00:27:34.270
权重矩阵的不同权重不同，

00:27:34.270 --> 00:27:35.905
呃，窗户上的字。

00:27:35.905 --> 00:27:38.470
这个RNN的优点是

00:27:38.470 --> 00:27:41.650
对每个输入应用完全相同的转换。

00:27:41.650 --> 00:27:45.835
这意味着如果它学习了处理一个输入的好方法，

00:27:45.835 --> 00:27:48.010
应用于序列中的每个输入。

00:27:48.010 --> 00:27:50.630
所以你可以看到它在这方面更有效率。

00:27:51.480 --> 00:27:54.805
好吧，那么这个模型的缺点是什么？

00:27:54.805 --> 00:27:58.270
一个是经常性的计算很慢。

00:27:58.270 --> 00:27:59.995
呃，就像你以前看到的，

00:27:59.995 --> 00:28:03.865
您必须根据以前的隐藏状态计算隐藏状态。

00:28:03.865 --> 00:28:06.925
这意味着你不能并行计算所有的隐藏状态。

00:28:06.925 --> 00:28:08.665
你必须按顺序计算它们。

00:28:08.665 --> 00:28:13.120
所以，特别是如果你想在一个相当长的输入序列上计算RNN，

00:28:13.120 --> 00:28:16.660
这意味着RNN的计算速度可能非常慢。

00:28:16.660 --> 00:28:20.425
RNN的另一个缺点是它不起作用，

00:28:20.425 --> 00:28:24.175
在实践中，要从许多步骤中获取信息是相当困难的。

00:28:24.175 --> 00:28:26.290
所以即使我说我们应该能记住

00:28:26.290 --> 00:28:28.930
学监和时钟，用来预测考试和我们的书，

00:28:28.930 --> 00:28:30.430
结果发现RNN，

00:28:30.430 --> 00:28:32.470
至少我在这节课上讲过的那些，

00:28:32.470 --> 00:28:35.305
不如你想象的那么好。

00:28:35.305 --> 00:28:39.295
嗯，我们稍后会进一步了解这两个缺点，

00:28:39.295 --> 00:28:42.610
我们将学习如何修复它们。

00:28:42.610 --> 00:28:46.900
我们现在有什么问题吗？是的。

00:28:46.900 --> 00:28:48.010
为什么我们假设什么是相同的？

00:28:48.010 --> 00:28:51.265
对不起，你能大声点吗？

00:28:51.265 --> 00:28:55.900
为什么我们假设wh应该是相同的？

00:28:55.900 --> 00:28:59.635
所以问题是，为什么你要假设wh是相同的？

00:28:59.635 --> 00:29:01.450
我想，这不完全是一个假设，

00:29:01.450 --> 00:29:04.390
在RNN的设计中，这更像是一个深思熟虑的决定。

00:29:04.390 --> 00:29:06.460
所以，RNN的定义是，

00:29:06.460 --> 00:29:10.450
在每一步上应用完全相同的权重的网络。

00:29:10.450 --> 00:29:13.800
所以，我想你为什么认为应该是，

00:29:13.800 --> 00:29:15.225
为什么这是个好主意？

00:29:15.225 --> 00:29:17.520
嗯，所以我说了一点为什么这是个好主意，

00:29:17.520 --> 00:29:18.690
这些优点，

00:29:18.690 --> 00:29:23.950
我想，这就是你为什么想这样做的原因。那能回答你的问题吗？

00:29:24.560 --> 00:29:29.020
打开他们的书，对吗？如果假设wh相同，

00:29:29.020 --> 00:29:31.420
你是说，呃，

00:29:31.420 --> 00:29:34.660
马尔可夫链，就像马尔可夫链。

00:29:34.660 --> 00:29:37.780
呃，传输，呃，

00:29:37.780 --> 00:29:42.955
打开人类情绪的转移概率，

00:29:42.955 --> 00:29:44.890
它们是一样的，

00:29:44.890 --> 00:29:50.940
但实际上是马尔可夫链。

00:29:50.940 --> 00:29:56.535
模型[听不见]的转移概率相同，

00:29:56.535 --> 00:30:00.895
所以[听不见]概率，

00:30:00.895 --> 00:30:07.105
这只是一个近似值，但它是另一个测试。

00:30:07.105 --> 00:30:08.240
可以。所以我认为[重叠]

00:30:08.240 --> 00:30:10.810
如果你假设wh可能是相同的，

00:30:10.810 --> 00:30:14.725
很好，因为你使用了很多参数，

00:30:14.725 --> 00:30:20.560
但这只是一个，这只是一个近似值。

00:30:20.560 --> 00:30:23.410
潜在的转移，呃，

00:30:23.410 --> 00:30:25.660
概率，不应该是一样的。特别是[重叠]

00:30:25.660 --> 00:30:28.835
可以。嗯，所以我认为问题是，考虑到这些

00:30:28.835 --> 00:30:30.540
学生们打开的词

00:30:30.540 --> 00:30:32.490
都是不同的，它们发生在不同的环境中，

00:30:32.490 --> 00:30:35.850
那么为什么我们每次都要应用相同的转换呢？

00:30:35.850 --> 00:30:37.440
所以这是一个很好的问题。

00:30:37.440 --> 00:30:41.670
我想，呃，这个想法是你在学习一个通用函数，而不仅仅是，你知道，

00:30:41.670 --> 00:30:43.535
如何对待学生，

00:30:43.535 --> 00:30:46.090
在这一个上下文中的一个词students。

00:30:46.090 --> 00:30:48.520
我们正在努力学习你如何

00:30:48.520 --> 00:30:51.070
应该处理到目前为止给出的一个词。

00:30:51.070 --> 00:30:55.090
到目前为止，你正在努力学习语言和上下文的一般表示法，

00:30:55.090 --> 00:30:57.055
这确实是一个非常困难的问题。

00:30:57.055 --> 00:31:00.175
嗯，我想你也提到了近似值。

00:31:00.175 --> 00:31:01.780
还有一件事要注意的是

00:31:01.780 --> 00:31:04.570
隐藏状态是向量，它们不仅仅是单个数字，对吗？

00:31:04.570 --> 00:31:06.670
它们是长度的向量，我不知道，500还是什么？

00:31:06.670 --> 00:31:09.610
因此，他们有相当大的容量来保存关于

00:31:09.610 --> 00:31:13.530
在不同的位置上有不同的东西。

00:31:13.530 --> 00:31:15.630
所以，我认为你可以

00:31:15.630 --> 00:31:18.255
在不同的环境中存储大量不同的信息，

00:31:18.255 --> 00:31:19.830
在隐藏状态的不同部分，

00:31:19.830 --> 00:31:21.960
但它确实是一个近似值

00:31:21.960 --> 00:31:24.575
某种程度上限制了您可以存储的信息量。

00:31:24.575 --> 00:31:26.845
好的，还有其他问题吗？对。

00:31:26.845 --> 00:31:29.410
因为你处理的是任何一个长度的框架，

00:31:29.410 --> 00:31:31.135
你在训练中用多长时间？

00:31:31.135 --> 00:31:35.035
你用来训练的长度会影响什么？

00:31:35.035 --> 00:31:39.355
好吧，问题是，如果你可以输入任何长度，

00:31:39.355 --> 00:31:41.950
培训期间的输入长度是多少？

00:31:41.950 --> 00:31:44.185
所以，我想在实践中，

00:31:44.185 --> 00:31:46.510
您可以选择输入的时间

00:31:46.510 --> 00:31:49.630
培训要么基于你的数据，要么基于

00:31:49.630 --> 00:31:52.615
嗯，你的效率很重要，所以也许你是人为的

00:31:52.615 --> 00:31:55.900
剪短它。嗯，另一个问题是什么？

00:31:55.900 --> 00:31:58.360
呃，那又有什么用呢？

00:31:58.360 --> 00:32:01.255
可以。所以问题是，什么取决于你使用的长度？

00:32:01.255 --> 00:32:04.075
所以，不，这是优势列表中的一个优点。

00:32:04.075 --> 00:32:07.165
模型尺寸不会随着输入时间的延长而增大，

00:32:07.165 --> 00:32:09.040
因为我们只是打开RNN

00:32:09.040 --> 00:32:11.245
只要我们愿意，一次又一次地施加相同的重量。

00:32:11.245 --> 00:32:13.930
不需要有更多的重量，因为你有一个更长的输入。

00:32:13.930 --> 00:32:16.795
[噪音]是的。

00:32:16.795 --> 00:32:24.235
所以，你提到的比率是如何[听不见]单词数的。

00:32:24.235 --> 00:32:28.405
[噪音]你问的是大写E还是小写E？

00:32:28.405 --> 00:32:29.485
呃，小写E。

00:32:29.485 --> 00:32:30.790
可以。所以，问题是，

00:32:30.790 --> 00:32:32.890
我们如何选择小写es的维数？

00:32:32.890 --> 00:32:34.300
呃，你可以，例如，

00:32:34.300 --> 00:32:37.120
假设这些只是预先训练过的单词向量，就像你，

00:32:37.120 --> 00:32:38.815
呃，用于作业一。

00:32:38.815 --> 00:32:39.715
更像Word2vec。

00:32:39.715 --> 00:32:41.140
是啊。例如，word2vec，

00:32:41.140 --> 00:32:42.610
你只需下载并使用它们，

00:32:42.610 --> 00:32:44.380
或者你从零开始学习，在这种情况下，

00:32:44.380 --> 00:32:46.930
你可以在训练开始时决定这些向量的大小。

00:32:46.930 --> 00:32:49.210
[噪音]好的。我现在就走。

00:32:49.210 --> 00:32:54.895
[噪音]所以，我们已经了解了RNN语言模型是什么，我们也已经了解了你会怎么做，

00:32:54.895 --> 00:32:56.845
嗯，向前跑一步，但问题是，

00:32:56.845 --> 00:32:59.080
你将如何训练RNN语言模型？

00:32:59.080 --> 00:33:02.230
你会怎么学？[噪音]

00:33:02.230 --> 00:33:03.850
所以，像往常一样，在机器学习中，

00:33:03.850 --> 00:33:06.670
我们的答案是，你会得到大量的文本，

00:33:06.670 --> 00:33:11.230
我们称之为单词x1到x大写t的序列，所以，

00:33:11.230 --> 00:33:15.115
将单词序列输入RNN语言模型，然后，

00:33:15.115 --> 00:33:19.615
我们的想法是，计算每个步骤t的输出分布y-hat t。

00:33:19.615 --> 00:33:21.700
我知道我在上一张照片上展示的，呃，

00:33:21.700 --> 00:33:23.560
幻灯片[噪音]只显示我们在最后一步做，

00:33:23.560 --> 00:33:26.140
但其想法是，你可以在每一步上计算这个。

00:33:26.140 --> 00:33:28.420
所以，这意味着你实际上在预测

00:33:28.420 --> 00:33:31.000
每一步下一个单词的概率。

00:33:31.000 --> 00:33:33.130
[噪音]好的。

00:33:33.130 --> 00:33:35.515
所以，一旦你这样做了，你就可以定义损失函数，

00:33:35.515 --> 00:33:37.120
你现在应该很熟悉了。

00:33:37.120 --> 00:33:39.190
这是[噪音]之间的交叉熵

00:33:39.190 --> 00:33:43.915
我们预测的概率分布y-hat t和真的，呃，

00:33:43.915 --> 00:33:47.260
分发，这是Y-Hat-抱歉，只是YT，

00:33:47.260 --> 00:33:49.570
这是一个热向量，呃，

00:33:49.570 --> 00:33:51.055
代表真正的下一个[噪音]单词，

00:33:51.055 --> 00:33:52.495
是xt加1。

00:33:52.495 --> 00:33:54.490
所以，正如你以前看到的，这个，呃，

00:33:54.490 --> 00:33:57.100
这两个向量之间的交叉熵[噪声]可以写出来。

00:33:57.100 --> 00:34:00.640
也作为负对数概率。

00:34:00.640 --> 00:34:05.635
最后，如果你平均每一步的交叉熵损失，呃，

00:34:05.635 --> 00:34:08.740
语料库时间步骤t中的每个t，然后，

00:34:08.740 --> 00:34:11.800
呃，这会让你在整个训练中损失惨重。

00:34:11.800 --> 00:34:16.360
[噪音]好的。

00:34:16.360 --> 00:34:18.475
所以，为了更清楚地描述这一点，

00:34:18.475 --> 00:34:20.080
呃，假设我们的语料库是，

00:34:20.080 --> 00:34:21.370
学生们开始考试，

00:34:21.370 --> 00:34:23.020
等等，它持续了很长时间。

00:34:23.020 --> 00:34:24.550
那么，我们要做的是，

00:34:24.550 --> 00:34:26.980
我们会对这段文字进行RNN，然后，

00:34:26.980 --> 00:34:30.535
在每一步中，我们都会预测概率[噪声]分布y-hats，

00:34:30.535 --> 00:34:31.780
然后，从每一个，

00:34:31.780 --> 00:34:33.310
你可以计算你的损失是多少，

00:34:33.310 --> 00:34:36.400
这是JT，然后，呃，在第一步，

00:34:36.400 --> 00:34:38.965
损失将是下一个单词的负对数概率，

00:34:38.965 --> 00:34:40.060
在这个例子中，

00:34:40.060 --> 00:34:42.040
学生，[噪音]等等。

00:34:42.040 --> 00:34:45.070
每一个都是下一个单词的负对数概率。

00:34:45.070 --> 00:34:47.515
[噪音]然后，一旦你计算了所有这些，

00:34:47.515 --> 00:34:49.585
你可以把它们[噪音]加起来平均，

00:34:49.585 --> 00:34:51.160
然后，这给了你最后的损失。

00:34:51.160 --> 00:34:56.260
[噪音]好的。所以，这里有个警告。

00:34:56.260 --> 00:34:59.935
嗯，计算整个语料库的损失和梯度，

00:34:59.935 --> 00:35:02.350
所有这些词x1到x大写t也是

00:35:02.350 --> 00:35:04.840
昂贵的[噪音]因为你的语料库可能真的很大。

00:35:04.840 --> 00:35:07.810
[噪音]所以，嗯，就像一个学生之前问的，

00:35:07.810 --> 00:35:10.555
实际上，你认为你的顺序是什么？

00:35:10.555 --> 00:35:12.580
所以，在实践中，你可能认为你的顺序，呃，

00:35:12.580 --> 00:35:14.590
比如一个句子或一个文件，

00:35:14.590 --> 00:35:17.270
一些较短的文本单位。

00:35:17.430 --> 00:35:20.890
所以，呃，你要做的另一件事就是，如果你记得的话，

00:35:20.890 --> 00:35:23.785
随机梯度下降允许您计算梯度

00:35:23.785 --> 00:35:26.980
对于小数据块而不是整个语料库。

00:35:26.980 --> 00:35:29.275
所以，在实践中，如果你在训练一种语言模式，

00:35:29.275 --> 00:35:32.830
实际上你可能要做的是计算一个句子的损失，

00:35:32.830 --> 00:35:35.290
但这实际上是一批句子，然后，

00:35:35.290 --> 00:35:37.945
你计算出这批句子的梯度，

00:35:37.945 --> 00:35:39.760
更新体重，然后重复。

00:35:39.760 --> 00:35:46.405
有什么问题吗？[噪音]好的。

00:35:46.405 --> 00:35:48.040
所以，呃，移到背面。

00:35:48.040 --> 00:35:51.055
别担心，不会有像上周那样多的反弹，

00:35:51.055 --> 00:35:53.230
但是，呃，这里有个有趣的问题，对吧？

00:35:53.230 --> 00:35:55.899
所以，呃，RNN的特点是

00:35:55.899 --> 00:35:58.975
是它们重复应用相同的权重矩阵。

00:35:58.975 --> 00:36:00.280
所以，问题是，

00:36:00.280 --> 00:36:02.215
[噪音]损失函数的导数是多少，

00:36:02.215 --> 00:36:03.610
假设，在步骤t上？

00:36:03.610 --> 00:36:08.635
这个损失对重复的重量矩阵wh的导数是多少？

00:36:08.635 --> 00:36:13.570
答案是损失的导数，呃，

00:36:13.570 --> 00:36:16.390
相对于重复权重的梯度是

00:36:16.390 --> 00:36:19.780
每次出现的梯度之和，

00:36:19.780 --> 00:36:21.355
这个方程就是这么说的。

00:36:21.355 --> 00:36:25.615
在右边，用垂直线表示的符号，我是说，

00:36:25.615 --> 00:36:30.670
当出现在第i步时，损失相对于Wh的导数。

00:36:30.670 --> 00:36:32.770
可以。那么，为什么是这样？

00:36:32.770 --> 00:36:35.260
[噪音]呃，为了说明这是真的，

00:36:35.260 --> 00:36:37.840
呃，[噪音]我会提醒你多变量链规则。

00:36:37.840 --> 00:36:42.535
这是汗学院关于多变量链规则的文章的截图，

00:36:42.535 --> 00:36:44.440
我建议你去看看

00:36:44.440 --> 00:36:46.630
想了解更多因为这很容易理解。

00:36:46.630 --> 00:36:48.220
嗯，它说的是，

00:36:48.220 --> 00:36:52.045
给定一个函数f[噪声]，它取决于x和y，

00:36:52.045 --> 00:36:56.140
它们都是变量t的函数，

00:36:56.140 --> 00:36:59.430
如果你想得到f对t的导数，

00:36:59.430 --> 00:37:04.380
然后你需要分别在x和y上做链式规则，然后把它们加起来。

00:37:04.380 --> 00:37:07.020
[噪音]所以，这是多变量链规则，

00:37:07.020 --> 00:37:10.510
[噪音]如果我们把这个应用到我们的场景中

00:37:10.510 --> 00:37:14.889
损失的导数jt相对于我们的重量矩阵wh，

00:37:14.889 --> 00:37:19.300
然后你可以把它看作一种图表[噪音]，其中wh有，呃，

00:37:19.300 --> 00:37:22.810
与所有这些世上的个体形象的关系，

00:37:22.810 --> 00:37:23.860
但这只是一种简单的关系，

00:37:23.860 --> 00:37:25.495
这只是平等，然后，

00:37:25.495 --> 00:37:29.690
wh的每一个出现都以不同的方式影响损失。

00:37:29.690 --> 00:37:34.080
那么，如果我们应用多变量链规则，

00:37:34.080 --> 00:37:37.470
然后它说损失的导数

00:37:37.470 --> 00:37:41.190
wh是这些链式法则的总和，

00:37:41.190 --> 00:37:45.600
但右边的表达式只是一个，因为它是平等关系，

00:37:45.600 --> 00:37:50.480
[噪音]然后，这就给出了我在上一张幻灯片上写的方程式。

00:37:50.480 --> 00:37:55.240
所以，这是一个证明，为什么损失的导数

00:37:55.240 --> 00:38:00.565
关于循环矩阵，是每次出现的导数之和。

00:38:00.565 --> 00:38:03.190
可以。所以，假设你相信我，也就是说，

00:38:03.190 --> 00:38:04.555
如何计算，呃，

00:38:04.555 --> 00:38:06.475
相对于重复重量的梯度。

00:38:06.475 --> 00:38:08.440
所以，还有一个问题是，

00:38:08.440 --> 00:38:10.720
实际中我们如何计算这个？

00:38:10.720 --> 00:38:16.660
[noise]所以，答案是你要做backprop来计算这个和，

00:38:16.660 --> 00:38:19.390
嗯，向后，从右到左，嗯，

00:38:19.390 --> 00:38:23.590
通过RNN，你将积累这个总数。

00:38:23.590 --> 00:38:24.940
所以，重要的是，

00:38:24.940 --> 00:38:28.435
你不应该分别计算这些东西，

00:38:28.435 --> 00:38:30.880
你应该通过累计来计算它们，比如，

00:38:30.880 --> 00:38:34.360
每一个都可以用前一个的形式来计算。

00:38:34.360 --> 00:38:39.130
[noise]所以，这个计算这些的算法，

00:38:39.130 --> 00:38:41.320
呃，每个梯度

00:38:41.320 --> 00:38:44.305
前一个被称为随时间的反向传播。

00:38:44.305 --> 00:38:47.650
而且，嗯，我一直认为这听起来比科幻小说要高得多。

00:38:47.650 --> 00:38:49.030
听起来像是时间旅行之类的，

00:38:49.030 --> 00:38:50.560
但其实很简单。

00:38:50.560 --> 00:38:53.290
呃，这就是你给的名字

00:38:53.290 --> 00:38:57.290
将反prop算法应用于循环神经网络。

00:38:57.960 --> 00:39:02.350
有什么问题吗？是的。[噪音]

00:39:02.350 --> 00:39:07.240
所以，似乎你如何分解这些批次关系到你的最终结果。

00:39:07.240 --> 00:39:15.700
[听不见]。

00:39:15.700 --> 00:39:21.460
所以，如果你把它分成更多[听不见]。

00:39:21.460 --> 00:39:23.605
可以。所以问题是，嗯，当然，

00:39:23.605 --> 00:39:27.865
你决定如何分解你的批次会影响你学习的方式，对吗？

00:39:27.865 --> 00:39:29.560
因为如果你选择，呃，

00:39:29.560 --> 00:39:31.660
一组数据将成为您的批处理，对吧，

00:39:31.660 --> 00:39:33.880
您将基于此进行更新，然后，

00:39:33.880 --> 00:39:36.760
你只能根据你从那里去的地方[噪音]来更新下一个。

00:39:36.760 --> 00:39:38.950
所以，如果您决定将不同的数据放入批处理中，

00:39:38.950 --> 00:39:40.495
那么你就可以迈出不同的一步了。

00:39:40.495 --> 00:39:42.910
所以，这是真的，[噪音]这就是为什么

00:39:42.910 --> 00:39:45.910
随机梯度下降只是

00:39:45.910 --> 00:39:49.660
真正的梯度下降，因为你计算的梯度

00:39:49.660 --> 00:39:53.950
对于一个批次而言，只是相对于

00:39:53.950 --> 00:39:56.095
呃，整个语料库的损失。

00:39:56.095 --> 00:39:58.165
所以，是的，它确实是一个近似值

00:39:58.165 --> 00:40:00.580
你选择如何将你的数据进行批量处理也很重要，

00:40:00.580 --> 00:40:03.040
这就是为什么，例如，整理数据是个好主意，

00:40:03.040 --> 00:40:05.575
每一个时代，改变它是一个好主意。

00:40:05.575 --> 00:40:09.130
但是SGD的核心理念是

00:40:09.130 --> 00:40:12.085
它应该是一个足够好的近似值，在许多步骤中，

00:40:12.085 --> 00:40:14.740
你会，呃，尽量减少你的损失。

00:40:14.740 --> 00:40:33.010
[噪音]还有其他问题吗？[噪音]是的。

00:40:33.010 --> 00:40:35.410
[噪音]所以，是，呃，是问题，

00:40:35.410 --> 00:40:37.180
当你计算向前推进时，

00:40:37.180 --> 00:40:40.345
你开始计算后向概率吗，甚至，比如说，在损失之前？

00:40:40.345 --> 00:40:41.620
这就是问题吗？[噪音]

00:40:41.620 --> 00:40:42.325
对。

00:40:42.325 --> 00:40:45.640
我不这么认为，对吧？因为你需要知道损失是什么

00:40:45.640 --> 00:40:49.030
计算损失对某事物的导数。

00:40:49.030 --> 00:40:50.560
所以，我认为你应该走到最后。

00:40:50.560 --> 00:40:51.760
所以，如果我们假设简单，

00:40:51.760 --> 00:40:54.490
在几个步骤的最后，只有一个损失，

00:40:54.490 --> 00:40:55.585
那你就得走到尽头，

00:40:55.585 --> 00:40:59.365
在计算导数之前先计算损失。

00:40:59.365 --> 00:41:02.200
但是我想你，你，你可以计算2的导数，

00:41:02.200 --> 00:41:04.240
一个事物相对另一个事物的某种相邻事物。

00:41:04.240 --> 00:41:05.470
[重叠]但是，是的。[噪音]

00:41:05.470 --> 00:41:07.780
在你前进的过程中，你需要跟踪什么，

00:41:07.780 --> 00:41:13.720
你将拥有的（听不见的）最终会得到损失。[听不见]

00:41:13.720 --> 00:41:15.865
对.所以，当你向前推进时，

00:41:15.865 --> 00:41:19.660
你当然必须坚持所有的干预因素。

00:41:19.660 --> 00:41:20.680
[噪音]好的。我现在就走。

00:41:20.680 --> 00:41:24.790
嗯，那是个数学难题，但是，

00:41:24.790 --> 00:41:27.130
嗯，现在，我们开始生成文本，

00:41:27.130 --> 00:41:28.675
之前有人问过。

00:41:28.675 --> 00:41:32.965
所以，嗯，就像我们使用n-gram语言模型生成文本一样，

00:41:32.965 --> 00:41:36.115
您还可以使用RNN语言模型生成文本，

00:41:36.115 --> 00:41:38.650
嗯，通过同样的重复取样技术。

00:41:38.650 --> 00:41:41.050
嗯，那么，这是一张如何工作的图片。

00:41:41.050 --> 00:41:43.990
你如何从你最初的隐藏状态开始H0，呃，

00:41:43.990 --> 00:41:46.330
我们可以把它作为

00:41:46.330 --> 00:41:49.060
模型或者我们初始化为零，或者类似的。

00:41:49.060 --> 00:41:51.340
那么，假设我们有第一个词my，

00:41:51.340 --> 00:41:54.235
我们假设我，嗯，把它提供给模型。

00:41:54.235 --> 00:41:57.235
那么，使用输入和初始隐藏状态，

00:41:57.235 --> 00:41:59.200
你可以得到我们的第一个隐藏状态h1。

00:41:59.200 --> 00:42:01.555
然后，我们可以计算，呃，

00:42:01.555 --> 00:42:04.765
概率分布，下一个是什么，

00:42:04.765 --> 00:42:07.435
然后我们可以用这个分布来取样一些单词。

00:42:07.435 --> 00:42:09.385
所以假设我们选取了“最爱”这个词。

00:42:09.385 --> 00:42:14.200
所以，我们的想法是在下一步使用输出的单词作为输入。

00:42:14.200 --> 00:42:16.960
所以，我们把最喜欢的加入RNN的第二步，

00:42:16.960 --> 00:42:18.220
我们得到了一个新的隐藏状态，

00:42:18.220 --> 00:42:20.784
我们再次得到一个新的概率分布，

00:42:20.784 --> 00:42:22.885
从中我们可以看到一个新词。

00:42:22.885 --> 00:42:25.675
所以，我们可以一次又一次地继续这个过程，

00:42:25.675 --> 00:42:27.685
这样我们就可以生成一些文本。

00:42:27.685 --> 00:42:29.500
所以，呃，这里我们已经生成了文本，

00:42:29.500 --> 00:42:30.760
我最喜欢的季节是春天，

00:42:30.760 --> 00:42:34.070
我们想走多久就走多久。

00:42:36.060 --> 00:42:39.130
好吧，那么，呃，让我们玩一玩。

00:42:39.130 --> 00:42:41.395
呃，你可以生成，

00:42:41.395 --> 00:42:43.885
使用RNN语言模型的文本。

00:42:43.885 --> 00:42:48.070
如果你在任何文本上训练RNN语言模型，

00:42:48.070 --> 00:42:51.340
然后您可以使用它来生成该样式的文本。

00:42:51.340 --> 00:42:53.380
事实上，这已经成为一种

00:42:53.380 --> 00:42:55.780
你可能见过的网络幽默类型。

00:42:55.780 --> 00:42:57.595
比如说，

00:42:57.595 --> 00:43:00.925
这里有一个针对奥巴马演讲的RNN语言模型，

00:43:00.925 --> 00:43:03.100
我在网上的一篇博文中找到了这个。

00:43:03.100 --> 00:43:07.120
这里是RNN语言模型生成的文本。

00:43:07.120 --> 00:43:11.350
“美国将加大应对新挑战的成本，

00:43:11.350 --> 00:43:15.520
美国人民将分享我们制造问题的事实。

00:43:15.520 --> 00:43:19.630
他们被袭击了，不得不这么说

00:43:19.630 --> 00:43:24.190
在战争的最后几天，我不能完成所有的任务。”

00:43:24.190 --> 00:43:27.130
[笑声]好的。

00:43:27.130 --> 00:43:30.205
所以，如果我们看看这个

00:43:30.205 --> 00:43:32.230
尤其是想想做了什么

00:43:32.230 --> 00:43:34.570
文本看起来就像我们从n-gram语言模型中得到的，

00:43:34.570 --> 00:43:36.160
关于黄金的价格。

00:43:36.160 --> 00:43:39.715
嗯，我想说这比那好得多。

00:43:39.715 --> 00:43:41.620
总体来说，它似乎更流畅。

00:43:41.620 --> 00:43:43.690
呃，我想说它有更多的

00:43:43.690 --> 00:43:48.535
一个持续的环境，在这种情况下，一次延长时间是有意义的，

00:43:48.535 --> 00:43:51.666
我想说这听起来也完全像奥巴马。

00:43:51.666 --> 00:43:53.035
所有这些都很好，

00:43:53.035 --> 00:43:55.735
但你可以看到，总体来说，它仍然很不连贯，

00:43:55.735 --> 00:43:58.930
就像我一样——读起来很困难，因为它没有真正意义，对吧？

00:43:58.930 --> 00:44:00.130
所以我必须仔细读单词。

00:44:00.130 --> 00:44:02.890
嗯，所以，是的，我想这场演出

00:44:02.890 --> 00:44:06.310
使用RNN生成文本可以获得一些进展，但是，

00:44:06.310 --> 00:44:09.610
嗯，离人类水平很远。下面是更多的例子。

00:44:09.610 --> 00:44:13.285
嗯，这里有一个RNN语言模型，是在哈利波特的书上训练的。

00:44:13.285 --> 00:44:17.095
这就是它所说的。”“对不起。”哈利惊慌失措地喊道。

00:44:17.095 --> 00:44:19.600
“我把扫帚留在伦敦。”是吗？

00:44:19.600 --> 00:44:21.880
“不知道。”几乎没有头的尼克说，

00:44:21.880 --> 00:44:23.740
在塞德里克附近低抛，

00:44:23.740 --> 00:44:26.980
从哈利肩上拿着最后一点蜜糖。

00:44:26.980 --> 00:44:29.290
为了回答他，公共休息室就在上面，

00:44:29.290 --> 00:44:33.025
四只手拿着一个闪亮的旋钮，蜘蛛感觉不到。

00:44:33.025 --> 00:44:34.855
他也到了队里。”

00:44:34.855 --> 00:44:38.065
所以，我再说一遍，这是相当流利的。

00:44:38.065 --> 00:44:40.000
听起来很像《哈利波特》的书。

00:44:40.000 --> 00:44:41.710
事实上，它的作用给我留下了很深的印象

00:44:41.710 --> 00:44:44.170
听起来像是哈利波特书中的声音。

00:44:44.170 --> 00:44:46.510
你甚至有一些性格特征，

00:44:46.510 --> 00:44:50.395
我想说哈利这个角色经常在书中惊慌失措，所以这似乎是对的。

00:44:50.395 --> 00:44:54.520
嗯，[笑声]但是我们有一些不好的东西，

00:44:54.520 --> 00:44:58.660
例如，第二段中的一个很长的句子很难阅读。

00:44:58.660 --> 00:45:01.490
呃，你有些毫无意义的东西。

00:45:01.490 --> 00:45:03.195
我不知道糖浆的魅力是什么。

00:45:03.195 --> 00:45:04.890
听起来很好吃，但我不认为是真的，

00:45:04.890 --> 00:45:07.790
嗯，总的来说，这简直是胡说八道。

00:45:07.790 --> 00:45:12.865
这是另一个例子。这里有一个RNN语言模型，它是在食谱上训练的。

00:45:12.865 --> 00:45:16.000
所以，呃，[笑声]这个很奇怪，

00:45:16.000 --> 00:45:18.565
标题是“巧克力农场烧烤”，

00:45:18.565 --> 00:45:20.950
里面有帕尔马干酪，

00:45:20.950 --> 00:45:25.555
椰奶，鸡蛋，食谱上说，把每一个意大利面放在一层一层的块状物上，

00:45:25.555 --> 00:45:29.500
将混合物放入中等大小的烤箱中，用文火炖至牢固。

00:45:29.500 --> 00:45:31.210
热的酒体新鲜，

00:45:31.210 --> 00:45:32.575
芥末橙和奶酪。

00:45:32.575 --> 00:45:35.815
把奶酪和盐放在一个大平底锅里，把面团搅拌在一起；

00:45:35.815 --> 00:45:38.140
加入配料，加入巧克力和胡椒粉。

00:45:38.140 --> 00:45:41.635
[笑声]嗯，我认为有一件事

00:45:41.635 --> 00:45:45.340
这里的食谱比散文更清楚，

00:45:45.340 --> 00:45:49.405
是不是无法记起（噪音）总体上发生了什么，对吗？

00:45:49.405 --> 00:45:53.020
因为你能说的菜谱很有挑战性，因为你需要记住

00:45:53.020 --> 00:45:57.100
你要做的东西的标题，在本例中是巧克力农场烧烤，

00:45:57.100 --> 00:45:59.470
实际上，你需要，你知道，到最后把它做好。

00:45:59.470 --> 00:46:01.060
嗯，你还需要记住原料是什么

00:46:01.060 --> 00:46:02.500
一开始你用过吗？

00:46:02.500 --> 00:46:05.230
在食谱中，如果你做了一些东西然后把它放进烤箱，

00:46:05.230 --> 00:46:07.720
你以后要把它拿出来，对吧？

00:46:07.720 --> 00:46:09.400
所以，显然不是真的

00:46:09.400 --> 00:46:11.890
记住总体上发生的事情或它试图做的事情，

00:46:11.890 --> 00:46:13.915
它似乎只是产生一种

00:46:13.915 --> 00:46:17.785
一般的配方句子，并按随机顺序排列。

00:46:17.785 --> 00:46:20.635
嗯，但是，我的意思是，我们可以看到它相当流利，

00:46:20.635 --> 00:46:23.350
它在语法上是正确的，听起来像是一个食谱。

00:46:23.350 --> 00:46:25.855
呃，但问题是这只是胡说八道。

00:46:25.855 --> 00:46:28.300
例如，把混合物塑造成

00:46:28.300 --> 00:46:31.345
适度的烤箱是语法上的，但它没有任何意义。

00:46:31.345 --> 00:46:33.295
好的，最后一个例子。

00:46:33.295 --> 00:46:37.510
所以，这里有一个RNN语言模型，它训练了绘画颜色名称。

00:46:37.510 --> 00:46:41.200
这是一个字符级语言模型的例子，因为

00:46:41.200 --> 00:46:44.845
它预测的是下一个角色而不是下一个单词。

00:46:44.845 --> 00:46:47.650
这就是它能想出新词的原因。

00:46:47.650 --> 00:46:49.840
另一个需要注意的是，这个语言模型

00:46:49.840 --> 00:46:52.090
受过训练，以某种输入为条件。

00:46:52.090 --> 00:46:55.780
所以这里输入的是颜色本身，我想用三个数字来表示，

00:46:55.780 --> 00:46:57.145
这可能是RGB数字。

00:46:57.145 --> 00:47:00.925
它产生了一些颜色的名称。

00:47:00.925 --> 00:47:02.140
我觉得这很有趣。

00:47:02.140 --> 00:47:04.060
我最喜欢的是臭豆子，

00:47:04.060 --> 00:47:05.140
在右下角。

00:47:05.140 --> 00:47:07.930
[笑声]嗯，这很有创意，

00:47:07.930 --> 00:47:10.210
[笑声]我觉得这些听起来有点像

00:47:10.210 --> 00:47:13.360
喜欢油漆颜色，但常常很奇怪。

00:47:13.360 --> 00:47:20.570
[笑声]爆炸光也很好。

00:47:20.910 --> 00:47:23.500
所以，呃，你会了解更多关于

00:47:23.500 --> 00:47:25.765
未来讲座中的角色级语言模型，

00:47:25.765 --> 00:47:28.870
你还将进一步了解如何调节语言模型

00:47:28.870 --> 00:47:32.440
基于某种输入，比如颜色，嗯，代码。

00:47:32.440 --> 00:47:34.330
所以，这些很有趣，

00:47:34.330 --> 00:47:35.890
呃，但我想警告你。

00:47:35.890 --> 00:47:38.920
嗯，你会在网上找到很多这样的文章，

00:47:38.920 --> 00:47:40.585
嗯，经常有头条新闻，比如，

00:47:40.585 --> 00:47:43.000
“我们强迫机器人观看，你知道，

00:47:43.000 --> 00:47:46.705
1000小时的科幻电影，它写了一个剧本，“差不多。

00:47:46.705 --> 00:47:50.800
嗯，所以，我的建议是你必须用一小撮盐吃这些，因为通常，

00:47:50.800 --> 00:47:53.080
呃，人们上网的例子是

00:47:53.080 --> 00:47:55.375
人类选择的手是最有趣的例子。

00:47:55.375 --> 00:47:58.660
就像我认为我今天展示的所有例子都是手工选择的

00:47:58.660 --> 00:48:02.200
被人类当作RNN提出的最有趣的例子。

00:48:02.200 --> 00:48:05.455
在某些情况下，它们甚至可能被人类编辑过。

00:48:05.455 --> 00:48:08.560
所以，嗯，是的，当你看这些例子的时候，你需要有点怀疑。

00:48:08.560 --> 00:48:10.195
[重叠]是的。

00:48:10.195 --> 00:48:12.925
所以，呃，在《哈利波特一号》中，

00:48:12.925 --> 00:48:16.630
有一个开场白，然后有一个收场白。

00:48:16.630 --> 00:48:18.745
所以，就像你期待RNN一样，

00:48:18.745 --> 00:48:22.000
就像当它把开场白写出来，并且不断地放更多的字，

00:48:22.000 --> 00:48:28.825
你预计收盘价的概率会随着你的去向增加还是减少？

00:48:28.825 --> 00:48:31.150
这是个很好的问题。所以，呃，

00:48:31.150 --> 00:48:32.515
问题是，呃，

00:48:32.515 --> 00:48:34.450
我们注意到在《哈利波特》的例子中，

00:48:34.450 --> 00:48:36.295
有一些开放式报价和一些封闭式报价。

00:48:36.295 --> 00:48:38.410
看起来这个模型没搞砸，对吧？

00:48:38.410 --> 00:48:40.075
所有这些开放式报价和封闭式报价，

00:48:40.075 --> 00:48:41.815
嗯，在正确的地方。

00:48:41.815 --> 00:48:44.455
所以，问题是，我们是否希望模型

00:48:44.455 --> 00:48:48.775
在“报价”段落中给出的报价关闭概率更高？

00:48:48.775 --> 00:48:51.115
所以，我应该说是的，而且

00:48:51.115 --> 00:48:54.220
这是最重要的——主要是解释为什么这项工作。

00:48:54.220 --> 00:48:56.500
嗯，有一些非常有趣的尝试工作

00:48:56.500 --> 00:48:58.540
看看隐藏的状态，呃，

00:48:58.540 --> 00:49:01.345
语言模型来看看它是否在跟踪诸如，

00:49:01.345 --> 00:49:03.610
我们是在开盘价还是在收盘价？

00:49:03.610 --> 00:49:06.430
有一些有限的证据表明

00:49:06.430 --> 00:49:09.370
也许在隐藏状态中有一些神经元，

00:49:09.370 --> 00:49:10.900
他们在跟踪一些事情，比如，

00:49:10.900 --> 00:49:12.550
我们目前是否在报价中？

00:49:12.550 --> 00:49:13.855
[噪音]。是啊。

00:49:13.855 --> 00:49:18.370
所以，就像你认为随着向右（重叠）的增加，概率会增加吗？

00:49:18.370 --> 00:49:22.270
所以，问题是，随着引文篇幅的延长，

00:49:22.270 --> 00:49:23.740
你认为优先还是

00:49:23.740 --> 00:49:26.770
输出闭式报价的概率应该增加？

00:49:26.770 --> 00:49:28.045
嗯，我不知道。

00:49:28.045 --> 00:49:31.420
也许吧。嗯，我想那很好，

00:49:31.420 --> 00:49:32.980
因为你不想要一个无限的引用，

00:49:32.980 --> 00:49:35.650
呃，但如果不是这样的话我不会感到惊讶的。

00:49:35.650 --> 00:49:39.400
就像我不会惊讶，如果其他一些更糟糕的语言模式，

00:49:39.400 --> 00:49:41.395
刚刚打开报价单，但从未关闭。

00:49:41.395 --> 00:49:44.815
呃，还有其他问题吗？是啊。

00:49:44.815 --> 00:49:47.605
W公制的尺寸是多少？

00:49:47.605 --> 00:49:50.710
可以。那么，问题是w度量的维度是什么？

00:49:50.710 --> 00:49:52.480
所以我们要回到网上。

00:49:52.480 --> 00:49:55.900
嗯，好吧。你在问我关于“W”或“W”或其他什么？

00:49:55.900 --> 00:49:56.610
是啊。

00:49:56.610 --> 00:49:58.960
所以，我们会

00:49:58.960 --> 00:50:01.435
呃，如果我们说隐藏的尺寸是N，

00:50:01.435 --> 00:50:07.240
那么w_h将是n乘n，如果我们假设嵌入件的尺寸为d，

00:50:07.240 --> 00:50:08.635
然后我们会，呃，

00:50:08.635 --> 00:50:12.550
也许是N，N，D。

00:50:12.550 --> 00:50:19.990
那能回答你的问题吗？[噪音]呃，

00:50:19.990 --> 00:50:23.380
关于生成还有其他问题吗？是的。

00:50:23.380 --> 00:50:28.030
那么，你说《哈利波特》中有一个长句子？

00:50:28.030 --> 00:50:28.425
是啊。

00:50:28.425 --> 00:50:33.640
在这个手写规则中，将RNN和LIKE结合起来是否有点实际？

00:50:33.640 --> 00:50:35.395
对不起的。结合起来是否可行？-

00:50:35.395 --> 00:50:37.810
有手写规则的书面清单的RNN。

00:50:37.810 --> 00:50:38.830
[重叠]

00:50:38.830 --> 00:50:39.880
可以。是啊。这是个很好的问题。

00:50:39.880 --> 00:50:42.220
所以问题是，它是否曾经实用

00:50:42.220 --> 00:50:44.980
结合RNN和手写规则列表？

00:50:44.980 --> 00:50:49.285
例如，不要让你的句子比这许多单词长。

00:50:49.285 --> 00:50:50.530
嗯，是的。

00:50:50.530 --> 00:50:54.070
我想说这可能是可行的，尤其是如果你对，呃，

00:50:54.070 --> 00:50:56.260
确保某些不好的事情不会发生，

00:50:56.260 --> 00:51:01.900
你可能会应用一些黑客规则，比如“是的”，迫使它早点结束。

00:51:01.900 --> 00:51:03.580
我是说，好吧。所以这就是所谓的光束搜索

00:51:03.580 --> 00:51:05.335
我们将在稍后的讲座中了解到，

00:51:05.335 --> 00:51:06.640
基本上不仅仅是，

00:51:06.640 --> 00:51:09.340
嗯，在每一步中选择一个单词并继续。

00:51:09.340 --> 00:51:12.325
它探索了许多不同的选择，你可以生成的单词。

00:51:12.325 --> 00:51:14.410
你可以在上面应用一些规则

00:51:14.410 --> 00:51:16.540
如果你有很多不同的选择，

00:51:16.540 --> 00:51:18.250
那你就可以摆脱

00:51:18.250 --> 00:51:21.265
如果你不喜欢它们，因为它们违反了你的一些规则。

00:51:21.265 --> 00:51:28.340
但是，嗯，这可能很难做到。还有其他问题吗？

00:51:29.490 --> 00:51:38.380
可以。嗯，我们已经讨论过从语言模型生成。

00:51:38.380 --> 00:51:40.630
呃，很不幸，你不能只用

00:51:40.630 --> 00:51:44.140
生成作为语言模型的评估指标。

00:51:44.140 --> 00:51:47.245
你确实需要某种，嗯，可测量的度量标准。

00:51:47.245 --> 00:51:52.015
因此，语言模型的标准评估指标被称为困惑。

00:51:52.015 --> 00:51:54.250
困惑的定义是

00:51:54.250 --> 00:51:58.480
基于语言模型的语料库逆概率。

00:51:58.480 --> 00:52:02.200
所以，如果你看它，你会发现这就是这个公式的意思。

00:52:02.200 --> 00:52:04.075
它的意思是每一个，呃，

00:52:04.075 --> 00:52:07.555
单词xt，小写t，在语料库中，

00:52:07.555 --> 00:52:10.420
我们正在计算这个词的概率

00:52:10.420 --> 00:52:13.630
到目前为止，一切都是如此，但它的反方向是一个。

00:52:13.630 --> 00:52:16.600
最后，当标准化这个大的

00:52:16.600 --> 00:52:19.960
嗯，按字数计算的产品，

00:52:19.960 --> 00:52:23.995
这就是Capital T.我们之所以这么做是因为如果我们不这么做，

00:52:23.995 --> 00:52:28.195
然后随着语料库的增大，困惑会越来越小。

00:52:28.195 --> 00:52:31.070
所以我们需要用这个因素来规范化。

00:52:31.140 --> 00:52:33.910
所以，你可以给你看这个，呃，

00:52:33.910 --> 00:52:38.470
困惑度等于交叉熵损失的指数jθ。

00:52:38.470 --> 00:52:41.470
所以如果你记得交叉熵损失，θ是，呃，

00:52:41.470 --> 00:52:44.305
我们用来训练语言模型的训练目标。

00:52:44.305 --> 00:52:46.555
而且，嗯，通过重新安排一些事情，

00:52:46.555 --> 00:52:50.890
你可以看到困惑实际上是交叉熵的指数。

00:52:50.890 --> 00:52:52.750
这是件好事，呃，

00:52:52.750 --> 00:52:55.750
因为如果我们训练语言模型

00:52:55.750 --> 00:52:58.900
使交叉熵损失最小化，

00:52:58.900 --> 00:53:04.070
然后你也在训练它来优化困惑。

00:53:04.800 --> 00:53:08.860
所以你应该记住，越低的困惑越好，

00:53:08.860 --> 00:53:12.640
因为困惑是语料库的逆概率。

00:53:12.640 --> 00:53:17.965
所以，如果你想让你的语言模型为语料库分配高概率，对吗？

00:53:17.965 --> 00:53:21.470
那就意味着你想降低困惑。

00:53:21.600 --> 00:53:28.480
呃，有问题吗？[噪音]好的。

00:53:28.480 --> 00:53:36.220
嗯，所以RNN近年来在改善困惑方面相当成功。

00:53:36.220 --> 00:53:39.880
这是最近的结果表，

00:53:39.880 --> 00:53:43.630
嗯，Facebook关于RNN语言模型的研究论文。

00:53:43.630 --> 00:53:46.600
而且，呃，你不必理解这张桌子的所有细节，

00:53:46.600 --> 00:53:48.055
但它告诉你的是，

00:53:48.055 --> 00:53:50.785
在上面，我们有N克语言模型。

00:53:50.785 --> 00:53:52.240
以及后面的各种，

00:53:52.240 --> 00:53:55.735
我们有一些日益复杂和庞大的RNN。

00:53:55.735 --> 00:53:58.945
你可以看到困惑的数字在减少，

00:53:58.945 --> 00:54:00.475
因为越低越好。

00:54:00.475 --> 00:54:02.770
所以RNN非常适合

00:54:02.770 --> 00:54:06.320
在过去的几年里建立更有效的语言模型。

00:54:08.910 --> 00:54:11.695
可以。所以缩小一点，

00:54:11.695 --> 00:54:13.120
你可能在想，呃，

00:54:13.120 --> 00:54:15.460
为什么我要关心语言建模？

00:54:15.460 --> 00:54:17.350
为什么很重要？我会说有

00:54:17.350 --> 00:54:19.735
语言建模之所以重要有两个主要原因。

00:54:19.735 --> 00:54:21.160
呃，所以第一个是，

00:54:21.160 --> 00:54:23.620
语言建模是一项基准任务，

00:54:23.620 --> 00:54:26.770
帮助我们衡量我们在理解语言方面的进展。

00:54:26.770 --> 00:54:28.540
因此，您可以将语言建模视为

00:54:28.540 --> 00:54:31.990
一个相当普遍的语言理解任务，对吧？

00:54:31.990 --> 00:54:35.425
因为预测任何一个词旁边的词，

00:54:35.425 --> 00:54:37.795
任何一种，呃，通用文本。

00:54:37.795 --> 00:54:40.975
嗯，这是一个非常困难和普遍的问题。

00:54:40.975 --> 00:54:43.330
为了擅长语言建模，

00:54:43.330 --> 00:54:45.340
你必须了解很多事情，对吗？

00:54:45.340 --> 00:54:46.780
你必须理解语法，

00:54:46.780 --> 00:54:48.115
你必须理解语法，

00:54:48.115 --> 00:54:49.615
你必须明白，

00:54:49.615 --> 00:54:51.115
嗯，逻辑和推理。

00:54:51.115 --> 00:54:52.570
你必须了解

00:54:52.570 --> 00:54:53.845
你知道，现实世界的知识。

00:54:53.845 --> 00:54:55.720
你必须了解很多事情才能

00:54:55.720 --> 00:54:57.970
能够正确进行语言建模。

00:54:57.970 --> 00:54:59.530
所以，我们关心它的原因是

00:54:59.530 --> 00:55:02.350
基准测试任务是因为如果您能够构建模型，

00:55:02.350 --> 00:55:05.050
哪种语言模式比之前的模式更好，

00:55:05.050 --> 00:55:07.930
那么你一定在上取得了一些进展

00:55:07.930 --> 00:55:11.620
至少是自然语言理解的一些子成分。

00:55:11.620 --> 00:55:14.470
所以，还有一个更具体的原因

00:55:14.470 --> 00:55:16.930
关注语言建模是

00:55:16.930 --> 00:55:19.990
许多NLP任务，特别是涉及

00:55:19.990 --> 00:55:23.560
生成文本或估计文本的概率。

00:55:23.560 --> 00:55:25.675
这里有很多例子。

00:55:25.675 --> 00:55:27.220
一种是预测性打字。

00:55:27.220 --> 00:55:29.170
这就是我们在讲座开始时展示的例子

00:55:29.170 --> 00:55:31.450
在手机上打字或在谷歌上搜索。

00:55:31.450 --> 00:55:35.185
呃，这对那些有运动障碍的人也很有用，

00:55:35.185 --> 00:55:39.595
因为它们是帮助人们用更少的动作交流的系统。

00:55:39.595 --> 00:55:41.920
另一个例子是语音识别。

00:55:41.920 --> 00:55:43.600
所以，在语音识别中

00:55:43.600 --> 00:55:45.820
一个人说话的录音

00:55:45.820 --> 00:55:49.975
而且常常有点吵，很难理解他们在说什么，你需要，

00:55:49.975 --> 00:55:51.700
呃，想想他们说了什么。

00:55:51.700 --> 00:55:55.300
所以这是一个例子，你必须估计不同的概率，

00:55:55.300 --> 00:55:58.210
呃，他们本可以说的话有不同的选择。

00:55:58.210 --> 00:56:00.445
同样，手写识别，

00:56:00.445 --> 00:56:02.410
有很多噪音的例子吗？

00:56:02.410 --> 00:56:05.470
你必须弄清楚那个人想说什么。

00:56:05.470 --> 00:56:07.810
嗯，拼写和语法修正还没有完成

00:56:07.810 --> 00:56:10.705
另一个例子是试图弄清楚某人的意思。

00:56:10.705 --> 00:56:12.340
这意味着你真的明白

00:56:12.340 --> 00:56:14.695
很可能是他们说了不同的话。

00:56:14.695 --> 00:56:19.555
一个有趣的应用程序是作者身份识别。

00:56:19.555 --> 00:56:22.480
所以假设你有一段文字，你想

00:56:22.480 --> 00:56:25.495
找出可能是谁写的，也许是你写的，

00:56:25.495 --> 00:56:29.830
嗯，好几个不同的作者，你有不同作者写的文章。

00:56:29.830 --> 00:56:31.285
例如，你可以

00:56:31.285 --> 00:56:34.720
在每个不同作者的文本上训练一个单独的语言模型。

00:56:34.720 --> 00:56:36.160
然后，因为，记住，

00:56:36.160 --> 00:56:39.805
语言模型可以告诉你给定文本的概率。

00:56:39.805 --> 00:56:42.430
然后你可以问所有不同的语言模式，

00:56:42.430 --> 00:56:45.790
嗯，文本和问题的可能性有多大，

00:56:45.790 --> 00:56:49.720
如果某个作者的语言模型说有可能

00:56:49.720 --> 00:56:55.000
这意味着文本、文本和问题更有可能是作者写的。

00:56:55.000 --> 00:56:57.820
嗯，其他例子包括机器翻译。

00:56:57.820 --> 00:56:59.200
这是一个巨大的，呃，

00:56:59.200 --> 00:57:01.390
语言模型的应用，

00:57:01.390 --> 00:57:03.565
因为这都是关于生成文本。

00:57:03.565 --> 00:57:05.740
嗯，类似地，总结是

00:57:05.740 --> 00:57:09.280
在给定一些输入文本的情况下，我们需要生成一些文本的任务。

00:57:09.280 --> 00:57:11.185
嗯，还有对话，

00:57:11.185 --> 00:57:14.980
并非所有对话代理都是RNN语言模型，但您可以

00:57:14.980 --> 00:57:19.285
构建一个对话代理，使用RNN语言模型生成文本。

00:57:19.285 --> 00:57:21.560
还有更多的例子。

00:57:21.560 --> 00:57:25.360
有什么问题吗？[笑声]是的。

00:57:25.360 --> 00:57:47.875
所以，我知道[听不见]

00:57:47.875 --> 00:57:49.945
好问题。所以，问题是，

00:57:49.945 --> 00:57:51.475
呃，对于其中一些例子，呃，

00:57:51.475 --> 00:57:55.315
例如语音识别或[噪音]图像字幕，

00:57:55.315 --> 00:57:59.290
输入的是音频或图像或不是文本的东西，对吗？

00:57:59.290 --> 00:58:01.780
所以，你不能用我们之前讨论过的方式来代表它。

00:58:01.780 --> 00:58:04.180
在这些例子中，

00:58:04.180 --> 00:58:06.460
您将有一些方法来表示输入，

00:58:06.460 --> 00:58:08.725
某种编码音频或图像的方法。

00:58:08.725 --> 00:58:13.315
呃，我现在从语言模型的角度提出它的原因是，这就是输入，

00:58:13.315 --> 00:58:15.685
但是您使用语言模型来获取输出，对吗？

00:58:15.685 --> 00:58:17.170
所以，语言模型[noise]生成

00:58:17.170 --> 00:58:19.345
我们之前看到的输出，呃，

00:58:19.345 --> 00:58:22.120
但我们稍后将进一步了解这些条件语言（噪声）模型。

00:58:22.120 --> 00:58:25.090
[噪音]还有人吗？

00:58:25.090 --> 00:58:29.020
[噪音]好的。

00:58:29.020 --> 00:58:32.965
[噪音]那么，呃，这是一个总结。

00:58:32.965 --> 00:58:36.730
如果我在演讲中找不到你，呃，或者你累了，

00:58:36.730 --> 00:58:38.770
嗯，现在是重新开始的好时机

00:58:38.770 --> 00:58:41.050
因为事情会变得更容易接近。

00:58:41.050 --> 00:58:43.045
可以。下面是我们今天所做的回顾。

00:58:43.045 --> 00:58:46.210
语言模型是预测下一个单词的系统，

00:58:46.210 --> 00:58:48.460
[噪音]和循环神经网络，

00:58:48.460 --> 00:58:50.590
是个新家庭，哦，对我们来说是个新家庭，

00:58:50.590 --> 00:58:53.710
一类接受顺序输入的神经网络。

00:58:53.710 --> 00:58:57.175
任何长度，在每一步上施加相同的重量，

00:58:57.175 --> 00:58:59.620
它可以选择性地在

00:58:59.620 --> 00:59:02.020
每一步或某些步骤，或没有任何步骤。

00:59:02.020 --> 00:59:04.945
[噪音]所以，不要混淆。

00:59:04.945 --> 00:59:08.305
循环神经网络与语言模型不同。

00:59:08.305 --> 00:59:12.970
嗯，我们今天已经看到RNN是构建语言模型的好方法，但是实际上，

00:59:12.970 --> 00:59:15.010
事实证明你可以使用RNN，

00:59:15.010 --> 00:59:17.710
呃，很多其他不同的东西不是语言建模。

00:59:17.710 --> 00:59:19.840
[噪音]这里有几个例子。

00:59:19.840 --> 00:59:24.085
[噪音]嗯，你可以用RNN做标记任务。

00:59:24.085 --> 00:59:26.320
因此，标记任务的一些例子是

00:59:26.320 --> 00:59:29.260
语音标记和命名实体识别的一部分。

00:59:29.260 --> 00:59:32.590
所以，这里的图片是语音标记的一部分，这就是任务。

00:59:32.590 --> 00:59:35.245
我们有一些输入文本，比如，

00:59:35.245 --> 00:59:37.645
惊吓的猫打翻了花瓶，

00:59:37.645 --> 00:59:39.385
你的工作是，

00:59:39.385 --> 00:59:42.085
给每个单词加上词性标签或标签。

00:59:42.085 --> 00:59:45.160
例如，cat是一个名词，knocked是一个动词。

00:59:45.160 --> 00:59:48.205
因此，您可以使用RNN在中执行此任务，

00:59:48.205 --> 00:59:50.350
就像我们想象的那样，你，呃，

00:59:50.350 --> 00:59:52.720
将文本输入RNN，[噪声]然后，

00:59:52.720 --> 00:59:53.905
在RNN的每一步上，

00:59:53.905 --> 00:59:55.705
你，呃，有输出，

00:59:55.705 --> 00:59:57.790
可能是关于什么，呃，

00:59:57.790 --> 01:00:01.775
标记你认为它是，然后，呃，你可以这样标记它。

01:00:01.775 --> 01:00:04.050
然后，对于命名实体识别，

01:00:04.050 --> 01:00:05.190
就这些，嗯，

01:00:05.190 --> 01:00:08.085
用它们的命名实体类型标记每个单词。

01:00:08.085 --> 01:00:11.820
所以，你也是这样做的。[噪音]好的。

01:00:11.820 --> 01:00:13.470
还有一件事你可以用RNN，

01:00:13.470 --> 01:00:16.200
嗯，你可以用它们来进行句子分类。

01:00:16.200 --> 01:00:19.080
所以，句子分类只是一个通用术语

01:00:19.080 --> 01:00:22.170
任何你想做句子或其他文字的任务，

01:00:22.170 --> 01:00:24.945
然后，你想把它分为几个类中的一个。

01:00:24.945 --> 01:00:28.120
所以，一个例子就是情绪分类。

01:00:28.120 --> 01:00:30.400
嗯，情绪分类是当你有某种

01:00:30.400 --> 01:00:32.680
输入文本，例如，总的来说，

01:00:32.680 --> 01:00:34.510
我很喜欢这部电影，然后，

01:00:34.510 --> 01:00:35.770
你想把它归类为

01:00:35.770 --> 01:00:38.095
积极或消极或[噪音]中立的情绪。

01:00:38.095 --> 01:00:40.090
所以，在这个例子中，这是一种积极的情绪。

01:00:40.090 --> 01:00:45.400
[噪音]所以，你可以用RNN来处理这个任务的一种方法是，呃，

01:00:45.400 --> 01:00:49.450
您可以使用RNN对文本进行编码，然后，

01:00:49.450 --> 01:00:53.350
你真正想要的是某种句子编码，这样你

01:00:53.350 --> 01:00:57.265
可以输出句子的标签，对吗？

01:00:57.265 --> 01:00:59.680
如果你有一个向量

01:00:59.680 --> 01:01:02.965
表示句子，而不是所有这些单独的向量。

01:01:02.965 --> 01:01:04.870
那么，你会怎么做？

01:01:04.870 --> 01:01:07.000
如何从RNN中获取句子编码？

01:01:07.000 --> 01:01:10.540
你能做的一件事就是，

01:01:10.540 --> 01:01:14.290
您可以使用最后的隐藏状态作为句子编码。

01:01:14.290 --> 01:01:18.460
所以，嗯，你认为这是个好主意的原因是，

01:01:18.460 --> 01:01:19.810
例如，在RNN中，

01:01:19.810 --> 01:01:22.675
我们认为，最终的隐藏状态是，

01:01:22.675 --> 01:01:25.735
嗯，这是你用来预测接下来会发生什么的事情，对吧？

01:01:25.735 --> 01:01:28.300
所以，我们假设最终的隐藏状态包含

01:01:28.300 --> 01:01:31.465
关于到目前为止所有文本的信息，对吗？

01:01:31.465 --> 01:01:34.990
因此，出于这个原因，你可能会认为这是一个很好的句子编码，

01:01:34.990 --> 01:01:36.460
我们可以用噪音来预测，

01:01:36.460 --> 01:01:39.040
这句话是什么感想？

01:01:39.040 --> 01:01:41.350
事实证明，通常情况下，这是一种更好的方法，

01:01:41.350 --> 01:01:42.595
通常是更有效的方法，

01:01:42.595 --> 01:01:46.240
是做一些事情，比如可能取元素的最大值，或者

01:01:46.240 --> 01:01:50.080
所有这些隐藏状态的元素含义，以获得句子编码，

01:01:50.080 --> 01:01:52.345
嗯，[噪音]还有，呃，

01:01:52.345 --> 01:01:54.640
这比使用最终隐藏状态更有效。

01:01:54.640 --> 01:01:58.490
[噪音]嗯，还有一些更高级的事情你也可以做。

01:01:59.310 --> 01:02:02.215
可以。[噪音]另一件事你可以用RNN

01:02:02.215 --> 01:02:05.335
是一个通用编码器模块。

01:02:05.335 --> 01:02:08.470
呃，这里有个回答问题的例子，

01:02:08.470 --> 01:02:10.480
但实际上，RNN的概念是

01:02:10.480 --> 01:02:15.085
通用编码器模块是非常常见的[噪声]并在许多不同的情况下使用，

01:02:15.085 --> 01:02:17.590
嗯，NLP的深度学习[噪音]架构。

01:02:17.590 --> 01:02:21.175
[噪音]那么，这里有一个回答问题的例子。

01:02:21.175 --> 01:02:23.410
呃，那么，假设，任务是，

01:02:23.410 --> 01:02:24.670
你有某种背景，

01:02:24.670 --> 01:02:26.110
在这种情况下，

01:02:26.110 --> 01:02:29.365
是关于贝多芬的维基百科文章，然后，

01:02:29.365 --> 01:02:31.210
你有个问题要问，

01:02:31.210 --> 01:02:33.070
贝多芬是什么国籍的？

01:02:33.070 --> 01:02:36.400
嗯，这实际上是从球队的挑战中得到的，

01:02:36.400 --> 01:02:38.680
这是默认最终项目的主题。

01:02:38.680 --> 01:02:41.770
所以，嗯，如果你选择做-做默认的最终项目，

01:02:41.770 --> 01:02:44.950
你将要建立解决这个问题的系统。

01:02:44.950 --> 01:02:49.930
所以，你可能会用RNN来处理这个问题，

01:02:49.930 --> 01:02:51.970
贝多芬是什么国籍的？

01:02:51.970 --> 01:02:56.215
然后，你可以利用这些隐藏的状态，呃，

01:02:56.215 --> 01:03:00.280
问题的RNN表示问题。

01:03:00.280 --> 01:03:03.580
我在这里故意含糊不清地说接下来会发生什么，

01:03:03.580 --> 01:03:05.200
但你的想法是你有[噪音]

01:03:05.200 --> 01:03:08.500
背景和问题都将以某种方式得到满足，

01:03:08.500 --> 01:03:10.900
也许你也会在上下文中使用RNN，

01:03:10.900 --> 01:03:14.485
为了得到你的答案，你会有更多的神经架构，

01:03:14.485 --> 01:03:15.895
这是德语。

01:03:15.895 --> 01:03:21.355
所以，这里的重点是RNN作为问题的编码器，

01:03:21.355 --> 01:03:23.920
也就是说，你从跑步中得到的隐藏状态

01:03:23.920 --> 01:03:26.650
问题的RNN代表问题。

01:03:26.650 --> 01:03:31.810
[噪音]呃，编码器是一个更大的神经系统的一部分，

01:03:31.810 --> 01:03:33.940
[噪音]这就是隐藏状态本身

01:03:33.940 --> 01:03:36.295
你感兴趣的，因为它们包含信息。

01:03:36.295 --> 01:03:38.140
所以，你可以，嗯，拿走，

01:03:38.140 --> 01:03:39.700
嗯，元素的最大值或平均值，

01:03:39.700 --> 01:03:41.005
就像我们在上一张幻灯片中显示的那样，

01:03:41.005 --> 01:03:44.170
为了得到问题的一个向量，但通常不会这样做。

01:03:44.170 --> 01:03:48.160
通常，你会，呃，做一些其他直接使用隐藏状态的事情。

01:03:48.160 --> 01:03:53.440
所以，这里的一般观点是RNN作为一种表示的方法非常强大，

01:03:53.440 --> 01:03:54.925
一系列文字，

01:03:54.925 --> 01:03:57.710
呃，为了进一步计算。

01:03:58.170 --> 01:04:02.935
可以。最后一个例子。所以，再次回到RNN语言模型，[噪音]呃，

01:04:02.935 --> 01:04:04.570
它们可以用来生成文本，

01:04:04.570 --> 01:04:07.300
有很多不同的应用程序。

01:04:07.300 --> 01:04:11.020
例如，语音识别，呃，你会有你的输入，

01:04:11.020 --> 01:04:13.345
哪一个是音频，正如一个学生之前所问的，

01:04:13.345 --> 01:04:15.865
这将，呃，以某种方式代表，

01:04:15.865 --> 01:04:19.480
然后，呃，也许你会对它进行神经编码，然后，

01:04:19.480 --> 01:04:22.615
使用RNN语言模型生成输出，

01:04:22.615 --> 01:04:24.354
在这种情况下，它将是一个转录。

01:04:24.354 --> 01:04:26.275
关于录音的内容。

01:04:26.275 --> 01:04:28.030
所以，你会有一些调节的方法，

01:04:28.030 --> 01:04:29.830
我们要多谈谈这是怎么运作的，呃，

01:04:29.830 --> 01:04:31.780
在以后的讲座中，你有办法

01:04:31.780 --> 01:04:35.230
在输入上调整RNN语言模型。

01:04:35.230 --> 01:04:38.920
所以，您将使用它生成文本，[噪声]，在本例中，

01:04:38.920 --> 01:04:41.335
可能是说，天气怎么样，

01:04:41.335 --> 01:04:44.590
问号。[重叠][噪音]

01:04:44.590 --> 01:04:54.220
是啊。[噪音]

01:04:54.220 --> 01:04:58.120
在语音识别中，[听不见]。

01:04:58.120 --> 01:05:00.100
可以。所以，问题是，在语音识别中，

01:05:00.100 --> 01:05:02.755
我们经常用单词错误率来评估，

01:05:02.755 --> 01:05:04.690
但你会用困惑来评价吗？

01:05:04.690 --> 01:05:07.690
[噪音]嗯，我其实对此不太了解。你知道吗，克里斯，

01:05:07.690 --> 01:05:09.250
他们用什么，呃，

01:05:09.250 --> 01:05:15.010
语音识别作为评估指标？[噪音]

01:05:15.010 --> 01:05:23.590
[听不见]字错误率[听不见]。

01:05:23.590 --> 01:05:25.375
答案是，你经常使用WER，

01:05:25.375 --> 01:05:27.550
嗯，为了逃避，但你也可能会使用困惑。

01:05:27.550 --> 01:05:29.500
是啊。还有其他问题吗？

01:05:29.500 --> 01:05:35.575
[噪音]好的。所以，嗯，

01:05:35.575 --> 01:05:38.350
这是一个条件语言模型的例子，

01:05:38.350 --> 01:05:39.970
它被称为条件语言模型

01:05:39.970 --> 01:05:41.725
因为我们有语言模型组件，

01:05:41.725 --> 01:05:44.740
但最重要的是，我们正在根据某种输入对其进行调节。

01:05:44.740 --> 01:05:48.580
所以，不像《哈利波特》中有趣的例子，我们只是，呃，

01:05:48.580 --> 01:05:51.460
基本上无条件地生成文本，

01:05:51.460 --> 01:05:52.750
我们根据培训数据进行培训，然后，

01:05:52.750 --> 01:05:54.820
我们刚开始用某种随机的种子

01:05:54.820 --> 01:05:56.305
然后，它无条件地生成。

01:05:56.305 --> 01:05:58.540
这被称为条件语言模型

01:05:58.540 --> 01:06:01.615
因为我们需要一些输入条件。

01:06:01.615 --> 01:06:05.980
嗯，机器翻译也是一个条件语言模型的例子。

01:06:05.980 --> 01:06:07.780
我们将在

01:06:07.780 --> 01:06:09.520
下周关于机器翻译的讲座。

01:06:09.520 --> 01:06:12.895
[噪音]好的。还有问题吗？

01:06:12.895 --> 01:06:14.320
我想你有点多余的时间。

01:06:14.320 --> 01:06:17.665
[噪音]是的。

01:06:17.665 --> 01:06:20.350
我有一个关于RNN的问题。

01:06:20.350 --> 01:06:25.345
[噪音]人们有没有把RNN结合起来？

01:06:25.345 --> 01:06:27.220
呃，建筑模式，

01:06:27.220 --> 01:06:29.965
嗯，有其他神经网络吗？

01:06:29.965 --> 01:06:31.885
说，你有，嗯，你知道，

01:06:31.885 --> 01:06:34.285
n以前可以做任何事情的层，

01:06:34.285 --> 01:06:35.410
在你的网络末端，

01:06:35.410 --> 01:06:36.880
你想让他们跑过去，

01:06:36.880 --> 01:06:39.160
嗯，五个重复层。

01:06:39.160 --> 01:06:40.810
人们会像那样混搭吗？

01:06:40.810 --> 01:06:42.190
或者这些，呃，[听不见]。[噪音]

01:06:42.190 --> 01:06:46.090
呃，问题是，

01:06:46.090 --> 01:06:48.580
您是否曾经将RNN与其他类型的体系结构结合在一起？

01:06:48.580 --> 01:06:49.870
所以，我认为答案是肯定的。

01:06:49.870 --> 01:06:51.595
[噪音]呃，你可能[噪音]你知道，呃，

01:06:51.595 --> 01:06:55.210
你可能有其他类型的架构，呃，

01:06:55.210 --> 01:06:58.540
为了产生将作为RNN输入的向量，

01:06:58.540 --> 01:07:00.280
或者您可以使用RNN的输出

01:07:00.280 --> 01:07:03.320
[噪音]并将其输入不同类型的神经网络。

01:07:06.390 --> 01:07:08.620
所以，是的。[噪音]还有其他问题吗？

01:07:08.620 --> 01:07:11.820
[噪音]好的。

01:07:11.820 --> 01:07:15.510
嗯，那么，在我们结束之前，呃，我有一个关于术语的注释。

01:07:15.510 --> 01:07:17.490
呃，当你读报纸的时候，

01:07:17.490 --> 01:07:20.915
你可能经常会发现这个短语香草RNN，

01:07:20.915 --> 01:07:23.065
当你看到“香草RNN”这个短语时，

01:07:23.065 --> 01:07:24.535
这通常意味着，呃，

01:07:24.535 --> 01:07:26.905
本课描述的RNN。

01:07:26.905 --> 01:07:30.460
所以，这些被称为香草RNN的原因是

01:07:30.460 --> 01:07:34.765
因为实际上还有其他更复杂的RNN口味。

01:07:34.765 --> 01:07:38.005
例如，有GRU和LSTM，

01:07:38.005 --> 01:07:40.330
下周我们会了解这两种情况。

01:07:40.330 --> 01:07:42.610
还有一件事我们下周要了解

01:07:42.610 --> 01:07:45.085
[噪音]实际上你可以得到一些多层RNN，

01:07:45.085 --> 01:07:48.250
也就是说，您将多个RNN堆叠在一起。

01:07:48.250 --> 01:07:50.935
[噪音]所以，呃，你要了解这些，

01:07:50.935 --> 01:07:53.875
但我们希望在课程结束时，

01:07:53.875 --> 01:07:56.905
你将能够阅读一篇研究论文并看到一个短语

01:07:56.905 --> 01:08:01.150
具有剩余连接和自我关注的堆叠双向LSTM，

01:08:01.150 --> 01:08:02.680
你就知道那是什么了。

01:08:02.680 --> 01:08:04.840
[噪音]所有的配料都是RNN。

01:08:04.840 --> 01:08:07.840
[笑声]好吧。谢谢您。今天就到此为止。

01:08:07.840 --> 01:08:15.910
下一次-下一次

01:08:15.910 --> 01:08:18.340
我们正在学习问题[噪音]和花哨的RNN。

01:08:18.340 --> 01:08:24.770
[NOISE]

