WEBVTT
Kind: captions
Language: en

00:00:04.760 --> 00:00:09.990
所以今天我们要学习自然语言的生成。

00:00:09.990 --> 00:00:12.460
嗯，这可能会有点不同于

00:00:12.460 --> 00:00:16.380
我以前的课是因为这将是一种更像调查的方式，

00:00:16.380 --> 00:00:17.745
很多尖端的，呃，

00:00:17.745 --> 00:00:21.060
正在NLG中进行的研究主题。

00:00:21.060 --> 00:00:22.800
所以在我们开始之前，

00:00:22.800 --> 00:00:24.315
我们有几个通知。

00:00:24.315 --> 00:00:26.370
呃，所以我想主要的公告是，

00:00:26.370 --> 00:00:28.290
非常感谢你们的辛勤工作。

00:00:28.290 --> 00:00:31.020
我知道，嗯，最近一两周很艰难。

00:00:31.020 --> 00:00:33.450
嗯，第五次任务真的很难，

00:00:33.450 --> 00:00:35.300
我认为，在八天内完成这项工作是一项挑战。

00:00:35.300 --> 00:00:38.000
所以我们非常感谢你付出的努力。

00:00:38.000 --> 00:00:40.910
嗯，我们也知道项目提案是，

00:00:40.910 --> 00:00:45.590
嗯，有时候有点难以理解对某些人的期望。

00:00:45.590 --> 00:00:47.990
嗯，所以，是的，这两个都是

00:00:47.990 --> 00:00:50.465
去年没有参加的今年的课程。

00:00:50.465 --> 00:00:52.115
嗯，所以你知道，

00:00:52.115 --> 00:00:55.265
我们必须和教员一起经历一些学习曲线。

00:00:55.265 --> 00:00:57.710
所以我们真的很想说谢谢你，呃，

00:00:57.710 --> 00:01:00.365
把所有的东西都放到这门课上。

00:01:00.365 --> 00:01:03.230
请继续向我们提供您的反馈

00:01:03.230 --> 00:01:06.990
现在和季度末的反馈调查。

00:01:07.080 --> 00:01:10.685
好的，下面是我们今天要做的事情的概述。

00:01:10.685 --> 00:01:13.955
所以今天我们要了解世界上发生了什么

00:01:13.955 --> 00:01:16.960
自然语言生成的神经方法。

00:01:16.960 --> 00:01:18.690
呃，那是个超级，

00:01:18.690 --> 00:01:22.065
超宽泛的标题，自然语言的生成。

00:01:22.065 --> 00:01:25.685
嗯，NLG涵盖了各种各样的研究领域

00:01:25.685 --> 00:01:27.800
几乎每个人都可以

00:01:27.800 --> 00:01:29.825
他们自己的课，我们可以教一个整体，

00:01:29.825 --> 00:01:34.035
整整四分之一的课程。

00:01:34.035 --> 00:01:37.475
嗯，但我们今天要介绍一些精选的东西。

00:01:37.475 --> 00:01:41.520
而且，呃，呃，主要是，呃，

00:01:41.520 --> 00:01:42.895
在这些事情的指引下，呃，

00:01:42.895 --> 00:01:46.040
我已经看到我觉得很酷、有趣或令人兴奋。

00:01:46.040 --> 00:01:48.140
所以这绝不是全面的，但是

00:01:48.140 --> 00:01:51.300
我希望你会喜欢我们要学的东西。

00:01:52.070 --> 00:01:56.660
好吧，特别是我们要先回顾一下

00:01:56.660 --> 00:02:01.070
已经知道自然语言的生成，以确保我们在同一个页面上。

00:02:01.070 --> 00:02:04.235
我们还将学习一些额外的解码算法。

00:02:04.235 --> 00:02:05.870
所以我们以前学过，呃，

00:02:05.870 --> 00:02:08.329
贪婪解码和波束搜索解码，

00:02:08.329 --> 00:02:10.280
但是今天我们要学习一些关于

00:02:10.280 --> 00:02:13.085
以及其他类型的解码算法。

00:02:13.085 --> 00:02:15.710
在那之后我们要经历，嗯，

00:02:15.710 --> 00:02:17.570
非常快的旅行

00:02:17.570 --> 00:02:21.860
不同的NLG任务及其神经方法的选择。

00:02:21.860 --> 00:02:25.580
在那之后，我们将讨论NLG研究中最大的问题，

00:02:25.580 --> 00:02:29.945
这就是NLG的评估，以及为什么它是如此棘手的情况。

00:02:29.945 --> 00:02:33.665
最后，我们将对NLG的研究进行总结。

00:02:33.665 --> 00:02:37.440
当前的趋势是什么？我们未来的发展方向是什么？

00:02:38.260 --> 00:02:47.875
可以。那么，呃，第一部分，让我们做一个概括。

00:02:47.875 --> 00:02:51.440
好吧，所以自然语言的产生只是为了定义它

00:02:51.440 --> 00:02:55.090
指我们在其中生成某种文本的任何设置。

00:02:55.090 --> 00:02:58.385
例如，NLG是一个重要的子组件

00:02:58.385 --> 00:03:01.400
很多不同的任务，比如机器翻译，

00:03:01.400 --> 00:03:04.370
我们已经见过了，嗯，抽象的总结，

00:03:04.370 --> 00:03:06.305
稍后我们会进一步了解，嗯，

00:03:06.305 --> 00:03:09.760
聊天和基于任务的对话。

00:03:09.760 --> 00:03:15.155
嗯，还有创造性的写作任务，比如写故事和写诗。

00:03:15.155 --> 00:03:17.420
NLG也是，

00:03:17.420 --> 00:03:19.610
嗯，免费回答问题。

00:03:19.610 --> 00:03:22.280
所以我知道你们很多人现在都在做球队项目，呃，

00:03:22.280 --> 00:03:26.030
这不是NLG任务，因为您只是从中提取答案，

00:03:26.030 --> 00:03:27.245
呃，源文件。

00:03:27.245 --> 00:03:29.510
但是还有其他的问答任务

00:03:29.510 --> 00:03:31.775
它确实有一个自然语言生成组件。

00:03:31.775 --> 00:03:35.870
图像字幕是另一个例子，

00:03:35.870 --> 00:03:38.610
呃，有NLG子组件的任务。

00:03:38.810 --> 00:03:43.625
所以NLG是许多不同的NLP任务中非常酷的组件。

00:03:43.625 --> 00:03:45.440
好吧，我们来回顾一下。

00:03:45.440 --> 00:03:47.450
所以我首先要回顾的是，

00:03:47.450 --> 00:03:48.725
呃，什么是语言建模？

00:03:48.725 --> 00:03:53.300
嗯，我注意到有些人对此有点困惑，我想，呃，

00:03:53.300 --> 00:03:56.300
可能是因为名称语言建模听起来可能意味着

00:03:56.300 --> 00:04:00.635
只是简单地编码语言，比如使用嵌入或其他方式表示语言。

00:04:00.635 --> 00:04:02.930
作为提醒语言建模，

00:04:02.930 --> 00:04:04.135
呃，有更确切的含义。

00:04:04.135 --> 00:04:09.245
到目前为止，语言建模是预测下一个单词的任务。

00:04:09.245 --> 00:04:11.300
所以任何产生

00:04:11.300 --> 00:04:16.420
执行此任务的条件概率分布称为语言模型。

00:04:16.420 --> 00:04:18.645
如果这种语言模式，

00:04:18.645 --> 00:04:20.025
嗯，系统是RNN，

00:04:20.025 --> 00:04:24.190
然后我们经常将其缩写为RNN语言模型。

00:04:24.350 --> 00:04:27.390
好吧，所以我希望，呃，你会记得的。

00:04:27.390 --> 00:04:29.060
接下来我们要回顾的是

00:04:29.060 --> 00:04:31.060
还记得什么是条件语言模型吗？

00:04:31.060 --> 00:04:35.040
条件语言建模的任务是当你预测

00:04:35.040 --> 00:04:37.910
接下来会有什么消息，但你也要做好准备

00:04:37.910 --> 00:04:41.585
还有一些其他的输入x以及到目前为止所有的单词。

00:04:41.585 --> 00:04:45.640
因此，回顾一下条件语言建模的一些例子，包括，

00:04:45.640 --> 00:04:49.145
在机器翻译中，你要调节源句子x，

00:04:49.145 --> 00:04:52.940
嗯，总结，你要对你要总结的输入文本进行调整。

00:04:52.940 --> 00:04:57.510
对话，你在调节你的对话历史等等。

00:04:57.680 --> 00:05:03.455
好吧，嗯，接下来我们要快速回顾一下你是如何训练RNN语言模型的？

00:05:03.455 --> 00:05:07.760
我想，它也可以是一个基于变形金刚的语言模型或者一个基于CNN的语言模型，

00:05:07.760 --> 00:05:11.090
现在你知道了，嗯，这可能是有条件的，也可能不是。

00:05:11.090 --> 00:05:15.620
所以我要提醒你的是，当你训练这个系统的时候，

00:05:15.620 --> 00:05:18.410
然后你输入你想输入的目标序列

00:05:18.410 --> 00:05:21.620
从语料库中生成目标句，

00:05:21.620 --> 00:05:24.290
也就是说你有一些你想要的序列

00:05:24.290 --> 00:05:27.785
生成并将其输入解码器RNN语言模型。

00:05:27.785 --> 00:05:31.385
然后它预测接下来会发生什么。

00:05:31.385 --> 00:05:35.010
所以最重要的是在训练中，

00:05:35.010 --> 00:05:39.430
我们正在把黄金，也就是参考目标句输入解码器，

00:05:39.430 --> 00:05:41.980
不管解码器预测的是什么。

00:05:41.980 --> 00:05:46.750
所以，即使我们说这是一个非常糟糕的解码器，不能预测正确的单词，

00:05:46.750 --> 00:05:48.550
呃，你知道，这根本不是在预测它们的高度，

00:05:48.550 --> 00:05:51.790
嗯，没关系，我们还是，嗯，

00:05:51.790 --> 00:05:55.595
将目标-黄金目标序列输入解码器。

00:05:55.595 --> 00:05:58.660
而且，嗯，我强调这一点是因为它会在以后出现，

00:05:58.660 --> 00:06:00.895
呃，这种训练方法叫做教师强迫。

00:06:00.895 --> 00:06:03.170
这可能是你在别处遇到的一个短语。

00:06:03.170 --> 00:06:05.975
所以，是的，它指的是老师，

00:06:05.975 --> 00:06:09.160
这有点像是黄金投入——正在迫使，呃，

00:06:09.160 --> 00:06:11.260
每一步都要用到的语言模型

00:06:11.260 --> 00:06:14.495
而不是在每一步都使用自己的预测。

00:06:14.495 --> 00:06:18.630
所以这就是你训练RNN语言模型的方法，它可能是有条件的。

00:06:18.630 --> 00:06:20.610
嗯，好吧。

00:06:20.610 --> 00:06:22.815
现在回顾一下解码算法。

00:06:22.815 --> 00:06:26.945
所以，呃，你已经有了训练过的语言模型，它可能是有条件的。

00:06:26.945 --> 00:06:29.345
问题是如何使用它生成文本？

00:06:29.345 --> 00:06:32.000
所以答案是你需要一个解码算法。

00:06:32.000 --> 00:06:34.640
解码算法是用来

00:06:34.640 --> 00:06:37.585
从经过培训的语言模型生成文本。

00:06:37.585 --> 00:06:39.680
所以，呃，在NMT演讲中

00:06:39.680 --> 00:06:42.470
几周前，我们学习了两种不同的解码算法。

00:06:42.470 --> 00:06:45.620
我们学习了贪婪解码和波束搜索。

00:06:45.620 --> 00:06:47.795
让我们快速回顾一下。

00:06:47.795 --> 00:06:51.215
贪婪解码是一个很简单的算法。

00:06:51.215 --> 00:06:53.030
在每一步，你只需采取什么

00:06:53.030 --> 00:06:55.895
根据语言模式最可能出现的单词。

00:06:55.895 --> 00:06:59.245
你可以处理argmax然后用它作为下一个词，

00:06:59.245 --> 00:07:01.340
你把它作为下一步的输入。

00:07:01.340 --> 00:07:03.860
你就这样一直走到最后

00:07:03.860 --> 00:07:06.620
或者当你达到某个最大长度时。

00:07:06.620 --> 00:07:09.805
我认为你们都很熟悉这一点，因为你们是在作业五中完成的。

00:07:09.805 --> 00:07:15.220
所以，嗯，是的，这张图展示了贪婪解码是如何产生句子的。

00:07:15.220 --> 00:07:17.415
正如我们之前所了解的，

00:07:17.415 --> 00:07:19.610
由于缺乏回溯和

00:07:19.610 --> 00:07:22.130
如果你做了错误的选择，就不能回去了，呃，

00:07:22.130 --> 00:07:25.050
贪婪解码的输出通常是，呃，

00:07:25.050 --> 00:07:30.280
很穷，就像它可能是不合文法的，或者它可能是不自然的，有点胡说八道。

00:07:30.280 --> 00:07:33.275
好的，让我们回顾一下波束搜索解码。

00:07:33.275 --> 00:07:38.660
因此，波束搜索是一种寻找高概率序列的搜索算法。

00:07:38.660 --> 00:07:43.610
所以如果我们要翻译的话，这个序列就是翻译单词的序列，

00:07:43.610 --> 00:07:48.200
嗯，一次跟踪多个可能的序列。

00:07:48.200 --> 00:07:51.980
所以核心思想是解码器的每一步，

00:07:51.980 --> 00:07:53.060
你要跟踪

00:07:53.060 --> 00:07:57.515
k最可能的部分序列，我们称之为假设。

00:07:57.515 --> 00:08:01.000
这里k是一个超超参数，叫做光束大小。

00:08:01.000 --> 00:08:02.570
所以这个想法是，嗯，

00:08:02.570 --> 00:08:05.870
考虑到许多不同的假设，我们将尝试有效地寻找

00:08:05.870 --> 00:08:07.790
一个很有可能的序列

00:08:07.790 --> 00:08:10.285
不能保证这是最佳的，

00:08:10.285 --> 00:08:12.870
最高概率序列。

00:08:12.870 --> 00:08:15.930
所以，呃，在光束搜索的最后，呃，

00:08:15.930 --> 00:08:17.930
你达到了某种停止的标准

00:08:17.930 --> 00:08:20.300
关于之前，但我不会再详细介绍。

00:08:20.300 --> 00:08:22.610
一旦你达到停车标准，

00:08:22.610 --> 00:08:25.175
你选择概率最高的序列，

00:08:25.175 --> 00:08:29.495
嗯，考虑到长度的一些调整，这就是你的输出。

00:08:29.495 --> 00:08:31.400
所以再做一次。

00:08:31.400 --> 00:08:35.435
这是我们在NMT波束搜索解码课上看到的图表。

00:08:35.435 --> 00:08:40.025
一旦它完成，在这个场景中，我们有一个2号的光束。

00:08:40.025 --> 00:08:43.180
这就是我们解决了这个勘探问题后的情况，

00:08:43.180 --> 00:08:45.420
这显示了我们探索的整棵树，

00:08:45.420 --> 00:08:49.145
然后我们找到了某种停止标准，我们确定了顶部，

00:08:49.145 --> 00:08:50.480
呃，假设和，呃，

00:08:50.480 --> 00:08:52.510
以绿色突出显示。

00:08:52.510 --> 00:08:55.815
所以在波束搜索解码的问题上，

00:08:55.815 --> 00:08:57.810
前几天我在看电视，

00:08:57.810 --> 00:09:00.710
我注意到西方世界的一些事情。

00:09:00.710 --> 00:09:06.020
我认为主机-[笑声]西方世界的人工智能主机可能使用光束搜索。

00:09:06.020 --> 00:09:08.840
这是我不希望在电视上看到的。

00:09:08.840 --> 00:09:10.945
[笑声]所以有这样一个场景，

00:09:10.945 --> 00:09:12.390
嗯，顺便说一下，西方世界，

00:09:12.390 --> 00:09:14.240
一个科幻系列，有这些，嗯，

00:09:14.240 --> 00:09:16.580
非常有说服力的人形人工智能系统。

00:09:16.580 --> 00:09:19.040
嗯，还有一个场景

00:09:19.040 --> 00:09:22.055
人工智能系统面临的现实是，

00:09:22.055 --> 00:09:23.975
嗯，她，我想是，

00:09:23.975 --> 00:09:29.850
嗯，不是人类，因为她在说话时看到了词汇的生成系统，

00:09:29.850 --> 00:09:31.680
我在看电视，我想，

00:09:31.680 --> 00:09:32.775
那是光束搜索吗？

00:09:32.775 --> 00:09:35.840
因为这张图很像这张图，

00:09:35.840 --> 00:09:38.580
嗯，但可能有更大的光束尺寸。

00:09:38.580 --> 00:09:40.605
所以，我觉得这很酷，因为，你知道，

00:09:40.605 --> 00:09:43.490
当你在电视上看到光束搜索时，人工智能已经成为主流。

00:09:43.490 --> 00:09:45.200
如果你放大的很厉害你就能看到

00:09:45.200 --> 00:09:49.470
在这个屏幕截图中还有一些令人兴奋的词，比如知识库，

00:09:49.470 --> 00:09:51.170
前链和后链，

00:09:51.170 --> 00:09:54.200
与前向支柱和后向支柱相同，

00:09:54.200 --> 00:09:57.185
还有模糊逻辑算法和神经网络。

00:09:57.185 --> 00:09:59.390
嗯，是的，光束搜索，

00:09:59.390 --> 00:10:00.875
我想，现在已经成为主流，

00:10:00.875 --> 00:10:04.100
嗯，这对西方世界来说已经足够好了，

00:10:04.100 --> 00:10:05.060
也许对我们来说已经足够好了。

00:10:05.060 --> 00:10:08.500
呃，所以用光束搜索，对吗？

00:10:08.500 --> 00:10:12.055
我们已经讨论过这个超参数k或者光束大小。

00:10:12.055 --> 00:10:14.110
还有一件事我们在上一节课上没有提到，

00:10:14.110 --> 00:10:16.660
现在我们要离开重述部分，嗯，

00:10:16.660 --> 00:10:20.980
改变光束尺寸的效果是什么？

00:10:20.980 --> 00:10:22.480
如果你有一个非常小的K，

00:10:22.480 --> 00:10:26.065
然后你会遇到类似贪婪解码的问题。

00:10:26.065 --> 00:10:27.370
实际上，如果k等于1，

00:10:27.370 --> 00:10:29.890
那么你实际上只是在贪婪地解码。

00:10:29.890 --> 00:10:32.410
同样的问题是，你知道，不符合文法，

00:10:32.410 --> 00:10:36.805
可能是不自然的，无意义的，只是一种不正确的输出。

00:10:36.805 --> 00:10:39.415
所以一旦我们变大K，

00:10:39.415 --> 00:10:41.305
如果你有更大的光束尺寸，

00:10:41.305 --> 00:10:46.750
然后你在做搜索算法，但是考虑到更多的假设，对吗？

00:10:46.750 --> 00:10:48.610
你有更大的搜索空间

00:10:48.610 --> 00:10:51.100
你在考虑更多不同的可能性。

00:10:51.100 --> 00:10:55.495
所以如果你这样做，我们经常会发现这减少了上面的一些问题。

00:10:55.495 --> 00:10:58.540
所以你不太可能有这种不合文法的东西，

00:10:58.540 --> 00:11:01.015
呃，你知道，输出不连贯。

00:11:01.015 --> 00:11:04.930
但是提高K也有一些缺点。所以，当然，

00:11:04.930 --> 00:11:06.969
K越大，计算成本就越高

00:11:06.969 --> 00:11:09.250
如果你想的话，那会很糟糕的，嗯，

00:11:09.250 --> 00:11:11.530
例如，生成你的，呃，

00:11:11.530 --> 00:11:12.850
你知道，大范围的输出，

00:11:12.850 --> 00:11:15.475
NMT示例的测试集。

00:11:15.475 --> 00:11:17.680
但更严重的是，

00:11:17.680 --> 00:11:19.870
增加k值会带来其他一些问题。

00:11:19.870 --> 00:11:23.245
例如，在NMT中，

00:11:23.245 --> 00:11:28.030
过多地增加光束尺寸实际上会降低Bleu分数。

00:11:28.030 --> 00:11:30.625
这有点违反直觉，对吧？

00:11:30.625 --> 00:11:32.875
因为我们在考虑光束搜索

00:11:32.875 --> 00:11:35.410
因为这个算法试图找到最优解。

00:11:35.410 --> 00:11:37.015
所以，如果你增加k，

00:11:37.015 --> 00:11:39.970
那么你只会找到更好的解决方案，对吧？

00:11:39.970 --> 00:11:44.440
嗯，所以我想这里的关键可能是最优性之间的区别

00:11:44.440 --> 00:11:46.300
就搜索问题而言

00:11:46.300 --> 00:11:48.895
一个高概率序列和布鲁分数，

00:11:48.895 --> 00:11:50.080
这是两个不同的东西，

00:11:50.080 --> 00:11:54.310
而且不能保证它们，嗯，是相对应的，对吧？

00:11:54.310 --> 00:11:57.850
我的意思是，布鲁分数和实际的翻译有区别，

00:11:57.850 --> 00:11:59.440
嗯，我们知道质量。

00:11:59.440 --> 00:12:01.720
所以如果你看看我联系的两份文件

00:12:01.720 --> 00:12:04.390
这就是那些证明，

00:12:04.390 --> 00:12:07.330
嗯，增加光束尺寸太多会降低Bleu分数。

00:12:07.330 --> 00:12:10.690
他们解释的主要原因是

00:12:10.690 --> 00:12:14.365
发生这种情况是因为当你把光束尺寸增加得太大时，

00:12:14.365 --> 00:12:18.370
然后你就产生了太短的翻译。

00:12:18.370 --> 00:12:22.720
所以我的意思是，这种解释在某种程度上说翻译太短了，

00:12:22.720 --> 00:12:24.130
所以他们的布鲁很低，因为他们

00:12:24.130 --> 00:12:26.230
可能遗漏了他们应该包含的单词。

00:12:26.230 --> 00:12:29.860
但问题是，为什么大梁的尺寸会让你翻译得很短？

00:12:29.860 --> 00:12:31.210
我觉得这很难回答。

00:12:31.210 --> 00:12:34.975
无论在哪里，在这两篇论文中，我都没有看到一个明确的解释。

00:12:34.975 --> 00:12:37.920
嗯，我想可能是更大的传球，

00:12:37.920 --> 00:12:41.565
我们有时会看到光束搜索，当你真的增加你的，呃，

00:12:41.565 --> 00:12:43.440
搜索空间，使搜索更加

00:12:43.440 --> 00:12:46.620
功能强大，可以考虑许多不同的选择。

00:12:46.620 --> 00:12:49.620
最终会发现这些高概率，

00:12:49.620 --> 00:12:53.205
嗯，序列其实不是你想要的。

00:12:53.205 --> 00:12:55.260
当然，它们是高概率的

00:12:55.260 --> 00:12:57.350
但它们不是你真正想要的。

00:12:57.350 --> 00:13:00.550
嗯，另一个例子是

00:13:00.550 --> 00:13:03.625
在诸如聊天对话之类的开放式任务中

00:13:03.625 --> 00:13:04.825
你想去的地方，嗯，

00:13:04.825 --> 00:13:07.330
跟你的谈话伙伴说些有趣的话，

00:13:07.330 --> 00:13:10.300
如果我们使用大光束尺寸的光束搜索，

00:13:10.300 --> 00:13:13.495
我们发现它可以为您提供一些真正通用的输出。

00:13:13.495 --> 00:13:16.405
嗯，我给你举个例子来说明我的意思。

00:13:16.405 --> 00:13:20.545
这些是聊天中的例子，

00:13:20.545 --> 00:13:22.825
呃，我正在做的对话项目。

00:13:22.825 --> 00:13:24.190
所以这里你有，呃，

00:13:24.190 --> 00:13:28.330
你的人类聊天伙伴说，像我这样的人多吃新鲜和生的食物，

00:13:28.330 --> 00:13:29.785
所以我节省了食品杂货。

00:13:29.785 --> 00:13:34.030
这就是聊天机器人根据波束大小所说的。

00:13:34.030 --> 00:13:37.790
我会让你读的。

00:13:43.590 --> 00:13:47.350
所以我想说这是你所看到的

00:13:47.350 --> 00:13:50.500
当你升高和降低光束尺寸[噪音]时发生。

00:13:50.500 --> 00:13:51.955
当你有近光灯尺寸时，

00:13:51.955 --> 00:13:54.700
嗯，这可能更像是一个话题。

00:13:54.700 --> 00:13:57.580
就像这里，我们可以看到健康的饮食，健康的饮食，

00:13:57.580 --> 00:13:59.710
我是一名护士，所以我不吃生食等，

00:13:59.710 --> 00:14:02.335
这与用户所说的有关，

00:14:02.335 --> 00:14:04.150
呃，但是英语有点差，对吧？

00:14:04.150 --> 00:14:06.100
有一些重复和，

00:14:06.100 --> 00:14:08.020
呃，这并不总是那么有意义，对吧？

00:14:08.020 --> 00:14:09.580
嗯，[噪音]但是，

00:14:09.580 --> 00:14:10.885
当你提高光束尺寸时，

00:14:10.885 --> 00:14:12.250
然后它就会收敛到

00:14:12.250 --> 00:14:17.185
一个安全的所谓正确的反应，但它是一种通用的和不太相关的，对吗？

00:14:17.185 --> 00:14:19.600
它在所有场景中都适用，你以什么为生？

00:14:19.600 --> 00:14:21.970
嗯，所以，那个，

00:14:21.970 --> 00:14:24.160
我在这里使用的数据集是，

00:14:24.160 --> 00:14:25.240
一个叫做“个人聊天”，

00:14:25.240 --> 00:14:26.440
稍后我会告诉你更多。

00:14:26.440 --> 00:14:28.240
嗯，但这是一个，

00:14:28.240 --> 00:14:31.315
它是一个聊天对话数据集，

00:14:31.315 --> 00:14:35.575
嗯，交谈伙伴有一个角色，这是一组特征。

00:14:35.575 --> 00:14:37.600
嗯，所以它一直在谈论当护士的原因，

00:14:37.600 --> 00:14:39.160
我想是因为它是在角色中。

00:14:39.160 --> 00:14:42.340
[噪音]但这里的重点是，嗯，

00:14:42.340 --> 00:14:45.685
我们有点不幸地与不妥协了，

00:14:45.685 --> 00:14:48.805
没有金凤花区，这很明显。

00:14:48.805 --> 00:14:50.410
我是说，有，有，是的，

00:14:50.410 --> 00:14:53.290
有点不太好的权衡，

00:14:53.290 --> 00:14:56.680
输出不好，英语不好，只是有一些很无聊的东西。

00:14:56.680 --> 00:15:00.800
所以这是我们用光束搜索得到的问题之一。

00:15:01.320 --> 00:15:03.790
可以。所以我们讨论过，呃，

00:15:03.790 --> 00:15:06.445
贪婪解码和波束搜索。对。

00:15:06.445 --> 00:15:13.000
所以波束大小取决于[听不见]

00:15:13.000 --> 00:15:14.050
问题是，我们能不能

00:15:14.050 --> 00:15:17.755
自适应光束大小取决于你所处的位置？

00:15:17.755 --> 00:15:19.060
你的意思是像在序列里？

00:15:19.060 --> 00:15:26.040
是啊。那是[听不见的]。

00:15:26.040 --> 00:15:29.220
是啊。我的意思是，我想我-我可能听说过一篇这样的研究论文？

00:15:29.220 --> 00:15:34.885
这种适应性的相似性提高了假设空间的能力。

00:15:34.885 --> 00:15:37.135
我是说，这听起来很难实施，呃，

00:15:37.135 --> 00:15:40.990
因为，你知道的，在你的GPU中，事物适合一个固定的空间。

00:15:40.990 --> 00:15:42.580
嗯，但我认为这是可能的，

00:15:42.580 --> 00:15:46.225
我想你得学习一下提高光束的标准，

00:15:46.225 --> 00:15:49.300
光束大小，是的。似乎是可能的。

00:15:49.300 --> 00:15:51.625
可以。所以我们讨论过，呃，

00:15:51.625 --> 00:15:53.365
波束搜索和贪婪解码。

00:15:53.365 --> 00:15:55.990
这是一个新的解码家族

00:15:55.990 --> 00:15:59.095
算法非常简单，嗯，基于采样的解码。

00:15:59.095 --> 00:16:03.235
所以我称之为纯采样，因为我不知道还能叫它什么。

00:16:03.235 --> 00:16:04.855
嗯，这只是，

00:16:04.855 --> 00:16:07.360
简单的取样方法，在每个，呃，

00:16:07.360 --> 00:16:08.890
译码器T的时间步，

00:16:08.890 --> 00:16:12.040
你只想从概率分布中随机抽取样本，

00:16:12.040 --> 00:16:13.780
呃，为了得到你的下一句话。

00:16:13.780 --> 00:16:15.490
所以这很简单。

00:16:15.490 --> 00:16:17.335
就像贪婪的解码。

00:16:17.335 --> 00:16:19.285
但不是用最重要的词，

00:16:19.285 --> 00:16:22.220
相反，只是从那个分布中取样。

00:16:22.350 --> 00:16:28.600
所以我称之为纯抽样的原因是为了区别于前n个抽样。

00:16:28.600 --> 00:16:30.640
同样，这通常被称为top-k

00:16:30.640 --> 00:16:33.400
但我已经称之为光束尺寸，

00:16:33.400 --> 00:16:36.340
我不想搞混了，所以我现在把它叫做前N抽样。

00:16:36.340 --> 00:16:38.935
嗯，这里的想法也很简单。

00:16:38.935 --> 00:16:40.585
在每个步骤t上，

00:16:40.585 --> 00:16:44.035
你想从你的概率分布中随机抽取样本，但是

00:16:44.035 --> 00:16:48.265
你将只限于最可能出现的前n个词。

00:16:48.265 --> 00:16:50.185
所以这是说，

00:16:50.185 --> 00:16:51.430
就像简单的，你知道，

00:16:51.430 --> 00:16:56.515
纯抽样方法，但你想截短你的概率分布，

00:16:56.515 --> 00:16:59.020
你知道，最可能出现的词。

00:16:59.020 --> 00:17:03.145
所以，呃，这里的想法有点像光束搜索，嗯，

00:17:03.145 --> 00:17:06.610
给你一个超参数是介于贪婪解码和

00:17:06.610 --> 00:17:08.935
你知道，呃，一个非常详尽的搜索。

00:17:08.935 --> 00:17:12.025
同样，这里有一个超参数n

00:17:12.025 --> 00:17:15.340
它可以带你在贪婪的搜索和纯粹的采样之间。

00:17:15.340 --> 00:17:16.630
如果你考虑一下这个，

00:17:16.630 --> 00:17:19.150
如果n是1，那么您将截断它的顶部。

00:17:19.150 --> 00:17:21.085
所以你只吃了贪婪的arg max。

00:17:21.085 --> 00:17:22.660
如果n是声乐大小，

00:17:22.660 --> 00:17:24.085
那么你根本就不截断它。

00:17:24.085 --> 00:17:25.510
你从每样东西中取样，

00:17:25.510 --> 00:17:27.790
那只是纯粹的抽样方法。

00:17:27.790 --> 00:17:31.000
所以这里，嗯，我希望应该很清楚，

00:17:31.000 --> 00:17:33.715
如果你想起来，如果你增加n，

00:17:33.715 --> 00:17:36.910
然后你会得到更多的多样性和风险输出，对吗？

00:17:36.910 --> 00:17:39.235
因为你，呃，给予更多，

00:17:39.235 --> 00:17:42.760
选择的越多，概率分布就越低，

00:17:42.760 --> 00:17:44.770
去做不太可能的事情。

00:17:44.770 --> 00:17:46.270
然后，如果你减少n，

00:17:46.270 --> 00:17:48.580
然后你会得到更多的通用安全输出，因为你

00:17:48.580 --> 00:17:51.890
更多地限制在高概率的选项上。

00:17:53.460 --> 00:17:56.440
所以这两种方法都比

00:17:56.440 --> 00:17:58.630
光束搜索，我认为这很重要，

00:17:58.630 --> 00:18:02.425
因为没有多个假设可以追踪，对吗？

00:18:02.425 --> 00:18:04.735
因为在波束搜索中，在解码器的每一步上，

00:18:04.735 --> 00:18:06.115
你有k个不同的，你知道，

00:18:06.115 --> 00:18:08.770
梁的尺寸，很多假设要跟踪。

00:18:08.770 --> 00:18:11.560
呃，但是这里，至少如果你只生成一个样本，

00:18:11.560 --> 00:18:12.760
只有一件事要追踪。

00:18:12.760 --> 00:18:14.440
所以，这是一个非常简单的算法。

00:18:14.440 --> 00:18:19.310
这就是这些基于采样的算法相对于波束搜索的一个优点。

00:18:21.200 --> 00:18:25.560
可以。所以，我想告诉你的最后一件事就是，

00:18:25.560 --> 00:18:27.165
嗯，软最高[噪音]温度。

00:18:27.165 --> 00:18:30.930
所以，如果你回忆起解码器的时间步骤t，

00:18:30.930 --> 00:18:34.590
你的语言模型计算某种概率分布，呃，

00:18:34.590 --> 00:18:39.030
通过将SoftMax函数应用于从某个地方获得的分数向量。

00:18:39.030 --> 00:18:42.735
就像从你的变压器或从你的RNN什么的。

00:18:42.735 --> 00:18:44.670
所以，又有了softmax函数。

00:18:44.670 --> 00:18:47.580
它的意思是，单词w的概率就是这个SoftMax函数，

00:18:47.580 --> 00:18:50.115
呃，给出，给出分数。

00:18:50.115 --> 00:18:55.080
所以，这里关于软最大温度的概念是

00:18:55.080 --> 00:19:01.200
温度超参数tau，你将把它应用到这个，呃，SoftMax上。

00:19:01.200 --> 00:19:04.920
所以，我们要做的就是把所有的分数分开，

00:19:04.920 --> 00:19:06.375
或者你可以称之为逻辑，

00:19:06.375 --> 00:19:08.565
通过温度超参数。

00:19:08.565 --> 00:19:10.545
所以，如果你再考虑一下，

00:19:10.545 --> 00:19:12.570
你会看到温度升高，

00:19:12.570 --> 00:19:13.800
越来越多，呃，

00:19:13.800 --> 00:19:19.935
超参数，这将使你的概率分布更加均匀。

00:19:19.935 --> 00:19:23.415
这种问题归结到你什么时候，

00:19:23.415 --> 00:19:25.830
当你把所有的分数乘以一个常数，

00:19:25.830 --> 00:19:28.980
嗯，这对SoftMax有什么影响？

00:19:28.980 --> 00:19:33.885
所以，当你按指数计算的时候，事情会变得越来越远还是越来越远？

00:19:33.885 --> 00:19:36.690
所以，这是你可以自己在纸上写的东西，

00:19:36.690 --> 00:19:38.520
但是作为一个，呃，

00:19:38.520 --> 00:19:40.125
一种记忆捷径，

00:19:40.125 --> 00:19:43.500
一个好的思考方法是，如果你提高温度，

00:19:43.500 --> 00:19:47.490
然后，分布类型融化，变软，糊状，均匀。

00:19:47.490 --> 00:19:48.810
如果你，呃，

00:19:48.810 --> 00:19:51.150
降低温度，就像让它变冷一样，

00:19:51.150 --> 00:19:54.690
概率分布变得更加尖锐，对吗？

00:19:54.690 --> 00:19:59.115
所以，就像那些被认为是高概率的事物变得更像，

00:19:59.115 --> 00:20:02.670
呃，与其他事情相比，概率非常高。

00:20:02.670 --> 00:20:05.535
嗯，我觉得这是一个简单的记忆方式。

00:20:05.535 --> 00:20:07.935
今天我得在纸上写出来然后，呃，

00:20:07.935 --> 00:20:09.135
我意识到，

00:20:09.135 --> 00:20:12.375
温度可视化的东西通常能让我更快地到达那里。

00:20:12.375 --> 00:20:18.480
所以，嗯，我要注意的一点是，SoftMax温度不是一种解码算法。

00:20:18.480 --> 00:20:21.120
我知道我把它放在解码算法部分，

00:20:21.120 --> 00:20:23.715
嗯，那只是因为它是一种东西，一种

00:20:23.715 --> 00:20:29.880
你可以在测试时做一些简单的事情来改变解码的方式，对吗？

00:20:29.880 --> 00:20:31.320
你不需要训练，呃，

00:20:31.320 --> 00:20:33.765
使用，SoftMax温度。

00:20:33.765 --> 00:20:36.225
所以，这不是解码算法本身。

00:20:36.225 --> 00:20:38.415
这是一种可以在测试时应用的技术

00:20:38.415 --> 00:20:41.040
结合解码算法。

00:20:41.040 --> 00:20:44.385
例如，如果你在做光束搜索或者做某种取样，

00:20:44.385 --> 00:20:48.615
然后你也可以应用一个最软的温度，嗯，来改变，

00:20:48.615 --> 00:20:54.160
你知道，这种风险对安全，嗯，权衡。

00:20:55.220 --> 00:21:03.060
有什么问题吗？可以。所以，这是

00:21:03.060 --> 00:21:06.270
总结一下我们刚刚学到的解码算法。

00:21:06.270 --> 00:21:09.375
贪婪解码是一种简单的方法。

00:21:09.375 --> 00:21:14.265
与其他系统相比，它的输出质量较低，至少可以进行波束搜索。

00:21:14.265 --> 00:21:17.160
光束搜索，特别是当你有一个远光灯的尺寸，呃，

00:21:17.160 --> 00:21:20.955
它通过许多不同的假设来搜索高概率输出。

00:21:20.955 --> 00:21:24.120
这通常会比贪婪的搜索提供更好的质量，呃，

00:21:24.120 --> 00:21:26.730
但是如果光束尺寸太高，你可以用这些，

00:21:26.730 --> 00:21:29.385
呃，我们以前讨论过的一些反直觉的问题。

00:21:29.385 --> 00:21:32.865
在那里你得到了一些高概率但不合适的输出。

00:21:32.865 --> 00:21:35.415
比如说，有些东西太普通或太短。

00:21:35.415 --> 00:21:37.290
我们稍后再谈。

00:21:37.290 --> 00:21:41.220
嗯，采样方法是一种获得更多多样性的方法，

00:21:41.220 --> 00:21:43.095
嗯，通过，通过随机性。

00:21:43.095 --> 00:21:46.380
嗯，好吧，随机性本身就是你的目标。

00:21:46.380 --> 00:21:49.485
嗯，所以，如果你想拥有某种，比如，

00:21:49.485 --> 00:21:51.930
开放式或创造性的一代环境，比如，

00:21:51.930 --> 00:21:53.910
嗯，创作诗歌或故事，

00:21:53.910 --> 00:21:56.370
那么抽样可能比

00:21:56.370 --> 00:21:59.700
因为你想有一种随机性来源，

00:21:59.700 --> 00:22:02.160
嗯，创造性地写不同的东西。

00:22:02.160 --> 00:22:07.170
Top-N采样允许您通过以下方式控制多样性：

00:22:07.170 --> 00:22:09.330
呃，换了N。最后，

00:22:09.330 --> 00:22:11.610
SoftMax温度是控制多样性的另一种方法。

00:22:11.610 --> 00:22:14.520
所以这里有很多不同的旋钮。

00:22:14.520 --> 00:22:16.260
这不是解码算法，

00:22:16.260 --> 00:22:20.190
这只是一种可以应用于任何解码算法的技术。

00:22:20.190 --> 00:22:22.830
尽管用它不合理

00:22:22.830 --> 00:22:26.370
贪婪的解码，因为即使你让它更尖锐或更平坦，

00:22:26.370 --> 00:22:29.950
argmax仍然是argmax，所以没有意义。

00:22:31.400 --> 00:22:34.350
可以。酷。我要转到第二部分。

00:22:34.350 --> 00:22:39.135
所以，呃，第二部分是NLG任务和神经方法。

00:22:39.135 --> 00:22:42.330
呃，正如前面提到的，这不是所有NLG的概述。

00:22:42.330 --> 00:22:43.620
那是完全不可能的。

00:22:43.620 --> 00:22:45.195
这将是一些精选的亮点。

00:22:45.195 --> 00:22:47.490
所以，特别是，我要从

00:22:47.490 --> 00:22:51.270
我对一个特定的NLG任务有相当深入的了解，

00:22:51.270 --> 00:22:53.250
这就是，呃，总结。

00:22:53.250 --> 00:22:57.660
那么，让我们从一个用于汇总的任务定义开始。

00:22:57.660 --> 00:23:02.325
嗯，一个合理的定义是：给定某种输入文本x，

00:23:02.325 --> 00:23:04.890
你想写一个短于

00:23:04.890 --> 00:23:07.825
并包含X的主要信息。

00:23:07.825 --> 00:23:11.360
所以，汇总可以是单文档，也可以是多文档。

00:23:11.360 --> 00:23:16.510
单文档意味着你只有一个文档x的摘要y。

00:23:16.510 --> 00:23:20.040
在多文档摘要中，你说你想写

00:23:20.040 --> 00:23:24.390
多个文档x_1到x_n的单个摘要y。

00:23:24.390 --> 00:23:28.980
通常情况下，x_1到x_n会有一些重叠的内容。

00:23:28.980 --> 00:23:32.040
例如，它们可能都是不同的新闻文章

00:23:32.040 --> 00:23:35.220
来自不同的报纸关于同一事件，对吗？

00:23:35.220 --> 00:23:39.030
因为写一个总结是有意义的，它从所有这些总结中得出。

00:23:39.030 --> 00:23:45.010
嗯，总结不同主题的东西没什么意义。

00:23:45.920 --> 00:23:48.015
还有，呃，

00:23:48.015 --> 00:23:51.270
总结中任务定义的细分。

00:23:51.270 --> 00:23:53.835
所以，我将通过一些数据集来描述它。

00:23:53.835 --> 00:23:58.455
呃，这里有一些不同的非常常见的数据集，特别是，

00:23:58.455 --> 00:24:01.800
神经总结，嗯，它们有点对应不同，

00:24:01.800 --> 00:24:04.035
比如，长度和不同的文本样式。

00:24:04.035 --> 00:24:05.430
所以，一个常见的问题是，

00:24:05.430 --> 00:24:07.050
嗯，千兆字数据集。

00:24:07.050 --> 00:24:09.360
这里的任务是你要从地图上

00:24:09.360 --> 00:24:13.710
一篇新闻文章的头一两句话，用来写标题。

00:24:13.710 --> 00:24:16.290
[噪音]你可以把它看作是句子压缩，

00:24:16.290 --> 00:24:19.140
尤其是当你从一个句子到标题

00:24:19.140 --> 00:24:22.710
从长句子到短标题式句子。

00:24:22.710 --> 00:24:26.955
下一个我，嗯，

00:24:26.955 --> 00:24:29.130
想告诉你这是，呃，

00:24:29.130 --> 00:24:31.320
这是一个中文摘要数据集，但我，

00:24:31.320 --> 00:24:33.690
我看到人们经常使用它。

00:24:33.690 --> 00:24:36.480
这是，呃，从一个微博上，

00:24:36.480 --> 00:24:39.945
嗯，人们写文章摘要的网站。

00:24:39.945 --> 00:24:42.270
所以，实际的总结任务是

00:24:42.270 --> 00:24:44.790
你有一段文字，然后你想，

00:24:44.790 --> 00:24:46.230
嗯，总结一下，

00:24:46.230 --> 00:24:48.180
我想，一句话的总结。

00:24:48.180 --> 00:24:51.120
另外一个，实际上是两个，

00:24:51.120 --> 00:24:55.650
是《纽约时报》和CNN/每日邮报，呃，数据集吗？

00:24:55.650 --> 00:24:57.180
所以，这些都是形式，

00:24:57.180 --> 00:24:59.940
你有一篇完整的新闻文章，实际上相当长

00:24:59.940 --> 00:25:03.690
几百个字，然后你想把它总结成，

00:25:03.690 --> 00:25:06.840
嗯，比如说，可能是一句话或是多句话的总结。

00:25:06.840 --> 00:25:10.560
呃，《纽约时报》是由，我想，呃，

00:25:10.560 --> 00:25:13.125
图书馆员或那些，

00:25:13.125 --> 00:25:16.440
嗯，为了图书馆的目的写摘要。

00:25:16.440 --> 00:25:18.885
呃，然后，呃，

00:25:18.885 --> 00:25:22.365
今天我写这张单子的时候发现了一个新的，

00:25:22.365 --> 00:25:25.845
相当新，就像过去六个月维基解密的数据集。

00:25:25.845 --> 00:25:27.840
所以，从我看来，

00:25:27.840 --> 00:25:31.950
你有一篇来自维基的文章，然后你想把它归结为

00:25:31.950 --> 00:25:34.200
有点巧妙的总结句

00:25:34.200 --> 00:25:37.185
摘自整个维基解密文章。

00:25:37.185 --> 00:25:38.790
它们有点像标题。

00:25:38.790 --> 00:25:42.390
所以，嗯，我看了这篇论文，似乎，嗯，

00:25:42.390 --> 00:25:45.840
这有点有趣，因为它是不同类型的文本。

00:25:45.840 --> 00:25:48.990
正如你可能已经注意到的，其他大部分都是基于新闻的，这是，

00:25:48.990 --> 00:25:51.825
呃，不是这样的，所以这会带来不同的挑战。

00:25:51.825 --> 00:25:57.360
嗯，总结的另一种划分是句子简化。

00:25:57.360 --> 00:26:00.690
所以，这是一个相关但实际上不同的任务。

00:26:00.690 --> 00:26:04.410
在总结中，你要写一些简短的，包含

00:26:04.410 --> 00:26:08.220
主要信息，但仍然可能是用同样复杂的语言写的，

00:26:08.220 --> 00:26:13.785
在句子简化中，您希望使用简单的方法重写源文本，

00:26:13.785 --> 00:26:15.420
呃，简单的语言，对吧？

00:26:15.420 --> 00:26:18.765
所以，就像简单的单词选择和简单的句子结构。

00:26:18.765 --> 00:26:21.240
这可能意味着它更短，但不一定。

00:26:21.240 --> 00:26:22.890
例如，呃，

00:26:22.890 --> 00:26:25.950
简单的wiki——wikipedia是一个标准的数据集。

00:26:25.950 --> 00:26:28.470
你的想法是，嗯，你知道，

00:26:28.470 --> 00:26:31.440
标准的维基百科，你有一个简单的维基百科版本。

00:26:31.440 --> 00:26:32.550
他们大多是排成一列，

00:26:32.550 --> 00:26:33.960
所以你想从地图上

00:26:33.960 --> 00:26:37.365
一个句子中的某个句子与[噪音]另一个句子中的对等句子。

00:26:37.365 --> 00:26:41.880
另一个数据来源是Newsela，它是一个网站，

00:26:41.880 --> 00:26:44.085
为孩子们重写新闻。

00:26:44.085 --> 00:26:46.320
实际上，我认为在不同的学习水平上。

00:26:46.320 --> 00:26:49.900
所以，对于它的简化程度，您有多种选择。

00:26:50.180 --> 00:26:54.690
可以。所以，嗯，所以

00:26:54.690 --> 00:26:59.085
这就是总结作为不同任务的定义或许多定义。

00:26:59.085 --> 00:27:00.810
所以，现在我要概述一下，

00:27:00.810 --> 00:27:02.190
主要是什么，呃，

00:27:02.190 --> 00:27:04.095
总结的技巧。

00:27:04.095 --> 00:27:06.390
所以，有两个主要的总结策略。

00:27:06.390 --> 00:27:10.560
嗯，你可以称之为提取性总结和抽象性总结。

00:27:10.560 --> 00:27:12.735
正如我之前暗示的那样，

00:27:12.735 --> 00:27:15.720
你只是在总结中选择

00:27:15.720 --> 00:27:19.050
部分原文形成摘要。

00:27:19.050 --> 00:27:22.770
这通常是整句话，但也许会比这更具体一些；

00:27:22.770 --> 00:27:24.825
可能，呃，短语或单词。

00:27:24.825 --> 00:27:27.360
而抽象的总结，你会

00:27:27.360 --> 00:27:31.275
使用NLG技术生成一些新文本。

00:27:31.275 --> 00:27:33.840
所以这个想法是，你知道，这是白手起家的一代。

00:27:33.840 --> 00:27:37.590
我的视觉隐喻是这样的，就像强调

00:27:37.590 --> 00:27:42.370
用荧光笔或钢笔自己写摘要的部分。

00:27:43.100 --> 00:27:47.160
我认为了解这两种技术的最高层次是

00:27:47.160 --> 00:27:50.610
提取性总结基本上比较容易，

00:27:50.610 --> 00:27:52.725
至少要建立一个体面的系统，

00:27:52.725 --> 00:27:57.120
因为选择东西可能比从头开始写文本容易。

00:27:57.120 --> 00:28:00.945
嗯，但是提取性总结是相当严格的，对吗？

00:28:00.945 --> 00:28:02.760
因为你不能真正解释什么，

00:28:02.760 --> 00:28:05.430
你真的不能做任何有力的句子压缩

00:28:05.430 --> 00:28:08.475
如果你只能选择句子。

00:28:08.475 --> 00:28:12.195
嗯，当然，抽象总结是一种范例

00:28:12.195 --> 00:28:15.645
更灵活，更能概括人类的想法，

00:28:15.645 --> 00:28:18.150
嗯，但正如所说，这很困难。

00:28:18.150 --> 00:28:23.835
所以，我会给你们一个很快的，关于神经前总结的观点。

00:28:23.835 --> 00:28:24.945
这里我们有，呃，

00:28:24.945 --> 00:28:26.700
这是一张图表，

00:28:26.700 --> 00:28:28.800
语音和语言处理书籍。

00:28:28.800 --> 00:28:33.120
所以，呃，神经前总结系统主要是提取的。

00:28:33.120 --> 00:28:35.370
就像神经前NMT一样，

00:28:35.370 --> 00:28:37.125
我们在NMT讲座中了解到的，

00:28:37.125 --> 00:28:40.395
它通常有一个管道，这就是这张图片所显示的。

00:28:40.395 --> 00:28:43.065
所以，一条典型的管道可能有三部分。

00:28:43.065 --> 00:28:46.170
首先，你有内容选择，呃，

00:28:46.170 --> 00:28:49.785
从源文档中选择一些要包含的句子。

00:28:49.785 --> 00:28:52.155
其次，你要做一些信息

00:28:52.155 --> 00:28:56.050
排序，这意味着选择我应该把这些句子按什么顺序排列。

00:28:56.050 --> 00:28:59.750
这是一个非常重要的问题，如果你是

00:28:59.750 --> 00:29:01.580
进行多文档摘要

00:29:01.580 --> 00:29:03.560
因为你的句子可能来自不同的文档。

00:29:03.560 --> 00:29:05.060
呃，最后，

00:29:05.060 --> 00:29:08.255
你要做一个句子意识，实际上，嗯，

00:29:08.255 --> 00:29:12.135
将所选句子转换为实际摘要。

00:29:12.135 --> 00:29:13.680
所以，尽管我们没有这样做，有点，

00:29:13.680 --> 00:29:15.825
自由格式的文本生成，

00:29:15.825 --> 00:29:19.290
可能会有某种编辑，例如，呃，简化，编辑，

00:29:19.290 --> 00:29:21.885
或者移除多余的零件，

00:29:21.885 --> 00:29:23.865
或修复连续性问题。

00:29:23.865 --> 00:29:26.220
例如，您不能引用

00:29:26.220 --> 00:29:28.920
像她一样的人，如果你一开始就没有介绍过他们。

00:29:28.920 --> 00:29:32.380
所以也许你需要把她改成那个人的名字。

00:29:33.180 --> 00:29:35.890
尤其是噪音

00:29:35.890 --> 00:29:37.945
这些神经前总结系统，

00:29:37.945 --> 00:29:41.230
有一些相当复杂的内容选择算法。

00:29:41.230 --> 00:29:43.450
嗯，例如，

00:29:43.450 --> 00:29:46.240
嗯，你会有一些句子评分功能。

00:29:46.240 --> 00:29:48.145
这是最简单的方法

00:29:48.145 --> 00:29:50.770
你可以把所有的句子单独打分吗？

00:29:50.770 --> 00:29:53.620
你可以根据特征来评分，比如，

00:29:53.620 --> 00:29:56.650
嗯，你知道，句子中有主题关键字吗？

00:29:56.650 --> 00:29:59.380
如果是这样，也许这是一个重要的句子，我们应该包括。

00:29:59.380 --> 00:30:02.725
你可以计算这些，

00:30:02.725 --> 00:30:06.760
例如，使用tf idf等统计信息的关键字。

00:30:06.760 --> 00:30:10.960
[噪音]您还可以使用非常基本但功能强大的功能，例如，

00:30:10.960 --> 00:30:12.925
呃，这个句子在文件里出现在哪里？

00:30:12.925 --> 00:30:14.260
如果它靠近文件的顶部，

00:30:14.260 --> 00:30:16.510
那么它更有可能是重要的。

00:30:16.510 --> 00:30:18.100
嗯，还有

00:30:18.100 --> 00:30:21.910
一些更复杂的内容选择算法，例如，

00:30:21.910 --> 00:30:25.420
有一些基于图的算法，它们将文档视为

00:30:25.420 --> 00:30:29.005
一组句子，这些句子是图的节点，

00:30:29.005 --> 00:30:30.760
你可以想象所有的句子，呃，

00:30:30.760 --> 00:30:33.190
句子对之间有一个边缘，

00:30:33.190 --> 00:30:36.760
边缘的重量就是句子的相似程度。

00:30:36.760 --> 00:30:39.925
那么，如果你从这个意义上考虑这个图表，

00:30:39.925 --> 00:30:43.600
那么现在你可以试着找出哪些句子是

00:30:43.600 --> 00:30:47.500
重要的是找出哪些句子是图表的中心。

00:30:47.500 --> 00:30:49.540
所以你可以运用一些一般的目的

00:30:49.540 --> 00:30:52.930
gla-图算法，以确定哪些[噪声]节点是中心，

00:30:52.930 --> 00:30:56.180
这是找到中心句子的方法。

00:30:56.340 --> 00:31:03.355
可以。所以，嗯，[噪音]作为一项任务回到总结。

00:31:03.355 --> 00:31:06.940
嗯，我们，我不记得我们是否已经谈过胭脂。

00:31:06.940 --> 00:31:08.140
我们当然讨论过布鲁。

00:31:08.140 --> 00:31:09.820
但我现在要告诉你胭脂是什么

00:31:09.820 --> 00:31:12.400
用于汇总的主要自动度量。

00:31:12.400 --> 00:31:17.695
因此，胭脂代表了以回忆为导向的评价依据。

00:31:17.695 --> 00:31:19.480
我不确定他们是不是第一个想到这件事

00:31:19.480 --> 00:31:21.790
或者如果他们这样做来匹配布鲁。

00:31:21.790 --> 00:31:24.610
嗯，这是，

00:31:24.610 --> 00:31:26.050
这是方程，呃，

00:31:26.050 --> 00:31:28.855
因为，好吧，我想是胭脂指数之一。

00:31:28.855 --> 00:31:31.210
我稍后会告诉你更多关于这意味着什么的信息，你可以

00:31:31.210 --> 00:31:34.105
在底部链接的原始纸张中阅读更多内容。

00:31:34.105 --> 00:31:38.095
所以，呃，总的来说，胭脂其实和布鲁很相似。

00:31:38.095 --> 00:31:40.015
这是基于N-克重叠。

00:31:40.015 --> 00:31:45.655
所以，与布鲁的一些主要区别是，胭脂没有简洁的处罚。

00:31:45.655 --> 00:31:47.230
嗯，我一会儿再谈。

00:31:47.230 --> 00:31:52.195
嗯，另一个大问题是，胭脂是基于召回，而布鲁是基于精确性。

00:31:52.195 --> 00:31:53.440
所以你可以在标题中看到它。

00:31:53.440 --> 00:31:57.115
[噪音]嗯，所以，如果你稍微考虑一下，

00:31:57.115 --> 00:32:02.245
我认为你可以说精确性对于机器翻译更重要。

00:32:02.245 --> 00:32:09.130
也就是说，您只想生成一个引用中出现的文本，

00:32:09.130 --> 00:32:12.520
翻译，然后避免

00:32:12.520 --> 00:32:14.770
一个非常保守的策略

00:32:14.770 --> 00:32:17.545
在一个很短的翻译中是非常安全的。

00:32:17.545 --> 00:32:20.035
这就是为什么你要加上简短的惩罚来确保

00:32:20.035 --> 00:32:23.035
它试图写足够长的东西。

00:32:23.035 --> 00:32:24.640
相比之下，

00:32:24.640 --> 00:32:26.290
回忆对于

00:32:26.290 --> 00:32:30.265
总结是因为你想包括所有的信息，

00:32:30.265 --> 00:32:33.190
信息-你总结中的重要信息，对吗？

00:32:33.190 --> 00:32:36.490
所以参考摘要中的信息是，

00:32:36.490 --> 00:32:38.080
呃，假设是重要信息。

00:32:38.080 --> 00:32:40.240
所以回忆意味着你抓住了所有这些。

00:32:40.240 --> 00:32:42.460
嗯，我想我-如果你认为你

00:32:42.460 --> 00:32:45.040
总结系统的最大长度限制，

00:32:45.040 --> 00:32:47.950
那这两种情况就可以权衡了，对吧？

00:32:47.950 --> 00:32:52.720
你想把所有的信息都包括进去，但不能太长时间做一个总结。

00:32:52.720 --> 00:32:55.435
所以我认为这就是为什么

00:32:55.435 --> 00:32:58.150
这两个不同任务的回忆和精确性。

00:32:58.150 --> 00:33:01.480
然而，令人困惑的是，通常是F1，

00:33:01.480 --> 00:33:03.910
这是精确性和召回版本的结合

00:33:03.910 --> 00:33:06.940
在总结文献中，胭脂无论如何都有报道。

00:33:06.940 --> 00:33:09.490
老实说，我不太清楚为什么这是，呃，

00:33:09.490 --> 00:33:11.140
可能是因为缺乏

00:33:11.140 --> 00:33:13.495
嗯，显式的最大长度约束。

00:33:13.495 --> 00:33:17.815
嗯，不管怎样，我，我试着搜索，但找不到答案。

00:33:17.815 --> 00:33:21.100
下面是关于胭脂的更多信息。

00:33:21.100 --> 00:33:22.840
嗯，如果你记得的话，

00:33:22.840 --> 00:33:24.940
布鲁被报告为一个数字，对吗？

00:33:24.940 --> 00:33:26.980
布鲁只是一个数字

00:33:26.980 --> 00:33:30.640
不同n克的精度组合

00:33:30.640 --> 00:33:32.950
通常是1-4，而

00:33:32.950 --> 00:33:36.910
每N克的红细胞评分通常单独报告。

00:33:36.910 --> 00:33:42.250
因此，最常报告的胭脂评分是胭脂-1、胭脂-2和胭脂-L。

00:33:42.250 --> 00:33:47.365
所以，胭脂红一号，不要和流氓一号混为一谈：一个星球大战的故事。

00:33:47.365 --> 00:33:49.060
嗯，我觉得自从那部电影上映后，

00:33:49.060 --> 00:33:51.610
我看到这么多人错误地输入了这个，我认为这是相关的。

00:33:51.610 --> 00:33:54.730
嗯，所以，胭脂-1是，呃，

00:33:54.730 --> 00:33:57.295
基于单图重叠，

00:33:57.295 --> 00:34:01.015
嗯，[噪音]和胭脂-2基于重瓣重叠。

00:34:01.015 --> 00:34:03.310
这是对布鲁的一种类比，除了，

00:34:03.310 --> 00:34:05.245
嗯，基于回忆，不是基于精确。

00:34:05.245 --> 00:34:10.450
更有趣的是胭脂-L，它是最长的常见子序列重叠。

00:34:10.450 --> 00:34:14.590
嗯，所以，这里的想法是你不仅对，呃，

00:34:14.590 --> 00:34:16.855
特定的n-grams匹配，但是，

00:34:16.855 --> 00:34:18.310
你知道，有多少，呃，怎么，

00:34:18.310 --> 00:34:23.240
你能找到两个词中出现的一系列词的时间。

00:34:23.520 --> 00:34:26.635
所以你可以，呃，更多地了解这些指标

00:34:26.635 --> 00:34:29.065
在上一页链接的文章中。

00:34:29.065 --> 00:34:31.495
还有一件很重要的事要注意的是现在有噪音

00:34:31.495 --> 00:34:35.200
一个方便的python实现的rouge和um，

00:34:35.200 --> 00:34:38.155
也许这不太明显为什么这么激动，

00:34:38.155 --> 00:34:40.420
但事实上很令人兴奋，因为很长一段时间，

00:34:40.420 --> 00:34:42.480
只有这个Perl脚本，嗯，

00:34:42.480 --> 00:34:46.365
这很难运行，也很难建立和理解。

00:34:46.365 --> 00:34:49.440
所以，嗯，有人曾经是个英雄，而且，呃，

00:34:49.440 --> 00:34:52.290
实现了一个纯Python版本的Rouge并检查它

00:34:52.290 --> 00:34:55.320
确实符合人们以前使用的Perl脚本。

00:34:55.320 --> 00:34:58.890
所以，如果你们中的任何一个人在使用胭脂或者为你们的项目做总结，呃，

00:34:58.890 --> 00:35:00.075
确保你，呃，

00:35:00.075 --> 00:35:02.530
去使用它，因为它可能会节省你一些时间。

00:35:02.530 --> 00:35:06.085
[噪音]好的。

00:35:06.085 --> 00:35:08.020
所以我们稍后再回到胭脂红。

00:35:08.020 --> 00:35:10.210
嗯，我知道你在作业4中考虑过

00:35:10.210 --> 00:35:12.790
布鲁作为度量衡的缺点

00:35:12.790 --> 00:35:16.555
可以肯定的是，胭脂有一些短处，而且是一个用来总结的指标。

00:35:16.555 --> 00:35:18.920
嗯，我们稍后再谈。

00:35:19.080 --> 00:35:23.230
可以。所以，我们将继续使用神经方法进行总结。

00:35:23.230 --> 00:35:27.969
[噪音]那么，呃，回到2015年，

00:35:27.969 --> 00:35:30.310
恐怕我再也没有戏剧表演了。

00:35:30.310 --> 00:35:32.710
[噪音]嗯，Rush等人

00:35:32.710 --> 00:35:35.590
发表了第一篇SEQ2SEQ总结论文。

00:35:35.590 --> 00:35:39.070
[噪音]所以，呃，他们把这个看成，

00:35:39.070 --> 00:35:41.395
你知道吗，NMT最近非常成功，

00:35:41.395 --> 00:35:44.500
为什么我们不把抽象总结看作一个翻译任务呢？

00:35:44.500 --> 00:35:48.565
因此，对其应用标准翻译seq2seq方法。

00:35:48.565 --> 00:35:51.910
所以他们就是这么做的，他们申请了，

00:35:51.910 --> 00:35:53.500
呃，一个标准的注意力模型，

00:35:53.500 --> 00:35:58.000
然后他们很好地完成了，嗯，Gigaword总结。

00:35:58.000 --> 00:35:59.620
那就是你所在的地方，嗯，

00:35:59.620 --> 00:36:03.130
从新闻文章的第一句话转换为标题。

00:36:03.130 --> 00:36:05.575
所以有点像，呃，句子压缩。

00:36:05.575 --> 00:36:10.570
所以至关重要的是，这和NMT的长度有着相同的数量级，对吧？

00:36:10.570 --> 00:36:13.810
因为NMT是一个句子对一个句子，而这是一个句子对一个句子，

00:36:13.810 --> 00:36:15.805
可能最多两句两句。

00:36:15.805 --> 00:36:18.310
所以这个效果很好，你可以变得很好，嗯，

00:36:18.310 --> 00:36:20.920
使用这种方法生成标题或句子压缩。

00:36:20.920 --> 00:36:23.515
[噪音]好的。

00:36:23.515 --> 00:36:25.510
从那以后，从2015年开始，

00:36:25.510 --> 00:36:29.380
神经抽象归纳法有了很大的发展。

00:36:29.380 --> 00:36:31.435
你可以这样说，嗯，

00:36:31.435 --> 00:36:33.865
将这些发展情况集中在，

00:36:33.865 --> 00:36:35.440
嗯，一系列主题。

00:36:35.440 --> 00:36:38.020
所以有一个主题是让复制更容易。

00:36:38.020 --> 00:36:41.050
嗯，这似乎很明显，因为在总结中，你知道，

00:36:41.050 --> 00:36:44.035
你要复制每一个，相当多的单词甚至短语，

00:36:44.035 --> 00:36:46.615
但不要复制太多。

00:36:46.615 --> 00:36:48.130
呃，另一件事是如果你成功了

00:36:48.130 --> 00:36:49.630
太容易复制，那么你复制太多。

00:36:49.630 --> 00:36:52.600
所以，还有其他的研究显示如何防止过多的复制。

00:36:52.600 --> 00:36:58.135
[噪音]嗯，下一件事是某种层次或多层次的关注。

00:36:58.135 --> 00:36:59.470
正如我刚才所展示的，

00:36:59.470 --> 00:37:01.690
注意力一直很关键，嗯，

00:37:01.690 --> 00:37:04.000
抽象神经综述。

00:37:04.000 --> 00:37:05.605
所以有一些工作要看，你知道，

00:37:05.605 --> 00:37:08.890
我们能不能把注意力放在更高的层次上，

00:37:08.890 --> 00:37:12.100
低成本精细版所以

00:37:12.100 --> 00:37:16.030
我们可以在高层次和低层次进行选择。

00:37:16.030 --> 00:37:18.985
另一个相关的事情是

00:37:18.985 --> 00:37:21.700
一些更全局的内容选择。

00:37:21.700 --> 00:37:23.515
所以如果你还记得我们谈论的时候，

00:37:23.515 --> 00:37:26.020
管道神经前总结，

00:37:26.020 --> 00:37:28.435
他们有这些不同的内容选择算法。

00:37:28.435 --> 00:37:30.250
我想你可以这么说，

00:37:30.250 --> 00:37:32.110
嗯，有点幼稚，

00:37:32.110 --> 00:37:34.630
注意：seq2seq不一定

00:37:34.630 --> 00:37:37.495
选择内容进行总结的最佳方法，

00:37:37.495 --> 00:37:40.885
也许你需要一种更为全球化的战略，在那里你可以选择什么是重要的。

00:37:40.885 --> 00:37:44.049
当你做这个小规模的总结时，这里就不那么明显了，

00:37:44.049 --> 00:37:45.430
但是如果你想象你在总结

00:37:45.430 --> 00:37:48.294
一篇完整的新闻文章，你要选择哪些信息，

00:37:48.294 --> 00:37:50.455
决定每个解码器步骤，

00:37:50.455 --> 00:37:53.170
选择什么似乎不是最全球化的战略。

00:37:53.170 --> 00:37:56.005
呃，我们还有什么？

00:37:56.005 --> 00:37:59.410
呃，有一种强化学习，可以直接最大化

00:37:59.410 --> 00:38:01.300
胭脂或其他你可能有的离散目标

00:38:01.300 --> 00:38:03.820
关心，比如总结的长度。

00:38:03.820 --> 00:38:07.495
我这里说离散，因为胭脂是不可分的，

00:38:07.495 --> 00:38:09.640
呃，生成输出的函数。

00:38:09.640 --> 00:38:12.160
你知道，没有简单的方法来区分

00:38:12.160 --> 00:38:14.200
以通常的方式在训练中了解这一点。

00:38:14.200 --> 00:38:20.170
呃，我在这张单子上的最后一点是

00:38:20.170 --> 00:38:24.040
恢复神经系统前的想法，比如我提到的那些图形算法

00:38:24.040 --> 00:38:25.960
早点把他们变成

00:38:25.960 --> 00:38:32.005
这些新的seq2seq抽象神经系统，我相信还有更多。

00:38:32.005 --> 00:38:34.150
所以，我给你看一些，嗯，

00:38:34.150 --> 00:38:37.660
尤其是因为即使你对总结不是特别感兴趣，

00:38:37.660 --> 00:38:40.930
我们将在这里探讨的许多想法实际上是适用的。

00:38:40.930 --> 00:38:45.260
到NLG的其他领域或只是NLP深入学习的其他领域。

00:38:45.300 --> 00:38:48.700
所以，清单上的第一件事就是让复制更容易，

00:38:48.700 --> 00:38:50.875
这似乎是你想解决的第一件事，

00:38:50.875 --> 00:38:53.335
如果你只需要注意基本的seq2seq。

00:38:53.335 --> 00:38:55.795
所以，嗯，复制机制，

00:38:55.795 --> 00:38:58.900
可能存在于汇总之外。

00:38:58.900 --> 00:39:03.160
原因，你为什么要这样做是因为基本的seq2seq需要注意，

00:39:03.160 --> 00:39:05.590
他们擅长写流利的输出，我们知道，

00:39:05.590 --> 00:39:09.835
但他们很难正确地复制稀有词等细节。

00:39:09.835 --> 00:39:13.210
所以复制机制只是说

00:39:13.210 --> 00:39:17.950
嗯，让我们有一个明确的机制来复制单词。

00:39:17.950 --> 00:39:20.140
例如，您可以使用

00:39:20.140 --> 00:39:25.375
把注意力分配到某种程度上选择你要复制的内容。

00:39:25.375 --> 00:39:28.890
嗯，如果你允许两种复制

00:39:28.890 --> 00:39:32.235
在你的语言模型中以通常的方式生成单词，

00:39:32.235 --> 00:39:37.220
然后，现在您有了一种混合的提取/抽象方法来进行总结。

00:39:37.220 --> 00:39:40.360
所以，有好几篇论文，它们提出了

00:39:40.360 --> 00:39:43.330
一些复制机制的变体，我想，

00:39:43.330 --> 00:39:45.040
之所以存在多重性是因为

00:39:45.040 --> 00:39:48.730
关于如何实现这一点，您可以做出一些不同的选择，

00:39:48.730 --> 00:39:53.380
这意味着有几个不同版本的拷贝机制。

00:39:53.380 --> 00:39:56.155
所以，嗯，是的，这里有几份文件，你可以看一下。

00:39:56.155 --> 00:39:58.690
我要给你看一张纸上的图表

00:39:58.690 --> 00:40:01.150
几年前我和克里斯在一起。

00:40:01.150 --> 00:40:04.915
所以，这只是一个例子，说明如何实现复制机制。

00:40:04.915 --> 00:40:06.505
所以，我们这样做，

00:40:06.505 --> 00:40:08.485
我们是说在每个解码器步骤中，

00:40:08.485 --> 00:40:11.590
你要计算这个概率pgen，这是

00:40:11.590 --> 00:40:15.370
生成下一个单词而不是复制它的概率，

00:40:15.370 --> 00:40:19.090
我们的想法是，这是根据你目前的情况来计算的，

00:40:19.090 --> 00:40:20.935
您当前的解码器隐藏状态。

00:40:20.935 --> 00:40:22.585
所以，一旦你做到了，

00:40:22.585 --> 00:40:24.790
那么你的注意力分布应该是

00:40:24.790 --> 00:40:27.280
正常，你有自己的输出，

00:40:27.280 --> 00:40:31.360
你知道，发电分布和正常一样，你要用这个pgen，

00:40:31.360 --> 00:40:32.545
这只是一个标量。

00:40:32.545 --> 00:40:35.049
你可以用它来组合，

00:40:35.049 --> 00:40:38.005
将这两个概率分布混合在一起。

00:40:38.005 --> 00:40:40.120
那么，这个方程告诉你的是，

00:40:40.120 --> 00:40:41.410
是说呃，

00:40:41.410 --> 00:40:44.230
最终输出分布

00:40:44.230 --> 00:40:45.595
接下来会有什么消息，

00:40:45.595 --> 00:40:47.080
这是一种说法，你知道，

00:40:47.080 --> 00:40:48.685
它是产生的概率

00:40:48.685 --> 00:40:51.895
乘以你的概率分布

00:40:51.895 --> 00:40:54.279
但复制的可能性

00:40:54.279 --> 00:40:57.220
还有你当时所做的。

00:40:57.220 --> 00:41:01.555
所以，最重要的是，你把注意力作为你的复制机制。

00:41:01.555 --> 00:41:03.610
所以，注意力在这里是一种双重责任。

00:41:03.610 --> 00:41:07.885
这对发电机来说都很有用

00:41:07.885 --> 00:41:10.000
你知道，呃，也许你会选择重新措辞

00:41:10.000 --> 00:41:12.460
但作为一种复制机制，它也很有用。

00:41:12.460 --> 00:41:15.430
我认为这是这些不同的论文所做的不同的事情之一。

00:41:15.430 --> 00:41:18.940
我想，我看过一份报纸，可能有两份分开的，呃，

00:41:18.940 --> 00:41:21.685
注意力分配，一份用于复印，一份用于主治医师。

00:41:21.685 --> 00:41:24.460
嗯，你可以做出不同的选择，例如，

00:41:24.460 --> 00:41:27.430
d1 pgen是一种介于0和

00:41:27.430 --> 00:41:30.730
或者你想让它成为一件困难的事情，要么是零要么是一。

00:41:30.730 --> 00:41:33.970
嗯，你也可以决定

00:41:33.970 --> 00:41:37.000
你想让PGEN在培训期间接受监督吗？

00:41:37.000 --> 00:41:40.165
是否要对数据集进行注释，说明这些内容是复制的，

00:41:40.165 --> 00:41:43.540
这些事情不是，或者你只是想端到端地学习它？

00:41:43.540 --> 00:41:46.075
所以有多种方法可以做到这一点，嗯，

00:41:46.075 --> 00:41:48.980
这已经变得很漂亮，很标准了。

00:41:50.100 --> 00:41:52.990
好吧，复制机制看起来像，

00:41:52.990 --> 00:41:55.960
似乎是个明智的想法，但他们有一个大问题，

00:41:55.960 --> 00:41:58.330
这就是我之前提到的问题是，

00:41:58.330 --> 00:41:59.665
他们复制的太多了。

00:41:59.665 --> 00:42:03.309
嗯，所以，当你——当你运行这些系统进行总结时，

00:42:03.309 --> 00:42:05.530
你发现他们最终复制了很多

00:42:05.530 --> 00:42:08.860
长短语，有时甚至是整句话，呃，

00:42:08.860 --> 00:42:11.860
不幸的是，你梦想有一个抽象的总结系统，

00:42:11.860 --> 00:42:13.795
因为你的，嗯，

00:42:13.795 --> 00:42:16.510
你知道，复制增强的seq2seq系统

00:42:16.510 --> 00:42:20.035
不幸的是，它坍塌成了一个主要的提取系统。

00:42:20.035 --> 00:42:22.060
另一个问题是，

00:42:22.060 --> 00:42:25.165
复制机制模型是它们不擅长

00:42:25.165 --> 00:42:28.600
总体内容选择，尤其是输入文档较长时，

00:42:28.600 --> 00:42:30.250
这就是我之前暗示的。

00:42:30.250 --> 00:42:33.580
嗯，我们假设，你在总结一些

00:42:33.580 --> 00:42:37.090
长得像一篇新闻文章，有几百个字那么长，

00:42:37.090 --> 00:42:38.995
你想写几句话的摘要。

00:42:38.995 --> 00:42:41.575
这似乎不是最明智的选择

00:42:41.575 --> 00:42:44.350
在写你的几句总结的每一步，

00:42:44.350 --> 00:42:46.390
但你又在选择该做什么，

00:42:46.390 --> 00:42:48.325
选择什么，总结什么。

00:42:48.325 --> 00:42:52.795
最好是在一开始就做一个全球性的决定，然后总结一下。

00:42:52.795 --> 00:42:56.560
所以，是的，问题是，没有选择内容的总体策略。

00:42:56.560 --> 00:43:03.825
所以，呃，这是我找到的一张纸。不，还没有。

00:43:03.825 --> 00:43:08.450
可以。那么，如何才能更好地进行神经总结的内容选择呢？

00:43:08.450 --> 00:43:12.010
所以，如果你还记得我们在神经系统前的总结中，

00:43:12.010 --> 00:43:14.890
你在管道中有完全不同的阶段，对吗？

00:43:14.890 --> 00:43:16.870
你有内容选择阶段

00:43:16.870 --> 00:43:20.260
文本生成阶段的表面实现。

00:43:20.260 --> 00:43:22.750
但在我们的Seq2Seq注意力系统中，

00:43:22.750 --> 00:43:25.240
这两个阶段完全混合在一起，对吗？

00:43:25.240 --> 00:43:28.780
你正在逐步实现文本生成，

00:43:28.780 --> 00:43:31.735
然后在每一个上面，你也在做内容选择。

00:43:31.735 --> 00:43:35.305
所以，是的，这不合理。

00:43:35.305 --> 00:43:37.510
所以，我找到了一份文件，

00:43:37.510 --> 00:43:39.745
呃，去年出版的我想，

00:43:39.745 --> 00:43:42.160
这是一种很好的

00:43:42.160 --> 00:43:47.050
这个问题的简单解决方案叫做自下而上的总结。

00:43:47.050 --> 00:43:51.715
所以，在本文中，如果你看-如果你看这个数字，

00:43:51.715 --> 00:43:53.260
嗯，主要的想法很简单。

00:43:53.260 --> 00:43:57.370
它说，首先你要有一个内容选择阶段，这是

00:43:57.370 --> 00:44:01.990
只是，呃，作为一个神经序列标记模型问题，对吧？

00:44:01.990 --> 00:44:04.660
运行源文档并

00:44:04.660 --> 00:44:07.615
你把每一个单词都标记为include或not include。

00:44:07.615 --> 00:44:09.790
所以，你只是有点决定什么看起来很重要，

00:44:09.790 --> 00:44:11.680
看起来应该把它写进摘要什么的

00:44:11.680 --> 00:44:15.625
不是，然后自下而上的注意力阶段说，

00:44:15.625 --> 00:44:18.010
现在你将看到一个注意力系统。

00:44:18.010 --> 00:44:19.945
这将生成摘要。

00:44:19.945 --> 00:44:21.610
你打算戴个面具吗？

00:44:21.610 --> 00:44:23.125
你知道，施加一个硬约束

00:44:23.125 --> 00:44:26.905
你不能注意那些被标记为“不包括”的词。

00:44:26.905 --> 00:44:30.595
所以，这是非常简单但有效的，嗯，

00:44:30.595 --> 00:44:34.090
因为它是一个更好的整体内容选择策略，因为

00:44:34.090 --> 00:44:38.800
这是第一个内容选择阶段，通过序列标记，你只是，

00:44:38.800 --> 00:44:42.730
只做选择的事情，而不同时做生成的事情，

00:44:42.730 --> 00:44:44.800
我认为这是一个更好的方法

00:44:44.800 --> 00:44:47.815
更好地决定包括哪些内容，然后分别进行，

00:44:47.815 --> 00:44:49.900
这也意味着作为一个巨大的副作用，

00:44:49.900 --> 00:44:53.500
在生成模型中，对长序列的复制较少。

00:44:53.500 --> 00:44:56.830
嗯，因为如果你不被允许处理事情，

00:44:56.830 --> 00:44:58.225
你不应该包括，

00:44:58.225 --> 00:45:01.960
那么很难复制一个很长的序列，对吧？

00:45:01.960 --> 00:45:05.320
就像你想复制一个完整的句子

00:45:05.320 --> 00:45:08.980
很多都不包括单词，

00:45:08.980 --> 00:45:11.635
那么你就不能复制一个长序列，你必须把它分解。

00:45:11.635 --> 00:45:12.970
所以，模型最终会做什么，

00:45:12.970 --> 00:45:14.320
是不是必须跳过，

00:45:14.320 --> 00:45:17.110
跳过要包含的部分，然后强制执行

00:45:17.110 --> 00:45:20.650
更抽象地将各个部分组合在一起。是的。

00:45:20.650 --> 00:45:25.510
他们是如何反对掩盖决定的，因为看起来-

00:45:25.510 --> 00:45:28.720
因为在训练期间[听不见]掩盖决定。

00:45:28.720 --> 00:45:32.035
是的，我想可能是单独训练。

00:45:32.035 --> 00:45:33.610
我是说，你可以去看看报纸。

00:45:33.610 --> 00:45:35.890
我过去几天读了很多论文，我记不清了。

00:45:35.890 --> 00:45:37.990
我想可能是单独训练，但他们可能

00:45:37.990 --> 00:45:40.660
试过一起训练，但效果不太好。

00:45:40.660 --> 00:45:42.860
我不确定。你可以去看看。

00:45:43.200 --> 00:45:48.745
可以。所以，我想告诉你的另一篇论文是一篇论文，呃，

00:45:48.745 --> 00:45:53.965
利用强化学习直接最大化胭脂进行神经总结。

00:45:53.965 --> 00:45:56.275
所以这是两年前的一篇论文。

00:45:56.275 --> 00:45:58.360
主要的想法是他们可以使用RL

00:45:58.360 --> 00:46:01.870
直接优化在这种情况下，胭脂-L，公制。

00:46:01.870 --> 00:46:06.010
因此，相比之下，标准的最大训练可能性是

00:46:06.010 --> 00:46:07.840
我们一直在讨论的培训目标

00:46:07.840 --> 00:46:10.390
到目前为止，整个班级都在学习语言模型，

00:46:10.390 --> 00:46:13.840
它是一个不可微函数，不能直接优化胭脂-L。

00:46:13.840 --> 00:46:16.870
所以他们，呃，他们使用了这个RL技术

00:46:16.870 --> 00:46:21.820
计算训练中的胭脂红分数，然后呃，

00:46:21.820 --> 00:46:26.110
使用强化学习对模型进行反向传播。

00:46:26.110 --> 00:46:33.220
所以，这篇论文的有趣发现是，如果他们只是使用了RL目标，

00:46:33.220 --> 00:46:36.040
然后他们确实得到了更高的胭脂分数。

00:46:36.040 --> 00:46:38.470
这样他们就可以成功地优化

00:46:38.470 --> 00:46:40.240
他们的目标是要达到的红-L指标

00:46:40.240 --> 00:46:42.760
优化，但问题是当你这样做的时候，

00:46:42.760 --> 00:46:44.725
你会得到较低的人类判断分数。

00:46:44.725 --> 00:46:47.050
所以，在右边，我们看到只有RL的模型

00:46:47.050 --> 00:46:51.775
实际上，可读性很差，关联性很强，人的判断得分。

00:46:51.775 --> 00:46:57.235
这比最大似然监督训练系统更糟糕。

00:46:57.235 --> 00:47:00.685
这是他们博客上的一句话，

00:47:00.685 --> 00:47:02.950
“我们观察到，我们的模型具有最高的胭脂红分数

00:47:02.950 --> 00:47:05.335
还生成了几乎不可读的摘要。”

00:47:05.335 --> 00:47:06.760
所以，这是-这是，

00:47:06.760 --> 00:47:08.140
嗯，我想是个问题吧？

00:47:08.140 --> 00:47:11.170
如果您试图直接为度量进行优化，

00:47:11.170 --> 00:47:13.450
然后你可能会发现你是一种游戏

00:47:13.450 --> 00:47:16.680
度量标准，而不是为真正的任务进行优化，对吧，

00:47:16.680 --> 00:47:20.550
因为我们知道，正如我们知道的，布鲁并不是一个完美的比喻

00:47:20.550 --> 00:47:22.530
实际的翻译质量，胭脂也是。

00:47:22.530 --> 00:47:26.255
不是一个完美的类比，嗯，总结质量。

00:47:26.255 --> 00:47:28.660
但是他们做了一些很酷的事情，也就是说他们发现如果

00:47:28.660 --> 00:47:31.419
把这两个目标结合起来，

00:47:31.419 --> 00:47:33.025
所以他们有点，呃，你知道，

00:47:33.025 --> 00:47:36.910
预测语言模型序列目标，然后他们也喜欢产生

00:47:36.910 --> 00:47:41.305
一个总体的总结，得到一个高的胭脂分数目标，你把它们结合在一起，

00:47:41.305 --> 00:47:45.370
然后你可以得到一个更好的人，呃，判断分数，

00:47:45.370 --> 00:47:48.220
最后，这是我们最需要的，呃，

00:47:48.220 --> 00:47:49.930
实际汇总质量的度量。

00:47:49.930 --> 00:47:54.340
[噪音]好的。

00:47:54.340 --> 00:47:57.280
所以，我要继续对话，

00:47:57.280 --> 00:48:01.750
这是一个不同的NLG任务系列。

00:48:01.750 --> 00:48:05.590
嗯，所以，真正的对话包含了各种各样的设置。

00:48:05.590 --> 00:48:06.700
我们不打算把它们全部覆盖，

00:48:06.700 --> 00:48:08.800
但这里是对所有不同种类的概述

00:48:08.800 --> 00:48:11.185
当人们说对话时可能意味着的任务。

00:48:11.185 --> 00:48:15.490
嗯，有任务导向的对话，这种对话指的是任何环境，

00:48:15.490 --> 00:48:18.205
你想在谈话中做点什么。

00:48:18.205 --> 00:48:19.690
例如，如果你有

00:48:19.690 --> 00:48:23.590
辅助任务，假设你有，你知道，也许，呃，

00:48:23.590 --> 00:48:27.040
对话代理试图帮助人类用户

00:48:27.040 --> 00:48:30.700
比如提供客户服务或建议，

00:48:30.700 --> 00:48:32.890
回答问题，帮助用户，

00:48:32.890 --> 00:48:35.950
你知道，完成一项任务，比如购买或预订一些东西。

00:48:35.950 --> 00:48:38.350
嗯，这些是虚拟系统执行的任务

00:48:38.350 --> 00:48:41.740
你的手机可以，也可以。

00:48:41.740 --> 00:48:46.585
嗯，另一类面向任务的对话任务是合作任务。

00:48:46.585 --> 00:48:49.150
你有两个探员

00:48:49.150 --> 00:48:52.120
试图通过对话共同解决一项任务。

00:48:52.120 --> 00:48:54.715
嗯，与之相反的是敌对的。

00:48:54.715 --> 00:48:58.600
如果你有两个探员想在一个任务中竞争，

00:48:58.600 --> 00:49:01.400
比赛通过对话进行。

00:49:02.340 --> 00:49:08.950
[噪音]所以，呃，与任务导向对话相反的是，呃，社会对话。

00:49:08.950 --> 00:49:13.600
所以我想这就是除了社交之外没有明确任务的地方。

00:49:13.600 --> 00:49:16.105
所以聊天对话，嗯，

00:49:16.105 --> 00:49:20.200
只是对话而已，你只是为了社交或是为了公司。

00:49:20.200 --> 00:49:24.910
嗯，我也看过一些治疗或心理健康对话方面的工作，

00:49:24.910 --> 00:49:26.740
我不确定这应该是任务还是社交，

00:49:26.740 --> 00:49:28.105
这是一种混合，呃，

00:49:28.105 --> 00:49:30.580
但我想这就是我们的目标

00:49:30.580 --> 00:49:34.285
也许可以为人类用户提供某种情感支持。

00:49:34.285 --> 00:49:40.030
嗯，作为一个非常简单的概述，

00:49:40.030 --> 00:49:42.070
呃，深度学习，呃，

00:49:42.070 --> 00:49:45.070
文艺复兴改变了对话研究，

00:49:45.070 --> 00:49:48.595
我想你可以说，在某种程度上，在深入学习之前，

00:49:48.595 --> 00:49:50.620
嗯，开放式的困难，

00:49:50.620 --> 00:49:53.830
自由形式的自然语言生成意味着，

00:49:53.830 --> 00:49:55.405
对话系统通常是，

00:49:55.405 --> 00:49:58.360
呃，不做自由形式的NLG。

00:49:58.360 --> 00:50:00.730
他们可能会使用预定义的模板，这意味着

00:50:00.730 --> 00:50:03.775
一个模板，你只需在其中填入一些内容，

00:50:03.775 --> 00:50:06.700
或者你可以从

00:50:06.700 --> 00:50:09.625
为了找到答案，

00:50:09.625 --> 00:50:11.380
你知道，一个适合用户的响应。

00:50:11.380 --> 00:50:13.330
这些绝不是简单的系统，

00:50:13.330 --> 00:50:16.180
他们有一些非常复杂的事情，比如决定，你知道，

00:50:16.180 --> 00:50:19.570
他们的对话状态是什么，应该使用什么模板，等等-

00:50:19.570 --> 00:50:23.905
到目前为止，所有的自然语言理解都是理解语境的组成部分。

00:50:23.905 --> 00:50:26.455
但是，呃，一个影响是，

00:50:26.455 --> 00:50:28.825
深入的学习是，呃，

00:50:28.825 --> 00:50:31.915
从2015年开始，也就是现在，嗯，

00:50:31.915 --> 00:50:34.615
标准化了，有，呃，

00:50:34.615 --> 00:50:38.440
就像总结一样，很多论文将seq2seq方法应用于对话。

00:50:38.440 --> 00:50:43.435
这使得人们对开放的、自由形式的对话系统产生了新的兴趣。

00:50:43.435 --> 00:50:45.760
所以，如果你想看看

00:50:45.760 --> 00:50:48.130
那些早期的SEQ2seq对话文件看起来像，

00:50:48.130 --> 00:50:53.090
嗯，这里有两种早期的方法，比如第一种应用seq2seq。

00:50:55.530 --> 00:51:00.400
可以。所以人们很快就应用了seq2seq，

00:51:00.400 --> 00:51:03.160
NMT的对话方法，但很快就变成了

00:51:03.160 --> 00:51:06.130
很明显，这种幼稚的应用

00:51:06.130 --> 00:51:08.560
标准NMT方法

00:51:08.560 --> 00:51:13.915
一些严重的普遍的缺陷，当应用到一个任务，如闲聊对话。

00:51:13.915 --> 00:51:16.960
这比总结更真实。

00:51:16.960 --> 00:51:21.145
那么，这些严重的普遍性缺陷的例子是什么呢？

00:51:21.145 --> 00:51:24.430
嗯，一种是一般性或无聊的反应，

00:51:24.430 --> 00:51:26.710
稍后我将详细介绍这些内容。

00:51:26.710 --> 00:51:29.005
另一个是不相关的反应。

00:51:29.005 --> 00:51:30.175
所以那时候，呃，

00:51:30.175 --> 00:51:32.200
对话探员说了点什么

00:51:32.200 --> 00:51:35.065
这与用户所说的有点无关。

00:51:35.065 --> 00:51:36.700
嗯，另一个是重复，

00:51:36.700 --> 00:51:38.080
这很基本，但是，

00:51:38.080 --> 00:51:39.640
呃，事情经常发生。

00:51:39.640 --> 00:51:44.275
嗯，这也是在话语中的重复，也可能是在话语中的重复。

00:51:44.275 --> 00:51:47.485
啊，另一个困难是，

00:51:47.485 --> 00:51:48.910
呃，有点缺乏背景，

00:51:48.910 --> 00:51:50.800
就像不记得谈话的历史。

00:51:50.800 --> 00:51:53.710
显然，如果你不以整个对话历史为条件，

00:51:53.710 --> 00:51:57.190
你的对话代理无法使用它，但这是一个挑战，尤其是如果你

00:51:57.190 --> 00:52:01.315
有很长的对话历史，以找出如何有效的条件。

00:52:01.315 --> 00:52:04.060
另一个问题是缺乏一致的角色。

00:52:04.060 --> 00:52:05.380
所以如果你，呃，

00:52:05.380 --> 00:52:09.970
就像我在前一张幻灯片上提到的那两篇论文一样，很天真，

00:52:09.970 --> 00:52:14.395
如果你幼稚地训练一种标准的seq2seq模型，可能会采取，呃，

00:52:14.395 --> 00:52:16.480
你知道用户的最后一句话，然后回话，

00:52:16.480 --> 00:52:18.955
或者甚至可能是整个对话史，然后说些什么。

00:52:18.955 --> 00:52:22.675
通常你的对话代理会有一个完全不一致的角色，

00:52:22.675 --> 00:52:26.800
就像有一刻他们会说它生活在欧洲，然后会说它生活在，

00:52:26.800 --> 00:52:29.770
我不知道，中国或者其他什么东西，但这是没有意义的。

00:52:29.770 --> 00:52:31.915
所以我要经历，呃，

00:52:31.915 --> 00:52:34.810
其中一些问题，并给你一点关于它们的详细信息。

00:52:34.810 --> 00:52:37.870
所以首先，这个不相关的反应问题。

00:52:37.870 --> 00:52:40.960
因此，更详细地说，您的问题是seq2seq经常

00:52:40.960 --> 00:52:44.080
生成一些与用户的话语无关的响应。

00:52:44.080 --> 00:52:47.155
所以它可能是无关的，因为它只是一般的，

00:52:47.155 --> 00:52:49.150
这意味着这有点像

00:52:49.150 --> 00:52:51.610
一般的响应问题

00:52:51.610 --> 00:52:54.160
因为模型选择了改变，

00:52:54.160 --> 00:52:56.845
把主题改成无关的东西。

00:52:56.845 --> 00:52:59.020
所以有一个解决方案，很多，

00:52:59.020 --> 00:53:00.880
有很多不同的文件，

00:53:00.880 --> 00:53:04.735
有点攻击这个不相关的反应问题，但是只有一个，

00:53:04.735 --> 00:53:07.015
例如，呃，

00:53:07.015 --> 00:53:09.835
你应该改变培训目标。

00:53:09.835 --> 00:53:12.760
所以不要试图优化，嗯，

00:53:12.760 --> 00:53:15.520
从输入s到响应t的映射，以便

00:53:15.520 --> 00:53:18.625
你在最大化给定s的条件概率，

00:53:18.625 --> 00:53:22.435
相反，你应该最大限度地利用相互之间的信息。

00:53:22.435 --> 00:53:24.235
所以这就是为什么在这里。

00:53:24.235 --> 00:53:26.530
所以最大限度的相互信息，呃，

00:53:26.530 --> 00:53:29.140
你可以这样改写目标，

00:53:29.140 --> 00:53:31.915
如果你想看更多的细节，你可以看这篇论文。

00:53:31.915 --> 00:53:35.830
但你的想法是想找到你对那种，呃，

00:53:35.830 --> 00:53:38.680
最大化这件事，就像说，

00:53:38.680 --> 00:53:41.680
考虑到投入，这是可能的，但是

00:53:41.680 --> 00:53:44.920
有点像它自身的概率比。

00:53:44.920 --> 00:53:49.510
所以如果t是非常高的可能性，

00:53:49.510 --> 00:53:52.600
然后它就会受到惩罚，就像比例一样

00:53:52.600 --> 00:53:56.440
对于给定输入的概率，它只是独立的概率。

00:53:56.440 --> 00:53:59.650
所以我的想法是这是为了让你泄气，嗯，

00:53:59.650 --> 00:54:04.240
只是说一般的事情，只是有一个高点自己。

00:54:04.240 --> 00:54:08.995
嗯，这就是不相关的反应问题。

00:54:08.995 --> 00:54:10.780
正如我刚才暗示的，有，

00:54:10.780 --> 00:54:12.040
两者之间的联系非常紧密

00:54:12.040 --> 00:54:16.870
不相关的响应问题和一般或无聊的响应问题。

00:54:16.870 --> 00:54:21.490
所以来看看一般性或者无聊的反应问题。

00:54:21.490 --> 00:54:27.730
[噪音]所以我想

00:54:27.730 --> 00:54:32.230
有一些非常简单的修复方法，

00:54:32.230 --> 00:54:35.470
在一定程度上改善了无聊的反应问题。

00:54:35.470 --> 00:54:38.410
你是否真正触及问题的核心是另一个问题。

00:54:38.410 --> 00:54:42.310
但是一些简单的测试时间修正，例如，

00:54:42.310 --> 00:54:46.885
你可以直接提高搜索率，增加搜索中稀有词的权重。

00:54:46.885 --> 00:54:49.675
所以你可以说，所有罕见的词都会让他们，呃，

00:54:49.675 --> 00:54:51.880
记录概率然后现在我们

00:54:51.880 --> 00:54:54.220
更可能在光束搜索过程中产生它们。

00:54:54.220 --> 00:54:56.410
你可以做的另一件事是你可以用例如，

00:54:56.410 --> 00:55:00.535
一种采样解码算法而不是波束搜索，我们之前讨论过，

00:55:00.535 --> 00:55:02.350
嗯，或者你可以用，哦，是的，

00:55:02.350 --> 00:55:04.195
你也可以使用SoftMax温度。

00:55:04.195 --> 00:55:07.105
那是另一回事。所以这些是

00:55:07.105 --> 00:55:12.040
这种测试时间的修正，你可以认为是一种后期干预，对吗？

00:55:12.040 --> 00:55:16.000
所以早期的干预可能会对你的模型进行不同的培训。

00:55:16.000 --> 00:55:20.005
所以我称这种调节修复是因为这些修复与

00:55:20.005 --> 00:55:23.965
嗯，把你的模特调整到一些有助于减少无聊的事情上。

00:55:23.965 --> 00:55:26.320
所以一个例子是，也许你应该

00:55:26.320 --> 00:55:28.930
解码器对某种附加上下文。

00:55:28.930 --> 00:55:31.150
呃，举个例子，有一些研究表明，

00:55:31.150 --> 00:55:33.625
如果你在聊天，那么也许你应该，呃，

00:55:33.625 --> 00:55:36.280
去列举一些与

00:55:36.280 --> 00:55:38.710
用户所说的，然后只是在你

00:55:38.710 --> 00:55:41.350
生成然后你更有可能说出某种内容

00:55:41.350 --> 00:55:44.080
与你以前说的无聊的事情相比，它充满了乐趣。

00:55:44.080 --> 00:55:46.870
啊，另一个选择是你可以训练

00:55:46.870 --> 00:55:50.770
检索和优化模型，而不是从头生成模型。

00:55:50.770 --> 00:55:53.440
所以通过检索和提炼，我的意思是，呃，

00:55:53.440 --> 00:55:55.825
假设你有某种语料库，

00:55:55.825 --> 00:55:57.400
只是一般的话语，

00:55:57.400 --> 00:56:00.460
你可以说的话，然后你可以做一个样本，呃，

00:56:00.460 --> 00:56:01.855
从那个测试集，

00:56:01.855 --> 00:56:03.775
这是训练集，

00:56:03.775 --> 00:56:06.895
然后编辑它以适应当前的情况。

00:56:06.895 --> 00:56:10.630
因此，这是一种非常强大的生产方法

00:56:10.630 --> 00:56:14.800
更为多样、人性化、有趣的话语，嗯，

00:56:14.800 --> 00:56:19.555
因为你可以从样品中得到所有的细粒度细节，

00:56:19.555 --> 00:56:23.665
啊，说出话来，然后根据需要进行编辑，以适应当前的情况。

00:56:23.665 --> 00:56:26.740
所以我的意思是，这类方法可能有缺点

00:56:26.740 --> 00:56:30.145
很难对其进行编辑，使其真正适应这种情况，

00:56:30.145 --> 00:56:32.410
嗯，但这当然是一种有效地

00:56:32.410 --> 00:56:36.530
更多的多样性，嗯，还有兴趣。

00:56:37.170 --> 00:56:40.810
关于重复问题的主题，

00:56:40.810 --> 00:56:43.105
这是我们注意到的另一个主要问题，

00:56:43.105 --> 00:56:45.790
嗯，把seq2seq应用于，呃，chitchat。

00:56:45.790 --> 00:56:48.970
嗯，还有一些简单的解决方案和更复杂的解决方案。

00:56:48.970 --> 00:56:52.150
嗯，一个简单的解决方法就是你可以阻止重复

00:56:52.150 --> 00:56:55.915
在波束搜索过程中，N-grams通常是非常有效的。

00:56:55.915 --> 00:56:57.595
我的意思是，呃，

00:56:57.595 --> 00:56:59.815
在搜索光束的时候，当你考虑的时候，

00:56:59.815 --> 00:57:01.510
你知道，我的假设是什么？

00:57:01.510 --> 00:57:05.110
这是概率分布中的前k种，你说，

00:57:05.110 --> 00:57:09.535
好吧，任何构成重复n-gram的东西都会被扔掉。

00:57:09.535 --> 00:57:11.590
所以当我说构成一个重复的n-gram时，

00:57:11.590 --> 00:57:14.470
我的意思是如果你接受了这个词，

00:57:14.470 --> 00:57:19.630
你现在能创造一个重复吗？比如说两克重的，重的，嗯，

00:57:19.630 --> 00:57:23.500
如果我们决定禁止所有重复的大字、三角或其他东西，

00:57:23.500 --> 00:57:26.620
然后你只需要检查每一个可能的单词

00:57:26.620 --> 00:57:30.700
在光束搜索中观察是否会产生一个重复的n-gram。

00:57:30.700 --> 00:57:32.440
我的意思是，这个很好用，

00:57:32.440 --> 00:57:34.780
这决不是一种原则性的解决方案，对吧？

00:57:34.780 --> 00:57:37.495
如果我们觉得应该有更好的方法来学习不要重复，嗯，

00:57:37.495 --> 00:57:39.790
但作为一种，呃，

00:57:39.790 --> 00:57:42.535
有效的黑客，我认为有效，效果很好。

00:57:42.535 --> 00:57:44.830
所以更复杂的解决方案是，

00:57:44.830 --> 00:57:47.920
例如，您可以训练称为覆盖机制的东西。

00:57:47.920 --> 00:57:50.530
嗯，所以在Seq2seq，这主要是，呃，

00:57:50.530 --> 00:57:53.800
受到机器翻译设置的启发，

00:57:53.800 --> 00:57:56.440
覆盖机制是防止

00:57:56.440 --> 00:57:58.630
从关注到关注的机制

00:57:58.630 --> 00:58:01.810
同一个词多次或多次。

00:58:01.810 --> 00:58:03.655
这里的直觉是，

00:58:03.655 --> 00:58:06.595
也许重复是由重复的注意力引起的。

00:58:06.595 --> 00:58:08.620
所以如果你经常处理同样的事情，

00:58:08.620 --> 00:58:09.970
也许你会重复一遍，

00:58:09.970 --> 00:58:11.605
你知道，相同的输出多次。

00:58:11.605 --> 00:58:13.690
所以如果你阻止了重复的注意力，

00:58:13.690 --> 00:58:15.280
防止重复输出。

00:58:15.280 --> 00:58:18.190
所以这确实很管用，但肯定是，

00:58:18.190 --> 00:58:21.490
嗯，更复杂的是要实现，

00:58:21.490 --> 00:58:23.635
不太方便，

00:58:23.635 --> 00:58:25.120
嗯，我不知道，

00:58:25.120 --> 00:58:28.075
在某些情况下，似乎简单的解决方案是，

00:58:28.075 --> 00:58:29.530
嗯，更简单，也同样有效。

00:58:29.530 --> 00:58:32.740
呃，其他复杂的解决方案

00:58:32.740 --> 00:58:35.800
也许你可以定义一个训练目标来阻止重复。

00:58:35.800 --> 00:58:38.320
呃，你可以试着，嗯，

00:58:38.320 --> 00:58:41.125
定义一些可微的东西，但其中一个，

00:58:41.125 --> 00:58:45.600
困难是因为你在接受老师的强制训练，对吗？

00:58:45.600 --> 00:58:47.055
你总是喜欢看的地方，

00:58:47.055 --> 00:58:48.435
到目前为止，黄金投入，

00:58:48.435 --> 00:58:50.700
那么你从来没有真正做过

00:58:50.700 --> 00:58:53.010
你产生你自己的输出并开始重复你自己。

00:58:53.010 --> 00:58:55.845
所以在那种情况下很难定义惩罚。

00:58:55.845 --> 00:58:58.350
所以这可能是一个不可微函数。

00:58:58.350 --> 00:59:00.255
就像这样，

00:59:00.255 --> 00:59:03.745
保罗等人的论文是，

00:59:03.745 --> 00:59:06.250
优化胭脂，也许我们，呃，

00:59:06.250 --> 00:59:11.455
优化以避免重复，这是输入的一个离散函数。

00:59:11.455 --> 00:59:14.425
呃，我要跳过讲故事。

00:59:14.425 --> 00:59:16.195
所以在讲故事时，

00:59:16.195 --> 00:59:19.015
现在有很多有趣的神经讲故事的工作正在进行。

00:59:19.015 --> 00:59:22.285
其中大部分都使用某种提示来写故事。

00:59:22.285 --> 00:59:24.610
例如，呃，

00:59:24.610 --> 00:59:28.115
写一个有图像或提示的故事

00:59:28.115 --> 00:59:32.715
或者写下故事的下一句话。

00:59:32.715 --> 00:59:37.645
这里有一个从图像生成故事的例子。

00:59:37.645 --> 00:59:40.360
有趣的是我们有这样一幅图像

00:59:40.360 --> 00:59:42.940
是一张爆炸的照片

00:59:42.940 --> 00:59:44.740
那么，给你

00:59:44.740 --> 00:59:48.475
以泰勒·斯威夫特的歌词形式写的关于图像的故事。

00:59:48.475 --> 00:59:52.015
所以它说，你必须是夜空中唯一的灯泡，我想，

00:59:52.015 --> 00:59:55.225
哦，天啊，我的心太黑了，我想你了，我保证。

00:59:55.225 --> 00:59:58.690
有趣的是，没有任何直接的、有监督的，

00:59:58.690 --> 01:00:02.620
你知道，图像字幕爆炸数据集和泰勒斯威夫特歌词。

01:00:02.620 --> 01:00:05.890
嗯，他们是分开学的。

01:00:05.890 --> 01:00:12.220
他们是如何做到这一点的，他们使用了一种常见的句子编码空间。

01:00:12.220 --> 01:00:15.160
所以他们用这种特殊的句子编码

01:00:15.160 --> 01:00:18.205
跳过思想载体，然后他们训练，

01:00:18.205 --> 01:00:21.880
嗯，这个可可图像的字幕，呃，

01:00:21.880 --> 01:00:24.790
从图像到编码的系统

01:00:24.790 --> 01:00:28.105
然后分别训练，

01:00:28.105 --> 01:00:30.370
呃，一个语言模型，一个条件语言模型

01:00:30.370 --> 01:00:33.010
泰勒·斯威夫特歌词的编码句子。

01:00:33.010 --> 01:00:35.230
因为你有这个共享的编码空间，

01:00:35.230 --> 01:00:38.305
你现在可以把这两个放在一起，然后从照片上看，

01:00:38.305 --> 01:00:41.050
对于嵌入，对于泰勒-斯威夫特风格的输出，

01:00:41.050 --> 01:00:43.790
我觉得很漂亮，很神奇。

01:00:44.220 --> 01:00:46.600
哇，我真的迷失了，迷失了时间的轨迹。

01:00:46.600 --> 01:00:48.250
所以我，我想我得快点。

01:00:48.250 --> 01:00:55.140
所以，嗯，我们有一些非常令人印象深刻的故事，

01:00:55.140 --> 01:00:57.900
发电系统，最近，嗯，

01:00:57.900 --> 01:01:00.795
这是一个例子，

01:01:00.795 --> 01:01:02.580
呃，一个系统，

01:01:02.580 --> 01:01:03.840
呃，准备一个新的数据集，

01:01:03.840 --> 01:01:05.745
在你写一个故事的地方给出了提示，

01:01:05.745 --> 01:01:08.165
他们让人印象深刻，

01:01:08.165 --> 01:01:10.900
非常完善的卷积语言模型，

01:01:10.900 --> 01:01:13.975
生成给定输入的故事的seq-to-seq系统。

01:01:13.975 --> 01:01:15.640
我不想把这些细节都讲一遍，

01:01:15.640 --> 01:01:18.070
但是如果你想退房，我鼓励你，

01:01:18.070 --> 01:01:20.950
故事生成的最新技术是什么，你应该看看这个。

01:01:20.950 --> 01:01:23.110
有很多不同的有趣的事情发生了

01:01:23.110 --> 01:01:25.780
非常花哨的注意力和曲折等等，

01:01:25.780 --> 01:01:29.695
他们创造了一些非常有趣的，嗯，令人印象深刻的故事。

01:01:29.695 --> 01:01:31.749
所以这里，如果你看这个例子，

01:01:31.749 --> 01:01:36.280
我们有一些非常有趣的，嗯，有点，

01:01:36.280 --> 01:01:39.324
嗯，故事的生成是多样的，不是一般的，

01:01:39.324 --> 01:01:41.320
它的风格很戏剧化，很好，

01:01:41.320 --> 01:01:42.925
与提示相关。

01:01:42.925 --> 01:01:46.330
嗯，但我想你在这里可以看到

01:01:46.330 --> 01:01:49.855
最先进的故事生成系统可以做到这一点——嗯，

01:01:49.855 --> 01:01:51.715
虽然有点时髦，

01:01:51.715 --> 01:01:54.625
它主要是大气的和描述性的。

01:01:54.625 --> 01:01:56.140
这并不能真正推动剧情的发展。

01:01:56.140 --> 01:01:57.940
这里没有什么活动，对吧？

01:01:57.940 --> 01:02:02.305
嗯，所以问题是，当你生成的时间更长的时候，情况会更糟。

01:02:02.305 --> 01:02:04.180
当您生成长文本时，

01:02:04.180 --> 01:02:08.755
然后，它将主要停留在同一个想法上，而不带着新的想法前进。

01:02:08.755 --> 01:02:11.500
可以。所以我要向前跳很多，

01:02:11.500 --> 01:02:13.510
呃，对不起，应该有更好的计划。

01:02:13.510 --> 01:02:15.160
这里有很多你想查的信息

01:02:15.160 --> 01:02:17.335
关于诗歌的产生和其他事情。

01:02:17.335 --> 01:02:19.690
我要跳到前面去，因为我想去

01:02:19.690 --> 01:02:23.320
NLG评估部分因为这很重要。

01:02:23.320 --> 01:02:27.655
所以，嗯，我们已经讨论过自动评估指标fr nlg，

01:02:27.655 --> 01:02:30.760
我们知道这些词是基于重叠的度量，比如bleu，

01:02:30.760 --> 01:02:32.155
还有胭脂和流星，

01:02:32.155 --> 01:02:34.360
我们知道它们不适合机器翻译。

01:02:34.360 --> 01:02:37.780
啊，概括起来就更糟了

01:02:37.780 --> 01:02:41.770
因为总结比机器翻译更开放。

01:02:41.770 --> 01:02:44.170
这意味着有了这种死板的观念，

01:02:44.170 --> 01:02:45.835
如果你要匹配N克，

01:02:45.835 --> 01:02:47.380
更不有用。

01:02:47.380 --> 01:02:49.930
然后为了更开放的对话，

01:02:49.930 --> 01:02:51.580
那就是一种灾难。

01:02:51.580 --> 01:02:54.220
甚至连一个指标都不能给你一个好的信号，

01:02:54.220 --> 01:02:58.045
这也适用于任何其他开放式的，比如故事的产生。

01:02:58.045 --> 01:03:01.225
所以它已经被展示出来了，你可以在底部查看这张纸，

01:03:01.225 --> 01:03:05.125
这个词的重叠度量不适合对话。

01:03:05.125 --> 01:03:07.480
所以橙色的盒子给你看，

01:03:07.480 --> 01:03:13.855
对话类中人类得分与BLeu-2相关的一些图，

01:03:13.855 --> 01:03:15.415
布鲁的一些变种。

01:03:15.415 --> 01:03:18.490
问题是，你根本看不到什么相关性，对吧？

01:03:18.490 --> 01:03:21.420
特别是在这种对话环境下，啊，

01:03:21.420 --> 01:03:23.370
布鲁度量与

01:03:23.370 --> 01:03:26.565
人类判断它是否是一种良好的对话反应是，

01:03:26.565 --> 01:03:28.020
嗯，相关性是-我的意思是，

01:03:28.020 --> 01:03:29.040
它看起来有点不存在。

01:03:29.040 --> 01:03:30.720
它至少很弱。

01:03:30.720 --> 01:03:35.120
所以这是非常不幸的，还有其他一些报纸也显示了同样的情况。

01:03:35.120 --> 01:03:36.640
所以你可能会想，“好吧，

01:03:36.640 --> 01:03:38.920
我们可以使用哪些其他自动度量标准？

01:03:38.920 --> 01:03:40.600
“困惑怎么办？

01:03:40.600 --> 01:03:45.820
嗯，所以困惑当然能说明你的语言模式有多强大，

01:03:45.820 --> 01:03:48.085
但它并没有告诉你任何关于世代的事情。

01:03:48.085 --> 01:03:51.970
例如，如果你的deca解码算法在某种程度上不好，

01:03:51.970 --> 01:03:54.700
那么困惑就不会告诉你什么了，对吧？

01:03:54.700 --> 01:03:57.640
因为解码是你训练语言模型的应用。

01:03:57.640 --> 01:04:00.130
困惑可以判断你是否有一个强大的语言模式，

01:04:00.130 --> 01:04:01.840
但它不会告诉你，嗯，

01:04:01.840 --> 01:04:04.165
你这一代人一定有多好。

01:04:04.165 --> 01:04:07.330
所以你对自动评估的其他想法是，

01:04:07.330 --> 01:04:09.460
那么，基于嵌入词的度量呢？

01:04:09.460 --> 01:04:12.145
嗯，所以嵌入词的度量的主要思想是，

01:04:12.145 --> 01:04:14.515
呃，你想计算

01:04:14.515 --> 01:04:18.220
单词嵌入或可能是单词嵌入句子的平均值，

01:04:18.220 --> 01:04:20.530
不仅仅是单词本身的重叠。

01:04:20.530 --> 01:04:22.780
嗯，所以我的想法是这样的，而不仅仅是

01:04:22.780 --> 01:04:25.510
非常严格，只说完全相同的词，

01:04:25.510 --> 01:04:28.885
你说，“好吧，如果单词相似，并且在单词嵌入空间中，那么它们就可以计数。”

01:04:28.885 --> 01:04:31.900
所以这肯定更灵活，但不幸的是，

01:04:31.900 --> 01:04:34.360
我之前展示的同一篇论文表明

01:04:34.360 --> 01:04:37.315
与人类对质量的判断密切相关，

01:04:37.315 --> 01:04:39.955
至少对于他们正在研究的对话任务来说。

01:04:39.955 --> 01:04:43.285
在这里，中间的一列显示了人类之间的关系，

01:04:43.285 --> 01:04:47.515
判断，以及某种基于嵌入词的平均度量。

01:04:47.515 --> 01:04:49.540
所以，嗯，是的，看起来也不太好，

01:04:49.540 --> 01:04:51.400
相关性不强。

01:04:51.400 --> 01:04:54.970
所以如果我们没有足够的自动度量标准

01:04:54.970 --> 01:04:58.435
捕捉自然语言生成的整体质量，

01:04:58.435 --> 01:05:00.460
嗯，什么，我们能做什么？

01:05:00.460 --> 01:05:02.590
所以我认为通常的策略是，

01:05:02.590 --> 01:05:06.280
你最终定义了一些更加集中的自动度量标准

01:05:06.280 --> 01:05:10.705
捕获您可能感兴趣的生成文本的特定方面。

01:05:10.705 --> 01:05:13.640
例如，你可能对流利感兴趣，

01:05:13.640 --> 01:05:15.540
你可以通过运行

01:05:15.540 --> 01:05:18.734
一个训练有素的语言模型在你的文本上生成概率，

01:05:18.734 --> 01:05:23.505
这就相当于它的写作水平，你知道，好的，流利的，语法文本。

01:05:23.505 --> 01:05:28.000
嗯，如果你特别想用特定的风格生成文本，

01:05:28.000 --> 01:05:29.860
然后你可以采取一种语言模式

01:05:29.860 --> 01:05:32.185
在代表这种风格的语料库上训练，

01:05:32.185 --> 01:05:35.110
现在概率告诉你，它不仅是一个好的文本，

01:05:35.110 --> 01:05:36.685
但是它的样式是对的吗？

01:05:36.685 --> 01:05:38.875
嗯，还有一些其他的事情，

01:05:38.875 --> 01:05:41.185
你知道，多样性，嗯，

01:05:41.185 --> 01:05:43.900
你只需要统计一下

01:05:43.900 --> 01:05:45.940
你知道，你用了多少生僻字。

01:05:45.940 --> 01:05:48.250
嗯，与输入相关，

01:05:48.250 --> 01:05:50.710
你可以用输入计算一个相似度分数，

01:05:50.710 --> 01:05:52.555
还有一些简单的事情，比如，

01:05:52.555 --> 01:05:55.480
长度和重复，你肯定可以数，是的，

01:05:55.480 --> 01:05:58.075
它没有告诉你整体质量，

01:05:58.075 --> 01:06:00.535
但这些东西值得衡量。

01:06:00.535 --> 01:06:02.410
所以我想我的主要观点是，是的，

01:06:02.410 --> 01:06:04.960
我们的NLG评估非常困难。

01:06:04.960 --> 01:06:06.400
没有一种全面的衡量标准。

01:06:06.400 --> 01:06:08.860
通常情况下，它们会捕捉到这种整体质量。

01:06:08.860 --> 01:06:11.365
但是如果你测量很多这些东西，

01:06:11.365 --> 01:06:16.075
然后他们当然可以帮助你追踪一些你应该知道的重要事情。

01:06:16.075 --> 01:06:21.895
所以我们讨论了NLG的自动评估度量是多么的困难。

01:06:21.895 --> 01:06:23.710
我们来谈谈人类的评估。

01:06:23.710 --> 01:06:27.400
嗯，人类的判断被视为黄金标准，对吧？

01:06:27.400 --> 01:06:30.865
但是我们已经知道人类的评估是缓慢和昂贵的，

01:06:30.865 --> 01:06:34.165
嗯，但这是人类撤离的唯一问题吗？

01:06:34.165 --> 01:06:36.910
假设你有权

01:06:36.910 --> 01:06:40.060
比如说，你需要多少时间或金钱来做人类评估。

01:06:40.060 --> 01:06:42.115
嗯，这能解决你所有的问题吗？

01:06:42.115 --> 01:06:43.480
假设你有无限的人类撤离，

01:06:43.480 --> 01:06:44.980
这真的解决了你的问题吗？

01:06:44.980 --> 01:06:47.590
我的答案是，呃，不。

01:06:47.590 --> 01:06:49.630
这有点来自个人经验。

01:06:49.630 --> 01:06:53.590
嗯，进行人类评估本身是很难得到正确的。

01:06:53.590 --> 01:06:57.280
这一点都不容易，部分原因是人类做了很多奇怪的事情。

01:06:57.280 --> 01:06:59.905
人类，不像度量标准，

01:06:59.905 --> 01:07:02.125
自动度量，它们不一致，

01:07:02.125 --> 01:07:03.670
它们可能不合逻辑。

01:07:03.670 --> 01:07:05.290
有时候，他们只是厌倦了你的工作，

01:07:05.290 --> 01:07:06.760
他们不再关注了。

01:07:06.760 --> 01:07:09.580
呃，他们会误解你问的问题，

01:07:09.580 --> 01:07:12.400
有时他们做的事情不能真正解释他们为什么这么做。

01:07:12.400 --> 01:07:14.440
所以，嗯，作为一种案例研究

01:07:14.440 --> 01:07:16.540
我要告诉你的是，

01:07:16.540 --> 01:07:18.010
我以前做的一个项目，

01:07:18.010 --> 01:07:19.540
呃，建一些聊天机器人，

01:07:19.540 --> 01:07:23.485
结果证明，人类评估是项目中最困难的部分。

01:07:23.485 --> 01:07:26.230
所以我试着建立这些聊天机器人的个人聊天数据

01:07:26.230 --> 01:07:29.635
设置，特别是研究可控性。

01:07:29.635 --> 01:07:32.875
所以我们试图控制生成文本的各个方面，比如，

01:07:32.875 --> 01:07:34.045
无论你是否重复，

01:07:34.045 --> 01:07:35.395
你有多普通，

01:07:35.395 --> 01:07:37.615
类似于我们之前提到的问题。

01:07:37.615 --> 01:07:40.180
所以我们建立了这些模型来控制，

01:07:40.180 --> 01:07:42.085
我们所说的具体内容

01:07:42.085 --> 01:07:44.740
我们所说的与用户所说的有多大关联。

01:07:44.740 --> 01:07:46.090
在这里你可以看到，

01:07:46.090 --> 01:07:48.880
你知道，呃，我们的搭档说，“是的，

01:07:48.880 --> 01:07:51.745
我现在在学法律，“我们可以控制-

01:07:51.745 --> 01:07:54.715
转动这个控制旋钮，让我们说一些非常普通的话，比如，

01:07:54.715 --> 01:07:57.010
“哦，”然后像20点什么的

01:07:57.010 --> 01:07:59.470
只是完全疯狂，这只是所有罕见的词，你知道。

01:07:59.470 --> 01:08:01.510
你说的话之间有一个甜蜜的地方，

01:08:01.510 --> 01:08:03.955
“听起来很有趣。你学习多长时间了？”

01:08:03.955 --> 01:08:06.955
同样的，我们也有一个旋钮可以转动，

01:08:06.955 --> 01:08:11.260
确定我们所说的和他们所说的有多大的语义关联。

01:08:11.260 --> 01:08:13.540
所以，嗯，你知道，这有点有趣。

01:08:13.540 --> 01:08:16.615
这是一种控制NLG系统输出的方法。

01:08:16.615 --> 01:08:19.525
但事实上，我想告诉你人类的评估是多么的困难，

01:08:19.525 --> 01:08:22.885
所以我们有这些系统，我们想用人类的评估生成。

01:08:22.885 --> 01:08:26.230
所以问题是，你在这里是如何要求人类的质量判断的？

01:08:26.230 --> 01:08:29.800
嗯，你可以问一些简单的全面质量问题，

01:08:29.800 --> 01:08:31.975
比如，你知道，谈话进行得怎么样？

01:08:31.975 --> 01:08:33.670
用户参与了吗？

01:08:33.670 --> 01:08:34.990
嗯，或者比较一下，

01:08:34.990 --> 01:08:38.605
哪些用户给出了最好的响应？呃，像这样的问题。

01:08:38.605 --> 01:08:40.330
而且，你知道，我们尝试了很多，

01:08:40.330 --> 01:08:43.000
但他们都有重大问题。

01:08:43.000 --> 01:08:46.960
比如，这些问题必然是非常主观的，而且，

01:08:46.960 --> 01:08:49.150
不同的受访者有不同的期望，

01:08:49.150 --> 01:08:50.620
这会影响他们的判断。

01:08:50.620 --> 01:08:53.695
例如，如果你问，你认为这个用户是人类还是机器人？

01:08:53.695 --> 01:08:55.645
那么，那完全取决于

01:08:55.645 --> 01:09:00.220
这些受访者对机器人的了解或对机器人的看法，以及他们认为自己能做什么。

01:09:00.220 --> 01:09:03.940
另一个例子是，你会对这个问题产生灾难性的误解。

01:09:03.940 --> 01:09:05.140
例如，如果我们问，

01:09:05.140 --> 01:09:07.675
这个用户-这个聊天机器人有吸引力吗？

01:09:07.675 --> 01:09:09.475
然后有人回答说，“是的，

01:09:09.475 --> 01:09:11.020
它很吸引人，因为它总是回信“，

01:09:11.020 --> 01:09:12.310
这显然不是我们的意思。

01:09:12.310 --> 01:09:14.605
我们的意思是说他们是一个有吸引力的谈话伙伴，

01:09:14.605 --> 01:09:16.765
但他们只是在字面上假设，

01:09:16.765 --> 01:09:19.075
呃，那是什么意思。

01:09:19.075 --> 01:09:22.975
所以这里的问题是，整体质量取决于许多潜在因素，

01:09:22.975 --> 01:09:25.390
很难找到单身，

01:09:25.390 --> 01:09:28.720
只反映整体质量的整体问题。

01:09:28.720 --> 01:09:31.030
所以我们最终做了这个，我们最终打破了这个

01:09:31.030 --> 01:09:34.270
更多的品质因素。

01:09:34.270 --> 01:09:36.205
所以，呃，我们看到的是，

01:09:36.205 --> 01:09:39.820
你也许对聊天机器人的质量有一些全面的衡量，

01:09:39.820 --> 01:09:41.380
比如它有多吸引人，

01:09:41.380 --> 01:09:43.135
和他交谈是多么的愉快，

01:09:43.135 --> 01:09:45.685
也许这是人类的信仰。

01:09:45.685 --> 01:09:47.230
然后在下面，

01:09:47.230 --> 01:09:49.810
我们有点崩溃，因为这些更低的水平，呃，

01:09:49.810 --> 01:09:51.685
质量组成部分，如：

01:09:51.685 --> 01:09:53.290
你知道，呃，你有意思吗？

01:09:53.290 --> 01:09:55.150
你是在显示你在听吗？

01:09:55.150 --> 01:09:56.935
你问的问题够多了吗？

01:09:56.935 --> 01:09:59.620
然后在下面，我们有了这些可控制的属性，

01:09:59.620 --> 01:10:02.470
是我们转动的旋钮，然后目标是找出，

01:10:02.470 --> 01:10:04.525
嗯，这些东西是如何影响产出的。

01:10:04.525 --> 01:10:09.745
嗯，让我们看看。

01:10:09.745 --> 01:10:13.120
嗯，我们这里有很多发现，我想，

01:10:13.120 --> 01:10:16.440
也许我要强调的是，

01:10:16.440 --> 01:10:18.060
呃，这两种在中间。

01:10:18.060 --> 01:10:19.980
所以整体的度量参与度，

01:10:19.980 --> 01:10:23.085
这意味着享受，这真的很容易最大化。

01:10:23.085 --> 01:10:24.420
结果，呃，

01:10:24.420 --> 01:10:28.300
我们的机器人在交战方面接近人类的表现。

01:10:28.300 --> 01:10:30.730
嗯，但是整体度量人性，

01:10:30.730 --> 01:10:32.440
这是图灵测试度量，

01:10:32.440 --> 01:10:34.405
这根本不容易实现最大化。

01:10:34.405 --> 01:10:35.920
我们所有的机器人都是这样的，

01:10:35.920 --> 01:10:38.020
在人性方面远远低于人类，对吗？

01:10:38.020 --> 01:10:40.300
所以我们根本不相信自己是人类，

01:10:40.300 --> 01:10:41.785
这很有趣，对吧？

01:10:41.785 --> 01:10:44.035
就像，我们和人类一样愉快，

01:10:44.035 --> 01:10:46.630
但我们显然不是人类，对吧？

01:10:46.630 --> 01:10:50.245
比如说，人性和会话质量是不一样的。

01:10:50.245 --> 01:10:52.615
其中一件有趣的事就是，

01:10:52.615 --> 01:10:55.390
嗯，学习，我们不仅评估聊天机器人，

01:10:55.390 --> 01:10:57.655
我们还让人类相互评价，

01:10:57.655 --> 01:11:00.925
是不是，嗯，人类是次优的健谈者？

01:11:00.925 --> 01:11:05.170
嗯，他们在趣味性、流利性和听力方面得分很低。

01:11:05.170 --> 01:11:06.895
他们问得不够多，

01:11:06.895 --> 01:11:09.460
这就是为什么我们喜欢这种方法的原因

01:11:09.460 --> 01:11:13.150
与你交谈的人的表现是令人愉快的，因为我们，

01:11:13.150 --> 01:11:16.500
例如，打开提问旋钮，问更多问题，

01:11:16.500 --> 01:11:19.875
人们对此反应很好，因为人们喜欢谈论自己。

01:11:19.875 --> 01:11:22.290
所以，嗯，是的。

01:11:22.290 --> 01:11:23.610
我觉得这很有趣，对吧？

01:11:23.610 --> 01:11:26.820
因为它表明没有明显的问题，只需要问一个问题，对吗？

01:11:26.820 --> 01:11:27.990
因为如果你只是觉得，“哦，

01:11:27.990 --> 01:11:31.870
要问的一个问题很明显是敬业，或者很明显是人性，

01:11:31.870 --> 01:11:35.365
然后我们会得到完全不同的阅读，关于我们做得有多好，对吗？

01:11:35.365 --> 01:11:39.770
然而，问这些多个问题会给你更多的概述。

01:11:41.730 --> 01:11:46.780
我要跳过这个，因为时间不多。

01:11:46.780 --> 01:11:48.595
可以。这是最后一部分。

01:11:48.595 --> 01:11:51.670
呃，这是我对NLG研究的总结，

01:11:51.670 --> 01:11:54.340
当前的趋势以及我们未来的发展方向。

01:11:54.340 --> 01:11:59.020
所以，这里有三种令人兴奋的趋势要在NLG中识别。

01:11:59.020 --> 01:12:00.985
当然，你的里程可能会有所不同，

01:12:00.985 --> 01:12:02.860
你可能认为其他事情更有趣。

01:12:02.860 --> 01:12:05.110
所以，呃，我想的是

01:12:05.110 --> 01:12:08.635
首先将离散潜变量合并到NLG中。

01:12:08.635 --> 01:12:11.140
嗯，你应该去看看

01:12:11.140 --> 01:12:13.585
我跳过了幻灯片，因为有一些这样的例子。

01:12:13.585 --> 01:12:16.960
但其想法是，对于某些任务，例如

01:12:16.960 --> 01:12:19.360
讲故事或以任务为导向的对话

01:12:19.360 --> 01:12:21.100
你想做点什么。

01:12:21.100 --> 01:12:22.810
嗯，你可能想要一种

01:12:22.810 --> 01:12:25.540
你所说的事情的具体的、硬的概念

01:12:25.540 --> 01:12:30.160
就像你知道的，实体和人，事件和谈判等等。

01:12:30.160 --> 01:12:33.805
所以，呃，这里提到了什么样的模型

01:12:33.805 --> 01:12:38.860
这些连续的，呃，NLG方法中的离散潜在变量。

01:12:38.860 --> 01:12:42.520
第二个是严格的从左到右的一代的替代品。

01:12:42.520 --> 01:12:44.440
我真的很抱歉（笑声）我忽略了这么多事情。

01:12:44.440 --> 01:12:47.020
嗯，所以，最近有一些有趣的工作在尝试

01:12:47.020 --> 01:12:49.810
以从左到右以外的方式生成文本。

01:12:49.810 --> 01:12:51.160
例如，有一种

01:12:51.160 --> 01:12:55.735
并行生成的东西，或者写点东西，反复地改进它，呃，

01:12:55.735 --> 01:12:59.815
还有自上而下一代的想法，嗯，为了

01:12:59.815 --> 01:13:02.800
尤其是较长的文本，比如可能试图决定内容

01:13:02.800 --> 01:13:06.385
在写单词之前，把每个句子分开。

01:13:06.385 --> 01:13:08.620
第三个是

01:13:08.620 --> 01:13:11.530
教师强制的最大可能性训练的替代方法。

01:13:11.530 --> 01:13:14.320
因此，提醒你，教师强制培训的最大可能性是

01:13:14.320 --> 01:13:16.420
只是标准的训练方法

01:13:16.420 --> 01:13:19.210
到目前为止我们在课堂上一直在讲的一种语言模型。

01:13:19.210 --> 01:13:20.755
嗯，所以，你知道，

01:13:20.755 --> 01:13:23.200
有一些有趣的研究着眼于更全面的，

01:13:23.200 --> 01:13:25.735
嗯，句子层次而不是单词层次的目标。

01:13:25.735 --> 01:13:27.550
呃，所以，不幸的是，我和

01:13:27.550 --> 01:13:29.860
这张幻灯片，我没有时间把参考资料放进去，但我会的。

01:13:29.860 --> 01:13:31.990
把参考资料放在后面

01:13:31.990 --> 01:13:34.945
将出现在课程网站上，以便您稍后查看。

01:13:34.945 --> 01:13:39.820
可以。所以，作为一种概述，NLG研究，我们在哪里，我们要去哪里？

01:13:39.820 --> 01:13:41.950
嗯，我的比喻是

01:13:41.950 --> 01:13:46.210
大约五年前，全国语言计划和深度学习研究是一种蛮荒的西方。

01:13:46.210 --> 01:13:50.770
正确的？就像一切都是新的，嗯，我们不确定，

01:13:50.770 --> 01:13:52.300
NLP的研究不确定什么样的

01:13:52.300 --> 01:13:55.660
新的研究领域是因为，呃，你知道，

01:13:55.660 --> 01:13:58.645
嗯，神经方法改变了很多机器翻译，

01:13:58.645 --> 01:14:01.675
看起来他们可能会改变其他领域，但还不确定会改变多少。

01:14:01.675 --> 01:14:04.690
嗯，但现在你知道五年后，

01:14:04.690 --> 01:14:06.475
嗯，这没那么疯狂。

01:14:06.475 --> 01:14:09.125
我会说，你知道事情已经解决了很多

01:14:09.125 --> 01:14:13.140
标准实践已经出现了，而且肯定还有很多事情在改变。

01:14:13.140 --> 01:14:15.240
但是你知道社区里有更多的人，

01:14:15.240 --> 01:14:16.500
还有更多的标准做法，

01:14:16.500 --> 01:14:18.240
我们有TensorFlow和Pythorch。

01:14:18.240 --> 01:14:20.085
所以，你不再需要考虑梯度。

01:14:20.085 --> 01:14:22.665
所以，我想说，事情现在不那么疯狂了

01:14:22.665 --> 01:14:26.370
但我想说NLG似乎是最原始的部分之一

01:14:26.370 --> 01:14:29.880
剩下的部分原因是

01:14:29.880 --> 01:14:33.915
缺乏评估指标，因此很难判断我们在做什么。

01:14:33.915 --> 01:14:37.260
很难确定主要的方法是什么

01:14:37.260 --> 01:14:41.810
当我们没有任何可以清楚地告诉我们发生了什么事情的度量标准时就开始工作。

01:14:41.880 --> 01:14:44.710
所以，我很高兴看到的另一件事是

01:14:44.710 --> 01:14:47.830
神经NLG社区正在迅速扩大。

01:14:47.830 --> 01:14:51.040
嗯，所以，在早期，呃，

01:14:51.040 --> 01:14:55.390
人们主要是将成功的NMT方法转移到各种NLG任务中。

01:14:55.390 --> 01:14:58.870
嗯，但是现在我看到你知道，越来越有创造力的NLG技术

01:14:58.870 --> 01:15:02.725
特定于非NMT生成设置的合并。

01:15:02.725 --> 01:15:05.845
嗯，我再次敦促你回到我跳过的幻灯片中去。

01:15:05.845 --> 01:15:08.650
嗯，所以，我也在说越来越多的

01:15:08.650 --> 01:15:11.590
神经NLG研讨会和竞赛，尤其是

01:15:11.590 --> 01:15:14.470
专注于开放式NLG，就像我们

01:15:14.470 --> 01:15:18.055
know不太适合用于NMT的自动度量。

01:15:18.055 --> 01:15:22.720
所以，有一个神经生成车间，一个讲故事的车间，呃，

01:15:22.720 --> 01:15:26.470
各种各样的挑战，以及人们进入的地方，例如，

01:15:26.470 --> 01:15:28.870
会话对话代理，

01:15:28.870 --> 01:15:31.495
嗯，互相评价。

01:15:31.495 --> 01:15:33.520
所以，我认为这些不同，嗯，

01:15:33.520 --> 01:15:35.350
社区组织研讨会和

01:15:35.350 --> 01:15:38.710
比赛对组织一个社区真的很有帮助，

01:15:38.710 --> 01:15:44.080
提高再现性和标准评价、标准评价。

01:15:44.080 --> 01:15:46.300
嗯，这很好，不过我想说

01:15:46.300 --> 01:15:50.230
取得进展的最大障碍肯定还是评估。

01:15:50.230 --> 01:15:53.305
可以。所以，我最不想和你分享的

01:15:53.305 --> 01:15:56.260
是我在NLG工作中学到的八件事。

01:15:56.260 --> 01:15:58.930
所以，第一个任务是更加开放的任务，

01:15:58.930 --> 01:16:00.535
一切都变得越困难。

01:16:00.535 --> 01:16:03.655
评估变得更加困难，定义你正在做的事情变得更加困难，

01:16:03.655 --> 01:16:05.905
当你做得好的时候告诉别人会变得更困难。

01:16:05.905 --> 01:16:09.130
因此，出于这个原因，约束有时会使事情变得更受欢迎。

01:16:09.130 --> 01:16:15.370
因此，如果您决定限制您的任务，那么有时更容易完成它。

01:16:15.370 --> 01:16:19.120
嗯，下一个目标是具体的改进可以

01:16:19.120 --> 01:16:22.675
通常比旨在提高整体发电质量更易于管理。

01:16:22.675 --> 01:16:25.285
例如，如果你决定

01:16:25.285 --> 01:16:27.865
例如，增加你的模型的多样性，比如说

01:16:27.865 --> 01:16:31.270
更有趣的事情，这是一个更容易实现和衡量的事情，而不仅仅是

01:16:31.270 --> 01:16:35.875
说我们要做全面的发电质量，因为评估问题。

01:16:35.875 --> 01:16:40.285
下一个问题是如果你用你的语言模型来做NLG，

01:16:40.285 --> 01:16:44.860
那么，改进语言模型，使其在困惑中变得更好，这将给你带来

01:16:44.860 --> 01:16:46.960
可能更好的一代质量，因为你

01:16:46.960 --> 01:16:51.220
一种更强大的语言模式，但它不是提高生成质量的唯一途径，

01:16:51.220 --> 01:16:53.065
正如我们之前所说的，呃，

01:16:53.065 --> 01:16:56.995
除了语言模型之外，还有其他一些组件可以影响生成，

01:16:56.995 --> 01:17:00.340
这是问题的一部分，这不在培训目标中。

01:17:00.340 --> 01:17:03.655
嗯，我的下一个建议是你应该多看看你的产出，

01:17:03.655 --> 01:17:07.150
部分原因是你没有任何单一的度量标准可以告诉你发生了什么。

01:17:07.150 --> 01:17:10.270
很重要的一点是要经常观察你的产出来形成你自己的观点。

01:17:10.270 --> 01:17:12.745
这可能很费时，但可能值得一试。

01:17:12.745 --> 01:17:14.575
最后我和这些聊天机器人聊了起来

01:17:14.575 --> 01:17:17.440
在我为这个项目工作的那段时间里，我花了很多钱。

01:17:17.440 --> 01:17:21.640
可以。几乎完成了，所以，5个你需要一个自动度量，即使它是不完美的。

01:17:21.640 --> 01:17:23.050
所以，我知道你已经知道了，因为我们

01:17:23.050 --> 01:17:25.135
把它写在项目说明书上。

01:17:25.135 --> 01:17:29.200
呃，但我可能会把它改成你需要几个自动度量。

01:17:29.200 --> 01:17:30.760
我之前说过你会怎么追踪

01:17:30.760 --> 01:17:33.445
多个方面来全面了解正在发生的事情，

01:17:33.445 --> 01:17:36.100
我想说，你的全国人民大会任务越是开放，

01:17:36.100 --> 01:17:39.175
你越可能需要几个指标。

01:17:39.175 --> 01:17:43.195
如果你做了人的评估，你想让问题尽可能集中。

01:17:43.195 --> 01:17:45.100
所以，我发现如果你

01:17:45.100 --> 01:17:47.785
把这个问题定义为一个非常模糊的问题，

01:17:47.785 --> 01:17:49.885
然后你就开始敞开心扉，嗯，

01:17:49.885 --> 01:17:52.975
被访者有点误解你，呃，

01:17:52.975 --> 01:17:54.670
如果他们这样做，那实际上不是他们的错，

01:17:54.670 --> 01:17:57.475
这是你的错，你需要回答你的问题，这就是我学到的。

01:17:57.475 --> 01:17:59.860
下一件事是再现性

01:17:59.860 --> 01:18:03.580
在今天的全国人民党和深入学习的大问题，

01:18:03.580 --> 01:18:06.130
问题只在NLG中更大，

01:18:06.130 --> 01:18:08.380
我想这是另一种方式，它仍然是一个蛮荒的西部。

01:18:08.380 --> 01:18:10.930
所以，我想说，呃，那真的很棒，

01:18:10.930 --> 01:18:13.300
如果每个人都能公开释放

01:18:13.300 --> 01:18:15.985
他们写NLG论文时产生的输出。

01:18:15.985 --> 01:18:20.155
我认为这是一个很好的实践，因为如果您发布生成的输出，

01:18:20.155 --> 01:18:23.875
如果后来有人提出了一个伟大的自动度量标准，

01:18:23.875 --> 01:18:27.985
然后他们就可以获取生成的输出，然后以此为基础计算度量。

01:18:27.985 --> 01:18:30.040
但是如果他没有释放你的输出或者你

01:18:30.040 --> 01:18:32.470
以某种不完美的度量数发布，

01:18:32.470 --> 01:18:35.020
那么，未来的研究人员就没有什么可比的了。

01:18:35.020 --> 01:18:38.575
呃，最后，我的最后一个想法

01:18:38.575 --> 01:18:42.790
在NLG工作有时会很沮丧，

01:18:42.790 --> 01:18:45.745
嗯，因为事情可能很困难，很难知道你什么时候取得进展。

01:18:45.745 --> 01:18:48.805
但好处是它也很有趣。

01:18:48.805 --> 01:18:52.740
这是我最后一张幻灯片，这里是我和聊天机器人的一些奇怪的对话。

01:18:52.740 --> 01:18:54.000
[笑声]谢谢。

01:18:54.000 --> 01:19:37.000
[NOISE] [LAUGHTER] All right, thanks.

