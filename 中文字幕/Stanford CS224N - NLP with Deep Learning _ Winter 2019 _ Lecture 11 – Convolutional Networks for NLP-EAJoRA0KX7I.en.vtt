WEBVTT
Kind: captions
Language: en

00:00:04.280 --> 00:00:07.620
今天的计划就是我要说的

00:00:07.620 --> 00:00:10.710
是卷积神经网络的主题。

00:00:10.710 --> 00:00:13.920
所以本质上，嗯，实际上有很多

00:00:13.920 --> 00:00:17.700
这节课的内容包括了很多值得了解的事情，

00:00:17.700 --> 00:00:20.760
因为从本质上来说，这将是学习

00:00:20.760 --> 00:00:24.840
卷积神经网络在一个大咬为NLP。

00:00:24.840 --> 00:00:27.465
所以，嗯，说说公告，

00:00:27.465 --> 00:00:30.945
解释卷积神经网络的一般概念，

00:00:30.945 --> 00:00:33.270
然后就一点而言，

00:00:33.270 --> 00:00:38.490
我想详细介绍一下

00:00:38.490 --> 00:00:40.950
卷积神经网络用于

00:00:40.950 --> 00:00:44.235
文本分类，句子分类任务。

00:00:44.235 --> 00:00:47.040
嗯，首先是一种相当简单的，

00:00:47.040 --> 00:00:50.370
嗯，CNN是在2014年完成的，

00:00:50.370 --> 00:00:52.365
第二个是

00:00:52.365 --> 00:00:58.435
更复杂的CNN，在2017年做得更多。

00:00:58.435 --> 00:01:01.275
可以。但首先，有几个公告。

00:01:01.275 --> 00:01:06.360
嗯，首先是季度中反馈调查的最后提醒。

00:01:06.360 --> 00:01:08.700
你们中的很多人已经做过了。

00:01:08.700 --> 00:01:10.470
谢谢，非常感谢。

00:01:10.470 --> 00:01:14.340
嗯，但如果你还是把它推迟到最后一分钟，嗯，

00:01:14.340 --> 00:01:17.330
今晚午夜是你最后的机会，嗯，

00:01:17.330 --> 00:01:20.045
填写季度中调查以获得

00:01:20.045 --> 00:01:23.275
嗯，给我们反馈，得到你的半分。

00:01:23.275 --> 00:01:26.990
嗯，好吧。还有一件你应该考虑的事，

00:01:26.990 --> 00:01:29.510
我知道你们很多人都在考虑

00:01:29.510 --> 00:01:32.495
因为我昨天花了三个小时和人交谈，

00:01:32.495 --> 00:01:35.195
嗯，是关于最终项目的。

00:01:35.195 --> 00:01:39.060
嗯，所以要确保你有一些计划，嗯，

00:01:39.060 --> 00:01:40.725
准备好了，嗯，

00:01:40.725 --> 00:01:44.640
星期四下午4:00，嗯，下午4:30。

00:01:44.640 --> 00:01:47.535
我是说，特别是我们讨论过的，嗯，

00:01:47.535 --> 00:01:52.745
你今年要做的事情之一就是找到一些研究论文，

00:01:52.745 --> 00:01:57.730
读过它，并对它如何通知你的工作有一个总结和想法。

00:01:57.730 --> 00:02:01.550
嗯，然后确定你的日历里有，嗯，

00:02:01.550 --> 00:02:05.735
CS224N的最终项目海报会议，

00:02:05.735 --> 00:02:09.320
3月20日星期三晚上，

00:02:09.320 --> 00:02:12.480
我们在校友中心举行。

00:02:12.640 --> 00:02:19.940
可以。嗯，还有一种公告或者只是一些普通的事情需要考虑。

00:02:19.940 --> 00:02:23.060
嗯，我们现在正式进入下半场了。

00:02:23.060 --> 00:02:24.545
祝贺你。

00:02:24.545 --> 00:02:26.630
嗯，而且，你知道，

00:02:26.630 --> 00:02:31.880
我们还想教你一些基本的东西，

00:02:31.880 --> 00:02:34.700
实际上卷积神经网络就是其中之一。

00:02:34.700 --> 00:02:39.950
但是，我的意思是，尽管在下半场，我的意思是，

00:02:39.950 --> 00:02:44.480
事情开始变了，我们希望更多，嗯，

00:02:44.480 --> 00:02:49.970
为你成为真正的深入学习NLP研究人员或实践者做好准备。

00:02:49.970 --> 00:02:52.395
那么具体来说这意味着什么呢？

00:02:52.395 --> 00:02:55.745
嗯，讲座开始少了

00:02:55.745 --> 00:02:59.660
给出了如何构建一个非常基本的东西的每一个细节，

00:02:59.660 --> 00:03:02.629
更多的给你一些想法

00:03:02.629 --> 00:03:05.880
对在不同领域所做的一些工作进行分类。

00:03:05.880 --> 00:03:08.510
所以在某种程度上

00:03:08.510 --> 00:03:11.375
与一个项目或类似的事情相关。

00:03:11.375 --> 00:03:14.360
希望你能主动

00:03:14.360 --> 00:03:17.915
了解更多正在谈论的事情。

00:03:17.915 --> 00:03:22.100
嗯，也非常欢迎人们对事物的任何疑问，

00:03:22.100 --> 00:03:24.440
嗯，想知道更多。

00:03:24.440 --> 00:03:26.420
还有你应该知道的另一件事

00:03:26.420 --> 00:03:30.440
深入学习就是一旦我们突破了基本原则，

00:03:30.440 --> 00:03:33.350
我们教的很多东西都不是

00:03:33.350 --> 00:03:38.120
真正的科学或人们确信的东西，

00:03:38.120 --> 00:03:41.870
你知道，我在下半场教的大部分课程都很漂亮

00:03:41.870 --> 00:03:46.175
很多人认为2019年的良好实践。

00:03:46.175 --> 00:03:49.370
但是，你知道，事实是人们的想法

00:03:49.370 --> 00:03:53.390
深入学习的良好实践正在迅速改变。

00:03:53.390 --> 00:03:58.330
所以，如果你回到两年前，或者肯定回到四年前，对吗？

00:03:58.330 --> 00:04:01.640
人们过去相信的事情很多，

00:04:01.640 --> 00:04:04.850
现在人们对什么最有效有了不同的想法。

00:04:04.850 --> 00:04:09.530
很明显，到了2021年或2023年，

00:04:09.530 --> 00:04:12.350
关于什么会有不同的想法，

00:04:12.350 --> 00:04:14.090
嗯，人们认为最好。

00:04:14.090 --> 00:04:17.750
所以你必须接受这是，嗯，

00:04:17.750 --> 00:04:20.630
迅速崛起的新兴领域

00:04:20.630 --> 00:04:24.125
很好地理解基本原理以及事物是如何结合在一起的。

00:04:24.125 --> 00:04:27.740
但在那之后，相当多的知识是

00:04:27.740 --> 00:04:31.280
思考在此刻是好的，它随着时间的推移不断进化。

00:04:31.280 --> 00:04:34.745
如果你想留在这一领域，或者做一些有深度学习的事情，

00:04:34.745 --> 00:04:37.505
你还是得跟上它的变化。

00:04:37.505 --> 00:04:39.710
这几天叫做终身学习。

00:04:39.710 --> 00:04:41.810
这是一个非常时髦的概念。

00:04:41.810 --> 00:04:45.200
嗯，还有讲座，

00:04:45.200 --> 00:04:49.735
对于作业来说也是如此。

00:04:49.735 --> 00:04:51.720
嗯，而且，你知道，

00:04:51.720 --> 00:04:57.050
我们一直在努力做作业，所以他们开始很有介绍性，

00:04:57.050 --> 00:05:01.340
逐渐开始少用脚手架，

00:05:01.340 --> 00:05:03.395
我们希望，嗯，

00:05:03.395 --> 00:05:10.530
继续说，嗯，在作业5中用较少的人手。

00:05:10.530 --> 00:05:13.910
而且，你知道，我想我们希望做的是为你做好准备

00:05:13.910 --> 00:05:17.495
无论是最后的项目还是现实生活。

00:05:17.495 --> 00:05:21.005
我想我今天早上做了个类比，

00:05:21.005 --> 00:05:25.370
嗯，把这个和C序列比较一下，

00:05:25.370 --> 00:05:29.135
所以当CS106A和B有很多脚手架的时候，

00:05:29.135 --> 00:05:31.025
然后在CS107中，

00:05:31.025 --> 00:05:34.850
你应该学会如何诊断和解决问题。

00:05:34.850 --> 00:05:38.910
对于你自己来说，在一个类似的调试器中，

00:05:38.910 --> 00:05:41.010
嗯，对于神经网络，你知道，

00:05:41.010 --> 00:05:43.770
对于早期的作业，嗯，你知道，

00:05:43.770 --> 00:05:46.610
我们在这里给了你很多帮助，所有的

00:05:46.610 --> 00:05:49.490
这些测试是为了确保每一点都是好的，

00:05:49.490 --> 00:05:51.815
这就是如何构造事物。

00:05:51.815 --> 00:05:54.305
但是，你知道，在现实世界中，

00:05:54.305 --> 00:05:57.695
嗯，你只能建立和使用神经网络。

00:05:57.695 --> 00:06:00.259
如果你知道他们为什么不工作

00:06:00.259 --> 00:06:02.990
你必须改变什么才能让它们发挥作用。

00:06:02.990 --> 00:06:06.785
而且，你知道，事实上，正如我上周所说的，你知道，

00:06:06.785 --> 00:06:11.210
这往往是超过一半的工作，似乎很容易坚持下来。

00:06:11.210 --> 00:06:14.270
这是我的神经网络和对我有意义的部分，

00:06:14.270 --> 00:06:17.660
然后你可以用剩下的80%的时间

00:06:17.660 --> 00:06:21.230
抓挠你的头想知道为什么它实际上不起作用，

00:06:21.230 --> 00:06:24.335
以及你如何改变它，使其工作良好。

00:06:24.335 --> 00:06:29.810
嗯，我承认调试神经网络通常很困难，但是，你知道，

00:06:29.810 --> 00:06:34.190
我们的目标是你应该学会做一些事情，

00:06:34.190 --> 00:06:38.600
这是课程的学习目标之一。

00:06:38.600 --> 00:06:41.045
嗯，最后的小广告。

00:06:41.045 --> 00:06:43.370
如果你想读一本书，

00:06:43.370 --> 00:06:45.155
嗯，这周刚出去，

00:06:45.155 --> 00:06:48.305
有一本关于自然语言处理的新书

00:06:48.305 --> 00:06:51.575
作者：Delip Rao和Brian McMahan。

00:06:51.575 --> 00:06:53.990
他实际上住在三藩。

00:06:53.990 --> 00:06:56.660
嗯，所以，嗯，如果你想，

00:06:56.660 --> 00:06:58.310
当然，你可以买一本。

00:06:58.310 --> 00:07:00.230
但如果你不想，嗯，

00:07:00.230 --> 00:07:03.230
买了它，你就想看看它，嗯，

00:07:03.230 --> 00:07:09.110
斯坦福图书馆实际上拥有O'Reilly藏书的许可证。

00:07:09.110 --> 00:07:14.945
所以你可以从library.stanford.edu开始免费阅读。

00:07:14.945 --> 00:07:18.230
只有一个地方是图书馆

00:07:18.230 --> 00:07:21.710
16个同时获得Safari图书许可证。

00:07:21.710 --> 00:07:25.450
所以如果你也希望你的同学能免费阅读，

00:07:25.450 --> 00:07:29.944
如果你记得从网上的游猎书籍中注销，这真的很有帮助，

00:07:29.944 --> 00:07:32.270
嗯，等你看完了。

00:07:32.270 --> 00:07:34.785
嗯，是的，所以这是一种，

00:07:34.785 --> 00:07:36.420
我是说，在某种意义上，

00:07:36.420 --> 00:07:39.025
我希望你看这本书会感到，

00:07:39.025 --> 00:07:41.610
“伙计，我已经知道大部分的事情了。

00:07:41.610 --> 00:07:43.740
这不是一本非常高级的书。

00:07:43.740 --> 00:07:49.780
但是这是一个很好的关于如何使用pytorch和nlp的书面教程。”

00:07:49.780 --> 00:07:52.620
如果你不觉得你了解这本书的大部分内容，

00:07:52.620 --> 00:07:56.250
你可以让我知道，但我会有点难过。

00:07:56.250 --> 00:08:01.030
嗯，好的，嗯，好的。

00:08:01.030 --> 00:08:03.760
从今天开始。

00:08:03.760 --> 00:08:06.430
嗯，所以，我们花了很多时间在

00:08:06.430 --> 00:08:10.630
经常性的神经网络，它们对很多事情都很有用。

00:08:10.630 --> 00:08:15.670
嗯，但有些事情他们不擅长。

00:08:15.670 --> 00:08:21.280
所以，你知道，我们可能想知道一个短语，比如我的出生，

00:08:21.280 --> 00:08:23.800
或者像我的出生这样更大的词组，

00:08:23.800 --> 00:08:27.550
有点不独立，嗯，

00:08:27.550 --> 00:08:31.480
在循环神经网络中这些跨度的表示。

00:08:31.480 --> 00:08:35.365
我们可以得到整句话的前缀。

00:08:35.365 --> 00:08:38.815
当我们这样做的时候，嗯，双向的，嗯，

00:08:38.815 --> 00:08:42.100
经常性的神经网络，你可以说，‘好吧，

00:08:42.100 --> 00:08:45.670
等一下，你可以在两个方向上使用它，在某种程度上这是真的。

00:08:45.670 --> 00:08:49.120
我们可以从这个方向得到东西，从这个方向得到东西，

00:08:49.120 --> 00:08:51.265
但我们还是有点

00:08:51.265 --> 00:08:54.730
到句子一端或另一端的整个序列。

00:08:54.730 --> 00:08:57.790
我们不是只有几句话。

00:08:57.790 --> 00:09:03.595
通常，我们想找出句子的意思，

00:09:03.595 --> 00:09:06.280
所以，我们这里有两个问题。

00:09:06.280 --> 00:09:09.835
我们只有一些初始和最终的子序列。

00:09:09.835 --> 00:09:14.230
而且，如果你看这些表示，就像你说，

00:09:14.230 --> 00:09:18.820
把这最后一种状态作为本文意义的表示。

00:09:18.820 --> 00:09:20.080
你发现的，

00:09:20.080 --> 00:09:22.360
它是不是被

00:09:22.360 --> 00:09:27.640
最新的词汇以及他们试图预测的后面会发生什么，

00:09:27.640 --> 00:09:30.085
这就是我提到的部分原因

00:09:30.085 --> 00:09:33.280
上次问答，嗯，讲座，

00:09:33.280 --> 00:09:37.060
有了哨兵和训练，你就能做得更好。

00:09:37.060 --> 00:09:41.755
对整个，嗯，lstm结构有关注的东西。

00:09:41.755 --> 00:09:44.560
可以。但今天我们要看看

00:09:44.560 --> 00:09:48.565
另一种选择是卷积神经网络，

00:09:48.565 --> 00:09:53.485
通常简称为CNN或ConvNets。

00:09:53.485 --> 00:09:57.385
嗯，这些想法是，嗯，

00:09:57.385 --> 00:09:59.920
看，也许我们可以

00:09:59.920 --> 00:10:06.910
一定长度的每个子序列并计算它的表示，嗯，

00:10:06.910 --> 00:10:10.090
所以，你知道，如果我们有这样的文字，

00:10:10.090 --> 00:10:12.684
为保持政府开放而达成的初步协议，

00:10:12.684 --> 00:10:14.320
我们可以说，嗯，

00:10:14.320 --> 00:10:17.110
让我们把这三个单词的顺序都记下来，

00:10:17.110 --> 00:10:19.765
达成初步协议，

00:10:19.765 --> 00:10:21.385
一直保持下去，

00:10:21.385 --> 00:10:26.470
我们将计算这些序列的某种表示。

00:10:26.470 --> 00:10:30.250
所以，这不是一个很强的语言概念。

00:10:30.250 --> 00:10:33.430
正确的？我们不担心它是否是一个连贯的短语，

00:10:33.430 --> 00:10:36.310
这在语法和语言上是有效的，

00:10:36.310 --> 00:10:41.125
从认知上讲是合理的，我们只取某个长度的每个子序列。

00:10:41.125 --> 00:10:45.370
然后，一旦我们计算出它们的表示，

00:10:45.370 --> 00:10:48.025
我们来看看如何分组。

00:10:48.025 --> 00:10:55.900
可以。那么，让我们更详细地了解一下CNN是什么以及他们是如何工作的。

00:10:55.900 --> 00:11:01.900
嗯，是的，所以，有一个卷积的一般概念，你可以，也可以

00:11:01.900 --> 00:11:07.855
没有在数学或电气工程课上见过。

00:11:07.855 --> 00:11:12.010
还有卷积的特殊版本，

00:11:12.010 --> 00:11:15.310
离散卷积，你可以这么说

00:11:15.310 --> 00:11:18.910
您可以使用友好的求和符号，而不是整数。

00:11:18.910 --> 00:11:20.905
嗯，那是一个，

00:11:20.905 --> 00:11:22.480
这是一个离散卷积。

00:11:22.480 --> 00:11:25.495
我发现那个符号完全没用。

00:11:25.495 --> 00:11:27.040
所以，我甚至都不想解释。

00:11:27.040 --> 00:11:28.690
但我有很多例子，

00:11:28.690 --> 00:11:34.075
对于神经网络来说，卷积就其作为例子的作用而言是非常容易的。

00:11:34.075 --> 00:11:38.590
好吧，卷积神经网络的经典例子，

00:11:38.590 --> 00:11:40.270
在视觉应用程序中。

00:11:40.270 --> 00:11:44.605
所以，如果你下个季度做CS231N，

00:11:44.605 --> 00:11:47.770
基本上，你知道，前四个星期都在做

00:11:47.770 --> 00:11:51.715
卷积神经网络的所有变异和荣耀。

00:11:51.715 --> 00:11:55.540
嗯，还有一种基本的想法，嗯，

00:11:55.540 --> 00:11:57.894
错综复杂的视觉，

00:11:57.894 --> 00:12:02.410
就是你想识别事物，不管它们在图像中出现在哪里。

00:12:02.410 --> 00:12:05.620
所以，你有一种翻译和变异的特性，

00:12:05.620 --> 00:12:08.230
把卷积作为一种方法

00:12:08.230 --> 00:12:10.810
在图像中的不同地方发现一些东西，

00:12:10.810 --> 00:12:12.670
不管它出现在哪里。

00:12:12.670 --> 00:12:19.360
嗯，这就是我从Andrew Ng的UFLDL网站偷来的视觉例子。

00:12:19.360 --> 00:12:21.925
所以，什么是卷积？

00:12:21.925 --> 00:12:24.130
这里有补丁吗？

00:12:24.130 --> 00:12:26.815
但你可以把它看作一个向量，

00:12:26.815 --> 00:12:31.450
补丁的重量是这些红色的小数字，

00:12:31.450 --> 00:12:32.995
你要做的，

00:12:32.995 --> 00:12:40.345
是像这个动画那样在图像上滑动那个补丁。

00:12:40.345 --> 00:12:43.075
嗯，所以在每个位置，

00:12:43.075 --> 00:12:47.905
你将把每个红色数字乘以那个位置的黑色数字，

00:12:47.905 --> 00:12:50.080
然后你就把它们总结出来。

00:12:50.080 --> 00:12:53.245
所以，这就是离散卷积的作用，

00:12:53.245 --> 00:12:55.180
上面的符号就是这么说的，

00:12:55.180 --> 00:12:58.495
正确的？你把事情相乘，然后把它们总结起来，

00:12:58.495 --> 00:13:00.235
所以你要这么做，

00:13:00.235 --> 00:13:04.240
然后你用这些产品填充粉色，

00:13:04.240 --> 00:13:05.710
嗯，求和积。

00:13:05.710 --> 00:13:07.855
所以，这有点像，你拿着这些

00:13:07.855 --> 00:13:12.400
修补点产品并将其放入粉色矩阵中，

00:13:12.400 --> 00:13:14.815
这就是你的卷积特征。

00:13:14.815 --> 00:13:17.350
这是一个二维卷积，

00:13:17.350 --> 00:13:18.760
今天剩下的时间里，

00:13:18.760 --> 00:13:20.470
我们不会再看了。

00:13:20.470 --> 00:13:23.215
所以，这就是你所学的关于视觉的全部知识。

00:13:23.215 --> 00:13:28.390
嗯，现在我们要回去看看1d卷积，

00:13:28.390 --> 00:13:32.995
这就是人们使用卷积神经网络进行文本处理时所使用的方法。

00:13:32.995 --> 00:13:36.610
所以，文本卷积神经网络的起点，

00:13:36.610 --> 00:13:38.410
我们有意见吗？

00:13:38.410 --> 00:13:42.190
这是我的句子和每个词

00:13:42.190 --> 00:13:45.970
在这句话中，我有一个密集的词向量，

00:13:45.970 --> 00:13:51.325
我做了一个4D，想在我的例子中保持它的小，但通常如你所知，它更多。

00:13:51.325 --> 00:13:54.580
所以，我们的出发点是我们有一些投入，你知道，

00:13:54.580 --> 00:13:58.060
输入可能只是一个不被禁止的热编码，

00:13:58.060 --> 00:14:01.795
但是通常我们会有这些密集的词向量。

00:14:01.795 --> 00:14:06.310
所以，它和3D和2D差不多，

00:14:06.310 --> 00:14:08.185
除了我们只有一个维度。

00:14:08.185 --> 00:14:10.510
所以，我们有一个过滤器。

00:14:10.510 --> 00:14:14.410
嗯，这是我们的过滤器，

00:14:14.410 --> 00:14:21.680
所以我们的过滤器要做三个步骤和时间，三个字。

00:14:21.750 --> 00:14:25.930
这将在各个维度上起作用。

00:14:25.930 --> 00:14:28.240
所以，这些不同的维度

00:14:28.240 --> 00:14:32.500
卷积神经网络通常被称为信道。

00:14:32.500 --> 00:14:35.665
所以，我们在输入通道中工作，

00:14:35.665 --> 00:14:37.990
所以我们有一个这样的补丁。

00:14:37.990 --> 00:14:45.430
我们要把这个补丁放在前三个词的顶部。

00:14:45.430 --> 00:14:47.980
我的动画不如前一张幻灯片好。

00:14:47.980 --> 00:14:51.610
对不起的。我们要计算出点积，

00:14:51.610 --> 00:14:56.410
嗯，在这些之间，我在家把这个放进Excel就做到了。

00:14:56.410 --> 00:14:58.015
答案（笑声）是，

00:14:58.015 --> 00:15:01.255
就是这个积是-1.0。

00:15:01.255 --> 00:15:05.495
然后在那一点上，我们滑动，

00:15:05.495 --> 00:15:08.345
我们滑动这个，嗯，

00:15:08.345 --> 00:15:11.410
被称为内核或

00:15:11.410 --> 00:15:16.305
一个滤波器，它是我们用于卷积神经网络的补丁。

00:15:16.305 --> 00:15:21.520
我们把它滑下一个，再做这些术语的点积。

00:15:21.520 --> 00:15:28.955
结果是负半，我们继续往下滑，得到了什么，

00:15:28.955 --> 00:15:33.095
嗯，得到右边显示的输出。

00:15:33.095 --> 00:15:34.265
所以在这一点上，

00:15:34.265 --> 00:15:36.690
我们刚刚减刑，

00:15:36.690 --> 00:15:39.105
对一个向量。

00:15:39.105 --> 00:15:44.740
嗯，那似乎我们想做的不止这些。

00:15:44.740 --> 00:15:48.455
但是你会注意到的另一件事是

00:15:48.455 --> 00:15:52.500
我们的判决有点缩水，因为以前，你知道，

00:15:52.500 --> 00:15:57.710
我们有一个七个字的句子，但因为我只是滑倒了这三个字，

00:15:57.710 --> 00:15:59.615
嗯，内核在这里，

00:15:59.615 --> 00:16:03.015
最后我只有五个位置可以放进去。

00:16:03.015 --> 00:16:05.825
所以它变成了一个五个字的东西。

00:16:05.825 --> 00:16:08.960
嗯，首先要解决这个问题，

00:16:08.960 --> 00:16:14.030
通常，当人们做卷积神经网络时，他们会添加填充物。

00:16:14.030 --> 00:16:18.790
嗯，我能做的就是在

00:16:18.790 --> 00:16:25.805
两端然后做同样的技巧，说运行卷积。

00:16:25.805 --> 00:16:31.355
现在，我可以把我的3号过滤器放在7个不同的地方

00:16:31.355 --> 00:16:37.835
把它往下滑，我得到一个和输入长度相同的向量。

00:16:37.835 --> 00:16:40.650
嗯，你知道，有不同的方法，

00:16:40.650 --> 00:16:43.200
所以这是最常见的做事方式。

00:16:43.200 --> 00:16:46.765
它看起来很合理，因为它能保持大小。

00:16:46.765 --> 00:16:50.460
我是说，你知道，总有不止一种方法可以做到这一点。

00:16:50.460 --> 00:16:52.310
嗯，如果你真的想，

00:16:52.310 --> 00:16:54.390
你，噢，我不想要你，是的，

00:16:54.390 --> 00:16:59.560
噢，我做了，呃，

00:16:59.560 --> 00:17:05.855
我在幻灯片上犯了一个小错误，因为这个

00:17:05.855 --> 00:17:08.405
结果我马上就要到了

00:17:08.405 --> 00:17:12.790
但我还是在这里解释一下[笑声]。

00:17:12.790 --> 00:17:15.450
嗯，你知道，如果你想，

00:17:15.450 --> 00:17:19.740
你可以在两端加两步衬垫。

00:17:19.740 --> 00:17:24.290
所以你的第一个卷积，我们会看到零，零，

00:17:24.290 --> 00:17:30.585
10到的，然后卷积将实际增加输入的大小。

00:17:30.585 --> 00:17:35.915
是啊。但是，是的。所以我是说，

00:17:35.915 --> 00:17:38.565
到目前为止我们所做的，

00:17:38.565 --> 00:17:41.380
我们已经从这些词向量开始，

00:17:41.380 --> 00:17:46.335
卷积神经网络的术语长度为4。

00:17:46.335 --> 00:17:49.475
所以我们的输入有四个通道。

00:17:49.475 --> 00:17:53.030
但当我们回到这里的时候，嗯，

00:17:53.030 --> 00:17:56.515
我们只是根据这个制作，嗯，

00:17:56.515 --> 00:17:59.690
内核，一列输出。

00:17:59.690 --> 00:18:02.560
所以我们的输出只有一个通道。

00:18:02.560 --> 00:18:08.690
所以我们有点缩小了，从四列到一列。

00:18:08.690 --> 00:18:11.490
这可能看起来很糟糕。

00:18:11.490 --> 00:18:14.105
在许多方面，这是不好的。

00:18:14.105 --> 00:18:16.714
嗯，所以，很多时候，

00:18:16.714 --> 00:18:21.164
你想做的就是说，

00:18:21.164 --> 00:18:25.325
不是只有一个过滤器，

00:18:25.325 --> 00:18:29.260
为什么我没有几个过滤器呢？

00:18:29.260 --> 00:18:32.680
所以这里我有三个不同的过滤器

00:18:32.680 --> 00:18:36.620
这些过滤器是同样大小的三个，

00:18:36.620 --> 00:18:41.825
大小，内核大小乘以输入，

00:18:41.825 --> 00:18:46.145
矩阵的通道数。

00:18:46.145 --> 00:18:49.550
所以我有三个不同的过滤器，我要运行

00:18:49.550 --> 00:18:53.380
每一个沿着文本向下，在这里得到一列。

00:18:53.380 --> 00:18:56.510
现在，我得到了三列输出。

00:18:56.510 --> 00:18:59.675
所以我有一个三通道的输出。

00:18:59.675 --> 00:19:04.940
直观地认为这是针对这些过滤器的，

00:19:04.940 --> 00:19:07.505
好吧，你知道，对于我们在神经网络中所做的，

00:19:07.505 --> 00:19:11.040
我们将像其他一切一样，通过反向传播来学习它们。

00:19:11.040 --> 00:19:16.760
但我们希望这些过滤器能够以某种方式专门处理不同的事情。

00:19:16.760 --> 00:19:20.480
所以也许这个过滤器可以专门用于，

00:19:20.480 --> 00:19:22.355
这种语言有礼貌吗？

00:19:22.355 --> 00:19:26.725
每当看到礼貌的话，它就会产生很高的价值。

00:19:26.725 --> 00:19:29.850
也许，嗯，这个，嗯，

00:19:29.850 --> 00:19:35.605
我不知道，过滤器可以专门研究，

00:19:35.605 --> 00:19:38.795
吃东西，一看到字就有很高的价值

00:19:38.795 --> 00:19:42.430
关于食物，你知道这个过滤器会做第三件事。

00:19:42.430 --> 00:19:49.235
这就是人们有时谈论的意义，嗯，嗯，嗯，

00:19:49.235 --> 00:19:53.075
你得到的是不同功能的输出，因为你希望

00:19:53.075 --> 00:19:57.515
你会从文本中获得不同的潜在特征。

00:19:57.515 --> 00:20:02.555
可以。所以这给了我们一个代表性，这就是

00:20:02.555 --> 00:20:07.540
在我们的文本中找到学习功能的一种有用的方式。

00:20:07.540 --> 00:20:11.290
不过，我们经常想做的就是

00:20:11.290 --> 00:20:15.610
根据这些特点总结文本。

00:20:15.610 --> 00:20:18.030
所以你可能有个问题，嗯，

00:20:18.030 --> 00:20:20.045
在这段文字中，嗯，

00:20:20.045 --> 00:20:23.435
它有礼貌吗？它谈论食物吗？

00:20:23.435 --> 00:20:26.555
所以我们经常要做的另一个手术

00:20:26.555 --> 00:20:30.410
要做的是总结卷积网络的输出。

00:20:30.410 --> 00:20:32.750
最简单的方法是，

00:20:32.750 --> 00:20:35.110
用于1d卷积，

00:20:35.110 --> 00:20:37.635
称为随时间推移的最大池。

00:20:37.635 --> 00:20:40.080
所以，如果我们在一段时间内最大化游泳池，

00:20:40.080 --> 00:20:43.935
每个频道或其他称为功能，

00:20:43.935 --> 00:20:53.865
我们只是往下看，看看它的最大值是多少，0.3，1.6，1.4。

00:20:53.865 --> 00:20:55.775
嗯，所以，你知道，

00:20:55.775 --> 00:20:58.730
如果我用前两个的故事，嗯，

00:20:58.730 --> 00:21:00.700
过滤器，有点像是说，

00:21:00.700 --> 00:21:04.600
这不是很礼貌的文字，但实际上是关于食物的，对吧？

00:21:04.600 --> 00:21:06.300
我们在总结，

00:21:06.300 --> 00:21:08.460
嗯，我们在那里发现的。

00:21:08.460 --> 00:21:14.400
从某种意义上讲，最大池的概念

00:21:14.400 --> 00:21:18.640
是的，这东西是不是在任何地方被激活了？

00:21:18.640 --> 00:21:22.180
所以如果我们有礼貌和关于食物的事情，

00:21:22.180 --> 00:21:25.510
最大池的输出值将很高。

00:21:25.510 --> 00:21:28.600
如果句子中有一个明确的标记

00:21:28.600 --> 00:21:32.035
有礼貌或明显关于食物的东西。

00:21:32.035 --> 00:21:37.210
这通常是一个有用的概念，因为通常你想知道的是，

00:21:37.210 --> 00:21:42.260
你知道吗，这句话中有没有关于食物的讨论？

00:21:42.260 --> 00:21:46.150
还有一件事，你可以做其他的事。

00:21:46.150 --> 00:21:48.635
而不是，啊，Max Pooling，

00:21:48.635 --> 00:21:51.210
您可以改为执行平均池。

00:21:51.210 --> 00:21:55.405
在这里，你只需要计算这些数字的平均值。

00:21:55.405 --> 00:21:58.910
它有着不同的语义

00:21:58.910 --> 00:22:02.595
这件事的平均礼貌是多少，嗯，

00:22:02.595 --> 00:22:05.855
短信或平均多少，你知道，如何，

00:22:05.855 --> 00:22:10.265
这句话的百分之几是关于食物之类的。

00:22:10.265 --> 00:22:12.190
嗯，出于某种目的，

00:22:12.190 --> 00:22:13.680
这更好，因为，你知道，

00:22:13.680 --> 00:22:16.960
它将所有重要的构建平均化。

00:22:16.960 --> 00:22:18.900
我是说，很多时候，

00:22:18.900 --> 00:22:22.890
人们发现，实际上max pooling更好，因为，

00:22:22.890 --> 00:22:27.490
你知道，很多自然语言的信号是稀疏的。

00:22:27.490 --> 00:22:30.630
你知道，不管你多么有礼貌，

00:22:30.630 --> 00:22:32.940
你不会每个字都有礼貌的。

00:22:32.940 --> 00:22:37.430
你要说这样的名词和文章，

00:22:37.430 --> 00:22:40.390
介词和连词，

00:22:40.390 --> 00:22:42.635
这些都不是天生的礼貌，对吧？

00:22:42.635 --> 00:22:46.325
嗯，所以如果有礼貌的表现出来，

00:22:46.325 --> 00:22:51.470
然后这个句子变得有礼貌，而max pooling实际上更适合捕获它。

00:22:51.470 --> 00:22:54.430
嗯，当然还有一种你可以做的事

00:22:54.430 --> 00:22:58.115
敏池找到了最不活跃的事。

00:22:58.115 --> 00:23:01.135
嗯，它没什么用处，但你也可以用。

00:23:01.135 --> 00:23:04.380
可以。所以，嗯，如果你在Pythorn，

00:23:04.380 --> 00:23:07.365
这都是很容易做的事情。

00:23:07.365 --> 00:23:10.005
所以这里有一个方便的花花公子聚会。

00:23:10.005 --> 00:23:13.025
还有一个conv2d，你可能会猜到它的视觉效果。

00:23:13.025 --> 00:23:15.005
但是有一个conv1d，嗯，

00:23:15.005 --> 00:23:18.790
指定有多少个输入通道。

00:23:18.790 --> 00:23:20.725
那是我们的文字嵌入尺寸。

00:23:20.725 --> 00:23:22.735
有多少个输出通道？

00:23:22.735 --> 00:23:24.365
我们有三个。

00:23:24.365 --> 00:23:27.820
卷积核的大小是多少？

00:23:27.820 --> 00:23:29.525
所以我们展示的那些

00:23:29.525 --> 00:23:32.380
三个参数，然后还有各种其他参数。

00:23:32.380 --> 00:23:35.990
就像你可以说你想要一个填充物之类的。

00:23:35.990 --> 00:23:38.075
一旦你有了一个，

00:23:38.075 --> 00:23:39.695
你可以跑步

00:23:39.695 --> 00:23:44.355
你的卷积滤波器在输入端得到一个新的隐藏状态。

00:23:44.355 --> 00:23:46.220
如果你想要最大限度的游泳池，

00:23:46.220 --> 00:23:47.570
你只要麦克斯，

00:23:47.570 --> 00:23:51.750
嗯，通过它的输出，你得到了一个最大的集合输出。

00:23:51.750 --> 00:23:58.869
可以。这给了我们建立一种卷积神经网络的基础，

00:23:58.869 --> 00:24:01.150
嗯，对于，嗯，NLP。

00:24:01.150 --> 00:24:04.280
在那之前这有什么意义吗？

00:24:06.000 --> 00:24:10.570
是啊。可以。所以下一步就是表演

00:24:10.570 --> 00:24:15.265
你还可以做其他三四件事。

00:24:15.265 --> 00:24:18.325
嗯，我刚开始打这些幻灯片

00:24:18.325 --> 00:24:20.920
其他不太有用的概念，因为我

00:24:20.920 --> 00:24:23.590
有点想，哦，至少他们在全国人民党中的地位不高。

00:24:23.590 --> 00:24:28.090
但是，你知道，事实上，当我读到第二篇论文的时候，

00:24:28.090 --> 00:24:32.740
当我说复杂的卷积神经网络，实际上，

00:24:32.740 --> 00:24:37.750
在那篇论文中，他们尝试了我所说的没有人使用的所有这些东西。

00:24:37.750 --> 00:24:42.145
所以很高兴知道他们在看各种各样的论文。

00:24:42.145 --> 00:24:49.795
在这里，当我们做这些事情的时候，我们正在计算这些卷积，

00:24:49.795 --> 00:24:52.660
我们在各方面都在尝试。

00:24:52.660 --> 00:24:55.285
所以我们有一对零的暂定交易。

00:24:55.285 --> 00:24:58.420
然后就达成了初步协议，然后就达成了协议。

00:24:58.420 --> 00:25:00.970
所以我们只是走了一步

00:25:00.970 --> 00:25:04.765
被称为一步一步的时间。

00:25:04.765 --> 00:25:08.095
这是到目前为止最常见的事情。

00:25:08.095 --> 00:25:09.595
但你可以观察到，

00:25:09.595 --> 00:25:10.825
看，等一下，

00:25:10.825 --> 00:25:15.655
因为第一个卷积涉及零试探性的交易。

00:25:15.655 --> 00:25:18.085
我把这三个字都写进去了。

00:25:18.085 --> 00:25:25.225
即使我跳到下一个，交易联系，然后我做了保持政府，

00:25:25.225 --> 00:25:30.460
我还是会把句子中的每一个词都卷成一个或另一个。

00:25:30.460 --> 00:25:32.950
所以我可以做一半的计算

00:25:32.950 --> 00:25:35.635
在某种意义上，一切都还在那里。

00:25:35.635 --> 00:25:38.425
所以这被称为用两个跨步。

00:25:38.425 --> 00:25:42.130
然后我得到了一半排的东西。

00:25:42.130 --> 00:25:46.840
所以这是一种压缩表示和生成的方法

00:25:46.840 --> 00:25:52.855
从长句中删去一些较短的句子，稍后我们会看到它的用法。

00:25:52.855 --> 00:25:59.890
还有其他的方法来压缩从句子中得到的切割表示。

00:25:59.890 --> 00:26:05.710
所以有一个不同的概念，就是本地池。

00:26:05.710 --> 00:26:09.640
现在，如果你见过

00:26:09.640 --> 00:26:13.510
当人们谈论最大的合用和愿景时，

00:26:13.510 --> 00:26:16.960
它们通常是指本地池，而不是

00:26:16.960 --> 00:26:21.400
我第一次给你看的最长的合用时间。

00:26:21.400 --> 00:26:27.070
所以在这里，我们回到了我们开始的地方，我们已经完成了

00:26:27.070 --> 00:26:33.535
我们的尺寸三步一卷积，这是生产输出像以前。

00:26:33.535 --> 00:26:39.310
但现在，我要做的是本地游泳池，步幅为2。

00:26:39.310 --> 00:26:44.650
这就意味着我要把每两排排都集中起来

00:26:44.650 --> 00:26:47.110
一排，我可以再做一次

00:26:47.110 --> 00:26:50.680
无论是最大值还是平均值，或者任何对我有吸引力的东西。

00:26:50.680 --> 00:26:53.200
所以我选了前两排，

00:26:53.200 --> 00:26:54.970
我最大限度地去游泳，我得到这个。

00:26:54.970 --> 00:26:56.800
接下来的两排，

00:26:56.800 --> 00:26:58.555
我最大限度地去游泳，我得到这个。

00:26:58.555 --> 00:27:01.420
接下来的两个，接下来的两个，然后我把它垫起来。

00:27:01.420 --> 00:27:04.285
在底部，所以底部有两行。

00:27:04.285 --> 00:27:09.415
然后给我一个2步的最大本地池。

00:27:09.415 --> 00:27:13.300
从某种意义上说，这种效应是完全相同的，但是

00:27:13.300 --> 00:27:16.990
以不同的结果使用两个跨步

00:27:16.990 --> 00:27:20.530
我的卷积是因为我又把它简化为

00:27:20.530 --> 00:27:26.090
过去是八行的四行中的一行。

00:27:26.970 --> 00:27:29.935
是的，想象一下。

00:27:29.935 --> 00:27:33.640
好吧，那就是那个。

00:27:33.640 --> 00:27:35.410
你还能做什么？

00:27:35.410 --> 00:27:38.080
你可以做更多的事情使它变得复杂。

00:27:38.080 --> 00:27:43.765
人们有时做的另一件事是k-max池。

00:27:43.765 --> 00:27:49.510
所以这是一件更复杂的事情，它说得不错，

00:27:49.510 --> 00:27:53.530
而不是一直保持最大值，

00:27:53.530 --> 00:28:00.325
如果一个特征在句子中被激活了两三次，

00:28:00.325 --> 00:28:03.640
也许最好一直记录下

00:28:03.640 --> 00:28:07.375
在句子中激活，同时丢弃其余部分。

00:28:07.375 --> 00:28:09.070
所以在k-max池中，

00:28:09.070 --> 00:28:10.870
我在这里最多做两次，

00:28:10.870 --> 00:28:17.335
往下看这个列，就会发现该列的两个最高值。

00:28:17.335 --> 00:28:23.665
但是你把两个最高的值，不是按照最高到最低的顺序，

00:28:23.665 --> 00:28:26.620
但是按照它们在这些列中的顺序。

00:28:26.620 --> 00:28:28.840
所以是负0.2，

00:28:28.840 --> 00:28:32.230
这个0.3，是1.6，

00:28:32.230 --> 00:28:38.065
0.6，因为它反映了上面列的顺序。

00:28:38.065 --> 00:28:43.210
可以。快完成了，还有一个概念。

00:28:43.210 --> 00:28:52.285
这是另一种压缩数据的方法，即展开卷积。

00:28:52.285 --> 00:28:55.315
所以如果你有一个扩张卷积，

00:28:55.315 --> 00:29:01.870
所以在这里进行的扩张卷积并不真正有意义，但是你可以在哪里使用它

00:29:01.870 --> 00:29:08.440
一个展开卷积是，如果我把它放到另一个卷积层，

00:29:08.440 --> 00:29:13.540
我们可以有深度卷积网络，它有多个卷积层。

00:29:13.540 --> 00:29:20.560
所以展开卷积问题的想法是跳过一些行。

00:29:20.560 --> 00:29:24.295
所以如果你从顶部开始膨胀两个，

00:29:24.295 --> 00:29:27.460
你要拿第一个，第三个，

00:29:27.460 --> 00:29:31.870
第五排，用我的菲尔不好意思，

00:29:31.870 --> 00:29:32.980
我有不同的过滤器。

00:29:32.980 --> 00:29:38.305
将它们乘以我的过滤器，然后得到出现在这里的值。

00:29:38.305 --> 00:29:40.480
如果像一个人那样大步前进，

00:29:40.480 --> 00:29:46.900
然后你会使用，你会继续做下一个排列的行。

00:29:46.900 --> 00:29:51.025
所以这可以让你看到

00:29:51.025 --> 00:29:56.680
没有很多参数的句子的更大范围。

00:29:56.680 --> 00:29:59.065
所以你不必这样做。

00:29:59.065 --> 00:30:00.670
你可以说，看，

00:30:00.670 --> 00:30:07.015
我可以用5个内核大小的卷积来代替。

00:30:07.015 --> 00:30:08.470
然后他们会说五个，

00:30:08.470 --> 00:30:11.500
看到一行五个字，但我会

00:30:11.500 --> 00:30:17.230
一些更大的矩阵来指定我的特性。

00:30:17.230 --> 00:30:20.770
然而，这样我可以保持矩阵的小而不动

00:30:20.770 --> 00:30:25.105
在一个操作中看到一个更大的句子范围。

00:30:25.105 --> 00:30:30.670
是的，这个概念是关于一句话的多少

00:30:30.670 --> 00:30:36.490
SEE是卷积神经网络中的一个重要概念。

00:30:36.490 --> 00:30:39.940
因为，你知道，如果你从一句话的开头开始

00:30:39.940 --> 00:30:43.780
你只需要进行三乘三的卷积，嗯，

00:30:43.780 --> 00:30:47.995
你好像看到了这句话的三个词组。

00:30:47.995 --> 00:30:50.350
事实证明，在自然语言中，

00:30:50.350 --> 00:30:53.305
实际上已经是非常有用的表示了。

00:30:53.305 --> 00:30:56.920
因为把这些n-gram作为特征是

00:30:56.920 --> 00:31:01.165
对很多用途都很好，包括文本分类。

00:31:01.165 --> 00:31:05.680
但是如果你想更多地理解一个句子的语义，

00:31:05.680 --> 00:31:08.575
不知怎的，你想一下子看到更多。

00:31:08.575 --> 00:31:13.780
而且你有一些工具可以让你一次看到更多的信息，

00:31:13.780 --> 00:31:15.730
你可以使用更大的过滤器，

00:31:15.730 --> 00:31:16.870
你可以用，呃，

00:31:16.870 --> 00:31:18.460
内核大小5，7，

00:31:18.460 --> 00:31:20.650
九或是什么卷积。

00:31:20.650 --> 00:31:25.585
你可以做一些像展开卷积这样的事情，这样你就可以看到展开的图片。

00:31:25.585 --> 00:31:28.120
你能做的第三件事就是

00:31:28.120 --> 00:31:30.835
可以有卷积神经网络的深度。

00:31:30.835 --> 00:31:35.605
因为当你对卷积神经网络有更深入的了解时，你会看到更多。

00:31:35.605 --> 00:31:37.690
所以在第一层，

00:31:37.690 --> 00:31:43.150
现在，这些行中有三个单词的相关信息。

00:31:43.150 --> 00:31:46.630
如果你只是粘了第二层

00:31:46.630 --> 00:31:48.280
卷积神经网络

00:31:48.280 --> 00:31:51.670
它上面的一般性质和你

00:31:51.670 --> 00:31:55.450
前三行再卷积一次，然后

00:31:55.450 --> 00:32:00.940
然后是下一个，那些人知道你最初输入句子的五个单词。

00:32:00.940 --> 00:32:03.700
所以当你有一个更深的convnet堆栈时，

00:32:03.700 --> 00:32:07.495
开始了解越来越大的句子片段。

00:32:07.495 --> 00:32:09.970
可以。一切都好吗？

00:32:09.970 --> 00:32:12.530
有问题吗？

00:32:14.760 --> 00:32:22.899
不，很好，好的。所以，下一段基本上是再次向你展示这些东西，

00:32:22.899 --> 00:32:26.560
嗯，在一篇特定的论文中。

00:32:26.560 --> 00:32:27.850
所以这是，嗯，

00:32:27.850 --> 00:32:32.125
一篇由Yoon Kim撰写的论文，他是哈佛学生，

00:32:32.125 --> 00:32:36.460
可能还是个哈佛学生，嗯，在2014年。

00:32:36.460 --> 00:32:39.790
所以这是一篇相当早的论文。

00:32:39.790 --> 00:32:45.520
嗯，他想证明你可以用卷积神经网络

00:32:45.520 --> 00:32:47.500
干得好

00:32:47.500 --> 00:32:52.240
文本分类当你想分类的是一个句子。

00:32:52.240 --> 00:32:55.750
所以，你可能想做的是

00:32:55.750 --> 00:33:00.400
你在烂番茄网站上看到的电影评论片段，

00:33:00.400 --> 00:33:04.900
“这是肯定句还是否定句描述？”

00:33:04.900 --> 00:33:08.155
他建造的模型实际上是类似的

00:33:08.155 --> 00:33:11.695
科洛伯特和韦斯顿的卷积神经网络，

00:33:11.695 --> 00:33:14.980
嗯，在他们2011年的报纸上介绍说，

00:33:14.980 --> 00:33:18.100
之前我们讨论基于窗口的分类器时提到过。

00:33:18.100 --> 00:33:20.500
所以，在他们的论文中，他们实际使用

00:33:20.500 --> 00:33:25.600
基于窗口的分类器和卷积分类器。

00:33:25.600 --> 00:33:28.570
可以。嗯，所以是的，

00:33:28.570 --> 00:33:29.800
我已经说过了。

00:33:29.800 --> 00:33:34.210
所以他们的任务是句子分类，可能是感情用事。

00:33:34.210 --> 00:33:35.875
可能是其他的事情，比如，

00:33:35.875 --> 00:33:39.100
这句话是主观的还是客观的？

00:33:39.100 --> 00:33:42.040
所以客观的是主要新闻文章的意思

00:33:42.040 --> 00:33:45.295
主观是意见的本意。

00:33:45.295 --> 00:33:48.970
嗯，还有其他的问题分类。

00:33:48.970 --> 00:33:51.220
这是一个关于一个人的问题吗？

00:33:51.220 --> 00:33:53.200
位置，号码，还是什么？

00:33:53.200 --> 00:33:57.400
好吧，这就是他所做的。

00:33:57.400 --> 00:34:01.495
有点-这些幻灯片有点，嗯，

00:34:01.495 --> 00:34:06.880
使用他论文的符号，这与

00:34:06.880 --> 00:34:09.310
把数学写下来的方式记到我刚展示的内容上

00:34:09.310 --> 00:34:12.160
你，它真的在做同样的事情。

00:34:12.160 --> 00:34:16.930
所以我们从长度为k.um的词向量开始，

00:34:16.930 --> 00:34:24.610
句子是将所有这些词向量连接在一起，然后，

00:34:24.610 --> 00:34:27.280
当我们有一系列的词时，

00:34:27.280 --> 00:34:30.190
它是句子向量的一个子部分。

00:34:30.190 --> 00:34:36.310
所以卷积滤波器被表示为一个向量，因为

00:34:36.310 --> 00:34:42.100
在这里，他把整个句子的所有内容都展成一个长的矢量，

00:34:42.100 --> 00:34:44.515
而我会进入一个矩阵。

00:34:44.515 --> 00:34:51.070
嗯，所以3号卷积只是一个长度为hk的实向量，

00:34:51.070 --> 00:34:56.350
卷积滤波器的大小乘以单词的维数。

00:34:56.350 --> 00:35:01.210
嗯，那么，他要做什么来建造

00:35:01.210 --> 00:35:07.450
他的文本分类器使用不同大小的卷积。

00:35:07.450 --> 00:35:10.765
所以你可以有两个大小的卷积，

00:35:10.765 --> 00:35:16.000
如图所示大小为三个卷积，以及更大的卷积。

00:35:16.000 --> 00:35:23.140
所以，嗯，为了计算我们CNN的一个频道，我们

00:35:23.140 --> 00:35:26.620
然后做一个点积之间的权重向量

00:35:26.620 --> 00:35:30.415
特征乘以相同术语的子序列，

00:35:30.415 --> 00:35:35.035
他也有点偏袒我，我有点忽略了。

00:35:35.035 --> 00:35:41.110
嗯，然后通过非线性，

00:35:41.110 --> 00:35:43.390
嗯，我也没做过。

00:35:43.390 --> 00:35:46.045
嗯，但我们见过很多。

00:35:46.045 --> 00:35:49.810
嗯，所以，我们想做的是，

00:35:49.810 --> 00:35:53.410
嗯，特写，我们想，嗯，

00:35:53.410 --> 00:35:58.150
完成所有这些-对于内核大小为3的特性，

00:35:58.150 --> 00:36:00.880
我们要把句子从头到尾讲一遍。

00:36:00.880 --> 00:36:04.735
他做的另一件事虽然有点像漏斗，但有趣的是，

00:36:04.735 --> 00:36:08.920
他的窗户在符号上有点歪斜，对吧。

00:36:08.920 --> 00:36:11.695
有一个词和

00:36:11.695 --> 00:36:15.355
嗯，h减去右边的1个字。

00:36:15.355 --> 00:36:20.095
所以他在右边有填充物，

00:36:20.095 --> 00:36:25.810
大多数人在事物的两个方向上对称地做卷积。

00:36:25.810 --> 00:36:31.630
可以。所以，我们要为一系列的特性或者

00:36:31.630 --> 00:36:34.480
通道CI，因此计算

00:36:34.480 --> 00:36:38.680
正如我们所说的那样，我们错综复杂的陈述。

00:36:38.680 --> 00:36:43.435
可以。嗯，那他就照我们说的做。

00:36:43.435 --> 00:36:48.370
嗯，在pooling层中有最大的一段时间池可以捕获

00:36:48.370 --> 00:36:53.650
最相关的事情是给我们每个频道一个号码。

00:36:53.650 --> 00:37:01.465
嗯，我们有不同的特性，它们有不同的内核大小。

00:37:01.465 --> 00:37:08.230
嗯，这是他使用的另一个想法，这可能是个好主意。

00:37:08.230 --> 00:37:13.659
嗯，他知道你可以用各种方式思考的事情之一，

00:37:13.659 --> 00:37:17.350
嗯，比如说问答系统。

00:37:17.350 --> 00:37:21.605
嗯，所以他使用了预先训练过的词向量。

00:37:21.605 --> 00:37:28.980
嗯，但他所做的实际上是把矢量这个词翻了一番。

00:37:28.980 --> 00:37:32.475
所以，对于每个词，他都有两个矢量词的副本，

00:37:32.475 --> 00:37:37.290
所以你有两个频道和一个频道，

00:37:37.290 --> 00:37:42.375
冻住了，另一个他在训练时调整得很好。

00:37:42.375 --> 00:37:46.590
所以他试图从两个方面都得到最好的微调。

00:37:46.590 --> 00:37:51.770
不是很好的调优，所有这些都进入了最大池操作。

00:37:51.770 --> 00:38:01.614
可以。嗯，那么，在最大池之后，我们得到每个通道的一个数字，所以，

00:38:01.614 --> 00:38:06.760
嗯，他有三个卷积，三个，

00:38:06.760 --> 00:38:10.390
每种尺寸有四、五、100个特点。

00:38:10.390 --> 00:38:13.435
所以我们得到一个大小向量，

00:38:13.435 --> 00:38:15.670
嗯，300点，

00:38:15.670 --> 00:38:19.810
在这一点上，你得到了最后的向量，然后把它粘在一起。

00:38:19.810 --> 00:38:24.595
通过一个SoftMax，然后给出类的分类。

00:38:24.595 --> 00:38:31.495
嗯，所有这些都可以总结在这张图片中，如果它大到可以阅读的程度的话。

00:38:31.495 --> 00:38:32.800
这是我们的句子。

00:38:32.800 --> 00:38:34.855
我非常喜欢这部电影，

00:38:34.855 --> 00:38:39.310
你知道吗，我们的嵌入维度是5，

00:38:39.310 --> 00:38:42.265
然后在这个例子中这样做，

00:38:42.265 --> 00:38:46.930
每个内核大小有两个通道，

00:38:46.930 --> 00:38:52.030
我们考虑粒径为2、3和4的粒。

00:38:52.030 --> 00:38:57.205
嗯，然后我们得到了两个不同的。

00:38:57.205 --> 00:39:01.615
嗯，所以我们要，嗯，六个。

00:39:01.615 --> 00:39:04.405
这是我们的六个过滤器。

00:39:04.405 --> 00:39:07.180
嗯，我们应用这些。

00:39:07.180 --> 00:39:10.975
当我们-当我们应用那些没有填充的过滤器时，

00:39:10.975 --> 00:39:15.880
然后我们将得到尺寸为4的过滤器的这些输出，

00:39:15.880 --> 00:39:18.985
分别是五个和六个。

00:39:18.985 --> 00:39:23.065
嗯，所以一旦我们有了这些

00:39:23.065 --> 00:39:27.265
对于这两组数字中的每一个，我们都要进行一次最大池。

00:39:27.265 --> 00:39:30.880
所以，我们只取其中的最大值，

00:39:30.880 --> 00:39:36.715
嗯，输出特性给了我们这六个数字。

00:39:36.715 --> 00:39:43.060
我们可以把它们连接成一个向量，然后输入，

00:39:43.060 --> 00:39:50.120
嗯，关于情绪是积极的还是消极的这两类人中最温和的。

00:39:52.580 --> 00:39:55.590
嗯，这基本上就是模型。

00:39:55.590 --> 00:40:01.200
所以有点-所以这实际上是非常简单的，

00:40:01.200 --> 00:40:03.630
计算效率很高，

00:40:03.630 --> 00:40:06.780
如何建立文本分类器的模型。

00:40:06.780 --> 00:40:13.155
[噪音]嗯，是的，还有几件事要做，

00:40:13.155 --> 00:40:15.210
嗯，在其中一项任务中，

00:40:15.210 --> 00:40:17.700
我们讨论过辍学[噪音]你就用它。

00:40:17.700 --> 00:40:19.065
所以，嗯，你知道，

00:40:19.065 --> 00:40:21.705
希望你们现在都是辍学的高手。

00:40:21.705 --> 00:40:24.720
嗯，所以他用的是辍学，嗯，

00:40:24.720 --> 00:40:28.185
这是2014年，

00:40:28.185 --> 00:40:31.820
嗯，辍学论文只在2014年出版。

00:40:31.820 --> 00:40:34.895
我想，早在几年前就出现了一个早期版本。

00:40:34.895 --> 00:40:37.160
这还是相当早的时候，

00:40:37.160 --> 00:40:39.425
嗯，利用辍学的机会。

00:40:39.425 --> 00:40:41.135
所以在训练的时候，

00:40:41.135 --> 00:40:44.105
你得到了这种辍学向量，嗯，

00:40:44.105 --> 00:40:49.010
在这里你对伯努利随机变量进行抽样，你是，

00:40:49.010 --> 00:40:54.825
嗯，有点，每次你做事情的时候都会删除一些功能。

00:40:54.825 --> 00:40:58.200
在测试的时候，你不会辍学，

00:40:58.200 --> 00:41:02.130
但因为在你放弃很多东西之前，

00:41:02.130 --> 00:41:07.455
你的体重矩阵和你辍学的概率一样，

00:41:07.455 --> 00:41:09.000
所以你得到，有点，

00:41:09.000 --> 00:41:12.000
与以前相同比例的向量。

00:41:12.000 --> 00:41:15.075
嗯，就像我们在作业中讨论的那样，

00:41:15.075 --> 00:41:18.420
辍学是一种真正有效的正规化形式，

00:41:18.420 --> 00:41:20.580
广泛应用于神经网络。

00:41:20.580 --> 00:41:23.700
嗯，他不仅这么做，他还真的这么做了，

00:41:23.700 --> 00:41:27.600
另一种时髦的正规化形式。

00:41:27.600 --> 00:41:31.425
这就是SoftMax权重向量，

00:41:31.425 --> 00:41:35.280
他限制了二级标准，

00:41:35.280 --> 00:41:41.100
所以权重向量和SoftMax的平方范数，噪音，嗯，

00:41:41.100 --> 00:41:45.405
矩阵，嗯，到一个固定的数字s，

00:41:45.405 --> 00:41:47.460
这是一组超参数，

00:41:47.460 --> 00:41:49.515
实际设置为值3。

00:41:49.515 --> 00:41:53.055
嗯，如果你的体重过大，

00:41:53.055 --> 00:41:55.590
他们正在重新扫描，

00:41:55.590 --> 00:41:57.345
嗯，所以他们没有爆炸。

00:41:57.345 --> 00:42:00.210
嗯，这不是一件很常见的事情。

00:42:00.210 --> 00:42:03.690
我不确定这很有必要，但是，嗯，

00:42:03.690 --> 00:42:05.850
我想它给了你一些-我是说，

00:42:05.850 --> 00:42:09.045
我想通过向你展示这个的一些细节，

00:42:09.045 --> 00:42:10.590
我的希望是，

00:42:10.590 --> 00:42:13.680
给你一些关于你可以玩很多东西的想法

00:42:13.680 --> 00:42:17.025
如果你想尝试不同的东西，

00:42:17.025 --> 00:42:19.020
嗯，最后的项目。

00:42:19.020 --> 00:42:21.000
嗯，好吧。

00:42:21.000 --> 00:42:24.120
这里是他的一些最终超参数。

00:42:24.120 --> 00:42:27.360
所以他用的是非线性

00:42:27.360 --> 00:42:30.765
嗯，三、四、五号窗户，

00:42:30.765 --> 00:42:35.790
卷积，每种尺寸的100个特征或通道，

00:42:35.790 --> 00:42:38.535
嗯，像往常一样中途退学。

00:42:38.535 --> 00:42:41.865
嗯，你的辍学率提高了几个百分点，

00:42:41.865 --> 00:42:43.860
这其实很常见。

00:42:43.860 --> 00:42:47.835
嗯，有点L2约束，S等于3，

00:42:47.835 --> 00:42:50.175
小批量50，

00:42:50.175 --> 00:42:52.635
300维字向量，

00:42:52.635 --> 00:42:55.755
训练以最大化开发集性能。

00:42:55.755 --> 00:42:58.830
可以。这是一张大桌子，

00:42:58.830 --> 00:43:00.690
你知道，我太懒了，嗯，

00:43:00.690 --> 00:43:06.570
重做这些不同的文本分类数据集的性能。

00:43:06.570 --> 00:43:08.460
嗯，有很多不同的。

00:43:08.460 --> 00:43:11.820
所以这两个都是斯坦福情感树库。

00:43:11.820 --> 00:43:14.565
这是主观的客观语言。

00:43:14.565 --> 00:43:19.650
这是问题的分类，问的是人名和地点吗？

00:43:19.650 --> 00:43:20.790
一家公司或什么的。

00:43:20.790 --> 00:43:24.150
嗯，这是，嗯，

00:43:24.150 --> 00:43:26.280
谈论，某种程度上，一种观点，

00:43:26.280 --> 00:43:28.335
这是另一个分类问题。

00:43:28.335 --> 00:43:30.885
消费者报告是另一种情绪。

00:43:30.885 --> 00:43:36.210
嗯，很多数据集，然后这里有很多模型。

00:43:36.210 --> 00:43:41.580
所以模型-一些模型在这里或这里，

00:43:41.580 --> 00:43:46.020
是传统的基于特征的分类器。

00:43:46.020 --> 00:43:48.000
嗯，特别是，

00:43:48.000 --> 00:43:52.230
嗯，在2012年的时候，我和王有点像，

00:43:52.230 --> 00:43:56.025
有点指出，通过采取某些步骤

00:43:56.025 --> 00:44:00.720
具有n-gram特征和其他形式的规范化，

00:44:00.720 --> 00:44:03.420
你可以得到相当好的结果

00:44:03.420 --> 00:44:06.960
只是传统的特征，嗯，基于分类器。

00:44:06.960 --> 00:44:12.045
很多人把它作为一个基准来证明你可以做得更好。

00:44:12.045 --> 00:44:14.360
嗯，上面的那些，

00:44:14.360 --> 00:44:18.200
我们小组非常喜欢树结构神经网络吗？

00:44:18.200 --> 00:44:22.805
在20世纪10年代初，然后在最高层，

00:44:22.805 --> 00:44:24.695
呃，他的CNN模特。

00:44:24.695 --> 00:44:26.510
如你所见，

00:44:26.510 --> 00:44:27.875
这是一种混合。

00:44:27.875 --> 00:44:30.870
有时CNN的模特会赢，

00:44:30.870 --> 00:44:33.015
就像这个专栏和这个专栏，

00:44:33.015 --> 00:44:36.015
有时，它不会像在这些专栏中那样获胜。

00:44:36.015 --> 00:44:38.010
但总的来说

00:44:38.010 --> 00:44:40.260
你从这看不到的是，你知道，

00:44:40.260 --> 00:44:43.140
这非常简单，嗯，

00:44:43.140 --> 00:44:46.335
卷积神经网络模型实际上是这样的，

00:44:46.335 --> 00:44:48.720
嗯，这个系统不错。

00:44:48.720 --> 00:44:54.720
嗯，你可以用这个结果表来反驳，

00:44:54.720 --> 00:45:01.285
再说一次，就像写你的提案-项目提案，嗯，

00:45:01.285 --> 00:45:07.250
你应该做的一件事就是想想你在读什么，嗯，

00:45:07.250 --> 00:45:10.100
因为，你知道，很多论文并不完美

00:45:10.100 --> 00:45:13.130
有理由对他们所声称的进行反驳。

00:45:13.130 --> 00:45:17.785
有时候，如果你考虑到他们的要求，以及是否合理，嗯，

00:45:17.785 --> 00:45:20.895
这是有原因的，或者有想法的

00:45:20.895 --> 00:45:24.405
你如何做不同的事情或展示不同的东西。

00:45:24.405 --> 00:45:27.315
我的意思是，你能和他争吵的主要原因，

00:45:27.315 --> 00:45:31.365
嗯，Yoon Kim的结果表是，

00:45:31.365 --> 00:45:35.385
他已经说过了，因为我有几张幻灯片，嗯，

00:45:35.385 --> 00:45:37.980
辍学者给你的陈述

00:45:37.980 --> 00:45:41.220
这种神经网络的精确度提高了2%到4%。

00:45:41.220 --> 00:45:45.210
[噪音]嗯，但这些系统大部分是因为

00:45:45.210 --> 00:45:49.365
年龄更大，在辍学之前就已经完成了，

00:45:49.365 --> 00:45:51.390
嗯，没有利用辍学。

00:45:51.390 --> 00:45:55.170
但是，你知道，上面的任何一种神经网络系统

00:45:55.170 --> 00:45:59.445
可能会使用辍学，可能会给他们一些，

00:45:59.445 --> 00:46:01.140
嗯，百分比也增加了。

00:46:01.140 --> 00:46:05.385
所以可以说，这是一种有偏见的、不公平的比较。

00:46:05.385 --> 00:46:10.635
正确的做法是比较所有的系统，嗯，使用辍学。

00:46:10.635 --> 00:46:12.120
但是，你知道，

00:46:12.120 --> 00:46:13.890
尽管如此，你知道，

00:46:13.890 --> 00:46:16.980
这仍然是一个漂亮的-很多人注意到

00:46:16.980 --> 00:46:20.820
这篇论文是因为它表明使用这种非常简单的方法，

00:46:20.820 --> 00:46:23.190
非常快速的卷积结构，

00:46:23.190 --> 00:46:27.070
可以为文本分类提供强有力的结果。

00:46:28.250 --> 00:46:31.005
嗯，就是这样。

00:46:31.005 --> 00:46:33.765
对.总之，

00:46:33.765 --> 00:46:38.475
你知道，一些你应该为项目考虑的事情，否则，

00:46:38.475 --> 00:46:44.370
我们正在有效地构建一个更大的工具包，其中包含您可以使用的各种工具，

00:46:44.370 --> 00:46:48.135
嗯，对于项目或者未来的工作或者其他什么。

00:46:48.135 --> 00:46:49.635
所以从一开始，

00:46:49.635 --> 00:46:53.250
我们有矢量词，然后我们就可以建立一个

00:46:53.250 --> 00:46:57.105
向量模型只需取向量这个词并求其平均值。

00:46:57.105 --> 00:47:01.080
而且，你知道，这实际上是一个非常好的基线。

00:47:01.080 --> 00:47:03.960
在很多情况下，我们建议你做一些项目，

00:47:03.960 --> 00:47:05.085
你应该用那个。

00:47:05.085 --> 00:47:06.270
看看效果如何，

00:47:06.270 --> 00:47:07.965
确保你工作得更好。

00:47:07.965 --> 00:47:10.605
我的意思是，你可以做得更好，

00:47:10.605 --> 00:47:14.490
如果你在上面增加一些额外的relu层，

00:47:14.490 --> 00:47:18.015
这是一个在深度平均网络中被探索的想法。

00:47:18.015 --> 00:47:22.290
嗯，然后我们看了一些非常简单的窗户模型。

00:47:22.290 --> 00:47:23.850
你只是拿着这些

00:47:23.850 --> 00:47:27.585
五个字窗口，并在其上计算一个前馈网络，

00:47:27.585 --> 00:47:32.835
对于只需要局部上下文的单词分类问题，它们工作得很好。

00:47:32.835 --> 00:47:36.045
比如说，词性标签或者内质网。

00:47:36.045 --> 00:47:39.390
但之后我们又看了一些其他的模型。

00:47:39.390 --> 00:47:45.405
所以，嗯，CNN对文本分类很有帮助，嗯，

00:47:45.405 --> 00:47:49.590
它们非常好，因为它们在GPU上的并行性非常好，

00:47:49.590 --> 00:47:51.840
这是我以后会再来讲的。

00:47:51.840 --> 00:47:57.510
所以，它们只是排序——表示句子意思的一般排序。

00:47:57.510 --> 00:47:59.099
他们实际上是一个有效率的，

00:47:59.099 --> 00:48:02.295
用途广泛，方法好，使用比较多。

00:48:02.295 --> 00:48:05.460
然后，它们与循环神经网络形成了某种对比。

00:48:05.460 --> 00:48:07.800
递归神经网络有一些优点。

00:48:07.800 --> 00:48:10.080
它们在认知上更有说服力，

00:48:10.080 --> 00:48:12.120
因为你在阅读课文，

00:48:12.120 --> 00:48:14.145
嗯，明白它的意思了。

00:48:14.145 --> 00:48:16.830
嗯，复发性神经网络对

00:48:16.830 --> 00:48:19.800
比如序列标记和分类，

00:48:19.800 --> 00:48:23.385
建立语言模型来预测接下来会发生什么。

00:48:23.385 --> 00:48:26.910
嗯，他们结合注意力可以做得很好。

00:48:26.910 --> 00:48:29.565
嗯，但它们也有一些缺点。

00:48:29.565 --> 00:48:33.870
它们比卷积神经网络慢得多，如果你想的话

00:48:33.870 --> 00:48:38.300
做的是找出一个句子的某种整体意义表示，

00:48:38.300 --> 00:48:39.845
你知道，“这是什么意思？

00:48:39.845 --> 00:48:41.375
这两个，嗯，

00:48:41.375 --> 00:48:43.850
短语相互解释？”

00:48:43.850 --> 00:48:46.730
现在有很多结果表明

00:48:46.730 --> 00:48:49.805
用循环神经网络不能取得更好的效果。

00:48:49.805 --> 00:48:55.440
使用卷积神经网络等技术可以获得更好的结果。

00:48:55.550 --> 00:49:05.010
可以。[噪音]所以在下一步中[噪音]是，

00:49:05.010 --> 00:49:09.675
有点像，朝着我们的公司前进，

00:49:09.675 --> 00:49:12.375
嗯，卷积体系结构示例。

00:49:12.375 --> 00:49:14.010
所以在开始之前，

00:49:14.010 --> 00:49:18.525
我只是想介绍一些我们没有看到的概念，

00:49:18.525 --> 00:49:22.625
当我们这样做的时候，所有这些都会出现。

00:49:22.625 --> 00:49:26.360
所以我们花了很多时间在序列模型部分，

00:49:26.360 --> 00:49:32.345
谈论门控模型或门控循环单元和LSTM单元。

00:49:32.345 --> 00:49:36.080
但大门的概念是一般的，我们可以

00:49:36.080 --> 00:49:40.130
有种想法，我们可以计算一些东西，

00:49:40.130 --> 00:49:42.175
穿过去，嗯，

00:49:42.175 --> 00:49:47.370
一个乙状体非线性，得到一个介于0和1之间的值，

00:49:47.370 --> 00:49:50.385
或者一个介于0和1之间的向量。

00:49:50.385 --> 00:49:52.980
然后用向量做一个Hadamard积

00:49:52.980 --> 00:49:55.860
它的值和0之间的门。

00:49:55.860 --> 00:49:59.490
所以这就意味着你也可以应用

00:49:59.490 --> 00:50:04.110
当你建立多层网络时，门是垂直的。

00:50:04.110 --> 00:50:07.845
在连续的LSTM被证明之后，

00:50:07.845 --> 00:50:11.780
这是一个非常成功的想法，

00:50:11.780 --> 00:50:13.730
人们开始探索，

00:50:13.730 --> 00:50:19.445
我们怎么能有，利用这些跳过连接和选通的思想，

00:50:19.445 --> 00:50:21.425
垂直方向？

00:50:21.425 --> 00:50:23.480
这里有两个版本。

00:50:23.480 --> 00:50:26.450
这个很简单，

00:50:26.450 --> 00:50:30.965
但一个非常成功的，基本上只是一个跳过连接。

00:50:30.965 --> 00:50:36.890
所以这被称为残数块，用于残数网络，

00:50:36.890 --> 00:50:38.690
也被称为resnet。

00:50:38.690 --> 00:50:42.470
嗯，所以在一个剩余块中，对于每个块，

00:50:42.470 --> 00:50:48.440
您只允许一个值跳到下一个，嗯，层。

00:50:48.440 --> 00:50:52.535
或者你可以把它穿过一个转换区，

00:50:52.535 --> 00:50:56.825
典型的conv块是通过卷积层，

00:50:56.825 --> 00:50:59.600
然后你会经历一个相对非线性的过程，

00:50:59.600 --> 00:51:03.250
另一个卷积层，当你出来的时候，

00:51:03.250 --> 00:51:05.430
你只要把这两个值加起来。

00:51:05.430 --> 00:51:07.710
所以这和

00:51:07.710 --> 00:51:11.820
求和值和lstm一样神奇。

00:51:11.820 --> 00:51:15.165
然后你把它的输出通过另一个relu，

00:51:15.165 --> 00:51:18.705
这里的这个叫做剩余块

00:51:18.705 --> 00:51:22.950
然后通常你会把剩余的块堆叠在一起。

00:51:22.950 --> 00:51:25.230
嗯，这里有个小把戏，

00:51:25.230 --> 00:51:28.320
嗯，你需要用填充物，对吧？

00:51:28.320 --> 00:51:33.000
嗯，因为在一天结束的时候，既然你想总结这两条路径，

00:51:33.000 --> 00:51:35.355
你要它们的尺寸一样。

00:51:35.355 --> 00:51:36.585
如果你，有点，

00:51:36.585 --> 00:51:40.200
如果它们在conv块中收缩，您将无法对它们求和。

00:51:40.200 --> 00:51:45.120
所以你想在每个阶段都有一个填充物，这样它们在这里的大小是一样的，

00:51:45.120 --> 00:51:47.437
这样你就可以把它们加在一起。

00:51:47.437 --> 00:51:54.500
嗯，这是，嗯，一个不同版本的块

00:51:54.500 --> 00:51:57.470
有点像LSTM，实际上

00:51:57.470 --> 00:52:01.710
该区块由J_rgen Schmidhuber和学生开发，

00:52:01.710 --> 00:52:06.000
谁是同一个人谁是背后的LSTM，你可以看到同样的想法。

00:52:06.000 --> 00:52:08.150
它被称为公路街区。

00:52:08.150 --> 00:52:10.800
所以在某种程度上是相似的。

00:52:10.800 --> 00:52:16.080
你知道，你有点想移动一个跳过的身份X

00:52:16.080 --> 00:52:23.085
一个非线性块，或者你可以让它通过完全相同的东西，conv，relu，conv。

00:52:23.085 --> 00:52:26.480
区别在于不同于这个，

00:52:26.480 --> 00:52:29.165
这一次有明确的大门，

00:52:29.165 --> 00:52:33.290
嗯，这个T型门和C型门。

00:52:33.290 --> 00:52:39.230
所以你把这条路和这条路相乘

00:52:39.230 --> 00:52:42.280
在一个门旁边，有点像

00:52:42.280 --> 00:52:47.130
我们以前看到的get输入门，然后将它们相加。

00:52:47.130 --> 00:52:50.670
所以那种感觉更

00:52:50.670 --> 00:52:56.285
强大，但实际上并不清楚它更强大。

00:52:56.285 --> 00:52:59.460
我的意思是，这个其实有一个非常简单的

00:52:59.460 --> 00:53:03.070
语义，因为如果你想到这个的语义

00:53:03.070 --> 00:53:05.930
默认情况是你走路

00:53:05.930 --> 00:53:11.015
这样你就可以把你的价值向前推，什么也不做。

00:53:11.015 --> 00:53:14.900
嗯，那么，这个街区的任务是做什么，

00:53:14.900 --> 00:53:18.155
就是要学习一个三角洲

00:53:18.155 --> 00:53:21.750
你对什么都不做有什么样的偏离。

00:53:21.750 --> 00:53:25.210
嗯，这是一个很好的简单语义，嗯，

00:53:25.210 --> 00:53:28.680
在神经网络中学习似乎很好。

00:53:28.680 --> 00:53:31.390
嗯，这种已经

00:53:31.390 --> 00:53:36.500
更复杂的表面语义学，因为你要，你知道，

00:53:36.500 --> 00:53:43.005
在Hadamard产品中，身份的某些部分乘以这种门

00:53:43.005 --> 00:53:49.880
这个conv块的一些部分乘以另一个gate t，得到一个hadamard产品。

00:53:49.880 --> 00:53:53.980
所以那种感觉更强大

00:53:53.980 --> 00:53:58.325
给了我更多的控制，因为我可以拿不同的碎片等等。

00:53:58.325 --> 00:54:01.620
如果你想得再久一点，我是说，

00:54:01.620 --> 00:54:05.380
从数学上讲，你

00:54:05.380 --> 00:54:09.500
它可以代表你用它可以做的任何事情。

00:54:09.500 --> 00:54:13.530
想这个的方法是，嗯，

00:54:13.530 --> 00:54:19.410
你知道，在这里你只保留了部分身份，

00:54:19.410 --> 00:54:26.840
嗯，但你能做的是保留整个身份，把它当作你的工作

00:54:26.840 --> 00:54:30.095
减去这个不保留的位

00:54:30.095 --> 00:54:34.440
在这里，在conv块，理论上可以做到。

00:54:34.440 --> 00:54:39.480
嗯，你可以用这个函数来计算任何东西，

00:54:39.480 --> 00:54:42.830
实际上，您可以使用一个，um，resnet块进行计算。

00:54:42.830 --> 00:54:47.185
嗯，然后就像在神经网络领域一样，

00:54:47.185 --> 00:54:49.330
问题不是，嗯，

00:54:49.330 --> 00:54:53.190
某种计算证明——可以计算也可以不计算。

00:54:53.190 --> 00:54:58.455
归根结底就是学习和规范化问题

00:54:58.455 --> 00:55:01.345
不管其中一个是否能证明

00:55:01.345 --> 00:55:05.270
在学习体系结构中更好。

00:55:06.430 --> 00:55:09.680
可以。第二个概念。

00:55:09.680 --> 00:55:11.860
嗯，批处理规范化。

00:55:11.860 --> 00:55:17.405
所以当人们建立深卷积神经网络时，

00:55:17.405 --> 00:55:21.680
嗯，在2015年的活动中，

00:55:21.680 --> 00:55:27.065
嗯，它们几乎总是使用批处理规范化层，因为

00:55:27.065 --> 00:55:32.685
这会让你的生活变得更好，如果他们不使用批处理规范化层，

00:55:32.685 --> 00:55:37.070
他们通常使用人们提出的另一种不同的想法

00:55:37.070 --> 00:55:42.165
例如，层规范化，这是为了做同样的事情。

00:55:42.165 --> 00:55:46.090
嗯，那么什么样的批处理规范化呢？

00:55:46.090 --> 00:55:50.650
我的意思是，我想你们中的很多人都会在某个地方看到台阶或者

00:55:50.650 --> 00:55:56.305
否则，做z变换的想法意味着你需要你的数据，

00:55:56.305 --> 00:55:59.100
你算出它的意思，你算出它的意思

00:55:59.100 --> 00:56:03.970
标准差，然后通过减法和

00:56:03.970 --> 00:56:07.710
乘法使你有一组数据

00:56:07.710 --> 00:56:12.360
平均值为零，标准偏差为一。

00:56:12.360 --> 00:56:14.680
大多数人都看到了，对吧？

00:56:14.680 --> 00:56:23.500
是啊？嗯，所以批量规范化实际上就是这样做的，但方式很奇怪。

00:56:23.500 --> 00:56:27.770
所以你要做的是把每一个小批量都拿走。

00:56:27.770 --> 00:56:31.875
所以不管你在一个小批量中随机抽取32个例子，

00:56:31.875 --> 00:56:34.040
你让他们穿过一层

00:56:34.040 --> 00:56:37.355
你的神经网络就像我们以前看到的一样

00:56:37.355 --> 00:56:43.190
然后您获取这个小批处理的输出，然后对其进行z转换。

00:56:43.190 --> 00:56:47.295
嗯，然后它进入下一个convblock或者其他什么，

00:56:47.295 --> 00:56:49.410
下次你有不同的小批量，

00:56:49.410 --> 00:56:50.990
你只要变换它。

00:56:50.990 --> 00:56:52.290
所以看起来有点奇怪。

00:56:52.290 --> 00:56:56.600
你只是在这些小批量的输出上做这个。

00:56:56.600 --> 00:57:01.680
嗯，但事实证明这是一件非常有效的事情。

00:57:01.680 --> 00:57:05.980
这就意味着

00:57:05.980 --> 00:57:09.890
convblock类型总是具有相同的比例。

00:57:09.890 --> 00:57:13.720
所以它不会有太大的波动和混乱，而且它会

00:57:13.720 --> 00:57:18.225
使模型更可靠地训练，因为，

00:57:18.225 --> 00:57:22.860
你知道，你只需要对很多事情不那么挑剔。

00:57:22.860 --> 00:57:25.505
因为，你知道，我们谈过很多事情，

00:57:25.505 --> 00:57:28.180
关于初始化参数和

00:57:28.180 --> 00:57:31.130
设定你的学习率差不多是，嗯，

00:57:31.130 --> 00:57:34.310
你必须把事情的规模控制在适当的范围内，这样他们就不会

00:57:34.310 --> 00:57:37.810
太大或太小之类的。

00:57:37.810 --> 00:57:40.280
但是，如果您正在进行批处理规范化，

00:57:40.280 --> 00:57:42.490
你有点强悍，

00:57:42.490 --> 00:57:45.705
嗯，每次大小都一样。

00:57:45.705 --> 00:57:48.370
所以，你不必这么做

00:57:48.370 --> 00:57:51.200
其他的东西，它仍然倾向于，

00:57:51.200 --> 00:57:52.710
嗯，工作得很好。

00:57:52.710 --> 00:57:55.650
所以这是一个很好的技巧。

00:57:55.690 --> 00:57:59.800
可以。嗯，还有最后一件事要了解。

00:57:59.800 --> 00:58:02.070
嗯，有个概念，

00:58:02.070 --> 00:58:07.015
嗯，一个卷积的大小。

00:58:07.015 --> 00:58:11.240
嗯，实际上，我想我真的有点，嗯，

00:58:11.240 --> 00:58:14.680
重新命名了它-我把它命名错了，因为我记下了

00:58:14.680 --> 00:58:18.240
一个接一个的卷积，因为这是你通常看到的术语。

00:58:18.240 --> 00:58:22.530
但那是，嗯，视觉世界，你有二维卷积。

00:58:22.530 --> 00:58:26.135
所以我想我应该把这个叫做卷积。

00:58:26.135 --> 00:58:28.890
所以你可以有卷积，嗯，

00:58:28.890 --> 00:58:33.070
当你第一次看到它的时候，

00:58:33.070 --> 00:58:37.840
这似乎毫无意义，因为整个想法

00:58:37.840 --> 00:58:43.305
卷积的意思是，我拿着这个补丁，从中计算一些东西。

00:58:43.305 --> 00:58:48.330
如果我不看其他单词，

00:58:48.330 --> 00:58:50.510
当然，我什么也不算。

00:58:50.510 --> 00:58:54.975
但是在尺寸1卷积中实际发生的情况，

00:58:54.975 --> 00:58:59.160
如果你有很多频道

00:58:59.160 --> 00:59:03.850
在前一层中，如果你计算了它是什么，

00:59:03.850 --> 00:59:06.605
32个频道之类的。

00:59:06.605 --> 00:59:11.070
一个接一个卷积的作用是

00:59:11.070 --> 00:59:16.625
在这些通道上嵌入一个小小的完全连接的网络。

00:59:16.625 --> 00:59:18.910
所以你在做一个

00:59:18.910 --> 00:59:22.280
位置特定的完全连接网络，

00:59:22.280 --> 00:59:26.385
嗯，对于每一行数据。

00:59:26.385 --> 00:59:28.050
所以你可以这样做，

00:59:28.050 --> 00:59:29.590
嗯，有各种原因。

00:59:29.590 --> 00:59:31.920
你可以这样做，因为你想从

00:59:31.920 --> 00:59:34.870
有很多渠道可以减少渠道或

00:59:34.870 --> 00:59:37.460
你可以这样做只是因为你认为另一个非线性

00:59:37.460 --> 00:59:40.345
会有帮助的，这是一种非常便宜的方法。

00:59:40.345 --> 00:59:44.150
因为最重要的是如果你

00:59:44.150 --> 00:59:47.990
把完全连接的层覆盖在所有东西上，

00:59:47.990 --> 00:59:52.930
它们涉及许多参数，而将这些参数放入

00:59:52.930 --> 00:59:56.650
一个卷积涉及很少的参数

00:59:56.650 --> 01:00:00.310
因为你只是在做一个词的水平。

01:00:00.670 --> 01:00:03.765
嗯，好吧。

01:00:03.765 --> 01:00:08.585
嗯，两个随机的东西，然后我将进入我的复杂模型。

01:00:08.585 --> 01:00:10.540
嗯，这只是一种

01:00:10.540 --> 01:00:13.660
几乎是一种偏见-撇开不谈，但它只是表明

01:00:13.660 --> 01:00:17.110
你可以做一些不同的事情，你可以和它一起玩。

01:00:17.110 --> 01:00:20.065
我是说，当我们谈论机器翻译时，

01:00:20.065 --> 01:00:24.500
我们讨论了在

01:00:24.500 --> 01:00:29.930
2014年，机器翻译非常成功。

01:00:29.930 --> 01:00:32.680
但事实上，在它出现的前一年，

01:00:32.680 --> 01:00:34.855
嗯，有篇论文，嗯，

01:00:34.855 --> 01:00:41.255
Nal Kalchbrenner和Phil Blunsom在英国做神经机器翻译。

01:00:41.255 --> 01:00:44.015
实际上这是

01:00:44.015 --> 01:00:48.840
近代第一个神经机器翻译论文。

01:00:48.840 --> 01:00:50.400
如果你挖得够远，

01:00:50.400 --> 01:00:52.130
实际上有几个人试图利用

01:00:52.130 --> 01:00:54.135
机器翻译的神经网络

01:00:54.135 --> 01:00:58.445
在80年代和90年代，但这是第一次重新启动它，

01:00:58.445 --> 01:01:02.205
他们实际上并没有使用从SiC到SiC的体系结构。

01:01:02.205 --> 01:01:05.690
所以他们用的是编码器，

01:01:05.690 --> 01:01:08.490
他们使用卷积神经网络。

01:01:08.490 --> 01:01:13.430
所以他们有一堆卷积神经网络，逐渐缩小

01:01:13.430 --> 01:01:18.760
把输入往下拉，最后得到一个句子表示，

01:01:18.760 --> 01:01:22.960
然后他们用一个序列模型作为解码器。

01:01:22.960 --> 01:01:26.880
嗯，所以，嗯，你可以这样做

01:01:26.880 --> 01:01:30.520
尝试使用其他一些应用程序，

01:01:30.520 --> 01:01:33.800
使用卷积神经网络真的很容易。

01:01:33.800 --> 01:01:39.175
也有人研究使用卷积神经网络作为译码器，

01:01:39.175 --> 01:01:44.415
虽然这有点困难，让你的大脑周围，并没有使用太多。

01:01:44.415 --> 01:01:50.960
我想提的第二件事是，我们马上就要讨论了，

01:01:50.960 --> 01:01:57.305
到目前为止，我们已经对单词做了卷积模型，以便

01:01:57.305 --> 01:02:00.890
我们的谷粒正在有效地拾起

01:02:00.890 --> 01:02:06.050
这些单词是两个单词或三个单词子序列的n-gram单位。

01:02:06.050 --> 01:02:10.190
然后很快发展起来的想法也许很好

01:02:10.190 --> 01:02:14.705
在字符上使用卷积也很有用。

01:02:14.705 --> 01:02:17.105
所以，你可以运行卷积神经网络

01:02:17.105 --> 01:02:19.970
在文字上尝试和，

01:02:19.970 --> 01:02:22.640
嗯，生成一个单词嵌入，嗯，

01:02:22.640 --> 01:02:25.760
这个想法已经探索了很多，嗯，

01:02:25.760 --> 01:02:28.505
这是你们为完成任务要做的一部分

01:02:28.505 --> 01:02:31.715
五是建立一个字符级的convnet，

01:02:31.715 --> 01:02:35.180
嗯，为了改进机器翻译系统。

01:02:35.180 --> 01:02:40.250
我今天不想说太多关于这个的基础，嗯，

01:02:40.250 --> 01:02:44.270
因为星期四的讲座是在讨论子字模型

01:02:44.270 --> 01:02:49.055
我们将详细介绍不同型号的低音炮。

01:02:49.055 --> 01:02:53.300
但是，我想给你看一个骗局-一个复杂的

01:02:53.300 --> 01:02:58.010
卷积神经网络，也用于文本分类。

01:02:58.010 --> 01:03:01.685
所以，从本质上来说，这和尹金的模型是一样的。

01:03:01.685 --> 01:03:06.230
这个模型实际上是建立在字符之上的，

01:03:06.230 --> 01:03:07.700
它不是建立在文字之上的。

01:03:07.700 --> 01:03:10.640
所以，我们是它的基础，

01:03:10.640 --> 01:03:13.145
嗯，有个像词一样的模型。

01:03:13.145 --> 01:03:16.775
嗯，这是2017年的一篇论文，

01:03:16.775 --> 01:03:21.350
嗯，由，嗯，这里显示的四位作者，嗯，

01:03:21.350 --> 01:03:24.170
在Facebook人工智能研究中心工作的人，

01:03:24.170 --> 01:03:27.635
嗯，在法国，嗯，等等，

01:03:27.635 --> 01:03:30.320
他们有一个有趣的假设

01:03:30.320 --> 01:03:34.205
这篇论文本质上是说，你知道，

01:03:34.205 --> 01:03:42.530
到2017年，利用深度学习来实现愿景的人正在真正建立，

01:03:42.530 --> 01:03:47.600
很深的网络，发现它们很管用，

01:03:47.600 --> 01:03:49.790
更适合视觉任务。

01:03:49.790 --> 01:03:52.205
所以，从某种程度上说，

01:03:52.205 --> 01:03:58.490
突破是这些人一旦这些想法出现，

01:03:58.490 --> 01:04:04.445
它证明了这不仅仅是你可以建立一个六层或八层，

01:04:04.445 --> 01:04:07.580
嗯，视觉任务的卷积神经网络。

01:04:07.580 --> 01:04:09.200
你真的可以开始建造，

01:04:09.200 --> 01:04:14.270
非常深入的视觉任务网络，有几十个甚至数百个

01:04:14.270 --> 01:04:21.090
而且，这些模型在大量数据的训练下工作得更好。

01:04:21.210 --> 01:04:27.115
所以，嗯，如果这就是你脑子里的东西，然后你看，

01:04:27.115 --> 01:04:33.970
看看在自然语言处理中曾经发生过什么，现在确实发生了什么，

01:04:33.970 --> 01:04:36.410
观察结果是，你知道，

01:04:36.410 --> 01:04:38.390
这些全国人民党人有点可悲，

01:04:38.390 --> 01:04:43.550
他们声称他们正在进行深入学习，但他们仍在使用三层LSTM。

01:04:43.550 --> 01:04:46.475
当然，我们可以取得一些进展，嗯，

01:04:46.475 --> 01:04:53.735
通过建立类似视觉网络的深层网络并使用它们，

01:04:53.735 --> 01:04:57.035
嗯，对于自然语言处理目标。

01:04:57.035 --> 01:05:01.415
所以，这正是他们所说的。

01:05:01.415 --> 01:05:08.930
因此，他们设计并建造了一个非常深的网络，看起来像一个视觉堆栈，

01:05:08.930 --> 01:05:14.900
嗯，作为一个基于字符的卷积神经网络。

01:05:14.900 --> 01:05:20.660
嗯，所以，我这里有它的照片，但足够深，它适合它。

01:05:20.660 --> 01:05:23.390
这张幻灯片有一点可读性[笑声]

01:05:23.390 --> 01:05:26.150
是一个挑战，但我们可以尝试看看这个。

01:05:26.150 --> 01:05:27.260
所以，在底部，

01:05:27.260 --> 01:05:29.240
我们有文字，嗯，

01:05:29.240 --> 01:05:33.965
这是一个字符序列，所以，嗯，

01:05:33.965 --> 01:05:36.980
对于文本，嗯，所以，

01:05:36.980 --> 01:05:40.640
当人们在

01:05:40.640 --> 01:05:44.930
图片通常所有图片的大小相同。

01:05:44.930 --> 01:05:50.225
正确的。你可以把每张图片做成300像素乘300像素或者类似的东西。

01:05:50.225 --> 01:05:53.375
所以，它们对NLP也有同样的作用，嗯，

01:05:53.375 --> 01:05:55.490
它们有一个尺寸，嗯，

01:05:55.490 --> 01:05:59.690
文档的长度为1024个字符。

01:05:59.690 --> 01:06:03.710
如果它比这个长，他们会截断它并保留第一部分。

01:06:03.710 --> 01:06:06.470
如果它比那个短，他们就把它垫到

01:06:06.470 --> 01:06:11.315
1024号，然后他们会把它放进他们的牌堆里。

01:06:11.315 --> 01:06:15.440
所以，第一部分是每个角色，

01:06:15.440 --> 01:06:18.200
他们现在要学一个角色嵌入

01:06:18.200 --> 01:06:22.145
他们的人物嵌入是维度16。

01:06:22.145 --> 01:06:29.540
那么，这段文字现在是16乘以1024，嗯，那么，

01:06:29.540 --> 01:06:33.770
他们将把它通过一个卷积层

01:06:33.770 --> 01:06:38.210
您有三个和64个输出通道的内核大小。

01:06:38.210 --> 01:06:44.150
所以你现在有了一个64倍于1024的东西。

01:06:44.150 --> 01:06:47.900
你现在用卷积块把它粘起来。

01:06:47.900 --> 01:06:52.085
我将在下一张幻灯片中解释这个卷积块的细节，但是，

01:06:52.085 --> 01:06:56.360
你应该考虑一下我之前展示的Resnet图片

01:06:56.360 --> 01:07:01.310
可以是通过一些卷积，也可以使用这个可选的快捷方式。

01:07:01.310 --> 01:07:05.180
另一个resnet，另一个剩余块

01:07:05.180 --> 01:07:08.765
在那里你可以通过卷积是一个可选的捷径，

01:07:08.765 --> 01:07:15.020
嗯，然后他们以人们通常想象的方式进行本地池。

01:07:15.020 --> 01:07:17.990
所以，通常人们在视觉系统中做什么

01:07:17.990 --> 01:07:21.530
你是在缩小图像的尺寸，嗯，

01:07:21.530 --> 01:07:25.820
通过在每个方向上对维度进行二分之一的池。

01:07:25.820 --> 01:07:27.020
但与此同时，

01:07:27.020 --> 01:07:29.014
你在你的神经网络里这样做，

01:07:29.014 --> 01:07:31.715
你扩大了频道的数量，

01:07:31.715 --> 01:07:34.130
因此，你在

01:07:34.130 --> 01:07:38.105
同时使X中的通道变小，

01:07:38.105 --> 01:07:39.710
图像的Y大小。

01:07:39.710 --> 01:07:44.120
所以，除了这些一维卷积，它们做的完全一样。

01:07:44.120 --> 01:07:49.760
所以，在我们的1024字符中有64个通道之前，

01:07:49.760 --> 01:07:54.425
嗯，嵌入，嗯，文档。

01:07:54.425 --> 01:07:57.110
所以，现在我们合用，嗯，所以，

01:07:57.110 --> 01:08:03.605
我们将有512个位置，有点像成对的字符，

01:08:03.605 --> 01:08:06.440
嗯，但是我们现在有128个频道

01:08:06.440 --> 01:08:09.380
然后他们一遍又一遍地重复，对吗？

01:08:09.380 --> 01:08:11.690
所以，还有两个卷积块

01:08:11.690 --> 01:08:14.285
解释的更多，但他们是一些剩余的块。

01:08:14.285 --> 01:08:17.960
他们又把它放在一起，做同样的事情。

01:08:17.960 --> 01:08:21.305
现在有256个，

01:08:21.305 --> 01:08:26.900
像四个字符块的位置，有256个通道，

01:08:26.900 --> 01:08:31.460
嗯，我指的不够高，但他们又重复了一遍，然后又游泳了。

01:08:31.460 --> 01:08:33.590
所以，现在他们有了，嗯，

01:08:33.590 --> 01:08:36.710
128个位置，大约8个字符

01:08:36.710 --> 01:08:40.775
每一个都有512个通道。

01:08:40.775 --> 01:08:45.080
它们再次聚集，它们又有卷积块，嗯，

01:08:45.080 --> 01:08:47.570
看吧，因为我说过

01:08:47.570 --> 01:08:50.060
奇怪的想法会出现，就在那里，

01:08:50.060 --> 01:08:55.325
他们在做K max池，他们保持了8个最强的值，

01:08:55.325 --> 01:08:57.290
嗯，在每个频道。

01:08:57.290 --> 01:08:59.300
嗯，在那一点上，

01:08:59.300 --> 01:09:05.195
他们有512码8码的，嗯，所以，

01:09:05.195 --> 01:09:08.510
有点像八个字符序列中的八个

01:09:08.510 --> 01:09:11.705
被认为对分类很重要，

01:09:11.705 --> 01:09:15.455
保留，但按频道排序，共有512个频道

01:09:15.455 --> 01:09:19.475
然后通过三个完全连接的层。

01:09:19.475 --> 01:09:22.190
所以，典型的视觉系统在顶部

01:09:22.190 --> 01:09:25.355
在末端有两个完全连接的层，

01:09:25.355 --> 01:09:28.055
嗯，最后一个，

01:09:28.055 --> 01:09:31.835
实际上是向你的SoftMax进纸。

01:09:31.835 --> 01:09:36.080
所以，它的大小是

01:09:36.080 --> 01:09:41.330
可能是正-负两类的课程，不同于专题课。

01:09:41.330 --> 01:09:44.000
嗯，所以，是的，基本上就像

01:09:44.000 --> 01:09:47.180
一个视觉堆栈，但他们将把它用于语言。

01:09:47.180 --> 01:09:48.890
嗯，好吧。

01:09:48.890 --> 01:09:52.340
所以，我手上的解释是

01:09:52.340 --> 01:09:57.515
这些卷积块，但看起来有点像我们以前的照片，或者，

01:09:57.515 --> 01:09:59.975
嗯，部门稍微复杂一点。

01:09:59.975 --> 01:10:02.420
所以你在做，嗯，

01:10:02.420 --> 01:10:05.840
三号卷积块

01:10:05.840 --> 01:10:10.430
根据您在序列中的位置卷积一些通道。

01:10:10.430 --> 01:10:13.490
然后你就要像我们一样把它变成一个批量标准

01:10:13.490 --> 01:10:17.075
讨论了如何通过一个relu非线性，

01:10:17.075 --> 01:10:21.320
把这三件事都重复一遍或者记住

01:10:21.320 --> 01:10:25.550
是不是这种跳过的连接正好绕过了这个块的外部。

01:10:25.550 --> 01:10:31.190
所以这是一个残留的样式块，嗯，所以，

01:10:31.190 --> 01:10:34.550
这是一种复杂的体系结构，你可以把它组合起来

01:10:34.550 --> 01:10:38.675
如果你敢在PyTorch尝试你的最终项目。

01:10:38.675 --> 01:10:42.775
嗯，是的，嗯，所以，

01:10:42.775 --> 01:10:46.090
对于实验来说

01:10:46.090 --> 01:10:52.570
他们感兴趣并想强调的是

01:10:52.570 --> 01:10:55.670
这些传统的句子和

01:10:55.670 --> 01:10:58.970
文本分类数据集已在其他论文中使用。

01:10:58.970 --> 01:11:02.465
像Yoon Kim的论文实际上相当小。

01:11:02.465 --> 01:11:10.550
所以，像腐烂的番茄数据集实际上只有10000个例子，5000个，

01:11:10.550 --> 01:11:13.550
正5000负，他们有点

01:11:13.550 --> 01:11:17.180
需要像ImageNet一样

01:11:17.180 --> 01:11:20.435
深入学习的模式，真正展示他们的价值和远见

01:11:20.435 --> 01:11:24.155
这可能确实显示了这样一个巨大模型的价值。

01:11:24.155 --> 01:11:28.070
嗯，你需要有非常大的数据集。

01:11:28.070 --> 01:11:29.855
所以，它们会变得更大，

01:11:29.855 --> 01:11:32.000
嗯，文本分类数据集。

01:11:32.000 --> 01:11:36.065
这是亚马逊评论的正负数据集，嗯，

01:11:36.065 --> 01:11:39.500
他们有360万份文件，

01:11:39.500 --> 01:11:43.030
嗯，Yelp审查了65万份文件。

01:11:43.030 --> 01:11:45.100
这么大的数据集，

01:11:45.100 --> 01:11:48.230
嗯，这是他们的实验。

01:11:48.230 --> 01:11:50.930
可以。上面的数字，呃，

01:11:50.930 --> 01:11:55.940
对于印刷在文献中的最好的先前结果的不同数据集，

01:11:55.940 --> 01:11:58.640
如果你读到，嗯，

01:11:58.640 --> 01:12:03.200
脚注，嗯，有一些东西，他们想成为明星。

01:12:03.200 --> 01:12:07.040
所以，旁边有星星的人用

01:12:07.040 --> 01:12:13.225
他们不使用的外部同义词库。[噪音]

01:12:13.225 --> 01:12:15.640
杨氏法，嗯，

01:12:15.640 --> 01:12:18.610
使用一些我切断的特殊技术。

01:12:18.610 --> 01:12:21.580
嗯，还有一件事要提的是这些数字，

01:12:21.580 --> 01:12:24.175
它们是错误率，所以低是好的。

01:12:24.175 --> 01:12:26.410
嗯，所以你得到的越低越好。

01:12:26.410 --> 01:12:30.940
所以这些都是他们的结果。

01:12:30.940 --> 01:12:34.770
嗯，那么你能从这些结果中得到什么呢？

01:12:34.770 --> 01:12:39.545
嗯，你能注意到的第一件事就是这些结果，

01:12:39.545 --> 01:12:42.100
更深层次的网络运行得更好，对吗？

01:12:42.100 --> 01:12:44.845
所以，我给你看的那个，

01:12:44.845 --> 01:12:48.490
呃，好吧，不，我想我有这张照片的那张不是完整的。

01:12:48.490 --> 01:12:52.720
嗯，但它们有深度为9，17的，

01:12:52.720 --> 01:12:56.680
29在卷积层的数目上，

01:12:56.680 --> 01:13:01.150
最深的总是最有效的。

01:13:01.150 --> 01:13:04.255
所以，这就是深层网络的证明。

01:13:04.255 --> 01:13:07.570
嗯，这并没有持续下去，嗯，

01:13:07.570 --> 01:13:10.690
这里一个有趣的脚注是，

01:13:10.690 --> 01:13:11.935
嗯，我想他们认为，

01:13:11.935 --> 01:13:13.225
哦，这很酷。

01:13:13.225 --> 01:13:19.315
为什么我们不尝试一个更深层的，有47层，看看效果如何？

01:13:19.315 --> 01:13:23.635
而且，我的意思是，结果有点有趣。

01:13:23.635 --> 01:13:26.125
所以，对于47层，

01:13:26.125 --> 01:13:28.855
它的效果比这个差一点点。

01:13:28.855 --> 01:13:32.050
嗯，从某种意义上说你，

01:13:32.050 --> 01:13:37.900
他们显示了残余层的分类结果非常有效。

01:13:37.900 --> 01:13:40.705
所以，他们做了一个让我们试着训练的实验

01:13:40.705 --> 01:13:45.325
不使用剩余连接的47层网络。

01:13:45.325 --> 01:13:47.455
而且，情况更糟。

01:13:47.455 --> 01:13:49.870
数字下降了大约百分之二。

01:13:49.870 --> 01:13:52.825
他们训练了一个有残余连接的人，

01:13:52.825 --> 01:13:58.870
事实上，数字只差一点点。

01:13:58.870 --> 01:14:02.485
他们差了0.1%。

01:14:02.485 --> 01:14:05.515
所以，你知道，他们也差不多是在工作。

01:14:05.515 --> 01:14:10.300
但是，这和想象中的情况有点不同，

01:14:10.300 --> 01:14:15.145
因为对于人们在视觉上使用的那种残余网络，

01:14:15.145 --> 01:14:19.990
这有点像人们使用的最小深度。

01:14:19.990 --> 01:14:23.485
所以，如果你通常在视觉上使用剩余网络，

01:14:23.485 --> 01:14:25.915
您可以使用resnet-34。

01:14:25.915 --> 01:14:29.215
如果你的记忆力真的很差，想要一个小模型，

01:14:29.215 --> 01:14:32.980
但是你知道如果你使用resnet-50你会得到更好的结果，

01:14:32.980 --> 01:14:36.730
事实上，如果你使用resnet-101，它会更好地工作。

01:14:36.730 --> 01:14:39.625
嗯，所以你知道，

01:14:39.625 --> 01:14:41.410
是否与

01:14:41.410 --> 01:14:44.350
语言或数据量等，

01:14:44.350 --> 01:14:47.915
你还没有达到你在视觉上所能达到的深度。

01:14:47.915 --> 01:14:50.620
嗯，但其他结果，嗯，

01:14:50.620 --> 01:14:54.190
所以他们在这里比较的另一件事是他们在比较

01:14:54.190 --> 01:14:59.245
用三种不同的方法把东西串起来。

01:14:59.245 --> 01:15:02.950
所以，你可以用，嗯，

01:15:02.950 --> 01:15:06.715
回旋的步伐，

01:15:06.715 --> 01:15:09.745
您可以使用本地MaxPooling，

01:15:09.745 --> 01:15:12.805
你可以使用kmaxpooling。

01:15:12.805 --> 01:15:14.350
嗯，他们是将军，

01:15:14.350 --> 01:15:16.945
正如你所看到的，它们的数字略有不同。

01:15:16.945 --> 01:15:19.990
每个人都赢了，还有一个，

01:15:19.990 --> 01:15:23.890
这些数据集中的至少一个或实际上至少两个。

01:15:23.890 --> 01:15:27.430
但MaxPooling不仅赢得了四个数据集，

01:15:27.430 --> 01:15:29.890
如果你看看数字，

01:15:29.890 --> 01:15:32.245
MaxPooling总是做得很好。

01:15:32.245 --> 01:15:34.495
因为MaxPooling在这里做得很好，

01:15:34.495 --> 01:15:38.140
虽然回旋步幅效果不好，

01:15:38.140 --> 01:15:41.605
在这里，MaxPooling工作得很好，

01:15:41.605 --> 01:15:45.685
而且，嗯，K现金池的运作有点糟糕。

01:15:45.685 --> 01:15:50.890
所以，他们最后的建议是你应该经常使用，嗯，

01:15:50.890 --> 01:15:53.680
只是简单的MaxPooling，

01:15:53.680 --> 01:15:55.389
看起来不错，

01:15:55.389 --> 01:15:57.340
嗯，没有别的了。

01:15:57.340 --> 01:16:01.315
嗯，这真的值得你费心考虑去做。

01:16:01.315 --> 01:16:10.540
可以。嗯，我还有什么其他的结论要说吗？

01:16:10.540 --> 01:16:13.285
可以。嗯，我想大部分都是这样。

01:16:13.285 --> 01:16:17.440
我想他们总的意思是你可以做得非常好，嗯，

01:16:17.440 --> 01:16:20.470
使用convnets的文本分类系统，

01:16:20.470 --> 01:16:22.630
你应该把这个信息拿走。

01:16:22.630 --> 01:16:26.170
可以。所以，只剩下几分钟了。

01:16:26.170 --> 01:16:30.145
还有一件事我想提，

01:16:30.145 --> 01:16:33.505
但我想我会很快提到它，

01:16:33.505 --> 01:16:36.505
如果你想的话，你可以看得更详细。

01:16:36.505 --> 01:16:38.785
所以，我们有这种情况

01:16:38.785 --> 01:16:44.065
重复出现的神经网络是NLP的标准组成部分，

01:16:44.065 --> 01:16:49.405
但他们有一个很大的问题，就是不能很好地进行比较。

01:16:49.405 --> 01:16:53.800
我们获得快速计算深度学习的方法是

01:16:53.800 --> 01:16:58.180
这些东西很好地并行，这样我们就可以把它们粘在GPU上。

01:16:58.180 --> 01:17:05.425
只有当GPU可以同时进行多次相同的计算时，它们才是快速的，

01:17:05.425 --> 01:17:08.440
这对于卷积神经网络来说是微不足道的，

01:17:08.440 --> 01:17:13.225
因为准确地说，你在做同样的计算-计算每个位置。

01:17:13.225 --> 01:17:17.740
但这并不是经常性神经网络发生的事情，因为你必须

01:17:17.740 --> 01:17:19.990
算出位置1的值

01:17:19.990 --> 01:17:22.930
在开始计算位置2的值之前，

01:17:22.930 --> 01:17:26.320
用于位置3的值。

01:17:26.320 --> 01:17:28.975
嗯，这是一项工作，嗯，

01:17:28.975 --> 01:17:33.030
有时由CS224N共同导师完成

01:17:33.030 --> 01:17:37.620
Richard Socher和Salesforce Research的一些人

01:17:37.620 --> 01:17:40.110
一句话，我们如何才能两全其美？

01:17:40.110 --> 01:17:43.485
我们怎么能得到像

01:17:43.485 --> 01:17:49.650
循环神经网络，但不具有坏的计算性能？

01:17:49.650 --> 01:17:53.160
所以他们的想法是，

01:17:53.160 --> 01:18:00.550
而不是做标准的LSTM样式的计算，你知道，

01:18:00.550 --> 01:18:07.090
根据前一个时间段更新的候选值和您的门，

01:18:07.090 --> 01:18:13.510
也许我们可以做的是在时间之间建立一种关系

01:18:13.510 --> 01:18:20.155
负1和时间进入卷积神经网络的最大池层。

01:18:20.155 --> 01:18:26.260
所以，我们在计算一个候选门、一个遗忘门和一个输出门。

01:18:26.260 --> 01:18:30.700
但这些，这些候选人还有，嗯，

01:18:30.700 --> 01:18:38.500
门控值通过计算在池层内完成，

01:18:38.500 --> 01:18:44.350
嗯，通过，嗯，呃，呃，卷积运算。

01:18:44.350 --> 01:18:46.150
所以，这有点像，

01:18:46.150 --> 01:18:47.650
它不，它，你知道，

01:18:47.650 --> 01:18:53.065
如果没有免费的午餐，你就不能真正的再发生，也不能支付罚金。

01:18:53.065 --> 01:18:56.764
这给了你一种假象重现，因为你

01:18:56.764 --> 01:19:02.244
在每个时间片上模拟相邻元素之间的关联，

01:19:02.244 --> 01:19:06.310
但它只是在当地制定的，而不是被推进，

01:19:06.310 --> 01:19:08.200
嗯，在一层。

01:19:08.200 --> 01:19:10.240
但他们发现的是，

01:19:10.240 --> 01:19:14.320
如果你利用这个想法使你的人际网络更深入，

01:19:14.320 --> 01:19:15.970
那么，你又开始了，

01:19:15.970 --> 01:19:18.010
扩大你的影响力。

01:19:18.010 --> 01:19:22.090
所以，你得到了一定数量的信息。

01:19:22.090 --> 01:19:25.330
他们的结论是你可以

01:19:25.330 --> 01:19:28.870
建立这些模型并让它们工作，

01:19:28.870 --> 01:19:32.050
你知道，在这张幻灯片上不一定更好，

01:19:32.050 --> 01:19:33.625
嗯，它通常说得更好。

01:19:33.625 --> 01:19:37.540
嗯，你可以让他们像LSTM那样工作，

01:19:37.540 --> 01:19:41.650
但是你可以让他们更快的工作，因为你在逃避

01:19:41.650 --> 01:19:46.555
标准的循环操作，并将其保持为您可以并行的操作，

01:19:46.555 --> 01:19:49.945
嗯，在MaxPooling操作中。

01:19:49.945 --> 01:19:53.035
嗯，是的，所以这是一种

01:19:53.035 --> 01:19:57.250
一种有趣的替代方法，尝试获得一些好处。

01:19:57.250 --> 01:20:01.825
我认为从长远来看，这不是最终胜出的想法。

01:20:01.825 --> 01:20:05.740
下周我们将讨论变压器网络，

01:20:05.740 --> 01:20:09.655
这似乎是目前获得最多动力的想法。

01:20:09.655 --> 01:20:12.830
Okay. I'll stop there for today. Thanks a lot.

