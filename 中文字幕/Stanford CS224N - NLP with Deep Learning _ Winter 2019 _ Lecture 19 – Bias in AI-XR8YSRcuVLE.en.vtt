WEBVTT
Kind: captions
Language: en

00:00:04.460 --> 00:00:07.800
Okay. Hi everyone, uh, let's get started.

00:00:07.800 --> 00:00:10.935
Um, so Chris is traveling this week so he's not here.

00:00:10.935 --> 00:00:13.320
But I'm very excited to say that today we've got

00:00:13.320 --> 00:00:17.640
Margaret Mitchell who is a Senior Research Scientist at Google AI.

00:00:17.640 --> 00:00:20.435
She's going to tell us about, uh, the latest

00:00:20.435 --> 00:00:23.625
work defining and understanding and improving

00:00:23.625 --> 00:00:27.255
the situation with bias in artificial intelligence.

00:00:27.255 --> 00:00:30.395
Uh, Margaret has a background working in NLP and deep learning,

00:00:30.395 --> 00:00:33.440
so I'm really interested to hear what she has to say today. Take it away.

00:00:33.440 --> 00:00:36.620
Great, thank you. Um, can you guys hear me okay?

00:00:36.620 --> 00:00:39.230
I'm not sure if this mic is exactly picking up my voice,

00:00:39.230 --> 00:00:41.300
everything's cool? Okay, cool.

00:00:41.300 --> 00:00:44.285
Um, so this work is, uh,

00:00:44.285 --> 00:00:46.250
the product of a ton of different people and

00:00:46.250 --> 00:00:48.520
collaborators that I've tried to put up here.

00:00:48.520 --> 00:00:53.330
Um, some students at Stanford also Johns Hopkins, Google,

00:00:53.330 --> 00:00:58.820
Facebook and Microsoft are all represented, cool.

00:00:58.820 --> 00:01:05.915
So, um, for those of you who haven't seen the set of slides before,

00:01:05.915 --> 00:01:08.480
what do you see here? Just shout it out.

00:01:08.480 --> 00:01:10.130
Bananas.

00:01:10.130 --> 00:01:12.130
Bananas. Okay what else?

00:01:12.130 --> 00:01:13.345
Stickers.

00:01:13.345 --> 00:01:14.570
Stickers. What else?

00:01:14.570 --> 00:01:20.810
[NOISE] Shelves. What else?

00:01:20.810 --> 00:01:21.995
Bunches of bananas.

00:01:21.995 --> 00:01:24.540
Bunches of bananas. What else?

00:01:24.580 --> 00:01:28.025
Yellow, ripe bananas.

00:01:28.025 --> 00:01:30.500
You said ripe bananas, good.

00:01:30.500 --> 00:01:34.730
So you can add [LAUGHTER] bananas with stickers on them.

00:01:34.730 --> 00:01:37.400
You can start doing, like, embedded clauses, you know,

00:01:37.400 --> 00:01:41.180
bunches of bananas with stickers on them on shelves in the store to get, kinda, crazy.

00:01:41.180 --> 00:01:44.875
But we don't tend to say yellow bananas, right?

00:01:44.875 --> 00:01:47.160
So given something like this,

00:01:47.160 --> 00:01:51.455
we might say green bananas or we might say unripe bananas.

00:01:51.455 --> 00:01:55.130
Given an image like this we might say ripe bananas or,

00:01:55.130 --> 00:01:57.340
uh, bananas with spots on them.

00:01:57.340 --> 00:02:00.935
Uh, if you're me, you might say bananas that are good for banana bread.

00:02:00.935 --> 00:02:05.240
Um, but given an image like this or something like this in the real world,

00:02:05.240 --> 00:02:07.955
we tend not to mention the yellowness.

00:02:07.955 --> 00:02:12.710
And the reason for this is because yellow is prototypical for bananas.

00:02:12.710 --> 00:02:15.545
So the idea of prototypes, uh,

00:02:15.545 --> 00:02:18.620
stems from prototype theory which goes back to the early '70s,

00:02:18.620 --> 00:02:21.795
uh, coming out of the work of Eleanor Rosch and colleagues.

00:02:21.795 --> 00:02:23.750
Um, and it's this idea that there are

00:02:23.750 --> 00:02:28.335
some stored central prototypical notions of objects,

00:02:28.335 --> 00:02:31.559
um, that we access as we're operating,

00:02:31.559 --> 00:02:33.065
uh, throughout the world.

00:02:33.065 --> 00:02:36.950
There's some disagreement about whether these prototypes are

00:02:36.950 --> 00:02:42.320
actual exemplars of objects or something like a distribution over what's likely,

00:02:42.320 --> 00:02:45.440
but there is general agreement that we do have some, sort of,

00:02:45.440 --> 00:02:49.340
sense of what's typical and what's a typical of the things in

00:02:49.340 --> 00:02:55.200
the world and we tend to notice and talk about the things that are atypical.

00:02:55.400 --> 00:02:59.330
Um, so this is a riddle that I

00:02:59.330 --> 00:03:02.710
heard in middle school that worked a little bit more at that time,

00:03:02.710 --> 00:03:04.895
um, some of you might have heard it before.

00:03:04.895 --> 00:03:06.350
A man and his son are in

00:03:06.350 --> 00:03:09.875
a terrible accident and are rushed to the hospital in critical care.

00:03:09.875 --> 00:03:12.245
The doctor looks at the boy and exclaims,

00:03:12.245 --> 00:03:13.775
"I can't operate on this boy,

00:03:13.775 --> 00:03:17.070
he's my son," How could this be? [NOISE].

00:03:17.070 --> 00:03:19.190
Two dads?

00:03:19.190 --> 00:03:22.905
Two dads or he has a mum who's a doctor, right.

00:03:22.905 --> 00:03:25.920
Otherwise known as a female doctor,

00:03:25.920 --> 00:03:29.090
which might be contracted- contrasted with doctor.

00:03:29.090 --> 00:03:31.920
Um, in a study they did,

00:03:31.920 --> 00:03:33.120
uh, when they first, sort of,

00:03:33.120 --> 00:03:36.450
put forward this riddle at Boston University,

00:03:36.450 --> 00:03:38.375
they found that the majority of test subjects

00:03:38.375 --> 00:03:41.510
overlooked the possibility that the doctor could be a she.

00:03:41.510 --> 00:03:45.695
And that included men, women and self-described feminists.

00:03:45.695 --> 00:03:46.940
So the point is that,

00:03:46.940 --> 00:03:49.100
these, kinds of, uh,

00:03:49.100 --> 00:03:52.880
ways of talking about things and assumptions that we make,

00:03:52.880 --> 00:03:58.070
aren't necessarily something that speaks to a negative intent,

00:03:58.070 --> 00:04:01.670
but something that speaks to how we actually store representations in

00:04:01.670 --> 00:04:05.600
our minds and how we access those representations as we interact,

00:04:05.600 --> 00:04:07.285
uh, in the world.

00:04:07.285 --> 00:04:11.825
So this, uh, this affects what we can learn when we're learning from text.

00:04:11.825 --> 00:04:15.395
So, um, this is work from 2013,

00:04:15.395 --> 00:04:17.300
where they took a look at what was, sort of,

00:04:17.300 --> 00:04:21.355
most likely, what would you learn if you were just learning from raw text,

00:04:21.355 --> 00:04:24.440
um, what were some things that were common in the world?

00:04:24.440 --> 00:04:28.070
Um, they found that in this setup

00:04:28.070 --> 00:04:32.090
something like murdering was ten times more likely than blinking.

00:04:32.090 --> 00:04:34.850
And the reason for this is because people tend

00:04:34.850 --> 00:04:38.150
not to mention these typical things that go without saying.

00:04:38.150 --> 00:04:42.500
We don't tend to mention things like blinking and breathing,

00:04:42.500 --> 00:04:47.250
but we do mention atypical events like murder and that affects the, kind of,

00:04:47.250 --> 00:04:50.960
things a machine can learn from texts that we put out in the world,

00:04:50.960 --> 00:04:52.490
because it's been subject to all of

00:04:52.490 --> 00:04:57.135
these filtering processes that we have as humans before we, uh, communicate.

00:04:57.135 --> 00:05:01.690
Um, this issue in particular is known as Human Reporting Bias.

00:05:01.690 --> 00:05:04.010
Which is that the frequency with which people write

00:05:04.010 --> 00:05:06.650
about actions, outcomes or properties,

00:05:06.650 --> 00:05:09.470
is not a reflection of real-world frequencies or

00:05:09.470 --> 00:05:13.010
the degree to which a property is characteristic of a class of individuals,

00:05:13.010 --> 00:05:14.990
but says a lot more about how we're actually

00:05:14.990 --> 00:05:18.205
processing the world and what we think is remarkable.

00:05:18.205 --> 00:05:22.350
So this affects everything a system can learn.

00:05:22.350 --> 00:05:24.620
Um, in a typical machine learning paradigm,

00:05:24.620 --> 00:05:29.210
one of the first steps is to collect and potentially annotate training data.

00:05:29.210 --> 00:05:32.230
From there a model can be trained,

00:05:32.230 --> 00:05:34.965
uh, from there, uh,

00:05:34.965 --> 00:05:38.340
media can be filtered rank- ranked, aggregated,

00:05:38.340 --> 00:05:39.900
generated in some way,

00:05:39.900 --> 00:05:42.985
um, and from there people see the output.

00:05:42.985 --> 00:05:46.699
And we like to think of this as a relatively straightforward pipeline,

00:05:46.699 --> 00:05:49.100
um, but at the very start, uh,

00:05:49.100 --> 00:05:51.080
even before we're collecting the data,

00:05:51.080 --> 00:05:52.895
actually within the data itself,

00:05:52.895 --> 00:05:55.940
are a host of different kinds of human biases.

00:05:55.940 --> 00:05:58.580
So things like stereotyping, things like prejudice,

00:05:58.580 --> 00:06:03.080
things like racism and that's embedded within the data before we collect it.

00:06:03.080 --> 00:06:05.600
Then as we collect and annotate data,

00:06:05.600 --> 00:06:07.745
further biases become introduced.

00:06:07.745 --> 00:06:11.815
So things like sampling errors, confirmation bias, um,

00:06:11.815 --> 00:06:15.050
uh, in-group bias and out-group bias and I'll talk about these,

00:06:15.050 --> 00:06:16.070
um, a little bit.

00:06:16.070 --> 00:06:19.625
Oh, and I should mention feel free to ask questions as I go,

00:06:19.625 --> 00:06:21.380
um, totally fine to just,

00:06:21.380 --> 00:06:23.825
kind of, interact, uh, throughout.

00:06:23.825 --> 00:06:27.290
So here are some of the biases that I think are

00:06:27.290 --> 00:06:30.320
relatively important for work in AI and machine learning.

00:06:30.320 --> 00:06:32.570
There's hundreds you can go into,

00:06:32.570 --> 00:06:34.370
um, but some of the ones that I've, sort of,

00:06:34.370 --> 00:06:36.750
become the most aware of working in this space,

00:06:36.750 --> 00:06:39.680
um, are these sets and I'll go through each of these a bit.

00:06:39.680 --> 00:06:42.140
Um, so I talked about reporting bias earlier,

00:06:42.140 --> 00:06:45.470
which is, uh, which affects what we can learn from text.

00:06:45.470 --> 00:06:48.890
Um, another example of a kind

00:06:48.890 --> 00:06:52.135
of bias that really affects what we can learn from text is selection bias.

00:06:52.135 --> 00:06:54.835
So, uh, a lot of times that we,

00:06:54.835 --> 00:06:57.440
a lot of times when we get data annotated we'd use something

00:06:57.440 --> 00:07:00.230
like Amazon's Mechanical Turk, um,

00:07:00.230 --> 00:07:03.890
and the distribution of workers across the world is not even, sort of,

00:07:03.890 --> 00:07:06.485
uniform distribution, it's actually, um,

00:07:06.485 --> 00:07:09.920
concentrated in India, the US and then some in Europe.

00:07:09.920 --> 00:07:12.065
So this leaves out South America,

00:07:12.065 --> 00:07:13.250
this leaves out Africa,

00:07:13.250 --> 00:07:16.220
this leaves out a lot of China and that affects the, kind of,

00:07:16.220 --> 00:07:20.640
things that we'll be able to learn about the world when we have things annotated.

00:07:20.920 --> 00:07:25.250
Um, another kind of bias is Out-group Homogeneity Bias,

00:07:25.250 --> 00:07:29.405
which is the tendency to see out-group members as more alike than in-group members.

00:07:29.405 --> 00:07:32.420
And this is gonna affect what people are able to describe

00:07:32.420 --> 00:07:35.575
and talk about when they're annotating things such as emotion.

00:07:35.575 --> 00:07:38.565
So, uh, so for example we have these two, like,

00:07:38.565 --> 00:07:42.265
adorable puppies on the left here and they're looking at these four cats.

00:07:42.265 --> 00:07:44.715
Um, these are all different black cats,

00:07:44.715 --> 00:07:46.200
very different in different ways,

00:07:46.200 --> 00:07:50.825
but the two puppies look at the cats and they see four cats basically the same.

00:07:50.825 --> 00:07:53.720
And it's kind of trivial to understand how that also extends to

00:07:53.720 --> 00:07:56.795
human cognition and how we also process people.

00:07:56.795 --> 00:07:59.869
Um, it's this- it's the sense we have that the,

00:07:59.869 --> 00:08:01.915
the cohort that we're in,

00:08:01.915 --> 00:08:03.560
the people that we interact with,

00:08:03.560 --> 00:08:06.230
those are the kinds of people that are nuanced and

00:08:06.230 --> 00:08:08.930
everybody else is somehow less nuanced,

00:08:08.930 --> 00:08:10.715
has less detail to them.

00:08:10.715 --> 00:08:14.870
It's a trick our minds play on us in order to help us process the world,

00:08:14.870 --> 00:08:19.380
but it affects how we talk about it and it affects further how we annotate it.

00:08:19.930 --> 00:08:24.055
Um, this leads to stuff like biased data representations.

00:08:24.055 --> 00:08:27.170
So it's possible that you have an appropriate amount of data for

00:08:27.170 --> 00:08:30.710
every possible human group you can think of in your data,

00:08:30.710 --> 00:08:32.945
um, but it might be the case that some groups

00:08:32.945 --> 00:08:35.090
are represented less positively than others.

00:08:35.090 --> 00:08:37.190
And if we have time I'll go into, uh,

00:08:37.190 --> 00:08:39.970
a long- a longer example of that.

00:08:39.970 --> 00:08:43.045
Um, it also leads to things like biased labels.

00:08:43.045 --> 00:08:46.160
So, um, this is an issue that came up when we were

00:08:46.160 --> 00:08:49.580
getting some annotations for Inclusive Images competition,

00:08:49.580 --> 00:08:54.605
asking people to annotate things like bride and wedding and groom.

00:08:54.605 --> 00:08:57.100
And we found that given three different kinds of bride,

00:08:57.100 --> 00:08:58.685
wedding and groom images,

00:08:58.685 --> 00:09:03.465
um, ones that were more Western, European American, uh,

00:09:03.465 --> 00:09:06.985
got the appropriate labels and ones that weren't,

00:09:06.985 --> 00:09:09.190
just got, sort of, more generic person,

00:09:09.190 --> 00:09:11.235
kinds of, labels, uh,

00:09:11.235 --> 00:09:17.960
not able to actually tease out what's actually happening in these images.

00:09:17.960 --> 00:09:21.160
Compounding this issue are biases in interpretation

00:09:21.160 --> 00:09:24.145
when the model outputs, uh, its decisions.

00:09:24.145 --> 00:09:27.820
So, um, one, one issue is confirmation bias,

00:09:27.820 --> 00:09:30.820
which is the tendency to search for, interpret, favor,

00:09:30.820 --> 00:09:34.180
recall information in a way that confirms preexisting beliefs.

00:09:34.180 --> 00:09:36.520
And so a lot of times when we, uh,

00:09:36.520 --> 00:09:40.120
build end-to-end systems and try and test our hypotheses,

00:09:40.120 --> 00:09:42.325
we're kind of just testing it towards, uh,

00:09:42.325 --> 00:09:46.525
things that we want to be true and analyzing the results in a way that will,

00:09:46.525 --> 00:09:49.000
uh, help confirm what we want to be true.

00:09:49.000 --> 00:09:52.120
Um, overgeneralization, which is coming to

00:09:52.120 --> 00:09:55.765
a conclusion based on information that's too general or not specific enough.

00:09:55.765 --> 00:09:58.165
Um, this is an issue that happens a lot of times

00:09:58.165 --> 00:10:01.555
in the analysis of deep learning model results um,

00:10:01.555 --> 00:10:03.760
where it's assumed that there's,

00:10:03.760 --> 00:10:05.470
there's some kind of general, uh,

00:10:05.470 --> 00:10:08.515
conclusion that can be taken away when really it's actually just,

00:10:08.515 --> 00:10:10.615
uh, an effect of really skewed data.

00:10:10.615 --> 00:10:13.900
Um, this is also closely related to overfitting which

00:10:13.900 --> 00:10:17.275
is kind of the machine learning version of overgeneralization,

00:10:17.275 --> 00:10:20.050
which is where you're still making predictions and outcomes,

00:10:20.050 --> 00:10:23.965
but it's based on a small set of possible features, um,

00:10:23.965 --> 00:10:28.600
so it's not actually capturing the space of the correct features for the outcome,

00:10:28.600 --> 00:10:31.795
uh, the desired output prediction correctly.

00:10:31.795 --> 00:10:35.320
Um, there's also a correlation fallacy,

00:10:35.320 --> 00:10:37.975
which is confusing correlation with causation.

00:10:37.975 --> 00:10:40.510
And this happens a lot again in talking about what

00:10:40.510 --> 00:10:41.920
machine learning models are learning and

00:10:41.920 --> 00:10:44.575
deep learning models are learning in particular, um,

00:10:44.575 --> 00:10:47.095
where just because things happen together,

00:10:47.095 --> 00:10:49.240
doesn't mean that one is causing the other,

00:10:49.240 --> 00:10:50.770
but, uh, models don't tell you

00:10:50.770 --> 00:10:52.900
anything- deep learning models directly don't

00:10:52.900 --> 00:10:55.150
tell you anything about the causal relations.

00:10:55.150 --> 00:10:58.030
And so it's easy to think that some output that is predicted

00:10:58.030 --> 00:11:01.195
based on a correlation is actually something that's causal,

00:11:01.195 --> 00:11:03.310
and I'll talk about some examples of this too.

00:11:03.310 --> 00:11:07.120
Um, a further issue is automation bias,

00:11:07.120 --> 00:11:10.870
and this really affects the machine learning models we put out there in the world that

00:11:10.870 --> 00:11:15.010
then get used by people in systems like justice systems.

00:11:15.010 --> 00:11:17.215
Um, so that's the tendency to, um,

00:11:17.215 --> 00:11:19.750
favor the suggestions of

00:11:19.750 --> 00:11:24.535
automatic predictions of models that output predictions over the,

00:11:24.535 --> 00:11:27.880
um, uh, over the different kinds of um,

00:11:27.880 --> 00:11:29.740
suggestions of another human.

00:11:29.740 --> 00:11:33.070
Um, and this happens even in the face of contradictory evidence.

00:11:33.070 --> 00:11:36.085
So, if a system is telling you, you know, "This,

00:11:36.085 --> 00:11:40.810
this is the score or this is the risk of this individual",

00:11:40.810 --> 00:11:45.970
then we're more likely to think it's true because it came out of a mathematical system,

00:11:45.970 --> 00:11:48.850
and we automatically sort of see this as something more objective,

00:11:48.850 --> 00:11:50.860
something more mathematical, that something's going to

00:11:50.860 --> 00:11:53.440
be more true than humans some- somehow.

00:11:53.440 --> 00:11:56.155
Um, and that's automation bias.

00:11:56.155 --> 00:11:58.915
So, um, rather than this kind of

00:11:58.915 --> 00:12:02.215
clean straightforward pipeline that we have in machine learning,

00:12:02.215 --> 00:12:06.430
um, we have human bias coming in at the very start in the data, um,

00:12:06.430 --> 00:12:10.570
and then human bias coming in in data collection, annotation,

00:12:10.570 --> 00:12:14.290
and then further getting propagated through the system as we train on that data,

00:12:14.290 --> 00:12:17.605
um, as we start putting outputs based on that data,

00:12:17.605 --> 00:12:19.195
as people act on that data.

00:12:19.195 --> 00:12:21.385
And this creates a feedback loop where

00:12:21.385 --> 00:12:25.555
the kinds of things that we output for people to act on,

00:12:25.555 --> 00:12:27.520
um, are then, are then,

00:12:27.520 --> 00:12:31.345
then serves as further training data for input into your system,

00:12:31.345 --> 00:12:36.640
so you end up amplifying even further these different kinds of implicit biases.

00:12:36.640 --> 00:12:41.935
This is known as a Bias Network Effect or Bias "Laundering", I like to call it.

00:12:41.935 --> 00:12:47.530
And so, the message is that human data perpetuates human biases.

00:12:47.530 --> 00:12:51.400
And then as as machine learning or deep learning learns from human data,

00:12:51.400 --> 00:12:54.025
the results is a bias network effect.

00:12:54.025 --> 00:13:01.210
So, I want to steer clear of the idea that if I say bias or someone says bias that equals bad,

00:13:01.210 --> 00:13:03.130
it's a little bit more nuanced than that.

00:13:03.130 --> 00:13:05.650
Um, so there are all kinds of things that people

00:13:05.650 --> 00:13:08.155
mean when they're talking about bias, um,

00:13:08.155 --> 00:13:12.415
and even the same bias can be good in some situations and bad in some situations,

00:13:12.415 --> 00:13:14.590
so bias in statistics and ML.

00:13:14.590 --> 00:13:17.410
Um, we, we talk about the bias of an estimator which is

00:13:17.410 --> 00:13:20.980
the difference between the predictions and the truth, the ground truth.

00:13:20.980 --> 00:13:24.085
Uh, we talk about the bias term in linear regression.

00:13:24.085 --> 00:13:26.649
Um, we also have cognitive biases,

00:13:26.649 --> 00:13:28.210
and I talked about that in the beginning,

00:13:28.210 --> 00:13:30.220
and not all of those are negative or,

00:13:30.220 --> 00:13:31.675
or have to be, uh,

00:13:31.675 --> 00:13:33.400
or have to be seen as negative.

00:13:33.400 --> 00:13:36.430
So optimism is another kind of bias that we can

00:13:36.430 --> 00:13:39.640
have that affects our worldview and the way we sort of process things.

00:13:39.640 --> 00:13:42.250
Um, and even things like recency bias and

00:13:42.250 --> 00:13:45.760
confirmation bias are just ways that our minds can like, um,

00:13:45.760 --> 00:13:50.080
handle the combinatorial explosion of all the different things that can be

00:13:50.080 --> 00:13:52.030
true in the world and put it down to something

00:13:52.030 --> 00:13:55.060
tractable that we can sort of operate with in the real world.

00:13:55.060 --> 00:13:58.345
Um, so algorithmic bias is what a lot

00:13:58.345 --> 00:14:01.645
of people mean in headlines and whatnot when we're talking about bias,

00:14:01.645 --> 00:14:03.790
which is, uh, more about unjust,

00:14:03.790 --> 00:14:07.510
unfair or prejudicial treatment of people that's an output of,

00:14:07.510 --> 00:14:09.610
a automated decision system.

00:14:09.610 --> 00:14:12.505
Um, and the focus here is really on, uh,

00:14:12.505 --> 00:14:15.460
unjust, unfair or prejudicial treatment of people.

00:14:15.460 --> 00:14:19.315
So, a lot of the work in this space right now is focusing on trying to understand,

00:14:19.315 --> 00:14:22.480
what does it mean to be unjust from an algorithm,

00:14:22.480 --> 00:14:25.570
what does it mean to be unfair from an algorithm,

00:14:25.570 --> 00:14:27.190
and how can we handle this,

00:14:27.190 --> 00:14:30.490
how can we sort of mitigate these issues in order to be able to keep

00:14:30.490 --> 00:14:34.675
developing technology that's useful for people without worsening social divides.

00:14:34.675 --> 00:14:41.170
Um, and I felt the Guardian put it really well a few years ago.

00:14:41.170 --> 00:14:45.295
Um, they said, "Although neural networks might be said to write their own programs,

00:14:45.295 --> 00:14:50.125
they do so towards goals set by humans using data collected for human purposes.

00:14:50.125 --> 00:14:52.420
If the data is skewed, even by accident,

00:14:52.420 --> 00:14:54.865
the computers will amplify injustice."

00:14:54.865 --> 00:14:58.540
And it really keyed in on this amplify injustice idea.

00:14:58.540 --> 00:15:01.555
Um, and let's talk about what that can mean.

00:15:01.555 --> 00:15:05.680
So, one of the avenues of deep learning research that's

00:15:05.680 --> 00:15:09.415
taken off in the past few years is predicting criminal behavior.

00:15:09.415 --> 00:15:14.020
Um, so, um, how many of you are familiar with Predictive Policing?

00:15:14.020 --> 00:15:17.995
[NOISE] Okay, like, half of the class.

00:15:17.995 --> 00:15:22.345
Okay. So, in predictive policing, algorithms, um,

00:15:22.345 --> 00:15:28.675
predict areas to deploy officers where crime is considered to be likely to occur.

00:15:28.675 --> 00:15:33.520
But the data that the- the- the models are trained off

00:15:33.520 --> 00:15:38.935
of is based on where police officers have already gone and made arrests.

00:15:38.935 --> 00:15:42.520
So, the systems are simply learning the patterns of bias that

00:15:42.520 --> 00:15:46.900
humans have and where do they go and where they are trying to decide to def- uh,

00:15:46.900 --> 00:15:48.820
to find crime, um,

00:15:48.820 --> 00:15:50.425
and then reflecting them back.

00:15:50.425 --> 00:15:52.930
So, because this system hones in on some of

00:15:52.930 --> 00:15:56.125
the top spots where people have been arrested,

00:15:56.125 --> 00:15:57.940
notice that's not the same of- that's

00:15:57.940 --> 00:16:00.490
the same thing as where crimes have been committed, right?

00:16:00.490 --> 00:16:02.440
It's where arrests have been made.

00:16:02.440 --> 00:16:04.720
Um, it means that the other areas that

00:16:04.720 --> 00:16:07.240
might be explored for crime don't get explored at all.

00:16:07.240 --> 00:16:09.175
That worsens the situation.

00:16:09.175 --> 00:16:10.795
Um, some neighborhoods, uh,

00:16:10.795 --> 00:16:14.080
get really acutely focused attention on them,

00:16:14.080 --> 00:16:16.810
and that heightens the chances of serious repercussions for

00:16:16.810 --> 00:16:19.735
even minor infractions, that means arrests.

00:16:19.735 --> 00:16:22.060
And that means a feedback loop of data that

00:16:22.060 --> 00:16:25.400
you will get an arrest in this place if you go there.

00:16:25.470 --> 00:16:33.085
Um, another, uh, sort of related issue in this space is, uh, predictive sentencing.

00:16:33.085 --> 00:16:36.100
Um, so there was a really nice article that came out

00:16:36.100 --> 00:16:39.280
from Pro- ProPublica a few years ago discussing this.

00:16:39.280 --> 00:16:41.950
Um, but when most defendants are booked in jail,

00:16:41.950 --> 00:16:44.320
they respond to a questionnaire called COMPAS.

00:16:44.320 --> 00:16:47.920
Um, and their answers are fed into this software system that

00:16:47.920 --> 00:16:51.640
generates scores that correspond to the risk of recidivism,

00:16:51.640 --> 00:16:53.500
that's the risk of um,

00:16:53.500 --> 00:16:55.330
er, making a crime again.

00:16:55.330 --> 00:16:57.970
Um, and the questions are used to gather data

00:16:57.970 --> 00:17:00.565
on the defendant's socio-economic status,

00:17:00.565 --> 00:17:03.070
family background, neighborhood crime,

00:17:03.070 --> 00:17:05.830
employment status, and other factors in order to reach

00:17:05.830 --> 00:17:11.785
some predictim- prediction of an individual's crime or criminal risk.

00:17:11.785 --> 00:17:17.470
Um, but what ends up happening is that it ends up focusing on the key bias issues

00:17:17.470 --> 00:17:19.600
that humans have and propagating it

00:17:19.600 --> 00:17:23.215
back with something that looks like an objective score.

00:17:23.215 --> 00:17:26.065
So, you're a lot more likely um,

00:17:26.065 --> 00:17:28.315
to be convicted of a crime, um,

00:17:28.315 --> 00:17:30.055
if you're black than if you're white,

00:17:30.055 --> 00:17:31.975
even if you've made the exact same crime.

00:17:31.975 --> 00:17:34.330
And the system will pick up on this,

00:17:34.330 --> 00:17:36.790
and will reflect this back to say that people who are

00:17:36.790 --> 00:17:39.580
black are more likely to have reci- like recidivism,

00:17:39.580 --> 00:17:41.020
more likely to convict a,

00:17:41.020 --> 00:17:43.165
uh, to make a crime again.

00:17:43.165 --> 00:17:49.885
Um, so this is an example of automation bias, preferring the output of a system, uh,

00:17:49.885 --> 00:17:54.085
in the face of overgeneralization, feedback loops,

00:17:54.085 --> 00:17:55.795
and correlation fallacy,

00:17:55.795 --> 00:18:00.620
confusing things that are occurring together as being somehow causal.

00:18:02.520 --> 00:18:06.985
There's another, uh, sort of area of research and, uh,

00:18:06.985 --> 00:18:12.010
startups looking at predicting criminality in particular from things like face images.

00:18:12.010 --> 00:18:14.890
So there's a company out there, uh, called Faception.

00:18:14.890 --> 00:18:18.520
They are based in Israel and they claim to be able to,

00:18:18.520 --> 00:18:21.625
um, use individual images, uh,

00:18:21.625 --> 00:18:25.150
with computer vision and machine learning technology for profiling people

00:18:25.150 --> 00:18:28.855
and revealing their personality based only on their facial image,

00:18:28.855 --> 00:18:32.395
um, recognizing things like high IQ,

00:18:32.395 --> 00:18:35.575
white-collar offender, pedophile, and terrorist.

00:18:35.575 --> 00:18:38.350
Um, and their main clients are Homeland Security,

00:18:38.350 --> 00:18:39.550
lots of other, uh,

00:18:39.550 --> 00:18:42.985
lots of other countries dealing with sort of public safety issues.

00:18:42.985 --> 00:18:45.760
They've not published any details about their methods,

00:18:45.760 --> 00:18:47.425
their sources of training data,

00:18:47.425 --> 00:18:49.090
or their quantitative results.

00:18:49.090 --> 00:18:50.980
We know that in light of automation bias,

00:18:50.980 --> 00:18:54.535
people will tend to think it just works even when it doesn't work well.

00:18:54.535 --> 00:18:57.700
Um, but there was a paper that came out wi- in

00:18:57.700 --> 00:19:01.765
a similar line predicting criminal, criminality,

00:19:01.765 --> 00:19:05.890
or purporting to predict criminality from individual face images,

00:19:05.890 --> 00:19:08.110
and that one had some results and,

00:19:08.110 --> 00:19:11.350
uh, some more details about the data that we could kinda dig

00:19:11.350 --> 00:19:15.100
into to understand where are these kinds of claims coming from.

00:19:15.100 --> 00:19:19.880
Um, so this was an article that was posted on Archive near the end of 2016.

00:19:19.880 --> 00:19:26.230
Um, and they said they were using less than 2,000 closely cropped images of faces, um,

00:19:26.230 --> 00:19:30.415
including wanted suspect ID pictures from specific regions,

00:19:30.415 --> 00:19:35.035
and they claimed that even based on this very small training dataset, um,

00:19:35.035 --> 00:19:37.540
that they were able to predict, uh,

00:19:37.540 --> 00:19:40.240
whether or not someone was likely to be a criminal,

00:19:40.240 --> 00:19:42.640
uh, greater than 90 percent accuracy.

00:19:42.640 --> 00:19:45.235
Um, and they got so lost in this,

00:19:45.235 --> 00:19:47.020
this idea that, uh,

00:19:47.020 --> 00:19:49.150
it's sort of funny to read to just take

00:19:49.150 --> 00:19:51.640
a step back and realize what's actually happening.

00:19:51.640 --> 00:19:52.840
So for example, one of

00:19:52.840 --> 00:19:57.640
their really great exciting claims was that the angle Theta from nose tip to

00:19:57.640 --> 00:19:59.770
two mouth corners is on average

00:19:59.770 --> 00:20:03.640
19.6 percent smaller for criminals than for non-criminals.

00:20:03.640 --> 00:20:05.980
This is otherwise known as smiling. [LAUGHTER]

00:20:05.980 --> 00:20:09.070
Uh, and [LAUGHTER] you know,

00:20:09.070 --> 00:20:11.170
exactly the kind of images people would

00:20:11.170 --> 00:20:13.780
use when trying to put out wanted criminal pictures,

00:20:13.780 --> 00:20:15.655
probably not really happy pictures.

00:20:15.655 --> 00:20:18.265
But you get so lost in the confirmation bias.

00:20:18.265 --> 00:20:20.230
You get so lost in the correlation and the

00:20:20.230 --> 00:20:22.930
feedback loops that you end up overlooking these

00:20:22.930 --> 00:20:25.945
really obvious kinds of things.

00:20:25.945 --> 00:20:29.010
Um, so that's an example of selection bias,

00:20:29.010 --> 00:20:32.880
experimenter's bias, confirmation bias, correlation fallacy,

00:20:32.880 --> 00:20:36.090
and feedback loops all coming together to create

00:20:36.090 --> 00:20:37.950
a deep learning system that people think is

00:20:37.950 --> 00:20:40.730
scary and can do things that it can't actually do.

00:20:40.730 --> 00:20:44.485
Um, one of the issues with this was that the media loved it.

00:20:44.485 --> 00:20:46.180
Like it was all over the news,

00:20:46.180 --> 00:20:49.060
and there's been similar kinds of things happening again and again.

00:20:49.060 --> 00:20:51.175
Media wants to sell the story,

00:20:51.175 --> 00:20:53.830
and so it's part of our job as researchers,

00:20:53.830 --> 00:20:55.315
that people who work on this stuff,

00:20:55.315 --> 00:20:58.615
to be very clear about what the technology is actually doing,

00:20:58.615 --> 00:21:00.760
uh, and make a distinction between what you

00:21:00.760 --> 00:21:03.520
might think it's doing and what it's actually doing.

00:21:03.520 --> 00:21:07.930
Um, so another issue that has come up recently, um,

00:21:07.930 --> 00:21:09.490
it's claiming to be able to predict

00:21:09.490 --> 00:21:13.660
internal qualities but specifically ones that are subject to discrimination,

00:21:13.660 --> 00:21:15.535
um, and loss of opportunity.

00:21:15.535 --> 00:21:18.610
So in particular, there was this work that came out that claimed

00:21:18.610 --> 00:21:21.565
to be able to predict whether or not someone was homosexual,

00:21:21.565 --> 00:21:23.905
just based on single face images.

00:21:23.905 --> 00:21:28.390
Um, now, it's important to know that the images that they used in the study included

00:21:28.390 --> 00:21:33.400
images that were from dating websites where people self-identified as straight or gay,

00:21:33.400 --> 00:21:37.045
and identified as whether they were looking for a partner who was straight or gay,

00:21:37.045 --> 00:21:40.015
and these became the sources of the training data,

00:21:40.015 --> 00:21:41.890
and still from this, uh.

00:21:41.890 --> 00:21:44.470
Oh! Before I go on, can you guys just understand

00:21:44.470 --> 00:21:47.500
just from that what the issue might have been?

00:21:47.500 --> 00:21:48.640
Rainbows.

00:21:48.640 --> 00:21:54.250
[LAUGHTER] I don't think that there was actually anything about rainbows,

00:21:54.250 --> 00:21:55.390
but that's really unfortunate.

00:21:55.390 --> 00:21:59.980
[LAUGHTER].

00:21:59.980 --> 00:22:00.280
[inaudible]

00:22:00.280 --> 00:22:03.475
Right. Yeah. So this has more to do with the presentation of the self,

00:22:03.475 --> 00:22:07.030
the presentation of the social self when you're trying to for example,

00:22:07.030 --> 00:22:08.784
attract a partner on a website,

00:22:08.784 --> 00:22:11.695
and less to do with how you look day to day.

00:22:11.695 --> 00:22:14.800
Um, and yet they kind of went to

00:22:14.800 --> 00:22:19.660
these large conclusions that aren't supported at all by the data or by their study,

00:22:19.660 --> 00:22:24.595
um, but things like consistent with a prenatal hormone theory of sexual orientation.

00:22:24.595 --> 00:22:28.630
Gay men and women tended to have gender atypical facial morphology.

00:22:28.630 --> 00:22:34.420
Now, none of the authors actually were prenatal hormone theory specialists, you know.

00:22:34.420 --> 00:22:37.120
They have doctor in their name so maybe that's a thing.

00:22:37.120 --> 00:22:39.865
Um, this was a Stanford professor and like I've,

00:22:39.865 --> 00:22:42.160
I've presented this a few times at Stanford and gotten into

00:22:42.160 --> 00:22:44.680
some like pretty harsh fights about this.

00:22:44.680 --> 00:22:46.570
So I'm ready if anyone wants to take me on.

00:22:46.570 --> 00:22:50.410
[LAUGHTER] But uh, me and my uh,

00:22:50.410 --> 00:22:52.495
some of my colleagues decided we'd,

00:22:52.495 --> 00:22:54.025
we'd play around with this a bit,

00:22:54.025 --> 00:22:56.755
and what we found was that a simple decision tree.

00:22:56.755 --> 00:22:59.580
Um, so I'm kind of assuming you guys know what a decision tree is.

00:22:59.580 --> 00:23:01.090
So, okay.

00:23:01.090 --> 00:23:04.270
Cool. So based on wearing makeup or wearing glasses,

00:23:04.270 --> 00:23:06.790
got us pretty close to the accuracy reported in

00:23:06.790 --> 00:23:09.670
the paper. That says nothing about internal hormones,

00:23:09.670 --> 00:23:11.200
that says nothing about any of that,

00:23:11.200 --> 00:23:13.795
and it says a lot about the physical presentation,

00:23:13.795 --> 00:23:15.535
the things that are on the surface.

00:23:15.535 --> 00:23:17.830
Um, it says a lot more about how people are

00:23:17.830 --> 00:23:20.515
presenting themselves than what is happening internally.

00:23:20.515 --> 00:23:23.530
Um, so the key thing that's recently kind of

00:23:23.530 --> 00:23:26.695
been overlooked is that deep learning is somehow,

00:23:26.695 --> 00:23:30.940
i- it's sort of considered that it's somehow magically going beyond surface level.

00:23:30.940 --> 00:23:34.420
But the point is that it's working on the surface level and working well.

00:23:34.420 --> 00:23:38.260
And in the face of confirmation bias and other kinds of bias factors,

00:23:38.260 --> 00:23:41.455
it's easy to assume that something else is happening that's not.

00:23:41.455 --> 00:23:43.720
Without critical examination, uh,

00:23:43.720 --> 00:23:46.660
for example simple baselines, uh,

00:23:46.660 --> 00:23:50.635
simple sanity checks, these kinds of things can just be ignored and,

00:23:50.635 --> 00:23:52.705
and not noticed at all.

00:23:52.705 --> 00:23:56.620
Um, so that's example of selection bias,

00:23:56.620 --> 00:24:00.530
um, experimenter's bias, and correlation fallacy.

00:24:00.780 --> 00:24:03.655
Okay. So now I'm going to talk to,

00:24:03.655 --> 00:24:05.680
talk about measuring algorithmic bias.

00:24:05.680 --> 00:24:10.690
So I just said a lot about different kinds of biases that come in in the data,

00:24:10.690 --> 00:24:13.870
in the collection, in the interpretation of the results.

00:24:13.870 --> 00:24:18.265
[NOISE] Let's talk about actually quantitatively measuring different kinds of biases.

00:24:18.265 --> 00:24:22.045
Um, so one of the key things that's, uh,

00:24:22.045 --> 00:24:25.750
emerged in a few different works and really ties nicely to a lot

00:24:25.750 --> 00:24:29.650
of fairness work is this idea of disaggregated evaluation.

00:24:29.650 --> 00:24:31.855
So in disaggregated evaluation,

00:24:31.855 --> 00:24:35.230
you evaluate across different subgroups as opposed to

00:24:35.230 --> 00:24:39.280
looking at one single score for your overall testing data set.

00:24:39.280 --> 00:24:41.620
Um, so, okay.

00:24:41.620 --> 00:24:44.200
You guys are probably familiar with the training testing data split.

00:24:44.200 --> 00:24:45.505
You kind of train on there,

00:24:45.505 --> 00:24:47.320
on your given training data,

00:24:47.320 --> 00:24:51.565
you test on your given testing data and then you repo- you report like precision,

00:24:51.565 --> 00:24:53.875
recall, F-score, things like that.

00:24:53.875 --> 00:24:57.280
Um, but what that masks is how well the system is actually

00:24:57.280 --> 00:25:01.315
working across different kinds of individuals and across different, different subgroups.

00:25:01.315 --> 00:25:04.660
Um, and so one just straightforward way to handle

00:25:04.660 --> 00:25:08.320
this is to actually evaluate with respect to those different subgroups.

00:25:08.320 --> 00:25:11.650
So creating for each sort of subgroup prediction pair.

00:25:11.650 --> 00:25:13.450
Um, so for an example,

00:25:13.450 --> 00:25:15.535
you might look at women face detection,

00:25:15.535 --> 00:25:18.205
men face detection, and look at how the,

00:25:18.205 --> 00:25:19.600
the error rates are,

00:25:19.600 --> 00:25:22.375
are different or are um, similar.

00:25:22.375 --> 00:25:27.145
Um, another important part of this is to look at things intersectionally,

00:25:27.145 --> 00:25:29.530
um, combining things, um,

00:25:29.530 --> 00:25:34.000
like gender and race at the same time and seeing how those, uh,

00:25:34.000 --> 00:25:36.370
how the error rates on those sorts of things

00:25:36.370 --> 00:25:39.700
change and how they're different across uh, different intersections.

00:25:39.700 --> 00:25:42.445
Um, and this is inspired by Kimberle Crenshaw.

00:25:42.445 --> 00:25:45.879
Um, who she, she pioneered intersectional research,

00:25:45.879 --> 00:25:48.175
uh, in critical race theory.

00:25:48.175 --> 00:25:51.805
Um, and she discussed the story of Emma DeGraffenreid, uh,

00:25:51.805 --> 00:25:55.585
who was a woman at General Motors, um,

00:25:55.585 --> 00:25:59.770
and she claimed that the company's hiring practices discriminated against black women.

00:25:59.770 --> 00:26:01.780
Um, but in her court opinion,

00:26:01.780 --> 00:26:04.795
the judges ruled that General Motors hired, um,

00:26:04.795 --> 00:26:10.255
many women for secretarial positions and many black people for factory roles,

00:26:10.255 --> 00:26:14.050
and thus they could not have discriminated against black women.

00:26:14.050 --> 00:26:16.690
What they failed to do was look at the intersection of

00:26:16.690 --> 00:26:19.270
the two and understand that the experience there might be

00:26:19.270 --> 00:26:21.160
fundamentally different than any of

00:26:21.160 --> 00:26:25.840
the experiences of either of these sort of subgroups in isolation.

00:26:25.840 --> 00:26:29.290
Um, and the same becomes true when you start looking

00:26:29.290 --> 00:26:32.590
at errors that are regularly made in deep learning systems.

00:26:32.590 --> 00:26:34.630
Um, so we've been able to uncover a lot of

00:26:34.630 --> 00:26:37.360
different kinds of unintended errors by looking not only at

00:26:37.360 --> 00:26:43.475
the disaggregated evaluation but also at intersectional disaggregated evaluation.

00:26:43.475 --> 00:26:46.645
Um, so I'm going to walk through a bit how this works.

00:26:46.645 --> 00:26:49.660
This is probably going to be review for most of you,

00:26:49.660 --> 00:26:52.330
but I think it's really important to understand this because it also

00:26:52.330 --> 00:26:55.645
ties to how we measure fairness and when we say like,

00:26:55.645 --> 00:26:58.690
uh, algorithmic fairness, what we're talking about.

00:26:58.690 --> 00:27:02.740
So um, the confusion matrix is a way, you guys.

00:27:02.740 --> 00:27:04.750
Okay. Are you guys familiar with the confusion matrix?

00:27:04.750 --> 00:27:06.250
[LAUGHTER]. I just want want to know where.

00:27:06.250 --> 00:27:09.085
Okay. Awesome. Cool. So you're familiar with the confusion matrix, right.

00:27:09.085 --> 00:27:11.425
So you have model predictions and references.

00:27:11.425 --> 00:27:14.470
Um, and you can kind of look at these as negative and positive,

00:27:14.470 --> 00:27:17.110
uh, binary classification, uh,

00:27:17.110 --> 00:27:19.495
kind of approach here where if

00:27:19.495 --> 00:27:23.200
the ground truth says something is true and the model predicts it's true,

00:27:23.200 --> 00:27:24.385
it's a true positive.

00:27:24.385 --> 00:27:25.780
If the ground truth says, uh,

00:27:25.780 --> 00:27:27.520
it's, it's, it's false,

00:27:27.520 --> 00:27:31.045
um, and the model predicts it's false, it's true negative.

00:27:31.045 --> 00:27:33.670
Um, and the errors that the kind of different issues that

00:27:33.670 --> 00:27:36.400
arise are false negatives and false positives.

00:27:36.400 --> 00:27:39.670
Um, so in false positives the, um,

00:27:39.670 --> 00:27:44.290
the ground truth says something is negative but the model predicts that it's positive.

00:27:44.290 --> 00:27:47.560
Uh, and then in false negatives, vice versa.

00:27:47.560 --> 00:27:49.765
Um, from these, you know,

00:27:49.765 --> 00:27:51.250
uh, basic kind of, uh,

00:27:51.250 --> 00:27:53.290
these basic breakdown of errors,

00:27:53.290 --> 00:27:55.330
you can get a few different metrics.

00:27:55.330 --> 00:28:01.105
Um, these metrics actually trivially map to a lot of different fairness criteria.

00:28:01.105 --> 00:28:02.965
So um, for example,

00:28:02.965 --> 00:28:04.720
if we're looking at something like

00:28:04.720 --> 00:28:09.880
a female versus male patient results and figuring out things like precision and recall,

00:28:09.880 --> 00:28:12.670
which is relatively common in NLP, um,

00:28:12.670 --> 00:28:16.405
if you have equal recall across your subgroups

00:28:16.405 --> 00:28:20.875
that's the same as the fairness criteria of equality of opportunity,

00:28:20.875 --> 00:28:23.440
um, I could work through the math.

00:28:23.440 --> 00:28:24.805
But I mean, this is basically just,

00:28:24.805 --> 00:28:27.475
just the main point that, that, uh,

00:28:27.475 --> 00:28:32.200
it says that given that something is true in the ground truth,

00:28:32.200 --> 00:28:35.365
the model should predict that it's true,

00:28:35.365 --> 00:28:37.660
uh, at equal rates across different subgroups.

00:28:37.660 --> 00:28:41.965
So this ends up being equivalent to having the same recall across different subgroups.

00:28:41.965 --> 00:28:45.730
Similarly, um, having the same precision across

00:28:45.730 --> 00:28:50.800
different subgroups is equivalent to a fairness criterion called predictive parity.

00:28:50.800 --> 00:28:55.285
And so as fairness has been defined again and again, um,

00:28:55.285 --> 00:28:58.960
it was originally some of these definitions came in

00:28:58.960 --> 00:29:02.980
1966 following the Civil Rights Act of 1964.

00:29:02.980 --> 00:29:05.875
Um, they were reinvented a few times, uh,

00:29:05.875 --> 00:29:09.895
and most recently reinvented in, uh, 2016.

00:29:09.895 --> 00:29:12.790
Um, but they all sort of boiled down to

00:29:12.790 --> 00:29:16.540
this disaggregated comparison across subgroups and the math,

00:29:16.540 --> 00:29:20.770
the metrics end being roughly equivalent to what we get from the confusion matrix,

00:29:20.770 --> 00:29:24.560
specifically in classification systems.

00:29:25.980 --> 00:29:29.530
So which kind of fairness metric do you use,

00:29:29.530 --> 00:29:31.870
what are the different criteria you want

00:29:31.870 --> 00:29:35.020
to use to look at the differences across different subgroups,

00:29:35.020 --> 00:29:37.780
that really comes down to the trade-offs

00:29:37.780 --> 00:29:39.925
between false positives and false negatives.

00:29:39.925 --> 00:29:42.025
So this is the same problem that you are dealing with

00:29:42.025 --> 00:29:44.515
when you're just figuring out how to evaluate generally.

00:29:44.515 --> 00:29:46.900
Um, there's no one fairness criteria and that is

00:29:46.900 --> 00:29:49.960
the fairness criteria and to rule them all, um,

00:29:49.960 --> 00:29:52.630
deciding which one is better than the other is the same as

00:29:52.630 --> 00:29:55.480
kind of trying to decide which is better, precision or recall, right?

00:29:55.480 --> 00:29:58.630
It depends on what the problem is and what you're interested in measuring.

00:29:58.630 --> 00:30:03.100
Um, so a case where false positives might be better than

00:30:03.100 --> 00:30:07.780
false negatives and so you want to prioritize something like a false positive rate,

00:30:07.780 --> 00:30:10.630
ah, across subgroups is privacy and images.

00:30:10.630 --> 00:30:15.370
So here a false positive is something that doesn't need to be blurred gets blurred.

00:30:15.370 --> 00:30:16.780
That's just kind of a bummer.

00:30:16.780 --> 00:30:19.540
Um, but a false negative would be something that needs to be

00:30:19.540 --> 00:30:22.405
blurred is not blurred and that can be identity theft.

00:30:22.405 --> 00:30:24.295
It's a much more serious issue.

00:30:24.295 --> 00:30:26.110
And so it's important to prioritize

00:30:26.110 --> 00:30:29.440
the evaluation metrics that stress the false negative rates.

00:30:29.440 --> 00:30:32.650
Um, an example where false negatives

00:30:32.650 --> 00:30:35.335
might be better than false positives is in spam filtering.

00:30:35.335 --> 00:30:41.635
So a false-negative could be an e-mail that's spam not caught so you see it in your inbox,

00:30:41.635 --> 00:30:44.350
that's usually just annoying, it's not a big deal.

00:30:44.350 --> 00:30:47.170
Um, but a false positive here would be e-mail flagged as

00:30:47.170 --> 00:30:50.245
spam and then removed from your inbox, which,

00:30:50.245 --> 00:30:52.780
you know, if its from a friend or a loved one,

00:30:52.780 --> 00:30:54.535
it can be, it can be a loss,

00:30:54.535 --> 00:30:56.620
maybe a job offer something like that.

00:30:56.620 --> 00:30:58.360
All right.

00:30:58.360 --> 00:31:02.560
So, um, I just kind of covered how AI can unintentionally lead to

00:31:02.560 --> 00:31:05.050
unjust outcomes and some of the things to do

00:31:05.050 --> 00:31:07.045
or some of the things to be aware of here,

00:31:07.045 --> 00:31:11.590
are the lack of insight into sources of bias in the data, in the model,

00:31:11.590 --> 00:31:16.690
lack of insight into the feedback loops from the original data that's collected

00:31:16.690 --> 00:31:21.519
as an example of what humans do to the data that's then repurposed,

00:31:21.519 --> 00:31:24.475
re-used, acted on, and then further fed in.

00:31:24.475 --> 00:31:28.060
Um, a lack of careful disaggregated evaluation,

00:31:28.060 --> 00:31:29.560
looking at the disparities,

00:31:29.560 --> 00:31:33.730
the differences between different subgroups in order to understand this bias,

00:31:33.730 --> 00:31:35.605
this difference across the subgroups.

00:31:35.605 --> 00:31:38.949
Um, and then human biases in interpreting, and accepting,

00:31:38.949 --> 00:31:40.525
and talking about the results,

00:31:40.525 --> 00:31:45.950
which then kind of further the media cycles and the hype around AI right now.

00:31:46.080 --> 00:31:50.875
Um, but it's up to us to influence how AI evolves.

00:31:50.875 --> 00:31:55.330
So I like to think of this in terms of short term,

00:31:55.330 --> 00:31:57.295
middle term, and long-term objectives.

00:31:57.295 --> 00:32:00.355
So short term today,

00:32:00.355 --> 00:32:05.170
we might be working on some specific model where we're trying to find some local optimum,

00:32:05.170 --> 00:32:07.705
we have a task, we have data, something like that.

00:32:07.705 --> 00:32:09.880
And that's sort of short-term objectives.

00:32:09.880 --> 00:32:14.590
Um, we might have a slightly longer-term objective of getting a paper published,

00:32:14.590 --> 00:32:16.990
or if you're an industry like getting a product launched,

00:32:16.990 --> 00:32:18.505
whatever it might be.

00:32:18.505 --> 00:32:23.440
Um, from there we might see our next endpoint is getting an award or,

00:32:23.440 --> 00:32:25.870
you know, maybe become sort of famous for something for

00:32:25.870 --> 00:32:28.600
a few minutes, something like that and that's cool.

00:32:28.600 --> 00:32:31.300
Um, but there's a longer-term objective that we

00:32:31.300 --> 00:32:34.165
can work towards as well at the same time.

00:32:34.165 --> 00:32:38.110
And that's something like a positive outcome for humans in their environment.

00:32:38.110 --> 00:32:42.130
So instead of just kind of focusing on these local decisions,

00:32:42.130 --> 00:32:43.555
these local optima and these

00:32:43.555 --> 00:32:48.370
sort of local paper by paper-based approaches to solving problems,

00:32:48.370 --> 00:32:51.400
you can also kind of think about what's the long-term objective.

00:32:51.400 --> 00:32:56.739
Where does this get me as I trace out an evolutionary path for artificial intelligence,

00:32:56.739 --> 00:32:58.555
down the line in 10 years,

00:32:58.555 --> 00:33:01.210
15 years, 20 years.

00:33:01.210 --> 00:33:06.220
Um, and one of the ways you can address this is by thinking,

00:33:06.220 --> 00:33:10.405
now how can the work I'm interested in now be best focused to help others?

00:33:10.405 --> 00:33:12.235
And that involves talking to experts,

00:33:12.235 --> 00:33:14.125
um, and kind of going outside your bubble,

00:33:14.125 --> 00:33:16.540
speaking across interdisciplinary fields like

00:33:16.540 --> 00:33:19.660
cognitive science which I've just talked a bit about.

00:33:19.660 --> 00:33:23.410
Um, so let's talk about some things we can do.

00:33:23.410 --> 00:33:25.825
So first off is data.

00:33:25.825 --> 00:33:32.095
Um, so a lot of the issues of bias and fairness,

00:33:32.095 --> 00:33:36.220
ah, in machine learning models really come down to the data.

00:33:36.220 --> 00:33:39.580
Unfortunately in machine learning and deep learning,

00:33:39.580 --> 00:33:42.670
working on data is really not seen as sexy.

00:33:42.670 --> 00:33:45.085
Ah, there's a few datasets, ah,

00:33:45.085 --> 00:33:47.350
that people use that are out there,

00:33:47.350 --> 00:33:48.580
that's what people use,

00:33:48.580 --> 00:33:50.710
and there's not a lot of analysis done on,

00:33:50.710 --> 00:33:55.000
on how well these datasets capture different truths about the world,

00:33:55.000 --> 00:33:56.740
how problematic they might be,

00:33:56.740 --> 00:34:01.945
[NOISE] um, but it's a pretty wide area that needs a lot of future,

00:34:01.945 --> 00:34:04.885
like lea- needs a lot of future additional work.

00:34:04.885 --> 00:34:08.740
Um, [NOISE] so we're going to understanding the data skews and the correlations.

00:34:08.740 --> 00:34:10.900
If you understand your data skews and the,

00:34:10.900 --> 00:34:13.960
ah, correlations that might be problematic in your data,

00:34:13.960 --> 00:34:17.305
then you can start working on either models that address those,

00:34:17.305 --> 00:34:20.980
or data augmentation approaches in order to sort of make

00:34:20.980 --> 00:34:24.115
the dataset a little bit better or a little bit more representative

00:34:24.115 --> 00:34:25.990
of how you want the world to be.

00:34:25.990 --> 00:34:30.280
Um, it's also important to abandon the single training set- testing

00:34:30.280 --> 00:34:34.420
set from similar distribution approach to advancing deep learning.

00:34:34.420 --> 00:34:37.975
So um, when we do projects in deep learning,

00:34:37.975 --> 00:34:39.610
you know, we tend to have the training set,

00:34:39.610 --> 00:34:43.330
and the testing set and then that's what we sort of benchmark on and prioritize,

00:34:43.330 --> 00:34:46.060
but the point is, as you move around different testing sets,

00:34:46.060 --> 00:34:48.160
you're gonna get vastly different results.

00:34:48.160 --> 00:34:50.440
Um, and so by keeping in

00:34:50.440 --> 00:34:55.810
this just sort of one training testing dat- training testing dataset paradigm,

00:34:55.810 --> 00:35:00.265
you're really likely to not notice issues that might otherwise be there.

00:35:00.265 --> 00:35:02.140
And one way to really focus in on them,

00:35:02.140 --> 00:35:05.155
is having a hard set of,

00:35:05.155 --> 00:35:09.070
of test cases, that you really wanna make sure the model does well on.

00:35:09.070 --> 00:35:12.055
So these are things that are particularly problematic.

00:35:12.055 --> 00:35:14.905
Things that would be really harmful to individuals,

00:35:14.905 --> 00:35:17.545
um, If they were to experience the output.

00:35:17.545 --> 00:35:21.730
Um, and you kinda collect those in a small test set and then it's really easy

00:35:21.730 --> 00:35:26.140
to evaluate on that test set as you benchmark improvements on your model,

00:35:26.140 --> 00:35:28.330
as you add different kinds of things to your model,

00:35:28.330 --> 00:35:30.025
in order to see, um,

00:35:30.025 --> 00:35:32.215
not just how your model is doing overall,

00:35:32.215 --> 00:35:33.940
in terms of your testing dataset,

00:35:33.940 --> 00:35:36.700
but how well you're doing in terms of these examples,

00:35:36.700 --> 00:35:38.590
you really want it to do well on.

00:35:38.590 --> 00:35:41.950
That you know that is going to be a problem if it doesn't do well on,

00:35:41.950 --> 00:35:43.900
and any sort of degradation in that,

00:35:43.900 --> 00:35:46.210
you might want to prioritize, um,

00:35:46.210 --> 00:35:50.815
to fix above degragaish- degradation and overall accuracy.

00:35:50.815 --> 00:35:53.530
Um, and it's also important to talk to experts

00:35:53.530 --> 00:35:56.335
about the additional signals that you can incorporate.

00:35:56.335 --> 00:36:00.490
Um, so we've put out a tool to help with this,

00:36:00.490 --> 00:36:03.220
ah, understanding data skews called facets,

00:36:03.220 --> 00:36:04.855
um, it's just available there.

00:36:04.855 --> 00:36:10.075
Um, and it's a really handy kinda visualizer for slicing, ah, understanding,

00:36:10.075 --> 00:36:13.330
um, you know, what some of the differences are between different subgroups

00:36:13.330 --> 00:36:16.630
and different representations and you can sort of dig in and explore a bit more.

00:36:16.630 --> 00:36:18.685
So this is just to sort of help people, ah,

00:36:18.685 --> 00:36:21.430
come to terms with the data that they're actually using and,

00:36:21.430 --> 00:36:23.050
and where there might be, um,

00:36:23.050 --> 00:36:25.795
unwanted associations or, or missing,

00:36:25.795 --> 00:36:26.920
missing kind of features.

00:36:26.920 --> 00:36:33.220
[NOISE] Um, another approach that's been put forward recently,

00:36:33.220 --> 00:36:36.565
ah, specifically on the data side is this data,

00:36:36.565 --> 00:36:38.725
datasheets for datasets approach.

00:36:38.725 --> 00:36:42.040
Um, so this is this idea that when you release a dataset,

00:36:42.040 --> 00:36:44.770
it's not enough to just release the dataset with like

00:36:44.770 --> 00:36:48.880
some pretty graphs and like talking about basic distributional information,

00:36:48.880 --> 00:36:52.330
you need to talk about who the annotators were, where they were,

00:36:52.330 --> 00:36:54.400
what the inter-annotator agreement was,

00:36:54.400 --> 00:36:56.500
what their background information was,

00:36:56.500 --> 00:36:59.005
um, motivation for the dataset.

00:36:59.005 --> 00:37:00.550
All these other kinds of details.

00:37:00.550 --> 00:37:03.625
So now you actually know that this isn't just a dataset,

00:37:03.625 --> 00:37:06.700
this is a dataset that has these specific biases.

00:37:06.700 --> 00:37:10.300
There's no such thing as a dataset that isn't biased in some way.

00:37:10.300 --> 00:37:14.905
A dataset by virtue of the fact that it's collected from the world as a subset,

00:37:14.905 --> 00:37:18.415
is a, is a biased set of the world in some way.

00:37:18.415 --> 00:37:20.440
The point is to make it clear what it is,

00:37:20.440 --> 00:37:22.015
how it is biased, what are the,

00:37:22.015 --> 00:37:23.545
what are the various biases,

00:37:23.545 --> 00:37:25.675
ah, that are important to know about in the dataset.

00:37:25.675 --> 00:37:29.410
So that's one of these ideas between- behind datasheets for datasets,

00:37:29.410 --> 00:37:32.185
releasing its datasets publicly.

00:37:32.185 --> 00:37:35.815
All right. Now let's switch a little bit to machine learning.

00:37:35.815 --> 00:37:40.375
Um, so there are a couple of techniques that I like to use. Um, I'll talk about two.

00:37:40.375 --> 00:37:42.835
One, ah, is bias mitigation,

00:37:42.835 --> 00:37:45.850
which is removing the signal for a problematic output.

00:37:45.850 --> 00:37:48.760
Um, so removing, ah, stereotyping,

00:37:48.760 --> 00:37:52.390
sexism, racism, trying to remove these kind of effects from the model.

00:37:52.390 --> 00:37:56.530
Um, this is also sometimes called de-biasing or unbiasing,

00:37:56.530 --> 00:38:00.520
but that's a little bit of a misnomer because you're- you're generally just kind of

00:38:00.520 --> 00:38:04.690
moving around bias based on a specific set of words for example,

00:38:04.690 --> 00:38:07.795
um, so to say it's unbiased is is not true.

00:38:07.795 --> 00:38:10.330
Um, but you are kind of mitigating bias with respect to

00:38:10.330 --> 00:38:13.675
some certain kinds of information that you provide it with.

00:38:13.675 --> 00:38:18.910
Um, and there's inclusion which is then adding signal for desired variables.

00:38:18.910 --> 00:38:22.105
So that's kind of the opposite side of bias mitigation.

00:38:22.105 --> 00:38:24.610
So increasing model performance with attention to

00:38:24.610 --> 00:38:28.075
subgroups or data slices with the worst performance.

00:38:28.075 --> 00:38:31.825
Um, so, ah, in order to,

00:38:31.825 --> 00:38:33.475
er, address inclusion, ah,

00:38:33.475 --> 00:38:36.865
kind of adding signal for under-represented sub-groups,

00:38:36.865 --> 00:38:39.910
one technique that's worked relatively well is multi-task learning.

00:38:39.910 --> 00:38:43.990
Um, so I've heard that you guys have studied multi-task learning which is great,

00:38:43.990 --> 00:38:46.390
um, so I'll tell you a bit about a case study here.

00:38:46.390 --> 00:38:48.235
Um, so this is work I did, ah,

00:38:48.235 --> 00:38:52.390
in collaboration with a UPenn World Well-being Project, ah,

00:38:52.390 --> 00:38:54.190
working directly with clinicians,

00:38:54.190 --> 00:38:56.530
and the goal was to create a system that could alert

00:38:56.530 --> 00:38:59.560
clinicians if there was a suicide attempt that was imminent.

00:38:59.560 --> 00:39:02.320
Um, and they wanted to understand the feasibility of

00:39:02.320 --> 00:39:05.650
these kinds of diagnoses when there were very few training,

00:39:05.650 --> 00:39:07.690
ah, training instances available.

00:39:07.690 --> 00:39:11.230
So that's similar to kind of the minority problem in datasets.

00:39:11.230 --> 00:39:14.125
Um, [NOISE]

00:39:14.125 --> 00:39:17.290
And, uh, in this work,

00:39:17.290 --> 00:39:19.015
we had two kinds of data.

00:39:19.015 --> 00:39:22.960
One was the internal data which was the electronic health records, um,

00:39:22.960 --> 00:39:27.190
with the- that was either provided by the patient or from the family.

00:39:27.190 --> 00:39:30.070
Um, it included mental health diagnoses,

00:39:30.070 --> 00:39:32.380
uh, suicide attempts or completions, um,

00:39:32.380 --> 00:39:34.750
if, if, if that were the case along with,

00:39:34.750 --> 00:39:36.175
uh, the user's, uh,

00:39:36.175 --> 00:39:37.915
the person's social media data.

00:39:37.915 --> 00:39:40.690
And that was the internal data that we did not publish on,

00:39:40.690 --> 00:39:43.000
but that we were able to work with clinicians on in

00:39:43.000 --> 00:39:45.685
order to understand if our methods were actually working.

00:39:45.685 --> 00:39:48.385
Um, the external data, the proxy data,

00:39:48.385 --> 00:39:50.845
the stuff that we could kinda publish on and talk about,

00:39:50.845 --> 00:39:52.045
was based on Twitter.

00:39:52.045 --> 00:39:53.910
Um, and this was, uh,

00:39:53.910 --> 00:39:57.810
using regular expressions in order to extract, uh,

00:39:57.810 --> 00:40:02.315
phases in Twitter feeds that had something that was kind of like diagnoses.

00:40:02.315 --> 00:40:04.960
So something like, I've been diagnosed with X,

00:40:04.960 --> 00:40:06.820
or I've tried to commit suicide.

00:40:06.820 --> 00:40:08.425
And that became kind of the,

00:40:08.425 --> 00:40:12.325
the proxy dataset and the corresponding social media feeds for,

00:40:12.325 --> 00:40:13.735
for those individuals, uh,

00:40:13.735 --> 00:40:16.270
for the actual diagnoses.

00:40:16.270 --> 00:40:22.840
Um, and the state-of-the-art in clinical medicine, uh,

00:40:22.840 --> 00:40:24.100
kind of until this work,

00:40:24.100 --> 00:40:26.425
there's been more recently but, uh, it's,

00:40:26.425 --> 00:40:30.595
it's sort of this single task logistic regress- lo- lo- logistic regression setup.

00:40:30.595 --> 00:40:32.080
Where you have some input features,

00:40:32.080 --> 00:40:35.185
and then you're making some output predictions like true or false.

00:40:35.185 --> 00:40:41.260
Um, you can add some layers and start making it deep learning which is much fancier.

00:40:41.260 --> 00:40:45.340
Um, you can have a bunch of tasks in order to

00:40:45.340 --> 00:40:49.420
do a bunch of logistic regression tasks for a clinical environment.

00:40:49.420 --> 00:40:51.850
Um, or you can use multitask learning, uh,

00:40:51.850 --> 00:40:55.900
which is taking the basic deep learning model and adding a bunch of heads to it,

00:40:55.900 --> 00:40:58.150
uh, predicted jointly at the same time.

00:40:58.150 --> 00:41:01.660
Um, and here we had a bunch of diagnosis data.

00:41:01.660 --> 00:41:04.420
So, um, we predicted things like depression,

00:41:04.420 --> 00:41:07.585
anxiety, uh, post-traumatic stress disorder.

00:41:07.585 --> 00:41:10.900
Um, we also added in gender because this is

00:41:10.900 --> 00:41:13.900
something that the clinicians told us actually, uh,

00:41:13.900 --> 00:41:16.180
had some correlation with some of these conditions,

00:41:16.180 --> 00:41:18.970
and that they actually used it in making decisions themselves,

00:41:18.970 --> 00:41:21.535
for whether or not someone was likely to,

00:41:21.535 --> 00:41:24.010
uh, attempt, uh, suicide or not.

00:41:24.010 --> 00:41:27.310
Um, and this also used this idea of comorbidity.

00:41:27.310 --> 00:41:32.935
So multi-task learning is actually kind of perfect for comorbidity in clinical domains.

00:41:32.935 --> 00:41:34.870
So comorbidity is, um,

00:41:34.870 --> 00:41:36.220
when you have one condition,

00:41:36.220 --> 00:41:38.185
you're a lot more likely to have another.

00:41:38.185 --> 00:41:39.820
Um, so people who have

00:41:39.820 --> 00:41:43.630
post-traumatic stress disorder are much more likely to have depression and anxiety.

00:41:43.630 --> 00:41:46.359
Um, and depression and anxiety tend to be cormorbid,

00:41:46.359 --> 00:41:48.695
so people who have one often have the other.

00:41:48.695 --> 00:41:52.320
So this points to the fact- this points to the idea that perhaps there's

00:41:52.320 --> 00:41:55.680
some underlying representation that is similar across them,

00:41:55.680 --> 00:41:57.990
that can be leveraged in a deep learning model,

00:41:57.990 --> 00:42:00.885
with individual heads further specifying,

00:42:00.885 --> 00:42:04.170
uh, each of the different kinds of conditions.

00:42:04.170 --> 00:42:07.960
Um, and so what we found was that as we moved from

00:42:07.960 --> 00:42:12.160
logistic regression to the single task deep learning to the multi-task deep learning,

00:42:12.160 --> 00:42:14.605
we were able to get significantly better results.

00:42:14.605 --> 00:42:17.725
And this was true both in the suicide risk case where we had a,

00:42:17.725 --> 00:42:19.510
a lot of data, as well as

00:42:19.510 --> 00:42:22.960
the post-traumatic stress disorder case where we had very little data.

00:42:22.960 --> 00:42:25.270
Um, the behavior here was a little bit different.

00:42:25.270 --> 00:42:28.660
So going from logistic regression to,

00:42:28.660 --> 00:42:30.490
um, single task deep learning,

00:42:30.490 --> 00:42:32.065
when we had, um,

00:42:32.065 --> 00:42:33.685
a lot of data, uh,

00:42:33.685 --> 00:42:36.295
as we did with the suicide risk, um,

00:42:36.295 --> 00:42:38.230
had the single task deep learning model

00:42:38.230 --> 00:42:40.690
working better than the logistic regression model.

00:42:40.690 --> 00:42:43.105
Um, but when we have very few instances, uh,

00:42:43.105 --> 00:42:46.270
this is where the deep learning models really struggled a lot more.

00:42:46.270 --> 00:42:50.320
Um, and so the logistic regression models were actually much better.

00:42:50.320 --> 00:42:54.130
But once we started adding heads for the cormorbid different kinds of conditions,

00:42:54.130 --> 00:42:55.780
the different kinds of tasks, um,

00:42:55.780 --> 00:42:57.400
that related to, you know,

00:42:57.400 --> 00:42:59.845
whether or not the person might be committing suicide, um,

00:42:59.845 --> 00:43:01.210
we were able to, uh,

00:43:01.210 --> 00:43:03.145
bump the accuracy way back up again.

00:43:03.145 --> 00:43:05.485
Um, and, it, you know,

00:43:05.485 --> 00:43:09.235
it's roughly 120 at-risk individuals that we were able to collect, uh,

00:43:09.235 --> 00:43:12.250
in the suicide case that we wouldn't have otherwise been able to,

00:43:12.250 --> 00:43:15.110
to notice as being at risk.

00:43:16.110 --> 00:43:20.050
Um, one of the approaches we took in this was to

00:43:20.050 --> 00:43:24.670
contextualize and consider the ethical dimensions of releasing this kind of technology.

00:43:24.670 --> 00:43:28.900
So, um, it's really common in NLP papers to give examples.

00:43:28.900 --> 00:43:31.030
Um, but this was an area where we decided that

00:43:31.030 --> 00:43:33.475
giving examples of like depressed language,

00:43:33.475 --> 00:43:35.860
could be used to discriminate against people,

00:43:35.860 --> 00:43:37.480
like at, you know, job,

00:43:37.480 --> 00:43:39.565
interviews, or something like that, you know,

00:43:39.565 --> 00:43:42.085
the sort of armchair psychology approach.

00:43:42.085 --> 00:43:45.240
So we decided that while it was important to talk about the technique,

00:43:45.240 --> 00:43:47.190
and the utility of multitask learning in

00:43:47.190 --> 00:43:51.840
a clinical domain and for bringing in inclusion of underrepresented subgroups,

00:43:51.840 --> 00:43:54.210
it had to be balanced with the fact that there was a lot of

00:43:54.210 --> 00:43:57.030
risk in talking about depression,

00:43:57.030 --> 00:44:00.270
and anxiety, and how those kinds of things could be predicted.

00:44:00.270 --> 00:44:03.435
Um, so we tried to take a more balanced approach here, um,

00:44:03.435 --> 00:44:07.020
and since then I've been putting ethical considerations in all of my papers.

00:44:07.020 --> 00:44:10.420
Um, it's becoming more and more common actually.

00:44:10.790 --> 00:44:15.565
Um, so another kind of approach that's now turning this on its head,

00:44:15.565 --> 00:44:18.535
where you're trying to remove some effect, um,

00:44:18.535 --> 00:44:20.230
mitigate bias in some way,

00:44:20.230 --> 00:44:22.390
is adversarial multi-task learning.

00:44:22.390 --> 00:44:24.010
So I just talked about multi-task learning,

00:44:24.010 --> 00:44:26.065
and I'll talk about the adversarial case.

00:44:26.065 --> 00:44:30.205
Um, and the idea in the adversarial case is that you have a few heads.

00:44:30.205 --> 00:44:32.470
Um, one is predicting the main task,

00:44:32.470 --> 00:44:35.200
and the other one is predicting the thing that you don't

00:44:35.200 --> 00:44:38.290
want to be affecting your model's predictions.

00:44:38.290 --> 00:44:42.670
So for example, something like whether or not someone should be promoted based on,

00:44:42.670 --> 00:44:44.650
uh, you know, their performance reviews,

00:44:44.650 --> 00:44:46.285
and things like that.

00:44:46.285 --> 00:44:49.555
Um, you don't want that to be affected by their gender.

00:44:49.555 --> 00:44:53.260
Ideally, gender is independent of a promotion decision.

00:44:53.260 --> 00:44:54.805
And so you can, uh,

00:44:54.805 --> 00:44:57.385
you can create a model for this that actually,

00:44:57.385 --> 00:44:59.620
uh, puts that independence, um,

00:44:59.620 --> 00:45:02.365
criteria in place by saying, uh,

00:45:02.365 --> 00:45:05.485
I want to minimize my loss on the promotion,

00:45:05.485 --> 00:45:07.885
while maximizing my loss on the gender.

00:45:07.885 --> 00:45:10.390
And so how we're doing that is just predicting gender,

00:45:10.390 --> 00:45:12.250
and then negating the gradient.

00:45:12.250 --> 00:45:15.010
So removing the effect of that signal.

00:45:15.010 --> 00:45:17.920
Um, this is another adversarial approach.

00:45:17.920 --> 00:45:20.950
So you might have been familiar with like generative adversarial networks.

00:45:20.950 --> 00:45:23.470
So this is like two discriminators, uh,

00:45:23.470 --> 00:45:25.435
two different task heads, uh,

00:45:25.435 --> 00:45:28.270
where one is trying to do the task that we care about,

00:45:28.270 --> 00:45:30.745
and the other one is removing the signal, uh,

00:45:30.745 --> 00:45:32.320
that we really don't want to,

00:45:32.320 --> 00:45:35.890
um, uh, be coming into play in our downstream predictions.

00:45:35.890 --> 00:45:37.900
Um, so this is a way of,

00:45:37.900 --> 00:45:39.670
uh, kind of putting this into practice.

00:45:39.670 --> 00:45:41.410
So the probability of your output,

00:45:41.410 --> 00:45:43.030
uh, predicted output given the,

00:45:43.030 --> 00:45:46.540
the ground truth and your sensitive attribute like gender, um,

00:45:46.540 --> 00:45:48.835
is equal across all the different, uh,

00:45:48.835 --> 00:45:52.060
sensitive attributes or equal across all the different genders.

00:45:52.060 --> 00:45:56.410
Um, and that's an example of equality of opportunity in supervised learning,

00:45:56.410 --> 00:45:57.775
being put into practice.

00:45:57.775 --> 00:46:00.220
So this is one of the key fairness definitions.

00:46:00.220 --> 00:46:01.750
It's equivalent to, uh,

00:46:01.750 --> 00:46:05.320
equal recall across different subgroups as I mentioned earlier.

00:46:05.320 --> 00:46:07.495
Um, and that's a model that will actually,

00:46:07.495 --> 00:46:10.270
uh, implement that or help you achieve that.

00:46:10.270 --> 00:46:13.375
Um, where you're saying that a classifier's output decisions should be the same

00:46:13.375 --> 00:46:16.480
across sensitive characteristics given what the,

00:46:16.480 --> 00:46:19.070
what the correct decision should be.

00:46:20.190 --> 00:46:23.365
Okay, so how are we on time?

00:46:23.365 --> 00:46:30.265
Cool. Are there any questions so far? Are we good?

00:46:30.265 --> 00:46:35.350
Okay, cool. So I'm gonna go into a little bit of a case study now, an end-to-end, uh,

00:46:35.350 --> 00:46:37.780
system that Google has been working on, uh,

00:46:37.780 --> 00:46:39.325
my colleagues have been working on, uh,

00:46:39.325 --> 00:46:42.790
that is in NLP domain and deals with some of these bias issues.

00:46:42.790 --> 00:46:46.675
Um, so you can find out more about this work, um,

00:46:46.675 --> 00:46:51.745
in papers at AIES in 2018 and FAT* tutorial 2019,

00:46:51.745 --> 00:46:56.410
um, called Measuring and Mitigating Unintended Bias in Text Classification.

00:46:56.410 --> 00:47:01.345
Um, and this came out of Conversation-AI which is a, uh,

00:47:01.345 --> 00:47:03.505
which is a product that's, um,

00:47:03.505 --> 00:47:07.720
like it's part of this- it's called a bet at Google.

00:47:07.720 --> 00:47:10.810
It's a kind of spin-off company called Jigsaw that

00:47:10.810 --> 00:47:14.245
focuses on trying to like combat abuse online.

00:47:14.245 --> 00:47:16.225
Um, and the Conversation-AI, uh,

00:47:16.225 --> 00:47:20.065
team is trying to use deep learning to improve online conversations.

00:47:20.065 --> 00:47:23.320
Um, and collaborate with a ton of different,

00:47:23.320 --> 00:47:25.375
uh, different people to do that.

00:47:25.375 --> 00:47:27.805
Um, so how this works is,

00:47:27.805 --> 00:47:30.460
oh you can try it out too, on perspectiveapi.com.

00:47:30.460 --> 00:47:33.970
So given some phrase like you're a dork, uh,

00:47:33.970 --> 00:47:41.358
it puts out a toxicity score associated to that like 0.91. [NOISE]

00:47:41.358 --> 00:47:44.140
Um, and the model starts sort of falsely associating

00:47:44.140 --> 00:47:47.305
frequently attacked identities with toxicity.

00:47:47.305 --> 00:47:50.245
So this is a kind of false positive bias.

00:47:50.245 --> 00:47:53.080
So I'm a proud tall person gets a model,

00:47:53.080 --> 00:47:55.360
uh, toxicity score of 0.18.

00:47:55.360 --> 00:47:56.995
I'm a proud, uh,

00:47:56.995 --> 00:48:00.925
gay person gets a toxicity model score of 0.69.

00:48:00.925 --> 00:48:06.805
And this is because these- the term gay tends to be used in really toxic situations.

00:48:06.805 --> 00:48:10.960
And so the model starts to learn that gay itself is toxic.

00:48:10.960 --> 00:48:12.730
But that's not actually what we want,

00:48:12.730 --> 00:48:15.865
and we don't want these kinds of predictions coming out of the model.

00:48:15.865 --> 00:48:22.705
Um, so, uh, the bias is largely caused here by the dataset imbalance.

00:48:22.705 --> 00:48:25.765
Again, this is data kinda coming and wearing its hat again.

00:48:25.765 --> 00:48:28.000
Um, so frequently attacked, uh,

00:48:28.000 --> 00:48:31.180
identities are really overrepresented in toxic comments.

00:48:31.180 --> 00:48:35.155
There's a lot of toxicity towards LGBTQ identities, um,

00:48:35.155 --> 00:48:37.240
it's really horrible to work on this stuff that like

00:48:37.240 --> 00:48:39.985
really [LAUGHTER] it can really affect you personally.

00:48:39.985 --> 00:48:42.790
Um, uh, and, uh,

00:48:42.790 --> 00:48:48.100
one of the approaches that the team took was just to add nontoxic data from Wikipedia.

00:48:48.100 --> 00:48:54.025
So helping to- helping the model to understand that these kinds of terms can be used in,

00:48:54.025 --> 00:48:58.040
you know, more positive sorts of contexts.

00:49:00.720 --> 00:49:03.940
One of the challenges with measuring, uh,

00:49:03.940 --> 00:49:06.295
how well the system was doing is that there's not

00:49:06.295 --> 00:49:11.725
a really nice way to have controlled toxicity evaluation.

00:49:11.725 --> 00:49:13.750
Um, so in real-world conversation,

00:49:13.750 --> 00:49:18.805
it can be kind of anyone's guess what the toxicity is of a specific sentence.

00:49:18.805 --> 00:49:21.070
Um, if you really wanna control for a different kind of

00:49:21.070 --> 00:49:24.040
subgroups or intersectional subgroups,

00:49:24.040 --> 00:49:25.810
and it can be even harder to get, uh,

00:49:25.810 --> 00:49:28.765
a real good data to evaluate properly.

00:49:28.765 --> 00:49:32.395
So what the team ended up doing was developing a synthetic data approach.

00:49:32.395 --> 00:49:35.185
Um, so this is kind of like a bias Mad Libs.

00:49:35.185 --> 00:49:37.465
Um, where you take template sentences [NOISE], um,

00:49:37.465 --> 00:49:41.380
and you use those for evaluation. This is the kind of, um,

00:49:41.380 --> 00:49:45.280
evaluation you'd want to use in addition to your target downstream

00:49:45.280 --> 00:49:47.650
ah, kind of dataset.

00:49:47.650 --> 00:49:51.685
But this helps you get at the biases specifically.

00:49:51.685 --> 00:49:55.840
So, um, some template phrase like I am a proud blank person,

00:49:55.840 --> 00:49:58.480
and then filling in different subgroup identities.

00:49:58.480 --> 00:50:01.660
And you don't want to release the model unless you see that

00:50:01.660 --> 00:50:04.990
the scores across these different kinds of, uh,

00:50:04.990 --> 00:50:08.860
these different kinds of template sentences with synthetic, uh,

00:50:08.860 --> 00:50:10.795
the synthetic template sentences, um,

00:50:10.795 --> 00:50:14.320
are relatively kind of the same across, ah, yeah.

00:50:14.320 --> 00:50:17.210
All of the different model runs.

00:50:17.460 --> 00:50:24.995
Cool. Um, so some assumptions that they made in this was that the dataset, um, uh,

00:50:24.995 --> 00:50:28.090
didn't have annotated bias and they didn't do

00:50:28.090 --> 00:50:31.525
any causal analysis because they were just trying to focus in particular,

00:50:31.525 --> 00:50:33.745
um, on this toxicity problem.

00:50:33.745 --> 00:50:36.880
Um, they used the CNN,

00:50:36.880 --> 00:50:39.610
ah, convolutional, yeah you guys know, blah, blah, blah.

00:50:39.610 --> 00:50:41.650
Uh, with pretrained chain GloVe embeddings.

00:50:41.650 --> 00:50:43.240
This is probably like your bread and butter.

00:50:43.240 --> 00:50:44.170
Pretrained GloVe embeddings.

00:50:44.170 --> 00:50:45.940
I'm sure you know all about this in Word2vec.

00:50:45.940 --> 00:50:48.250
Cool, uh, Keras implementation of this.

00:50:48.250 --> 00:50:53.470
Um, and, uh, and using these kind of data augmentation approaches, um,

00:50:53.470 --> 00:50:55.420
both a Wikipedia, uh,

00:50:55.420 --> 00:51:00.955
kind of approach as well as actually collecting positive statements about LGBTQ identity.

00:51:00.955 --> 00:51:03.730
So there's this project called Project Respect at Google,

00:51:03.730 --> 00:51:04.990
where we go out and,

00:51:04.990 --> 00:51:08.890
and talk to people who identify as queer or people who have friends who do,

00:51:08.890 --> 00:51:11.290
and like talk about this in a positive way,

00:51:11.290 --> 00:51:13.120
and we add this as data.

00:51:13.120 --> 00:51:17.035
Um, so we can actually know that this is can be a positive thing.

00:51:17.035 --> 00:51:21.385
Um, and in order to measure the model performance here, um,

00:51:21.385 --> 00:51:25.480
again it's looking at the differences across different subgroups and trying to

00:51:25.480 --> 00:51:29.695
compare also the subgroup performance to some sort of general distribution.

00:51:29.695 --> 00:51:31.930
So here they use AUC, um,

00:51:31.930 --> 00:51:34.630
where AUC is essentially the probability that a model will

00:51:34.630 --> 00:51:37.975
give a randomly sel- selected positive example,

00:51:37.975 --> 00:51:42.070
a higher score than a randomly selected, uh, negative example.

00:51:42.070 --> 00:51:44.890
So, um, here you can see some toxic comments and

00:51:44.890 --> 00:51:49.240
nontoxic comments with a example sort of low AUC.

00:51:49.240 --> 00:51:52.225
Um, here, ah, this is an

00:51:52.225 --> 00:51:53.770
example with a high AUC,

00:51:53.770 --> 00:51:58.000
so the model is doing a relatively good job of separating these two kinds of comments.

00:51:58.000 --> 00:52:02.350
Um, and there are different kinds of biases that they've defined in this work.

00:52:02.350 --> 00:52:05.080
So, uh, low subgroup performance means that

00:52:05.080 --> 00:52:08.005
the model performs worse on subgroup comments than it does,

00:52:08.005 --> 00:52:09.625
ah, on comments overall.

00:52:09.625 --> 00:52:14.150
And the metric they've introduced to measure this is called subgroup AUC.

00:52:14.220 --> 00:52:17.350
Um, another one is subgroup shift.

00:52:17.350 --> 00:52:19.960
And that's when the model systematically scores comments,

00:52:19.960 --> 00:52:22.120
um, from some subgroup higher.

00:52:22.120 --> 00:52:24.280
Um, so this is sort of like to the right.

00:52:24.280 --> 00:52:26.620
Um, and then there's also, uh,

00:52:26.620 --> 00:52:31.220
this Background Positive Subgroup Negative shifting to the left.

00:52:31.620 --> 00:52:36.775
Yeah. Um, yeah that's sort of saying what I said.

00:52:36.775 --> 00:52:38.980
It can go either way to the right or the left and there's just

00:52:38.980 --> 00:52:42.740
kind of different metrics that can define each of these.

00:52:42.780 --> 00:52:46.000
Cool. Um, and the results in this,

00:52:46.000 --> 00:52:49.765
ah, sort of going through not only just looking at, you know,

00:52:49.765 --> 00:52:53.380
qualitative examples, um, and general evaluation metrics,

00:52:53.380 --> 00:52:56.470
but also focusing in on some of the key metrics defined for this work,

00:52:56.470 --> 00:52:58.465
these sort of AUC-based approaches.

00:52:58.465 --> 00:53:01.450
And they were able to see significant differences in

00:53:01.450 --> 00:53:05.184
the original release which didn't account for any of these unintended biases,

00:53:05.184 --> 00:53:07.510
and downstream releases, uh, which did,

00:53:07.510 --> 00:53:10.195
which incorporated this kind of normative data

00:53:10.195 --> 00:53:14.690
that said the sort of things that we thought the model should be learning.

00:53:15.390 --> 00:53:18.250
Cool. Um, so, um,

00:53:18.250 --> 00:53:20.980
the last thing to keep in mind as you sort of develop and,

00:53:20.980 --> 00:53:25.840
and work towards, uh, creating deeper better models is to release responsibly.

00:53:25.840 --> 00:53:28.450
Um, so this is a project I've been working on with

00:53:28.450 --> 00:53:31.510
a ton of different people called Model Cards for Model Reporting.

00:53:31.510 --> 00:53:36.280
It's, uh, it's a little bit of like the next step after Datasheets for Datasets,

00:53:36.280 --> 00:53:41.335
um, where, um, Datasheets for Datasets focuses on information about the data.

00:53:41.335 --> 00:53:45.490
Ah, Model Cards for Model Reporting focuses on information about the model.

00:53:45.490 --> 00:53:47.935
Um, so it captures what it does,

00:53:47.935 --> 00:53:50.005
how it works, why it matters.

00:53:50.005 --> 00:53:55.600
Um, and one of the key ideas here is disaggregated in intersectional evaluation.

00:53:55.600 --> 00:53:57.055
So it's not enough, uh,

00:53:57.055 --> 00:54:00.430
any more to put out human-centered technology that just

00:54:00.430 --> 00:54:04.105
has some vague overall score associated to it.

00:54:04.105 --> 00:54:07.855
You actually need to understand how it works across different subpopulations.

00:54:07.855 --> 00:54:11.620
And you have to understand what the data is telling you that.

00:54:11.620 --> 00:54:14.530
Um, so here's some example details that a

00:54:14.530 --> 00:54:16.000
model card would have,

00:54:16.000 --> 00:54:17.410
um, who it's developed by,

00:54:17.410 --> 00:54:18.970
what the intended use is,

00:54:18.970 --> 00:54:22.715
so that it doesn't start being used in ways that it's not intended to be used.

00:54:22.715 --> 00:54:25.350
Um, the factors that are likely to be

00:54:25.350 --> 00:54:28.125
affected by disproportionate performance of the model.

00:54:28.125 --> 00:54:31.430
Um, so different kinds of identity groups, things like that.

00:54:31.430 --> 00:54:33.580
Um, the metrics that, ah,

00:54:33.580 --> 00:54:37.150
that you're deciding to use in order to understand the fairness of the model or

00:54:37.150 --> 00:54:42.025
the different performance of the model across different kinds of subgroups and factors,

00:54:42.025 --> 00:54:45.970
information about the evaluation data and training data.

00:54:45.970 --> 00:54:49.210
Um, as well as ethical considerations, um,

00:54:49.210 --> 00:54:51.010
so what were some of the things you took into

00:54:51.010 --> 00:54:53.800
account or what are some of the risks and benefits,

00:54:53.800 --> 00:54:56.935
um, that, uh, that are relevant to this model?

00:54:56.935 --> 00:54:59.455
Um, and additional caveats and recommendations.

00:54:59.455 --> 00:55:02.740
So for example, in the conversation AI case,

00:55:02.740 --> 00:55:04.270
they're working with synthetic data.

00:55:04.270 --> 00:55:08.395
So this is the sort of limitation of the evaluation that's important to understand, uh,

00:55:08.395 --> 00:55:11.020
because it can tell you a lot about the biases,

00:55:11.020 --> 00:55:13.660
but doesn't tell you a lot about how it works generally.

00:55:13.660 --> 00:55:19.570
[NOISE] And then the key component in the quantitative,

00:55:19.570 --> 00:55:21.730
uh, section of the model card is to have

00:55:21.730 --> 00:55:24.640
this both intersectional and disaggregated evaluation.

00:55:24.640 --> 00:55:28.300
And from here, you trivially get to different kinds of fairness definitions.

00:55:28.300 --> 00:55:30.925
The closer you get to parity across subgroups,

00:55:30.925 --> 00:55:34.840
the closer you're getting to something that is mathematically fair.

00:55:34.840 --> 00:55:39.175
Okay. So hopefully by paying attention to these kinds of approaches,

00:55:39.175 --> 00:55:41.080
taking into account all these kinds of things,

00:55:41.080 --> 00:55:44.080
we can move from majority representation of data in

00:55:44.080 --> 00:55:47.620
our models to something more like diverse representation,

00:55:47.620 --> 00:55:49.435
uh, from our ethical AI.

00:55:49.435 --> 00:55:50.905
Okay. That's it.

00:55:50.905 --> 00:56:02.540
Thanks. [APPLAUSE]

