WEBVTT
Kind: captions
Language: en

00:00:04.850 --> 00:00:07.410
可以。大家好。

00:00:07.410 --> 00:00:11.430
我们开始吧[噪音]好的。

00:00:11.430 --> 00:00:14.280
所以，今天的讲座，

00:00:14.280 --> 00:00:20.025
我们将要做的是研究具有树递归神经网络的主题。

00:00:20.025 --> 00:00:22.110
我是说，这实际上是，呃，

00:00:22.110 --> 00:00:26.660
我特别喜欢和喜欢的话题，

00:00:26.660 --> 00:00:32.975
因为事实上，当我们2010年开始在斯坦福大学为NLP做深入学习时，

00:00:32.975 --> 00:00:37.940
从2010年到2015年，

00:00:37.940 --> 00:00:42.320
我们正在研究的主要观点是关于你如何

00:00:42.320 --> 00:00:46.945
可以在神经网络中建立递归树结构。

00:00:46.945 --> 00:00:50.645
所以在某种程度上，我现在才开始做这件事有点可笑。

00:00:50.645 --> 00:00:53.420
我是说，这是有原因的，

00:00:53.420 --> 00:00:56.720
但我觉得有很多有趣的想法

00:00:56.720 --> 00:01:00.365
这与语言结构密切相关，

00:01:00.365 --> 00:01:02.720
所以这是件好事。

00:01:02.720 --> 00:01:04.625
但实际上，嗯，

00:01:04.625 --> 00:01:08.105
事实证明，这些想法难以扩展。

00:01:08.105 --> 00:01:11.870
在实践中不一定比

00:01:11.870 --> 00:01:15.140
我们花了更多时间在这类事情上

00:01:15.140 --> 00:01:19.100
意思是像看LSTM和变压器，

00:01:19.100 --> 00:01:20.570
像这样的事情。

00:01:20.570 --> 00:01:25.860
所以这就是为什么我们把他们分流到课程的末尾。

00:01:25.860 --> 00:01:29.090
但我想谈谈动机，

00:01:29.090 --> 00:01:31.190
你建造树结构的方法，

00:01:31.190 --> 00:01:34.220
以及神经网络，看看其中的一些可能性，

00:01:34.220 --> 00:01:36.065
嗯，我们探索了嗯，

00:01:36.065 --> 00:01:38.170
在这节课上。

00:01:38.170 --> 00:01:44.690
嗯，关于这门课的另一个事实是，这是我要上的最后一门课。

00:01:44.690 --> 00:01:47.060
嗯，下个星期再上两节课。

00:01:47.060 --> 00:01:49.320
别忘了下周，嗯，

00:01:49.320 --> 00:01:52.275
CS224N类，嗯，

00:01:52.275 --> 00:01:54.220
但是周二，嗯，

00:01:54.220 --> 00:01:57.280
我们已经邀请了最后一位演讲者，

00:01:57.280 --> 00:02:00.410
谁演说家很好，有很多有趣的东西

00:02:00.410 --> 00:02:04.865
在全国人民党和人工智能中谈论公平和道德。

00:02:04.865 --> 00:02:06.425
最后一场演讲，

00:02:06.425 --> 00:02:09.650
我的另一个博士生是

00:02:09.650 --> 00:02:13.270
我会告诉你最近发生的事情，

00:02:13.270 --> 00:02:16.965
2018年19月，在深度学习方面发生了什么，

00:02:16.965 --> 00:02:20.975
关于NLP和深度学习的一些最新发展。嗯，

00:02:20.975 --> 00:02:24.875
所以，嗯，让我们-我会在这一个结尾说我的告别。

00:02:24.875 --> 00:02:28.730
嗯，希望每个人都提交了，嗯，

00:02:28.730 --> 00:02:33.380
他们的，嗯，是他们最后一个项目的里程碑。

00:02:33.380 --> 00:02:37.310
如果你没有，你应该在-嗯开始你的里程碑，

00:02:37.310 --> 00:02:40.640
你知道，在这附近的某个地方，

00:02:40.640 --> 00:02:46.290
开始出现一些问题，人们的情况是什么都不起作用，

00:02:46.290 --> 00:02:48.855
一切都太慢了，你很惊慌。

00:02:48.855 --> 00:02:52.095
嗯，而且，嗯，这种情况发生了。

00:02:52.095 --> 00:02:54.030
嗯，我当然祝你好运。

00:02:54.030 --> 00:02:55.900
我是说，你能怎么办？

00:02:55.900 --> 00:03:00.785
我的意思是，当你有一些不工作的事情要解决的时候会很困难，

00:03:00.785 --> 00:03:02.255
为什么他们不工作，

00:03:02.255 --> 00:03:03.800
以及如何修复它们。

00:03:03.800 --> 00:03:08.660
我的意思是，我认为通常最好的办法就是回到过去

00:03:08.660 --> 00:03:14.405
简单地说，你可以开始工作，并从那里继续前进。

00:03:14.405 --> 00:03:18.980
拥有非常小的数据集也很有帮助。

00:03:18.980 --> 00:03:23.540
我真的推荐了一个10个项目的策略，

00:03:23.540 --> 00:03:28.510
或者20项数据集，检查您的模型是否工作正常，

00:03:28.510 --> 00:03:31.320
在列车上，这种数据100%准确。

00:03:31.320 --> 00:03:34.610
设置可以节省大量时间，

00:03:34.610 --> 00:03:39.170
在你得到一些简单的数据之后，

00:03:39.170 --> 00:03:41.780
那是个合适的时机，

00:03:41.780 --> 00:03:44.600
嗯，再向前展开。

00:03:44.600 --> 00:03:47.330
嗯，你一定要确保

00:03:47.330 --> 00:03:49.970
您可以完全过度使用培训数据集。

00:03:49.970 --> 00:03:51.180
有点，嗯，

00:03:51.180 --> 00:03:52.370
不是很好的证据，

00:03:52.370 --> 00:03:56.780
但它至少是正确实现模型的第一个良好需求。

00:03:56.780 --> 00:04:01.130
嗯，你，你知道的是

00:04:01.130 --> 00:04:03.920
一个成功的深度学习研究者实际上是

00:04:03.920 --> 00:04:07.565
努力完成任务，而不是浪费大量时间。

00:04:07.565 --> 00:04:10.300
所以它总是有帮助的，你知道，

00:04:10.300 --> 00:04:12.975
在你进行训练的过程中策划

00:04:12.975 --> 00:04:16.160
开发错误，这样你就可以判断事情是否有效，

00:04:16.160 --> 00:04:17.900
或者如果事情不起作用，

00:04:17.900 --> 00:04:20.930
你应该放弃，重新开始新的实验，

00:04:20.930 --> 00:04:25.505
这样可以节省你很多时间，让你完成更多的工作。

00:04:25.505 --> 00:04:27.350
所以一旦一切正常，

00:04:27.350 --> 00:04:30.080
有很多事情可以让它更好地工作。

00:04:30.080 --> 00:04:33.410
有正规化的L2和辍学，

00:04:33.410 --> 00:04:36.969
有时间进行超参数搜索，

00:04:36.969 --> 00:04:38.530
嗯，而且，你知道，

00:04:38.530 --> 00:04:42.620
经常做这些事情，对

00:04:42.620 --> 00:04:46.820
你的最终结果是，所以有时间做这些事情是很好的。

00:04:46.820 --> 00:04:48.680
但很明显，你想得到一些东西，嗯，

00:04:48.680 --> 00:04:51.590
在你开始之前先工作，嗯，

00:04:51.590 --> 00:04:54.380
有点鼓励人们

00:04:54.380 --> 00:04:57.470
如果你有什么问题，在上班时间顺便过来，

00:04:57.470 --> 00:05:00.530
我们会尽最大努力帮助这里

00:05:00.530 --> 00:05:05.205
我们所能做的，仅仅是被问题冷落的限制。

00:05:05.205 --> 00:05:08.240
好的，嗯，是的。

00:05:08.240 --> 00:05:13.595
所以，我想说一些关于，嗯，

00:05:13.595 --> 00:05:16.680
语言和语言理论，

00:05:16.680 --> 00:05:22.790
嗯，在激发这些树递归网络的上下文中。

00:05:22.790 --> 00:05:27.485
嗯，这是卡内基梅隆大学的一个艺术装置。

00:05:27.485 --> 00:05:29.150
作为一个全国人民党人，

00:05:29.150 --> 00:05:31.700
我真的很喜欢这个艺术装置。

00:05:31.700 --> 00:05:36.805
嗯，所以我们需要斯坦福工程学院周围更好的艺术设施。

00:05:36.805 --> 00:05:40.510
嗯，这是一袋文字艺术装置。

00:05:40.510 --> 00:05:42.710
包里有很多字。

00:05:42.710 --> 00:05:44.120
你看下面，

00:05:44.120 --> 00:05:47.015
有停止语，还有美国，

00:05:47.015 --> 00:05:49.135
从袋子里掉出来的，

00:05:49.135 --> 00:05:52.175
在地面上用停止语表示。

00:05:52.175 --> 00:05:55.730
漂亮的艺术品，对吗？所以，嗯，

00:05:55.730 --> 00:06:02.690
关于语言的NLP模型发现了一件有趣的事情，

00:06:02.690 --> 00:06:04.670
我认为这在

00:06:04.670 --> 00:06:08.600
比以前更深刻的学习世界是，

00:06:08.600 --> 00:06:11.510
孩子，你可以用一袋字做模特，对吧？

00:06:11.510 --> 00:06:15.920
你可以经常说，

00:06:15.920 --> 00:06:18.400
好吧，让我们得到我们的神经词向量，

00:06:18.400 --> 00:06:20.670
我们要取平均值或者最大值，

00:06:20.670 --> 00:06:21.975
或者像这样，

00:06:21.975 --> 00:06:23.580
什么都不做，

00:06:23.580 --> 00:06:26.300
这给了我一个很好的句子表达，或者

00:06:26.300 --> 00:06:30.035
我可以在分类器或其他东西中使用的文档表示。

00:06:30.035 --> 00:06:33.830
有时候，你可以做的不多，甚至更好。

00:06:33.830 --> 00:06:37.910
所以人们做了像深度平均网络这样的事情

00:06:37.910 --> 00:06:40.400
一袋词的输出模式和种类

00:06:40.400 --> 00:06:43.475
再多给它几层，然后改进。

00:06:43.475 --> 00:06:47.300
所以这完全不同于

00:06:47.300 --> 00:06:51.560
在语言学中，观察语言结构的主导因素是什么？

00:06:51.560 --> 00:06:58.340
在语言学中，重点是识别

00:06:58.340 --> 00:07:05.480
通过非常复杂的形式，大量的语言话语结构。

00:07:05.480 --> 00:07:10.455
我想这有点像乔姆斯基的极简主义句法树，

00:07:10.455 --> 00:07:15.260
最上面的那张图有点像是一幅由头部驱动的短语结构语法图。

00:07:15.260 --> 00:07:18.035
这是一个主要的理论，嗯，

00:07:18.035 --> 00:07:21.935
90年代在斯坦福大学开发的。

00:07:21.935 --> 00:07:25.070
嗯，但数据结构非常复杂，

00:07:25.070 --> 00:07:28.895
用来描述语言学的结构。

00:07:28.895 --> 00:07:32.645
这两件事之间有很大的差距。

00:07:32.645 --> 00:07:36.260
你可能会想，你知道，当然，

00:07:36.260 --> 00:07:41.180
中间有一些好的地方，我们有一定的结构，

00:07:41.180 --> 00:07:43.460
这将帮助我们做我们想做的。

00:07:43.460 --> 00:07:46.525
尤其是，嗯，

00:07:46.525 --> 00:07:49.745
如果我们想用语义解释语言，

00:07:49.745 --> 00:07:53.300
似乎我们不只是想有词汇载体，

00:07:53.300 --> 00:07:56.120
我们想要更大的词组的含义。

00:07:56.120 --> 00:07:59.705
所以这里是滑雪运动员跳过一个大亨，

00:07:59.705 --> 00:08:03.170
一个人在滑雪板上跳入空中。

00:08:03.170 --> 00:08:06.635
我们想说的是滑雪运动员

00:08:06.635 --> 00:08:10.790
意思基本上和滑雪板上的人一样。

00:08:10.790 --> 00:08:13.220
所以我们想要这些

00:08:13.220 --> 00:08:17.255
在语言学中是构成短语的语言，

00:08:17.255 --> 00:08:19.430
说它们有意义，

00:08:19.430 --> 00:08:21.830
我们希望能够比较它们的含义。

00:08:21.830 --> 00:08:26.870
现在，我们至少已经研究了一种工具，它允许我们使用大块的语言，对吗？

00:08:26.870 --> 00:08:30.320
因为我们研究了卷积神经网络

00:08:30.320 --> 00:08:34.805
用三个词来表示卷积神经网络，

00:08:34.805 --> 00:08:38.090
但最根本的区别在于

00:08:38.090 --> 00:08:41.254
人类语言你有这些有意义的部分，

00:08:41.254 --> 00:08:43.055
不同尺寸的。

00:08:43.055 --> 00:08:45.935
所以我们想说的是滑雪板

00:08:45.935 --> 00:08:50.120
在语义上相当于滑雪板上的人，

00:08:50.120 --> 00:08:52.790
但最上面的两个字很长，

00:08:52.790 --> 00:08:55.295
最下面的是五个字。

00:08:55.295 --> 00:08:59.200
如果我们能做到，嗯，

00:08:59.200 --> 00:09:03.230
我们不知何故想拥有这些组成部分，

00:09:03.230 --> 00:09:07.070
能够在神经网络中使用和表示它们。

00:09:07.070 --> 00:09:08.630
有点，嗯，

00:09:08.630 --> 00:09:11.870
什么的中心思想

00:09:11.870 --> 00:09:16.235
激发了一些树结构的神经网络，我将要向你们展示。

00:09:16.235 --> 00:09:21.380
还有一件相关的事情你可能会想，你知道，

00:09:21.380 --> 00:09:23.240
一个人在滑雪板上，

00:09:23.240 --> 00:09:27.950
人类如何理解这意味着什么？

00:09:27.950 --> 00:09:31.055
然后一个人在滑雪板上跳到空中，

00:09:31.055 --> 00:09:35.655
人们如何理解这意味着什么？

00:09:35.655 --> 00:09:41.030
这似乎是唯一可能的答案

00:09:41.030 --> 00:09:46.595
这就是通常所说的合成性原则。

00:09:46.595 --> 00:09:49.040
人们知道“人”这个词，

00:09:49.040 --> 00:09:50.240
他们知道这个词，

00:09:50.240 --> 00:09:52.760
他们知道“滑雪板”这个词，因此，

00:09:52.760 --> 00:09:55.880
他们可以计算出滑雪板上的含义，嗯，

00:09:55.880 --> 00:10:00.170
他们知道滑雪板上的人意味着什么

00:10:00.170 --> 00:10:05.900
组件的含义，并将它们组合成更大的部分。

00:10:05.900 --> 00:10:08.295
有一个F-有一个著名的，

00:10:08.295 --> 00:10:12.500
嗯，应用数学统计学家，嗯，

00:10:12.500 --> 00:10:17.235
在布朗大学，斯图·杰曼，我猜他总结的方式是，

00:10:17.235 --> 00:10:21.680
组合性原则要么是真的，要么上帝存在。

00:10:21.680 --> 00:10:24.285
嗯，为了[笑声]他是，嗯，

00:10:24.285 --> 00:10:27.200
好吧，你想怎么做就怎么做，但是，你知道，

00:10:27.200 --> 00:10:30.480
嗯，我觉得他说的很好，你知道，

00:10:30.480 --> 00:10:32.880
你可以把这些无限的

00:10:32.880 --> 00:10:36.085
无限长的句子和人类理解它们，

00:10:36.085 --> 00:10:39.365
必须让人们知道

00:10:39.365 --> 00:10:43.650
把意义结合起来并赋予更大意义的词语和方法，因为，

00:10:43.650 --> 00:10:47.945
你知道，人们理解句子还有其他什么可能。

00:10:47.945 --> 00:10:50.375
所以我们希望能够做到这一点。

00:10:50.375 --> 00:10:54.664
我们希望能够计算出较小元素的语义组合，

00:10:54.664 --> 00:10:57.415
找出更大的部分的意义。

00:10:57.415 --> 00:11:01.200
显然这不仅仅是语言上的问题，

00:11:01.200 --> 00:11:05.050
合成性，嗯，也出现在其他地方，对吧。

00:11:05.050 --> 00:11:10.620
所以，嗯，如果你想了解一些机器是如何工作的，

00:11:10.620 --> 00:11:14.195
你想知道的是它有不同的子组件。

00:11:14.195 --> 00:11:16.140
如果你能理解

00:11:16.140 --> 00:11:19.370
不同的子部件工作，以及它们如何装配在一起，

00:11:19.370 --> 00:11:24.605
嗯，那么你可能对整个场景的工作方式有了一些了解。

00:11:24.605 --> 00:11:31.170
嗯，和，嗯，合成性似乎是工作中的视觉。

00:11:31.170 --> 00:11:35.950
所以这里有一个场景，而且这个场景似乎有部分。

00:11:35.950 --> 00:11:38.650
所以有一些小的部分在一起，对吧。

00:11:38.650 --> 00:11:41.725
所以有人聚集成一大群人，

00:11:41.725 --> 00:11:44.860
还有一个屋顶，一个二楼，还有一个小屋顶。

00:11:44.860 --> 00:11:48.850
和一楼合影成这座教堂。

00:11:48.850 --> 00:11:54.275
所以，这也是一种组合场景，在其中，片段组合在一起。

00:11:54.275 --> 00:11:58.695
所以这似乎是为了理解语言，

00:11:58.695 --> 00:12:02.810
实际上，对于我们用来做情报的其他很多事情，

00:12:02.810 --> 00:12:05.495
我们需要能够理解

00:12:05.495 --> 00:12:09.335
从对小零件的了解中获得更大的东西。

00:12:09.335 --> 00:12:14.870
嗯，是的，所以计算上的-我之前提到过的最多，

00:12:14.870 --> 00:12:16.850
有时最著名的，嗯，

00:12:16.850 --> 00:12:20.480
语言学家是麻省理工学院的诺姆·乔姆斯基，

00:12:20.480 --> 00:12:24.480
嗯，你知道，真正的计算语言学家，

00:12:24.480 --> 00:12:28.355
很多时候都不是很友好，

00:12:28.355 --> 00:12:32.590
语言学语言学家，特别是一些诺姆·乔姆斯基的，嗯，

00:12:32.590 --> 00:12:36.250
语言理论是因为他从来没有

00:12:36.250 --> 00:12:39.970
同情机器学习的想法。

00:12:39.970 --> 00:12:44.075
或者一般来说，从数据中学习的一些经验能力。

00:12:44.075 --> 00:12:46.020
他一直都是，嗯，

00:12:46.020 --> 00:12:48.575
[噪音]想要拒绝这种存在。

00:12:48.575 --> 00:12:51.470
但是，嗯，如果我们还是要找一点，

00:12:51.470 --> 00:12:53.565
嗯，这方面的见解。

00:12:53.565 --> 00:12:58.115
嗯，你知道，这是乔姆斯基最近的一篇关于作者的论文，他们是

00:12:58.115 --> 00:13:03.120
试图给出一个人类语言的独特版本。

00:13:03.120 --> 00:13:05.280
从本质上说，他们，嗯，

00:13:05.280 --> 00:13:07.985
零开始是这样的吗？

00:13:07.985 --> 00:13:09.910
如果你在看，你知道，

00:13:09.910 --> 00:13:13.730
人类与其他相当聪明的生物的较量。

00:13:13.730 --> 00:13:17.790
他们认为人类的定义差异，嗯，

00:13:17.790 --> 00:13:22.095
他们有能力对递归进行建模。

00:13:22.095 --> 00:13:27.450
因此，本文认为

00:13:27.450 --> 00:13:30.390
人类要发展的语言是

00:13:30.390 --> 00:13:33.555
可以把较小的部分组合在一起做成更大的东西，

00:13:33.555 --> 00:13:38.495
在递归过程中，这就是定义新能力的方式。

00:13:38.495 --> 00:13:41.135
嗯，不确定我-不确定我是否相信，

00:13:41.135 --> 00:13:43.920
嗯，[笑声]你可以决定你的想法。

00:13:43.920 --> 00:13:46.325
但我想，嗯，

00:13:46.325 --> 00:13:51.390
当然是这样的-这是无可争辩的

00:13:51.390 --> 00:13:56.990
人类语言句子的结构有这些片段，

00:13:56.990 --> 00:14:01.260
嗯，然后形成层次结构的成分，或者

00:14:01.260 --> 00:14:05.555
在树上递归地变成更大的片段。

00:14:05.555 --> 00:14:11.190
尤其是你得到了这个递归式，你得到了一个小名词短语meat，

00:14:11.190 --> 00:14:15.375
然后出现在一个更大的名词短语中，比如意大利面加肉。

00:14:15.375 --> 00:14:17.640
你可以重复几次，

00:14:17.640 --> 00:14:19.530
给你一个递归结构。

00:14:19.530 --> 00:14:22.710
我有一个顶部蓝色的例子。

00:14:22.710 --> 00:14:25.180
所以站在他旁边的人

00:14:25.180 --> 00:14:28.035
购买你以前工作的公司的公司，

00:14:28.035 --> 00:14:32.675
嗯，整件事都是个大名词短语。

00:14:32.675 --> 00:14:36.280
嗯，但里面有一个名词短语，

00:14:36.280 --> 00:14:39.780
你以前工作的公司的老板，

00:14:39.780 --> 00:14:41.880
这是另一个大名词短语。

00:14:41.880 --> 00:14:43.820
在里面，嗯，

00:14:43.820 --> 00:14:47.260
有一些小的名词短语，比如，

00:14:47.260 --> 00:14:49.890
购买你以前工作的公司的公司。

00:14:49.890 --> 00:14:53.190
但是，你知道，它仍然包含在名词短语中，比如，

00:14:53.190 --> 00:14:55.055
你以前工作的公司。

00:14:55.055 --> 00:14:57.660
事实上，即使在里面，

00:14:57.660 --> 00:14:59.365
小名词短语，

00:14:59.365 --> 00:15:02.035
这就是你这个词。

00:15:02.035 --> 00:15:06.945
所以单个代词也是名词短语。

00:15:06.945 --> 00:15:11.000
嗯，所以只是一种结构

00:15:11.000 --> 00:15:13.565
你从中得到的语言

00:15:13.565 --> 00:15:16.895
层次结构及其内部的同类事物。

00:15:16.895 --> 00:15:20.465
我认为这是完全正确的。

00:15:20.465 --> 00:15:23.130
嗯，那时候的说法是，

00:15:23.130 --> 00:15:26.210
你知道，我们的语言是递归的，我的意思是，

00:15:26.210 --> 00:15:29.615
从形式上讲，不太清楚的是，

00:15:29.615 --> 00:15:33.365
嗯，很明显。

00:15:33.365 --> 00:15:36.850
这就是原因-说有些东西是递归的，

00:15:36.850 --> 00:15:39.500
它必须重复到无穷远，对吧。

00:15:39.500 --> 00:15:43.180
所以一旦你把任何东西绑起来，

00:15:43.180 --> 00:15:48.819
你说，“看，这是你刚刚给我的一个名词短语，有五个层次的嵌套。”

00:15:48.819 --> 00:15:52.615
有人要这么说，真是难以置信。

00:15:52.615 --> 00:15:54.630
所以一旦你有点

00:15:54.630 --> 00:15:56.570
嗯，想争论一下，

00:15:56.570 --> 00:15:57.960
好吧，即使他们这么说，

00:15:57.960 --> 00:16:01.110
没有人会说10级嵌套的名词短语。

00:16:01.110 --> 00:16:04.340
如果你像那样对它施加一些硬限制，嗯，

00:16:04.340 --> 00:16:08.970
在某种意义上，它并不是真正的递归的，因为它不会向外延伸到无穷大。

00:16:08.970 --> 00:16:10.290
但是，你知道，

00:16:10.290 --> 00:16:12.280
不管你怎么想，

00:16:12.280 --> 00:16:16.090
这并没有否定你得到这个层次结构的基本论点。

00:16:16.090 --> 00:16:19.900
用类似名词短语的东西构成结构，

00:16:19.900 --> 00:16:26.780
句子，动词短语，以一种没有明确界限的方式出现在彼此内部。

00:16:26.780 --> 00:16:30.200
就像我给你看一个复杂的句子，

00:16:30.200 --> 00:16:35.430
你可以说我可以把它放进去，使它成为一个更大，更复杂的句子，

00:16:35.430 --> 00:16:38.315
你对我说，然后说，

00:16:38.315 --> 00:16:39.940
嗯，我的判决，对吧。

00:16:39.940 --> 00:16:44.880
这就是它看起来是一个递归生成过程的意义，

00:16:44.880 --> 00:16:50.510
尽管事实上人们说的复杂句子是有限的。

00:16:50.510 --> 00:16:53.625
所以这就是

00:16:53.625 --> 00:16:57.605
在这些选区捕获，嗯，结构树。

00:16:57.605 --> 00:17:02.780
所以在我们讨论解析的早期，你们做了一些，

00:17:02.780 --> 00:17:05.365
我强调依赖分析。

00:17:05.365 --> 00:17:08.220
嗯，但另一种解析实际上是

00:17:08.220 --> 00:17:12.035
我今天要讲的模型是用的，

00:17:12.035 --> 00:17:15.605
这个想法是不是经常被称为选民？

00:17:15.605 --> 00:17:19.790
解析或语言学家通常称之为短语结构语法，

00:17:19.790 --> 00:17:24.635
嗯，或者在计算机科学中，形式语言理论。

00:17:24.635 --> 00:17:27.185
这些是上下文无关的语法，其中，嗯，

00:17:27.185 --> 00:17:29.525
我们有，嗯，这些，

00:17:29.525 --> 00:17:32.129
嗯，非终结词，比如名词短语，

00:17:32.129 --> 00:17:35.105
动词短语，在另一个名词短语里，

00:17:35.105 --> 00:17:36.715
在另一个动词短语里，

00:17:36.715 --> 00:17:38.879
在更多的动词短语里，

00:17:38.879 --> 00:17:40.775
在句子的前面。

00:17:40.775 --> 00:17:43.895
所以这些是我们选区的语法。

00:17:43.895 --> 00:17:47.585
当我们偶尔提到宾夕法尼亚树的时候，

00:17:47.585 --> 00:17:52.495
这是一种原始的宾夕法尼亚树，基本上，呃，

00:17:52.495 --> 00:17:54.005
短语结构语法，

00:17:54.005 --> 00:17:56.705
这里面有各种各样的额外注释，

00:17:56.705 --> 00:17:58.850
嗯，穿上节点。

00:17:58.850 --> 00:18:04.775
好吧，那么，为了捕捉其中的一些属性，你到底是怎么看的？

00:18:04.775 --> 00:18:07.925
好像我们想要一个神经模型

00:18:07.925 --> 00:18:11.405
它可以利用这种树结构。

00:18:11.405 --> 00:18:17.720
所以我们想做的是计算成分的语义相似性，

00:18:17.720 --> 00:18:20.255
我们不仅想要

00:18:20.255 --> 00:18:25.185
一个像我们在四分之一开始时从右开始的词向量空间，

00:18:25.185 --> 00:18:30.065
但我们希望能够采用更大的成分，如名词短语，

00:18:30.065 --> 00:18:31.565
我出生的国家，

00:18:31.565 --> 00:18:33.475
我出生的地方，

00:18:33.475 --> 00:18:35.535
也给他们一个意义。

00:18:35.535 --> 00:18:39.370
所以我们想做的是

00:18:39.370 --> 00:18:44.140
以组合的方式计算任何短语的含义，

00:18:44.140 --> 00:18:47.320
最终结果也是

00:18:47.320 --> 00:18:52.410
这些短语可能会粘在我们的向量空间模型中。

00:18:52.410 --> 00:18:56.300
所以我们还是要坚持我们的向量空间短语语义，

00:18:56.300 --> 00:18:59.625
我们要计算短语的意思。

00:18:59.625 --> 00:19:01.530
所以问题是，

00:19:01.530 --> 00:19:04.715
我们该怎么做呢？

00:19:04.715 --> 00:19:07.865
回答第一个问题就是我们要用

00:19:07.865 --> 00:19:11.375
合成性既然我们确信是对的，

00:19:11.375 --> 00:19:15.740
那么，好吧，合成性原理本质上说的是，

00:19:15.740 --> 00:19:20.170
如果你想算出一个句子的意思，或者这里说的是一个句子。

00:19:20.170 --> 00:19:24.340
但任何词组的意思，任何成分都是

00:19:24.340 --> 00:19:29.050
通过了解单词的含义来构建它，

00:19:29.050 --> 00:19:31.565
然后有结合这些含义的规则。

00:19:31.565 --> 00:19:34.280
从我出生的国家开始，

00:19:34.280 --> 00:19:37.310
我应该能够计算出我出生的意义，

00:19:37.310 --> 00:19:39.105
国家的意义，

00:19:39.105 --> 00:19:43.540
我出生的意义，然后我出生的国家的意义。

00:19:43.540 --> 00:19:47.680
所以我们有意义组成规则，可以计算

00:19:47.680 --> 00:19:52.520
对更大的成分或句子来说是向上的意思。

00:19:52.520 --> 00:19:57.290
嗯，这似乎是正确的做法。

00:19:57.290 --> 00:20:00.140
所以问题是，我们能，嗯，

00:20:00.140 --> 00:20:04.470
然后建立一个如何做到这一点的模型？

00:20:04.470 --> 00:20:08.630
好吧，这是一种简单的方法，好吧。

00:20:08.630 --> 00:20:16.105
所以我们-我们计算过的单词有单词向量。

00:20:16.105 --> 00:20:20.405
我们要做的就是解决问题，嗯-

00:20:20.405 --> 00:20:23.625
然后是这个句子的意思表示。

00:20:23.625 --> 00:20:26.680
在这一点上，我们有两件事要做。

00:20:26.680 --> 00:20:31.590
我们要通过分析来找出句子的正确结构，

00:20:31.590 --> 00:20:35.250
然后我们有意义的计算要做的

00:20:35.250 --> 00:20:39.515
找出这句话的意思表示。

00:20:39.515 --> 00:20:42.900
嗯，为了分析，我们可能正在构建，

00:20:42.900 --> 00:20:45.280
名词短语，介词短语，

00:20:45.280 --> 00:20:48.120
动词短语，句子类型单位，嗯，

00:20:48.120 --> 00:20:49.895
让“猫坐在垫子上”，

00:20:49.895 --> 00:20:51.380
然后会，什么，

00:20:51.380 --> 00:20:52.935
我们，如果我们有，

00:20:52.935 --> 00:20:57.220
然后我们可以运行一些有意义的计算程序，

00:20:57.220 --> 00:20:59.195
给我们一个向量空间，

00:20:59.195 --> 00:21:01.310
嗯，这些句子的意思。

00:21:01.310 --> 00:21:02.970
所以这就是我们想要的，

00:21:02.970 --> 00:21:04.270
就是同时做这两件事，

00:21:04.270 --> 00:21:07.355
再过一会儿，我给你举一个这样的例子

00:21:07.355 --> 00:21:10.660
有一种方法是你接近它。

00:21:10.660 --> 00:21:13.085
但在我这么做之前，先退一步

00:21:13.085 --> 00:21:15.935
有什么不同，对吗？

00:21:15.935 --> 00:21:18.395
在这里我们有我们的

00:21:18.395 --> 00:21:21.630
在某种意义上已经

00:21:21.630 --> 00:21:25.125
到目前为止我们班上的作业马工具，

00:21:25.125 --> 00:21:26.465
它给了你，

00:21:26.465 --> 00:21:30.605
它向你展示了我出生的国家的意义，

00:21:30.605 --> 00:21:32.920
你可以说这就是

00:21:32.920 --> 00:21:34.740
嗯，我出生的国家，

00:21:34.740 --> 00:21:36.980
或者我们讨论了其他的技巧，比如，

00:21:36.980 --> 00:21:39.979
在所有这些方面进行最大限度的整合，

00:21:39.979 --> 00:21:42.820
或者在外面有一个单独的节点，

00:21:42.820 --> 00:21:44.620
对这些问题的关注也是如此。

00:21:44.620 --> 00:21:49.240
所以它给了你一种表现，嗯，

00:21:49.240 --> 00:21:50.950
这意味着，

00:21:50.950 --> 00:21:54.785
任何，嗯，单词的子序列。

00:21:54.785 --> 00:21:57.410
嗯，但他们有点不同，对吧？

00:21:57.410 --> 00:21:59.290
这是什么，顶部，

00:21:59.290 --> 00:22:01.435
树递归神经网络，

00:22:01.435 --> 00:22:07.775
它需要一个句子或任何类型的短语具有树结构。

00:22:07.775 --> 00:22:10.390
所以我们知道它的组成部分是什么，

00:22:10.390 --> 00:22:14.935
但是我们正在研究意义表达

00:22:14.935 --> 00:22:20.800
对于对其句法结构敏感的短语，

00:22:20.800 --> 00:22:24.215
这些词是如何组合在一起形成短语的。

00:22:24.215 --> 00:22:27.875
而对于循环神经网络，我们

00:22:27.875 --> 00:22:31.549
只是以一种不经意的方式运行一个序列模型，

00:22:31.549 --> 00:22:33.500
说和计算东西，

00:22:33.500 --> 00:22:35.120
显而易见，

00:22:35.120 --> 00:22:38.390
它没有以任何明显的方式表示，

00:22:38.390 --> 00:22:41.970
我的出生，或者我的出生包含在其中。

00:22:41.970 --> 00:22:46.240
我们对整个序列只有一种意义表示，

00:22:46.240 --> 00:22:48.680
如果我们这样做，嗯，

00:22:48.680 --> 00:22:54.705
我们确实对句子的不同有意义的部分有意义表示。

00:22:54.705 --> 00:22:58.730
可以。这就解释了我们要做什么？

00:22:59.120 --> 00:23:01.625
可以。那我们怎么办？

00:23:01.625 --> 00:23:03.240
去做那件事？

00:23:03.240 --> 00:23:08.385
嗯，好吧，我们该怎么做的想法是，

00:23:08.385 --> 00:23:09.859
如果我们自下而上，

00:23:09.859 --> 00:23:14.245
在最底层我们有单词向量，

00:23:14.245 --> 00:23:19.890
所以我们要递归地计算更大成分的含义。

00:23:19.890 --> 00:23:25.280
所以如果我们想计算“在垫子上”的意思，我们能做的就是说，

00:23:25.280 --> 00:23:30.135
好吧，我们已经有了，在和mat之间已经有了意义的表示。

00:23:30.135 --> 00:23:33.910
如果我们能把它们输入神经网络，因为那是我们的

00:23:33.910 --> 00:23:37.820
一个工具，我们可以从中得到两样东西。

00:23:37.820 --> 00:23:42.055
我们可以得到一个很好的分数。

00:23:42.055 --> 00:23:44.420
这就是我们要用来解析的。

00:23:44.420 --> 00:23:49.450
我们会说，“你相信吗-你相信你能把”开“和

00:23:49.450 --> 00:23:55.055
“mat”形成一个好的组成部分，它是解析树的一部分？

00:23:55.055 --> 00:23:58.055
如果答案是真的这将是一个很大的正数，

00:23:58.055 --> 00:23:59.865
如果不是真的，那就是否定的，

00:23:59.865 --> 00:24:03.305
然后我们有了一个意义组合装置，

00:24:03.305 --> 00:24:05.270
上面写着，“好吧，嗯，

00:24:05.270 --> 00:24:07.480
如果你把这两件事结合起来，

00:24:07.480 --> 00:24:11.965
我们所组合的东西的意思表示是什么？”

00:24:11.965 --> 00:24:16.200
所以这是我们研究的第一个模型

00:24:16.200 --> 00:24:19.660
这是不是很简单？

00:24:19.660 --> 00:24:22.750
这是我们的意思组合，嗯，

00:24:22.750 --> 00:24:27.665
把两个向量连接起来的装置，

00:24:27.665 --> 00:24:30.230
我们把它们乘以一个矩阵，加上

00:24:30.230 --> 00:24:31.840
像往常一样有偏见，

00:24:31.840 --> 00:24:33.955
把它晒成棕褐色。

00:24:33.955 --> 00:24:35.345
这项工作已经够老了，

00:24:35.345 --> 00:24:36.815
这有点早，比如，

00:24:36.815 --> 00:24:38.110
雷鲁斯变得很受欢迎，

00:24:38.110 --> 00:24:41.735
不过，也许还是晒黑比较好，嗯，更适合，

00:24:41.735 --> 00:24:43.235
循环神经网络，

00:24:43.235 --> 00:24:47.580
这就是我们的意义组合，它给出了父母的意义。

00:24:47.580 --> 00:24:52.305
从侧面看，这句话是否好，

00:24:52.305 --> 00:24:55.370
我们采用了父向量表示法，

00:24:55.370 --> 00:24:58.949
再乘以另一个向量，

00:24:58.949 --> 00:25:01.990
这给了我们一个数字。

00:25:02.100 --> 00:25:06.180
嗯，如果你在我们做这个的时候考虑一下，

00:25:06.180 --> 00:25:10.699
你可能认为这不是一个完美的意义组合模型，

00:25:10.699 --> 00:25:14.800
稍后在课堂上，我将讨论一些更复杂的模型，

00:25:14.800 --> 00:25:17.880
嗯，然后我们开始探索。

00:25:17.880 --> 00:25:21.815
嗯，但这足以让我们走了，

00:25:21.815 --> 00:25:24.520
这给了我们一种建设的方式

00:25:24.520 --> 00:25:29.805
一个递归神经网络解析器，都找到了解析器，

00:25:29.805 --> 00:25:33.530
并为他们制定了一个意义表达。

00:25:33.530 --> 00:25:37.565
所以我们这样做是以最简单的方式，真的，

00:25:37.565 --> 00:25:39.580
有一个贪婪的解析器。

00:25:39.580 --> 00:25:42.620
所以如果我们从“猫坐在垫子上”开始，

00:25:42.620 --> 00:25:43.960
我们能做的就是说，

00:25:43.960 --> 00:25:47.040
好吧，也许你应该加入“猫”和“猫”的行列。

00:25:47.040 --> 00:25:48.220
我们试试看。

00:25:48.220 --> 00:25:49.865
通过我们的神经网络运行它，

00:25:49.865 --> 00:25:53.315
它会得到一个分数和一个意义表示，

00:25:53.315 --> 00:25:55.565
虽然我们可以尝试为“猫”和

00:25:55.565 --> 00:25:58.635
“sat”我们可以尝试为“sat”和“on”做这个。

00:25:58.635 --> 00:26:03.170
我们可以试着为“on”和“the”做，我们可以为“the”和“mat”做。

00:26:03.170 --> 00:26:06.665
在这一点上，我们会说，好的，好的，

00:26:06.665 --> 00:26:12.725
我们能把这些词矢量组合起来的最好短语是“猫”这个词。

00:26:12.725 --> 00:26:14.770
所以让我们承诺一下，

00:26:14.770 --> 00:26:17.360
它有这样的语义表示，

00:26:17.360 --> 00:26:20.825
在这一点上，我们基本上可以重复。

00:26:20.825 --> 00:26:25.190
现在，我们在那里做的所有工作都可以重用，因为没有任何改变，

00:26:25.190 --> 00:26:28.850
但我们现在也可以考虑加入“猫”

00:26:28.850 --> 00:26:32.965
作为一个有“SAT”的参与者，得到分数。

00:26:32.965 --> 00:26:34.930
所以现在我们决定，好吧，

00:26:34.930 --> 00:26:37.140
垫子是最好的建筑材料，

00:26:37.140 --> 00:26:41.455
为此，计算“在垫子上”的意思表示。

00:26:41.455 --> 00:26:44.025
看起来不错，一定要做到，

00:26:44.025 --> 00:26:46.080
一直跳下去，

00:26:46.080 --> 00:26:51.200
所以我们有一种机制来选择一个句子的语法分析，

00:26:51.200 --> 00:26:52.655
以贪婪的方式。

00:26:52.655 --> 00:26:55.300
但是，当我们研究依赖分析时，

00:26:55.300 --> 00:26:57.230
我们也在贪婪地这么做，对吗？

00:26:57.230 --> 00:27:00.955
嗯，并提出一个意义表示。

00:27:00.955 --> 00:27:06.105
可以。这是我们第一个建立树递归神经网络的模型，

00:27:06.105 --> 00:27:07.630
并将其用于解析。

00:27:07.630 --> 00:27:12.630
嗯，这里还有一些细节，

00:27:12.630 --> 00:27:16.200
其中一些可能不是超级的，

00:27:16.200 --> 00:27:18.335
嗯，这一点很重要，对吧？

00:27:18.335 --> 00:27:22.790
所以我们可以通过求和每个节点的得分来得到一棵树，

00:27:22.790 --> 00:27:25.290
嗯，为了锻炼，

00:27:25.290 --> 00:27:27.730
对于我们正在进行的优化，

00:27:27.730 --> 00:27:33.545
我们使用的是我们在其他地方看到的这种最大利润损失。

00:27:33.545 --> 00:27:37.780
嗯，最简单的方法就是贪婪。

00:27:37.780 --> 00:27:41.825
你只要，嗯，在每一点上找到最好的地方决策，

00:27:41.825 --> 00:27:42.845
做这个结构，

00:27:42.845 --> 00:27:43.985
继续前进。

00:27:43.985 --> 00:27:45.610
但如果你想做得更好，

00:27:45.610 --> 00:27:47.100
我们研究过这个，

00:27:47.100 --> 00:27:48.750
嗯，你可以说，

00:27:48.750 --> 00:27:50.910
嗯，我们可以做光束搜索。

00:27:50.910 --> 00:27:54.020
我们可以探索几种很好的合并方式，

00:27:54.020 --> 00:27:59.480
然后在树的高处决定哪一种是最好的合并方式，嗯。

00:27:59.480 --> 00:28:03.155
嗯，我们这节课没有讨论过，

00:28:03.155 --> 00:28:05.240
但更重要的是，嗯，

00:28:05.240 --> 00:28:08.490
以防人们看到，嗯，

00:28:08.490 --> 00:28:12.545
传统的选区分析，在这里你有符号，

00:28:12.545 --> 00:28:14.405
比如，NP或VP。

00:28:14.405 --> 00:28:19.240
嗯，有一些高效的动态编程算法

00:28:19.240 --> 00:28:24.660
在多项式时间内找到句子的最佳解析。

00:28:24.660 --> 00:28:26.190
所以在立方时间里。

00:28:26.190 --> 00:28:29.375
所以如果你有一个规则的上下文无关语法，那么，

00:28:29.375 --> 00:28:32.615
规则概率上下文无关语法，嗯，

00:28:32.615 --> 00:28:35.010
如果你想知道什么是最好的分析方法

00:28:35.010 --> 00:28:38.385
根据概率上下文无关语法的句子，

00:28:38.385 --> 00:28:43.265
你可以写一个三次动态规划算法，你可以找到它。

00:28:43.265 --> 00:28:47.560
很好。在CS224N的旧时代，

00:28:47.560 --> 00:28:51.605
嗯，在神经网络之前，我们曾经让每个人都这么做。

00:28:51.605 --> 00:28:57.240
老CS224N最让人头疼的任务

00:28:57.240 --> 00:29:02.380
正在编写这个动态程序来对一个句子进行上下文无关语法分析。

00:29:02.380 --> 00:29:05.620
嗯，有点悲伤的事实是，

00:29:05.620 --> 00:29:08.810
一旦你研究了这种神经网络表示法，

00:29:08.810 --> 00:29:12.975
你不能再写聪明的动态编程算法了，

00:29:12.975 --> 00:29:18.094
因为聪明的动态编程算法只在你有符号的时候才起作用

00:29:18.094 --> 00:29:23.580
因为如果是这样的话，

00:29:23.580 --> 00:29:26.110
你可以，你有点碰撞，对吧？

00:29:26.110 --> 00:29:29.075
你有很多方法来分析低层的东西，

00:29:29.075 --> 00:29:30.760
哪种，呃，

00:29:30.760 --> 00:29:33.485
结果是用不同的方法造出一个名词短语，

00:29:33.485 --> 00:29:35.850
或者用不同的方法来制作介词短语，

00:29:35.850 --> 00:29:38.825
因此，您可以保存使用动态编程的工作。

00:29:38.825 --> 00:29:40.645
如果你有这样的模特，

00:29:40.645 --> 00:29:44.490
因为你所建立的一切都是经过神经网络的各个层次，

00:29:44.490 --> 00:29:47.745
你有一个意义表示，一些高维向量，

00:29:47.745 --> 00:29:49.760
事情永远不会发生碰撞，

00:29:49.760 --> 00:29:53.070
所以你不能通过做动态编程来保存工作。

00:29:53.070 --> 00:29:58.520
所以，呃，你要么做指数研究来探索一切，

00:29:58.520 --> 00:30:03.950
或者你用某种光束来探索一堆可能的东西。

00:30:04.090 --> 00:30:09.260
是啊。嗯，实际上我们也应用了这个，

00:30:09.260 --> 00:30:11.915
嗯，同时视觉。

00:30:11.915 --> 00:30:14.330
所以这不是完全的

00:30:14.330 --> 00:30:17.300
一个模糊的动机，嗯，

00:30:17.300 --> 00:30:23.255
视觉场景中有一些我们实际上已经开始探索的部分，你可以，嗯，

00:30:23.255 --> 00:30:27.590
这些场景，然后算出，嗯，

00:30:27.590 --> 00:30:32.885
使用类似合成形式的场景表示。

00:30:32.885 --> 00:30:35.405
尤其是，

00:30:35.405 --> 00:30:40.715
嗯，有一种数据集被用于，嗯，

00:30:40.715 --> 00:30:43.894
视觉上的多类分割，

00:30:43.894 --> 00:30:48.830
从很小的补丁开始，然后你想把它们组合起来

00:30:48.830 --> 00:30:51.050
在某个场景的某个部分

00:30:51.050 --> 00:30:54.170
认识到照片的哪一部分是大楼，

00:30:54.170 --> 00:30:58.025
天空，道路，各种各样的课程。

00:30:58.025 --> 00:31:02.435
我们当时做得非常好，嗯，

00:31:02.435 --> 00:31:06.140
更好地使用这些树递归结构的神经网络之一

00:31:06.140 --> 00:31:11.435
在20世纪末的10年里，远见领域的工作比以前的工作要多。

00:31:11.435 --> 00:31:17.225
可以。那么我们怎样才能-怎样才能建立神经网络，

00:31:17.225 --> 00:31:19.550
嗯，就是这样吗？

00:31:19.550 --> 00:31:25.730
当我们开始探索这些树结构的神经网络时，

00:31:25.730 --> 00:31:29.480
我们认为这是一个很酷的原创想法，没有人

00:31:29.480 --> 00:31:33.305
以前曾成功地研究过树型神经网络。

00:31:33.305 --> 00:31:39.560
嗯，但事实证明我们错了，在20世纪90年代中期有两个德国人，

00:31:39.560 --> 00:31:44.765
嗯，实际上已经开始研究树结构的神经网络，并且已经研究出，

00:31:44.765 --> 00:31:45.950
嗯，他们的数学。

00:31:45.950 --> 00:31:49.535
所以对应于时间算法的反向传播，

00:31:49.535 --> 00:31:52.865
嗯，那是艾比在我们做重复性神经网络时说的。

00:31:52.865 --> 00:31:55.310
他们计算出树结构的案例

00:31:55.310 --> 00:31:58.310
称为反向传播，嗯，通过结构。

00:31:58.310 --> 00:32:02.810
嗯，这上面有几张幻灯片

00:32:02.810 --> 00:32:07.505
但我想我会跳过幻灯片。

00:32:07.505 --> 00:32:08.765
如果有人想看他们，

00:32:08.765 --> 00:32:10.610
他们在网上，你可以看看他们。

00:32:10.610 --> 00:32:14.720
我的意思是，实际上没有什么新的东西。

00:32:14.720 --> 00:32:17.420
所以如果你记得

00:32:17.420 --> 00:32:21.725
糟糕的伤疤或是这节课早期讲课的东西，

00:32:21.725 --> 00:32:26.450
嗯，神经网络的衍生物，以及它如何与循环神经网络一起工作。

00:32:26.450 --> 00:32:28.130
差不多，对吧。

00:32:28.130 --> 00:32:32.585
在不同层次的树结构中有这个循环矩阵。

00:32:32.585 --> 00:32:37.220
你在总结它出现的所有地方的导数。

00:32:37.220 --> 00:32:40.370
唯一的区别是因为我们现在有了树结构，

00:32:40.370 --> 00:32:43.040
你有点把事情一分为二。

00:32:43.040 --> 00:32:45.410
嗯，是的。

00:32:45.410 --> 00:32:48.560
所以向前推进，我们可以向前计算。

00:32:48.560 --> 00:32:51.245
然后当我们在后面支撑的时候，

00:32:51.245 --> 00:32:55.910
当我们进行反向传播时，我们会得到来自上面的错误信号。

00:32:55.910 --> 00:32:58.025
然后，我们结合起来，

00:32:58.025 --> 00:33:00.530
嗯，计算在这个节点上。

00:33:00.530 --> 00:33:03.350
然后我们把它以树形结构发回

00:33:03.350 --> 00:33:06.845
一直到我们下面的每一根树枝。

00:33:06.845 --> 00:33:11.960
所以这是我们的第一个版本，我们得到了一些不错的结果。

00:33:11.960 --> 00:33:16.985
我们得到了我给你看的很好的视力结果，看起来是这样的，

00:33:16.985 --> 00:33:19.490
嗯，有点好，嗯，

00:33:19.490 --> 00:33:23.030
用于解析和执行的语言-我们

00:33:23.030 --> 00:33:27.410
有些结果我还没有包括在这里，做一些解释，

00:33:27.410 --> 00:33:33.545
嗯，句子和句子之间的判断——它很好地模拟了事物。

00:33:33.545 --> 00:33:37.760
但当我们开始更多地思考它的时候

00:33:37.760 --> 00:33:41.780
很简单的神经网络功能不可能

00:33:41.780 --> 00:33:46.955
计算我们要计算的句子意义的类型。

00:33:46.955 --> 00:33:49.760
所以我们开始尝试

00:33:49.760 --> 00:33:52.970
一些更复杂的计算方法

00:33:52.970 --> 00:33:56.180
指组合函数和节点

00:33:56.180 --> 00:33:59.630
然后可以用来建立一个更好的神经网络。

00:33:59.630 --> 00:34:04.520
这张幻灯片上有一些-其中的一些要点。

00:34:04.520 --> 00:34:07.280
但是，你知道，对于第一个版本，我们只是

00:34:07.280 --> 00:34:10.130
坦率地说，神经网络的复杂性还不够，对吧？

00:34:10.130 --> 00:34:13.730
所以当我们有两个组分时，我们连接

00:34:13.730 --> 00:34:19.040
然后乘以一个重量，呃，重量矩阵。

00:34:19.040 --> 00:34:21.590
嗯，这基本上是我们所有的。

00:34:21.590 --> 00:34:27.740
嗯，我希望你在这节课上有更多的感觉。

00:34:27.740 --> 00:34:31.610
如果你只是连接并乘以一个权重矩阵，

00:34:31.610 --> 00:34:36.200
你并不是在为这两个向量之间的交互建模，对吧。

00:34:36.200 --> 00:34:39.500
因为你可以把这个重量矩阵看作是

00:34:39.500 --> 00:34:43.490
除以二分之一乘以这个向量，

00:34:43.490 --> 00:34:45.605
它的一半乘以这个向量。

00:34:45.605 --> 00:34:49.685
所以这两件事的意义并不相互作用。

00:34:49.685 --> 00:34:52.280
所以你必须建立你的神经网络，

00:34:52.280 --> 00:34:54.620
嗯，比这更复杂。

00:34:54.620 --> 00:34:59.944
但另一个看起来过于简单的方法是在第一个模型中，

00:34:59.944 --> 00:35:04.505
我们只有一个重量矩阵，可以用来做任何事情。

00:35:04.505 --> 00:35:08.120
而且，啊，至少如果你是一个语言学家

00:35:08.120 --> 00:35:12.050
想想你可能开始想的语言结构，

00:35:12.050 --> 00:35:14.630
等等，有时候你会

00:35:14.630 --> 00:35:17.645
一个动词和一个宾语名词短语。

00:35:17.645 --> 00:35:19.490
嗯，击球。

00:35:19.490 --> 00:35:24.125
有时候你会把一篇文章和一个名词放在一起，呃，球。

00:35:24.125 --> 00:35:29.150
有时你会做形容词修改蓝球。

00:35:29.150 --> 00:35:32.705
这些东西在语义上是非常不同的。

00:35:32.705 --> 00:35:36.980
你真的可以有一个重量矩阵吗？

00:35:36.980 --> 00:35:41.645
这个通用的构词功能是用来组合短语的意思吗？

00:35:41.645 --> 00:35:43.280
这可能有效吗？

00:35:43.280 --> 00:35:45.020
你可能会怀疑，

00:35:45.020 --> 00:35:46.820
嗯，不起作用。

00:35:46.820 --> 00:35:49.355
嗯，所以我要继续，嗯，

00:35:49.355 --> 00:35:53.150
展示一些不同的东西。

00:35:53.150 --> 00:35:57.965
但实际上，嗯，在我展示不同的东西之前，

00:35:57.965 --> 00:36:02.810
嗯，我要再展示一个版本，和第一件事有关，

00:36:02.810 --> 00:36:07.355
它实际上提供了一个非常成功和良好的解析器，

00:36:07.355 --> 00:36:10.250
嗯，为了做，嗯，

00:36:10.250 --> 00:36:14.195
上下文无关风格的选区分析。

00:36:14.195 --> 00:36:21.950
所以这是另一种摆脱解析的方式，完全贪婪。

00:36:21.950 --> 00:36:26.900
嗯，实际上是把G的两部分分开。

00:36:26.900 --> 00:36:31.880
我们的句子必须有一个树形结构，

00:36:31.880 --> 00:36:34.805
“让我们来计算这个句子的意思。”

00:36:34.805 --> 00:36:37.280
所以我的想法是，

00:36:37.280 --> 00:36:42.965
在决定什么是句子的良好树结构方面，

00:36:42.965 --> 00:36:47.015
这实际上是你可以用符号语法做得很好的事情。

00:36:47.015 --> 00:36:49.940
但是符号语法的问题不是

00:36:49.940 --> 00:36:53.270
他们不能把树结构放在句子上。

00:36:53.270 --> 00:36:55.790
你对这些语法的问题是，

00:36:55.790 --> 00:36:59.390
他们不能计算意义表示，他们不能

00:36:59.390 --> 00:37:03.710
非常擅长在可选树结构之间进行选择。

00:37:03.710 --> 00:37:07.175
但我们可以把这两部分分开。

00:37:07.175 --> 00:37:08.750
所以我们能做的就是说，

00:37:08.750 --> 00:37:13.175
让我们用一个规则的概率上下文无关语法

00:37:13.175 --> 00:37:16.580
为句子生成可能的树结构。

00:37:16.580 --> 00:37:19.205
我们可以生成一个k最佳列表并说，

00:37:19.205 --> 00:37:21.110
50个最好的是什么，嗯，

00:37:21.110 --> 00:37:24.230
这个句子的上下文无关语法结构？

00:37:24.230 --> 00:37:28.775
这是我们可以用动态编程算法非常有效地做到的。

00:37:28.775 --> 00:37:33.125
然后我们可以计算出一个神经网络，

00:37:33.125 --> 00:37:38.105
嗯，那就可以算出句子的意思表示了。

00:37:38.105 --> 00:37:41.675
嗯，这导致了这个，嗯，

00:37:41.675 --> 00:37:46.670
所谓的句法上的无约束递归神经网络。

00:37:46.670 --> 00:37:51.260
嗯，所以本质上这是说我们

00:37:51.260 --> 00:37:55.925
哈-每个节点和句子都有一个类别，

00:37:55.925 --> 00:37:58.940
嗯，一个符号上下文无关的语法。

00:37:58.940 --> 00:38:02.630
所以它们是A、B和C类，所以什么时候

00:38:02.630 --> 00:38:06.500
我们把事情放在一起，我们可以说，好的。

00:38:06.500 --> 00:38:10.430
我们有一条规则说，

00:38:10.430 --> 00:38:12.620
X去BC，

00:38:12.620 --> 00:38:15.365
所以在这里授权这个节点。

00:38:15.365 --> 00:38:18.800
所以解析的那部分是符号化的。

00:38:18.800 --> 00:38:21.050
然后-然后我们想，嗯，

00:38:21.050 --> 00:38:24.110
找出这个短语的意思。

00:38:24.110 --> 00:38:27.770
嗯，我说的第二个问题

00:38:27.770 --> 00:38:32.075
肯定只是有一种作曲的方法

00:38:32.075 --> 00:38:35.990
期望太多了

00:38:35.990 --> 00:38:40.535
动词和宾语的种类与形容词和名词的构成方式相同。

00:38:40.535 --> 00:38:42.980
所以我们有这样的想法，

00:38:42.980 --> 00:38:46.190
因为我们现在知道了

00:38:46.190 --> 00:38:51.215
孩子们，我们可能知道这是一个形容词，这是一个名词。

00:38:51.215 --> 00:38:55.010
我们可以做的是为

00:38:55.010 --> 00:38:58.895
组合取决于类别。

00:38:58.895 --> 00:39:01.790
而不是以前

00:39:01.790 --> 00:39:07.325
这是一个万能的重量矩阵，用来做所有有意义的合成。

00:39:07.325 --> 00:39:08.810
我们可以在这里，

00:39:08.810 --> 00:39:11.840
这是用于组合的权重矩阵

00:39:11.840 --> 00:39:15.439
一个形容词和一个名词的意义，它会计算，

00:39:15.439 --> 00:39:17.810
嗯，这个成分的意思。

00:39:17.810 --> 00:39:21.590
但是我们将有一个不同的权重矩阵来组合

00:39:21.590 --> 00:39:27.870
把一个限定词和一个名词短语或类似的东西的意思放在一起。

00:39:30.090 --> 00:39:34.450
可以。嗯，是的。

00:39:34.450 --> 00:39:37.360
所以我总是这么说，我想，

00:39:37.360 --> 00:39:40.825
嗯，我们希望能够快速完成任务。

00:39:40.825 --> 00:39:44.830
所以我们的解决办法是

00:39:44.830 --> 00:39:49.525
使用概率上下文无关语法查找可能的分析，

00:39:49.525 --> 00:39:55.120
嗯，然后只计算出我们对那些，嗯，很可能的意义。

00:39:55.120 --> 00:39:59.050
所以我们称这个结果为组合向量语法

00:39:59.050 --> 00:40:03.745
PCFG和树递归神经网络的组合。

00:40:03.745 --> 00:40:07.440
嗯，是的。

00:40:07.440 --> 00:40:11.010
所以，嗯，基本上在当时，

00:40:11.010 --> 00:40:14.235
这实际上提供了一个相当好的选区分析程序。

00:40:14.235 --> 00:40:16.845
所以这里有很多结果。

00:40:16.845 --> 00:40:20.040
最顶级的是我们的经典老款，嗯，

00:40:20.040 --> 00:40:25.285
斯坦福解析器，是一个pcfg，人们已经构建的解析器。

00:40:25.285 --> 00:40:32.080
这是我们在2013年完成的合成向量语法，

00:40:32.080 --> 00:40:34.690
它不是最好的解析器。

00:40:34.690 --> 00:40:38.155
尤金·查尼克在布朗大学做了一些更好的工作。

00:40:38.155 --> 00:40:41.980
但实际上我们有一个很好的解析器从系统中出来。

00:40:41.980 --> 00:40:46.495
但更有趣的是我们，

00:40:46.495 --> 00:40:50.845
我们不仅有一个旨在提供正确解析树的解析器。

00:40:50.845 --> 00:40:54.850
我们也在计算节点的意义表示。

00:40:54.850 --> 00:40:58.210
因此，

00:40:58.210 --> 00:41:02.140
您不仅可以看到节点的表示。

00:41:02.140 --> 00:41:06.355
你可以了解这些模型正在学习的权重矩阵，

00:41:06.355 --> 00:41:08.980
嗯，当它们结合在一起的意义。

00:41:08.980 --> 00:41:13.600
记住，我们有一些特定于类别的w矩阵，

00:41:13.600 --> 00:41:17.455
那是和孩子们一起研究的意思。

00:41:17.455 --> 00:41:20.875
嗯，这些有点难解释。

00:41:20.875 --> 00:41:24.130
但问题是，当我们加载这些矩阵时，

00:41:24.130 --> 00:41:27.310
我们将它们初始化为一对对角矩阵。

00:41:27.310 --> 00:41:32.125
因为有两个子矩阵，所以它们是由两个一个的矩形矩阵组成的。

00:41:32.125 --> 00:41:35.335
嗯，一半是，嗯，

00:41:35.335 --> 00:41:37.090
把左边的孩子

00:41:37.090 --> 00:41:39.535
另一半是正确的孩子。

00:41:39.535 --> 00:41:45.520
我们把它们初始化成类似于一个compi-two标识矩阵

00:41:45.520 --> 00:41:48.895
另一种是默认语义

00:41:48.895 --> 00:41:52.840
平均直到在

00:41:52.840 --> 00:41:55.690
在中，在权重向量中。

00:41:55.690 --> 00:42:02.070
在某种程度上，模型没有学到什么有趣的东西，

00:42:02.070 --> 00:42:08.325
沿着对角线你会看到黄色，在其他区域你会看到这种天蓝色。

00:42:08.325 --> 00:42:11.100
在某种程度上，它学到了一些东西

00:42:11.100 --> 00:42:14.355
有趣的是，从一个孩子的语义学中，

00:42:14.355 --> 00:42:17.770
然后你会在对角线上看到红色和橙色，

00:42:17.770 --> 00:42:22.465
还有深蓝色、绿色等等。

00:42:22.465 --> 00:42:26.200
所以你发现如果你训练这个模型，

00:42:26.200 --> 00:42:34.225
它是学习一个短语的哪些孩子实际上是重要的。

00:42:34.225 --> 00:42:36.370
嗯，这些人说如果你

00:42:36.370 --> 00:42:39.310
结合名词短语和协调，

00:42:39.310 --> 00:42:41.830
所以像“猫和”，

00:42:41.830 --> 00:42:45.055
大部分的语义必须在“猫”中找到

00:42:45.055 --> 00:42:48.760
在“and”中不会找到太多的语义。

00:42:48.760 --> 00:42:52.825
如果你把所有格代词组合在一起，

00:42:52.825 --> 00:42:55.045
像她或他的，

00:42:55.045 --> 00:42:58.615
嗯，里面有个名词短语，

00:42:58.615 --> 00:43:02.425
嗯，她的斑猫之类的。

00:43:02.425 --> 00:43:06.595
然后大部分的意义都在斑猫成分中被发现。

00:43:06.595 --> 00:43:10.555
所以它实际上是在学习句子的重要语义在哪里。

00:43:10.555 --> 00:43:18.460
嗯，有很多这样的例子。嗯，是的。

00:43:18.460 --> 00:43:21.850
这一类-所以这一类显示了

00:43:21.850 --> 00:43:26.575
修饰结构，形容词或副词，

00:43:26.575 --> 00:43:31.330
修改名词短语或形容词短语或

00:43:31.330 --> 00:43:35.995
只有一个形容词是名词短语的乘法。

00:43:35.995 --> 00:43:40.120
你似乎注意到，有一些特殊的维度

00:43:40.120 --> 00:43:44.200
有点捕捉修改的意思。

00:43:44.200 --> 00:43:50.395
所以第6维度和第11维度在这些不同的地方出现了，

00:43:50.395 --> 00:43:53.830
嗯，这里的组合，作为捕捉意义成分的一种方式。

00:43:53.830 --> 00:43:55.645
所以这有点整洁。

00:43:55.645 --> 00:44:00.430
所以这个稍微复杂一点的模型实际上工作得很好

00:44:00.430 --> 00:44:05.035
善于捕捉短语和句子的含义。

00:44:05.035 --> 00:44:07.105
所以在这次测试中，

00:44:07.105 --> 00:44:11.920
我们给系统一个测试句然后说，

00:44:11.920 --> 00:44:17.785
另一个是什么？什么是意义最相似的句子？

00:44:17.785 --> 00:44:22.120
这句话最接近我们语料库中的释义？

00:44:22.120 --> 00:44:25.990
所以如果所有的数据都是根据季节变化进行调整的，

00:44:25.990 --> 00:44:29.650
语料库中其他两个最相似的句子是，

00:44:29.650 --> 00:44:32.995
所有数字都根据季节性兽医波动进行了调整。

00:44:32.995 --> 00:44:34.270
这很容易。

00:44:34.270 --> 00:44:37.960
或者所有的数字都被调整以消除通常的季节性模式。

00:44:37.960 --> 00:44:40.240
这样看来效果不错。

00:44:40.240 --> 00:44:43.000
“奈特·里德尔不会对作者发表评论，

00:44:43.000 --> 00:44:46.360
哈斯科拒绝透露是哪个国家下的订单。”

00:44:46.360 --> 00:44:48.640
这里的语义有点不同，但它

00:44:48.640 --> 00:44:51.490
似乎它捕捉到了类似的东西。

00:44:51.490 --> 00:44:53.860
“嗯，海岸不会透露这些条款。”

00:44:53.860 --> 00:44:55.630
这真的很有趣，

00:44:55.630 --> 00:45:00.010
因为这个词的意思其实很相似，但它是用

00:45:00.010 --> 00:45:05.215
在单词和所使用的句法结构方面有很大的不同。

00:45:05.215 --> 00:45:09.820
好吧，那是进步，因为现在

00:45:09.820 --> 00:45:14.815
对于不同的成分类型，我们可以有不同的矩阵。

00:45:14.815 --> 00:45:22.015
嗯，但还是有理由认为我们没有足够的力量，

00:45:22.015 --> 00:45:28.225
那就是我们仍然在使用这个非常简单的构图结构

00:45:28.225 --> 00:45:34.960
这里我们将两个子向量连接起来，再乘以一个矩阵。

00:45:34.960 --> 00:45:37.885
这意味着这两个词，嗯，

00:45:37.885 --> 00:45:41.470
没有按照他们的意思互相交流。

00:45:41.470 --> 00:45:49.450
嗯，但是，嗯，看起来我们想让他们按照他们的意思互动，对吧？

00:45:49.450 --> 00:45:54.100
所以特别是如果你想到

00:45:54.100 --> 00:45:59.305
人类语言和人们在语言语义学中看到的东西，

00:45:59.305 --> 00:46:04.465
你得到的单词看起来像是修饰符或运算符。

00:46:04.465 --> 00:46:06.970
所以这个词非常，

00:46:06.970 --> 00:46:09.580
这一点本身并不重要。

00:46:09.580 --> 00:46:14.980
我的意思是它意味着某种东西，比如加强，或者更多，或者类似的东西，

00:46:14.980 --> 00:46:18.160
但你知道，这并没有真正意义，对吧？

00:46:18.160 --> 00:46:20.050
它没有任何表示。

00:46:20.050 --> 00:46:22.360
你不能给我看很多东西，对吧？

00:46:22.360 --> 00:46:25.135
你可以给我看椅子和笔，嗯，

00:46:25.135 --> 00:46:27.910
孩子们，但你不能给我看很多东西，

00:46:27.910 --> 00:46:32.695
“非常”的意思似乎是有东西在它后面，好的。

00:46:32.695 --> 00:46:39.580
这有一种算符的意思，就是在这个东西的尺度上增加，

00:46:39.580 --> 00:46:42.490
它可以在任何方向的刻度上增加。

00:46:42.490 --> 00:46:45.115
你可以很好也可以很坏。

00:46:45.115 --> 00:46:49.990
所以如果我们想捕捉到这种语义，

00:46:49.990 --> 00:46:53.425
似乎我们不能通过

00:46:53.425 --> 00:46:58.330
连接两个向量并将它们乘以一个矩阵。

00:46:58.330 --> 00:47:04.300
看来我们真正想说的是

00:47:04.300 --> 00:47:06.880
掌握好的意义和

00:47:06.880 --> 00:47:10.915
以某些方式对其进行修改，以产生新的意义。

00:47:10.915 --> 00:47:15.130
实际上，这是一种典型的方法，

00:47:15.130 --> 00:47:17.980
嗯，是在语言语义学上做的。

00:47:17.980 --> 00:47:20.530
所以在语言学的语义理论中，

00:47:20.530 --> 00:47:22.120
你通常会说，好吧，

00:47:22.120 --> 00:47:23.620
好有意义，

00:47:23.620 --> 00:47:29.530
“非常”是一个函数，它接受“好”的含义并返回“非常好”的含义。

00:47:29.530 --> 00:47:31.825
所以我们想，嗯，

00:47:31.825 --> 00:47:35.050
把它放入神经网络的一种方法。

00:47:35.050 --> 00:47:40.750
所以试着提出一个新的构图函数，关于如何做到这一点。

00:47:40.750 --> 00:47:44.530
你可以用各种方式思考

00:47:44.530 --> 00:47:48.340
这样做，其他人也有过一些不同的尝试。

00:47:48.340 --> 00:47:52.915
但从本质上说，我们头脑中的东西是好的，

00:47:52.915 --> 00:47:55.120
我们有词汇载体，

00:47:55.120 --> 00:48:01.990
如果我们想说，很好的意义，并返回一个新的意义，

00:48:01.990 --> 00:48:05.410
最明显的事情就是说

00:48:05.410 --> 00:48:08.830
有一个矩阵附在上面，因为我们可以使用，

00:48:08.830 --> 00:48:13.990
把这个矩阵乘以好的向量，我们得到一个新的，

00:48:13.990 --> 00:48:16.360
嗯，向量出来了。

00:48:16.360 --> 00:48:19.045
然后，好吧，

00:48:19.045 --> 00:48:21.310
问题是，呃，

00:48:21.310 --> 00:48:25.810
哪个词有向量，哪个词有矩阵？

00:48:25.810 --> 00:48:27.700
有点，嗯，

00:48:27.700 --> 00:48:30.190
很难知道答案。

00:48:30.190 --> 00:48:32.485
我的意思是，特别是，嗯，

00:48:32.485 --> 00:48:35.875
充当操作员的词可以，

00:48:35.875 --> 00:48:39.550
嗯，经常会修改自己。

00:48:39.550 --> 00:48:44.305
嗯，所以，嗯，你知道，

00:48:44.305 --> 00:48:49.680
好也可以-好也可以是一个操作员，对吗？

00:48:49.680 --> 00:48:52.740
所以从某种程度上来说，

00:48:52.740 --> 00:48:56.115
你可以有一个好人，这也算是一个接线员，

00:48:56.115 --> 00:48:58.290
这是在修改它。

00:48:58.290 --> 00:49:03.460
所以我们提出的想法是，我们不要试图预先决定这一切。

00:49:03.460 --> 00:49:07.630
为什么我们不说每个词和短语都有

00:49:07.630 --> 00:49:12.385
与之相连的是矩阵和向量。

00:49:12.385 --> 00:49:15.175
这是我们非常好的电影。

00:49:15.175 --> 00:49:16.645
所以每一个词，

00:49:16.645 --> 00:49:19.945
我们有一个向量意义，它有一个矩阵意义，

00:49:19.945 --> 00:49:23.530
然后当我们开始建立像“非常好”这样的短语时，

00:49:23.530 --> 00:49:28.795
它们也有向量意义和矩阵意义。

00:49:28.795 --> 00:49:32.635
所以我们提议的是，

00:49:32.635 --> 00:49:34.390
嗯，首先，

00:49:34.390 --> 00:49:37.015
我们，我们希望能够，嗯，

00:49:37.015 --> 00:49:40.765
计算向量的意义。

00:49:40.765 --> 00:49:47.005
所以要想解出一个短语的矢量意义就很好了。

00:49:47.005 --> 00:49:49.540
每个词都有一个矩阵意义。

00:49:49.540 --> 00:49:53.680
所以我们要把它们的对立矩阵和向量意义结合起来。

00:49:53.680 --> 00:49:56.860
所以我们要取矩阵的意义

00:49:56.860 --> 00:50:00.610
好的，乘以向量意义。

00:50:00.610 --> 00:50:03.910
我们将得到矩阵的意义

00:50:03.910 --> 00:50:07.525
乘以好的向量意义。

00:50:07.525 --> 00:50:11.185
所以这两样东西我们都有。

00:50:11.185 --> 00:50:17.620
然后我们会有一个像之前那样的神经网络层，把它们结合在一起。

00:50:17.620 --> 00:50:19.540
在红盒子里就是这样。

00:50:19.540 --> 00:50:22.045
然后这两个东西被连接起来，

00:50:22.045 --> 00:50:25.840
然后穿上我们以前给我们的那种神经网络层

00:50:25.840 --> 00:50:30.235
词组的最后一个矢量。

00:50:30.235 --> 00:50:34.675
然后我们还需要一个短语的矩阵意义。

00:50:34.675 --> 00:50:38.395
对于这个短语的矩阵意义，嗯。

00:50:38.395 --> 00:50:44.185
我们做了一个简单的模型，可能实际上不太好，也就是说，

00:50:44.185 --> 00:50:50.260
让我们把-um的两个矩阵连接起来，

00:50:50.260 --> 00:50:53.560
成分，乘以

00:50:53.560 --> 00:50:57.280
另一个矩阵，然后给我们一个矩阵，

00:50:57.280 --> 00:50:59.920
嗯，父节点的版本。

00:50:59.920 --> 00:51:05.605
所以这给了我们新的更复杂，更强大的组成程序。

00:51:05.605 --> 00:51:11.620
嗯，这看起来确实能做些好事，

00:51:11.620 --> 00:51:17.980
呃，呃，某种运算符语义，其中一个词修改了另一个词的含义。

00:51:17.980 --> 00:51:25.610
嗯，这是一个很好的事情，我们可以用它来做。

00:51:25.620 --> 00:51:30.040
嗯，我们希望能够解决

00:51:30.040 --> 00:51:34.330
修改另一个词的运算符的语义。

00:51:34.330 --> 00:51:40.450
令人难以置信的烦人，令人难以置信的可怕，令人难以置信的悲伤。

00:51:40.450 --> 00:51:44.260
嗯，不烦人，不可怕，不悲伤。

00:51:44.260 --> 00:51:48.340
[噪音]所以对比起来，

00:51:48.340 --> 00:51:54.205
我们的，嗯，旧型号和新型号。

00:51:54.205 --> 00:51:58.345
这个量表是从正到负的量表。

00:51:58.345 --> 00:52:03.210
所以这是完全负的到完全正的，好吗？

00:52:03.210 --> 00:52:06.750
所以你得到的对比度，

00:52:06.750 --> 00:52:09.910
呃，那是为了，嗯，

00:52:09.910 --> 00:52:15.290
简单的模型认为这是非常消极的，这并不令人恼火，

00:52:15.290 --> 00:52:19.070
虽然新模式认为这是相当中性的意思，

00:52:19.070 --> 00:52:22.615
这似乎是合理的。

00:52:22.615 --> 00:52:25.210
嗯，但不伤心，

00:52:25.210 --> 00:52:31.180
这意味着这有点积极，两个模特都试图捕捉到这一点，

00:52:31.180 --> 00:52:34.775
你知道，这里的结果有点矛盾，

00:52:34.775 --> 00:52:36.970
但是-但是他们似乎

00:52:36.970 --> 00:52:40.105
朝着我们想要的方向前进。对。

00:52:40.105 --> 00:52:42.640
“不悲伤”的例子中的基本事实是什么？

00:52:42.640 --> 00:52:45.745
哦，是的。所以这个基本事实

00:52:45.745 --> 00:52:49.510
是-我们实际上让一群人说，

00:52:49.510 --> 00:52:53.215
嗯，给“不悲伤”的含义打分，

00:52:53.215 --> 00:52:55.210
以1到10的比例。

00:52:55.210 --> 00:52:58.390
也许这不是一个很好的明确任务，因为你可以看到，

00:52:58.390 --> 00:53:01.630
在许多[笑声]中跳来跳去，

00:53:01.630 --> 00:53:05.350
嗯，我们的评分是多少？

00:53:05.350 --> 00:53:08.230
但是，这实际上是一种人类的判断。

00:53:08.230 --> 00:53:13.464
嗯，我们也用这个，

00:53:13.464 --> 00:53:15.220
嗯，模特说，“嗯，

00:53:15.220 --> 00:53:18.910
我们可以做语义分类任务吗？

00:53:18.910 --> 00:53:24.965
所以如果我们想了解不同名词短语之间的关系，

00:53:24.965 --> 00:53:28.255
所以这是一个数据集，

00:53:28.255 --> 00:53:32.695
两个名词短语之间有关系。

00:53:32.695 --> 00:53:37.390
我的公寓有一个很大的厨房

00:53:37.390 --> 00:53:43.840
一个组成部分，两个名词短语之间关系的一部分，

00:53:43.840 --> 00:53:49.210
不同类型的名词短语之间也存在着其他的关系。

00:53:49.210 --> 00:53:52.240
所以如果是电影里的战争，嗯，

00:53:52.240 --> 00:53:54.535
那是当时的一个信息主题，

00:53:54.535 --> 00:53:59.455
所以有一些交流媒介包含了一些主题关系。

00:53:59.455 --> 00:54:01.930
所以我们用这种

00:54:01.930 --> 00:54:05.860
神经网络构建我们的意义表达和

00:54:05.860 --> 00:54:06.940
然后穿过去

00:54:06.940 --> 00:54:12.395
另一个神经网络层作为分类器来看看我们做得有多好。

00:54:12.395 --> 00:54:15.580
所以我们得到了一些相当好的结果。

00:54:15.580 --> 00:54:18.970
所以这是一个数据集

00:54:18.970 --> 00:54:24.070
传统的NLP系统采用不同的机器学习方法。

00:54:24.070 --> 00:54:26.170
但在某种意义上，你知道，

00:54:26.170 --> 00:54:29.710
我们感兴趣的是我们似乎在

00:54:29.710 --> 00:54:32.440
一个更好的语义合成系统

00:54:32.440 --> 00:54:37.180
我们以前的递归神经网络得到了75%左右，

00:54:37.180 --> 00:54:40.255
然后我们的新一代得到了79%左右，

00:54:40.255 --> 00:54:45.280
我们可以通过在系统中添加更多的特性来进一步推动这一点。

00:54:45.280 --> 00:54:47.770
所以这就是进步，

00:54:47.770 --> 00:54:50.815
嗯，但我们没有停下来。

00:54:50.815 --> 00:54:55.375
我们一直在努力想出更好的做事方法。

00:54:55.375 --> 00:54:59.660
所以，尽管这里的情况很好，

00:54:59.660 --> 00:55:07.500
看起来这种做矩阵的方式不一定很好。

00:55:07.500 --> 00:55:09.675
它有两个问题。

00:55:09.675 --> 00:55:15.910
一个问题是它引入了大量的参数，因为，

00:55:15.910 --> 00:55:19.630
你知道，因为我们所做的一切，否则，

00:55:19.630 --> 00:55:22.360
单词有一个向量，嗯，

00:55:22.360 --> 00:55:28.090
也许有时候我们会用到很高的维向量，比如1024，

00:55:28.090 --> 00:55:31.900
嗯，[噪音]但是，你知道，这是一个相对较少的参数。

00:55:31.900 --> 00:55:35.305
而一旦我们在这里引入这个矩阵，

00:55:35.305 --> 00:55:40.540
我们得到了每个单词的平方附加参数的数目。

00:55:40.540 --> 00:55:43.300
基本上是因为

00:55:43.300 --> 00:55:46.690
能够计算这个模型的参数，

00:55:46.690 --> 00:55:49.180
我们正在使向量的大小变小。

00:55:49.180 --> 00:55:51.250
我们实际使用的是

00:55:51.250 --> 00:55:55.570
只有25维向量，所以25平方

00:55:55.570 --> 00:56:01.495
625，仍然安全，在我们可以计算的范围内有点得体。

00:56:01.495 --> 00:56:03.775
所以这是第一个问题。

00:56:03.775 --> 00:56:05.485
第二个问题是，

00:56:05.485 --> 00:56:08.620
我们没有很好的方法

00:56:08.620 --> 00:56:13.210
有点像是在构建大型短语的矩阵意义。

00:56:13.210 --> 00:56:14.350
我是说，你知道，

00:56:14.350 --> 00:56:17.830
这似乎是我们可以做的简单的事情，但事实并非如此，

00:56:17.830 --> 00:56:21.970
你知道，感觉一个很好的方法来获得一个短语的矩阵意义。

00:56:21.970 --> 00:56:24.760
所以我们想想出别的办法

00:56:24.760 --> 00:56:28.375
解决这两个问题的方法。

00:56:28.375 --> 00:56:33.940
然后，这导致了递归神经张量网络的研究。

00:56:33.940 --> 00:56:39.415
嗯，这些神经张量有个不错的主意，

00:56:39.415 --> 00:56:44.590
这是一个在其他地方实际使用的想法，包括，嗯，

00:56:44.590 --> 00:56:49.210
做一些知识图的向量嵌入之类的工作，

00:56:49.210 --> 00:56:51.550
这是个不错的主意。

00:56:51.550 --> 00:56:55.570
所以我想展示一下这个模型是如何工作的。

00:56:55.570 --> 00:56:58.930
嗯，但只是说，首先，

00:56:58.930 --> 00:57:03.670
我们应用这个模型的地方是关于情绪分析的问题。

00:57:03.670 --> 00:57:08.650
现在，我认为“情绪分析”这个词已经出现了好几次了。

00:57:08.650 --> 00:57:13.720
做，事实上我在上一节课中提到过。

00:57:13.720 --> 00:57:17.320
但我想我们已经五分钟没真正谈过了，嗯，

00:57:17.320 --> 00:57:19.450
在这节情绪分析课上，

00:57:19.450 --> 00:57:22.645
我会举个例子给你。

00:57:22.645 --> 00:57:24.910
嗯，情绪分析实际上

00:57:24.910 --> 00:57:30.940
自然语言处理中的一个非常常见和重要的应用。

00:57:30.940 --> 00:57:34.705
嗯，你在看一段文字，你在说，

00:57:34.705 --> 00:57:37.810
“是肯定的还是否定的？”

00:57:37.810 --> 00:57:42.160
嗯，这对很多人都很有用，

00:57:42.160 --> 00:57:46.955
商业应用，看产品评论或做品牌，

00:57:46.955 --> 00:57:51.530
嗯，意识和像这样的东西，看与事物有关的情感。

00:57:51.530 --> 00:57:55.540
在某种程度上，做情绪分析很容易，对吗？

00:57:55.540 --> 00:57:57.220
你可以说，

00:57:57.220 --> 00:57:58.840
“好吧，看一段文字。

00:57:58.840 --> 00:58:00.700
如果你看到爱的话，

00:58:00.700 --> 00:58:03.265
太好了，印象深刻了，太好了，那就是积极的。

00:58:03.265 --> 00:58:04.570
这是一个积极的评论。

00:58:04.570 --> 00:58:06.880
如果是说，糟糕和可怕，

00:58:06.880 --> 00:58:08.440
那就是负面评论。”

00:58:08.440 --> 00:58:13.420
在某种程度上，这是你可以使用的情绪分析的基准。

00:58:13.420 --> 00:58:18.805
只需选择单词特征或一袋单词中的所有单词。

00:58:18.805 --> 00:58:20.035
如果你这样做，

00:58:20.035 --> 00:58:22.780
你做得并不差，

00:58:22.780 --> 00:58:25.150
嗯，在情感分析中。

00:58:25.150 --> 00:58:26.650
如果你有更长的文件，

00:58:26.650 --> 00:58:31.135
在情感分析中，只看一袋字就可以给你90%的回报。

00:58:31.135 --> 00:58:32.335
但另一方面，

00:58:32.335 --> 00:58:35.140
事情往往会变得更棘手，对吧？

00:58:35.140 --> 00:58:38.020
所以，嗯，这是腐烂的西红柿做的。

00:58:38.020 --> 00:58:40.450
有了这个演员阵容和主题，

00:58:40.450 --> 00:58:43.480
这部电影应该更有趣，更有趣。

00:58:43.480 --> 00:58:47.004
如果你假装自己是一个字里行间的模特，

00:58:47.004 --> 00:58:52.840
这里面唯一一个明显充满感情的词，呃，

00:58:52.840 --> 00:58:57.969
有趣又有趣，这两个词都是很积极的词，

00:58:57.969 --> 00:59:04.615
嗯，但很明显，这实际上是对电影的一个糟糕的评论。

00:59:04.615 --> 00:59:07.360
好吧，我们该怎么知道呢？

00:59:07.360 --> 00:59:11.320
嗯，似乎我们要做的又是构图。

00:59:11.320 --> 00:59:15.310
我们得找一些短语，比如“应该

00:59:15.310 --> 00:59:21.070
有趣的是，”然后意识到这实际上是一个短语的否定意义。

00:59:21.070 --> 00:59:25.690
所以我们想探索我们如何看待

00:59:25.690 --> 00:59:33.310
词组和探索建立这些意义，就像在树上做意义组成。

00:59:33.310 --> 00:59:36.400
嗯，所以我们做的第一件事，嗯，

00:59:36.400 --> 00:59:42.490
我们是不是建立了一个情绪树的树库，让人们来评价情绪。

00:59:42.490 --> 00:59:45.910
所以这就导致了斯坦福情感之旅，

00:59:45.910 --> 00:59:49.675
这仍然是你经常看到的数据集，嗯，

00:59:49.675 --> 00:59:54.280
各种评估和一系列数据集。的确，

00:59:54.280 --> 00:59:57.175
上周它出现在迪卡普。

00:59:57.175 --> 01:00:00.545
嗯，所以我们在这里做的是，

01:00:00.545 --> 01:00:06.265
嗯，是电影里烂番茄的句子。

01:00:06.265 --> 01:00:13.450
我们分析它们以给出树的结构，然后我们要求机械土耳其人

01:00:13.450 --> 01:00:16.745
给不同的短语打分-不同的单词和

01:00:16.745 --> 01:00:21.460
情绪量表上的短语从非常积极到非常消极。

01:00:21.460 --> 01:00:25.660
所以很多东西都是白色的，因为它不是充满感情的，对吧？

01:00:25.660 --> 01:00:27.575
有句话是，

01:00:27.575 --> 01:00:29.710
还有像电影和

01:00:29.710 --> 01:00:33.325
这部电影没有任何感情，

01:00:33.325 --> 01:00:37.180
但是你有一些非常积极的东西

01:00:37.180 --> 01:00:42.025
然后以蓝色和红色显示的树和树的负片。

01:00:42.025 --> 01:00:45.265
而且-通常在情绪数据集中，

01:00:45.265 --> 01:00:49.720
人们只是把整个句子贴上标签说，

01:00:49.720 --> 01:00:53.140
“这是一个积极的句子或非常积极的句子。

01:00:53.140 --> 01:00:55.840
这是否定句或非常否定句。”

01:00:55.840 --> 01:01:01.810
最重要的是，我们在这里所做的不同之处在于句子中的每个短语

01:01:01.810 --> 01:01:08.170
根据我们的树结构，它的积极性或消极性被贴上了标签。

01:01:08.170 --> 01:01:11.080
嗯，也许并不奇怪，

01:01:11.080 --> 01:01:14.990
事实上你有很多这样的注解，嗯，

01:01:14.990 --> 01:01:20.400
只是改善了分类器的行为，因为你可以

01:01:20.400 --> 01:01:26.735
对句子中哪些词是肯定的或否定的，要做得更好。嗯。

01:01:26.735 --> 01:01:32.810
所以，这些是之前模型的结果。

01:01:32.810 --> 01:01:39.650
所以绿色是一个朴素的贝叶斯模型，除了它不仅使用个别的词，

01:01:39.650 --> 01:01:41.630
但它使用成对的词。

01:01:41.630 --> 01:01:45.590
如果你正在构建一个传统的分类器，

01:01:45.590 --> 01:01:49.940
想做情绪分析，而不是像主题分类，

01:01:49.940 --> 01:01:54.500
如果你也使用单词对功能，你会得到更好的结果。

01:01:54.500 --> 01:01:58.850
这是因为它为你做了一点婴儿的嗯组成。

01:01:58.850 --> 01:02:01.659
你不仅拥有不感兴趣的功能，

01:02:01.659 --> 01:02:03.610
但是你可以有一个不

01:02:03.610 --> 01:02:07.030
有趣的是，这可以让你模拟一定数量的东西。

01:02:07.030 --> 01:02:10.630
嗯，然后这些是我们的老一代神经网络，

01:02:10.630 --> 01:02:14.935
我们的原始树结构神经网络和我们的矩阵向量一。

01:02:14.935 --> 01:02:19.070
简单地说，对于这些固定的模型，

01:02:19.070 --> 01:02:23.810
仅仅拥有来自我们新的Treebank的更丰富的监管，

01:02:23.810 --> 01:02:26.330
它有点提高了每种车型的性能。

01:02:26.330 --> 01:02:29.450
所以即使是嗯，仅仅是嗯，

01:02:29.450 --> 01:02:33.695
朴素的贝叶斯模型的表现上升了4%左右，嗯，

01:02:33.695 --> 01:02:35.940
因为事实，嗯，

01:02:35.940 --> 01:02:38.405
它现在知道更多关于

01:02:38.405 --> 01:02:42.005
句子中的特定词是肯定的或否定的。

01:02:42.005 --> 01:02:47.075
嗯，但是这些表演都不是很好。

01:02:47.075 --> 01:02:53.120
嗯，所以我们仍然认为，我们可以建立更好的模型，如何做到这一点。

01:02:53.120 --> 01:02:56.390
嗯，特别是，如果你看句子

01:02:56.390 --> 01:02:59.450
各种各样的否定，

01:02:59.450 --> 01:03:01.505
应该更有趣的是，

01:03:01.505 --> 01:03:06.170
一般来说，这些模型仍然无法捕捉到它们的正确含义。

01:03:06.170 --> 01:03:11.600
因此，我们的第四种模式是如何做到这一点，

01:03:11.600 --> 01:03:15.965
这就是递归神经张量网络的概念。

01:03:15.965 --> 01:03:22.550
嗯，所以我们想做的就是回到仅仅拥有嗯，

01:03:22.550 --> 01:03:26.660
词的意义是载体，

01:03:26.660 --> 01:03:30.335
但是尽管如此

01:03:30.335 --> 01:03:34.355
有一个有意义的短语，其中两个向量

01:03:34.355 --> 01:03:36.140
相互作用。

01:03:36.140 --> 01:03:39.245
嗯，你知道，这种，

01:03:39.245 --> 01:03:41.810
这是我们当年的照片

01:03:41.810 --> 01:03:44.615
以双线性的方式做注意力，对吗？

01:03:44.615 --> 01:03:46.625
我们有两个词的向量。

01:03:46.625 --> 01:03:50.330
我们在中间插入了一个矩阵

01:03:50.330 --> 01:03:54.775
然后引起了注意，得到了一个注意力得分。

01:03:54.775 --> 01:03:59.245
让这两个向量相互作用，

01:03:59.245 --> 01:04:02.635
但它只产生一个数字作为输出。

01:04:02.635 --> 01:04:04.630
但有办法解决这个问题，

01:04:04.630 --> 01:04:10.295
也就是说，这里没有矩阵，

01:04:10.295 --> 01:04:14.555
我们可以在这里贴一个三维的立方体，

01:04:14.555 --> 01:04:19.220
哪个物理学家和深入学习的人现在称之为张量，对吗？

01:04:19.220 --> 01:04:22.550
张量就是更高的多维数组。

01:04:22.550 --> 01:04:24.560
在计算机科学术语中。

01:04:24.560 --> 01:04:28.595
如果我们把它变成张量，

01:04:28.595 --> 01:04:33.035
你知道，就像我们这里有很多层的矩阵。

01:04:33.035 --> 01:04:38.795
所以最终的结果是，我们在这里得到一个数，在这里得到一个数。

01:04:38.795 --> 01:04:42.979
所以总的来说，我们得到一个2号向量，

01:04:42.979 --> 01:04:46.355
在我的婴儿例子中我们需要的就是

01:04:46.355 --> 01:04:50.300
例如，我们只有这两个分量向量用于单词。

01:04:50.300 --> 01:04:52.250
但一般来说，我们有一个张量

01:04:52.250 --> 01:04:55.835
我们的词向量大小的额外提及维度。

01:04:55.835 --> 01:04:58.985
因此，我们得到一个词向量，即，

01:04:58.985 --> 01:05:03.800
我们将从组成中得到一个短语向量，它的大小与

01:05:03.800 --> 01:05:06.980
输入向量，并允许它们

01:05:06.980 --> 01:05:12.450
相互交流，找出整个事情的意义。

01:05:12.910 --> 01:05:17.390
可以。嗯，好吧。

01:05:17.390 --> 01:05:19.220
所以在那个时候，嗯，

01:05:19.220 --> 01:05:23.250
我们使用结果向量，嗯，

01:05:24.610 --> 01:05:28.265
所以我们有了神经张量网络。

01:05:28.265 --> 01:05:33.695
我们实际上把它和以前的那种层结合在一起，

01:05:33.695 --> 01:05:37.310
我们的第一批RNN，也许你不需要这样做，

01:05:37.310 --> 01:05:39.485
但我们也决定加上这个，

01:05:39.485 --> 01:05:42.470
把事情置于非线性状态

01:05:42.470 --> 01:05:45.770
给我们新的短语表达。

01:05:45.770 --> 01:05:49.190
我们把它建在树上，然后在最后，

01:05:49.190 --> 01:05:53.120
我们可以对任何短语的意思进行分类。

01:05:53.120 --> 01:05:56.900
以同样的方式使用SoftMax回归，我们可以

01:05:56.900 --> 01:06:00.585
用梯度下降训练这些权重来预测情绪。

01:06:00.585 --> 01:06:03.910
所以这真的很管用。

01:06:03.910 --> 01:06:05.245
尤其是，

01:06:05.245 --> 01:06:09.820
仅仅使用句子标签并没有起到更好的效果。

01:06:09.820 --> 01:06:13.194
但是如果我们用Treebank训练模型，

01:06:13.194 --> 01:06:15.820
然后我们就可以得到一种

01:06:15.820 --> 01:06:18.700
这大约是性能的百分之几，

01:06:18.700 --> 01:06:20.575
所以这看起来不错。

01:06:20.575 --> 01:06:21.880
尤其是，

01:06:21.880 --> 01:06:26.920
它似乎在实际理解意义成分方面做得更好。

01:06:26.920 --> 01:06:32.095
这是一个句子，其中有缓慢重复的部分，

01:06:32.095 --> 01:06:35.245
但是它有足够的香料来保持它的有趣。

01:06:35.245 --> 01:06:38.470
这里看到的模型非常善于理解。

01:06:38.470 --> 01:06:41.400
好吧，这部分句子是否定的，

01:06:41.400 --> 01:06:43.880
这部分句子是肯定的，

01:06:43.880 --> 01:06:46.310
实际上当你把两半粘在一起的时候，

01:06:46.310 --> 01:06:50.450
最终的结果是一个意义积极的句子。

01:06:50.450 --> 01:06:54.170
但是集中精神一点什么似乎

01:06:54.170 --> 01:06:58.610
好像这是第一次

01:06:58.610 --> 01:07:02.270
似乎能做得更好

01:07:02.270 --> 01:07:07.220
找出当你做否定之类的事情时会发生什么。

01:07:07.220 --> 01:07:11.975
所以这里我们有了它，它是令人难以置信的沉闷，它绝对不是沉闷。

01:07:11.975 --> 01:07:14.000
所以如果它绝对不沉闷，

01:07:14.000 --> 01:07:16.130
这实际上意味着它很好，对吧？

01:07:16.130 --> 01:07:20.480
我们能理解的意思是，这绝对不是枯燥的？

01:07:20.480 --> 01:07:25.610
所以，嗯，这些，这有点像

01:07:25.610 --> 01:07:31.505
当你有消极情绪时会发生什么，

01:07:31.505 --> 01:07:34.790
进一步否定的否定句。

01:07:34.790 --> 01:07:39.890
所以如果你从嗯开始，

01:07:39.890 --> 01:07:42.305
所以如果你这样做的话

01:07:42.305 --> 01:07:48.710
一个消极事物的否定应该变成适度的积极，对吗？

01:07:48.710 --> 01:07:54.065
所以如果你有迟钝是消极的，如果你说不迟钝，

01:07:54.065 --> 01:07:56.105
这并不意味着它是美妙的，

01:07:56.105 --> 01:07:58.400
但这意味着它是适度积极的。

01:07:58.400 --> 01:08:04.070
所以对于一种朴素的贝叶斯模型或者我们前面的模型，

01:08:04.070 --> 01:08:09.755
他们无法捕捉到那种从沉闷到不沉闷的感觉，

01:08:09.755 --> 01:08:14.135
你的意思计算结果不再是正的了。

01:08:14.135 --> 01:08:17.000
而这种神经张量网络

01:08:17.000 --> 01:08:22.470
捕捉到不沉闷的事实意味着它相当好。

01:08:22.750 --> 01:08:26.960
所以这就是进步。嗯，是的。

01:08:26.960 --> 01:08:31.460
所以我想这就是我现在要给你展示的关于申请的内容

01:08:31.460 --> 01:08:37.590
这些树结构的神经网络，对自然语言。

01:08:37.810 --> 01:08:43.190
嗯，你知道，我想我刚开始说的总结是

01:08:43.190 --> 01:08:48.275
你知道吗，它们是有趣的想法和语言联系。

01:08:48.275 --> 01:08:52.325
我是说，出于各种原因，

01:08:52.325 --> 01:08:55.100
这些想法不是嗯，

01:08:55.100 --> 01:08:59.570
近年来在自然语言处理方面取得了很大进展。

01:08:59.570 --> 01:09:04.610
你知道吗，一个人很诚实，人们发现，

01:09:04.610 --> 01:09:08.090
一旦你有了高维向量

01:09:08.090 --> 01:09:11.480
在我们研究过的序列模型中，

01:09:11.480 --> 01:09:15.980
无论它是指像LSTM模型之类的东西还是

01:09:15.980 --> 01:09:21.200
最新的上下文语言模型非常有效，嗯，

01:09:21.200 --> 01:09:24.890
不，不清楚这些模型总体上工作得更好。

01:09:24.890 --> 01:09:28.399
第二个原因是某种计算上的原因，

01:09:28.399 --> 01:09:35.495
也就是说，当你做统一计算的时候，GPU工作得很好。

01:09:35.495 --> 01:09:40.130
有序列模型的好处是，

01:09:40.130 --> 01:09:43.595
你只需要做一个行列式计算

01:09:43.595 --> 01:09:47.180
沿着序列或卷积神经网络，

01:09:47.180 --> 01:09:49.010
有一个决定因素，嗯，

01:09:49.010 --> 01:09:51.530
你做的计算，嗯，

01:09:51.530 --> 01:09:54.170
通过你的卷积层，因此，

01:09:54.170 --> 01:09:58.805
可以在GPU上高效地表示和计算事物。

01:09:58.805 --> 01:10:03.020
这些模型的最大问题是你的计算是什么

01:10:03.020 --> 01:10:07.475
这取决于你给句子分配的结构，

01:10:07.475 --> 01:10:12.110
每个句子都会有不同的结构，因此，

01:10:12.110 --> 01:10:15.200
无法将计算批处理到一组

01:10:15.200 --> 01:10:18.860
对句子进行相同的计算

01:10:18.860 --> 01:10:22.310
不同的句子，有点破坏了这种能力

01:10:22.310 --> 01:10:26.365
在大范围内高效地构建这些模型。

01:10:26.365 --> 01:10:31.145
我想我会在最后说几句。

01:10:31.145 --> 01:10:36.195
嗯，有趣的是，虽然这些还没有被大量使用，

01:10:36.195 --> 01:10:39.250
嗯，最近几年的语言，嗯，

01:10:39.250 --> 01:10:45.650
他们确实有一些用途，在不同的地方发现了不同的应用程序，

01:10:45.650 --> 01:10:48.215
嗯，有点可爱。

01:10:48.215 --> 01:10:52.850
嗯，这实际上是物理学的一个应用。

01:10:52.850 --> 01:10:56.020
嗯，我想我只需要看看这个

01:10:56.020 --> 01:10:58.890
所以我不知道这半个词是什么意思。

01:10:58.890 --> 01:11:04.295
嗯，但是，嗯，它说的是到目前为止在碰撞中最常见的结构

01:11:04.295 --> 01:11:10.295
大型强子对撞机是由高能强子（称为喷流）组成的平行喷流。

01:11:10.295 --> 01:11:14.135
这些喷流是由

01:11:14.135 --> 01:11:19.200
量子色动力学描述的夸克和胶子。

01:11:19.200 --> 01:11:21.420
有人知道这意味着什么吗？

01:11:21.420 --> 01:11:23.550
嗯，我希望你能跟在这里。

01:11:23.550 --> 01:11:26.970
嗯，一个引人注目的物理挑战是寻找

01:11:26.970 --> 01:11:32.000
高度增强的标准模型粒子衰变强子。

01:11:32.000 --> 01:11:36.935
不幸的是，有一个很大的背景来自于更平凡的喷气式飞机，

01:11:36.935 --> 01:11:41.090
量子色动力学过程。

01:11:41.090 --> 01:11:44.610
在这项工作中，我们提出了一个解决方案

01:11:44.610 --> 01:11:48.215
基于以下类比的射流分类

01:11:48.215 --> 01:11:52.470
量子色动力学与自然语言

01:11:52.470 --> 01:11:56.775
一些作品来自自然语言，嗯，加工。

01:11:56.775 --> 01:11:59.520
就像一个句子是由单词组成的

01:11:59.520 --> 01:12:02.865
遵循作为解析树组织的句法结构，

01:12:02.865 --> 01:12:08.050
喷气式飞机也由以下结构的4个动量组成：

01:12:08.050 --> 01:12:09.760
QCD和组织方式

01:12:09.760 --> 01:12:14.100
序列联合喷射算法的聚类历史。

01:12:14.100 --> 01:12:18.005
嗯，所以不管怎样，嗯，是的，有了这些喷气式飞机，你就知道他们得到了

01:12:18.005 --> 01:12:23.794
它们上面有一个树结构，它们使用树递归神经网络，

01:12:23.794 --> 01:12:25.100
嗯，来模拟一下。

01:12:25.100 --> 01:12:31.435
嗯，那有点远，不过再举一个例子，嗯，

01:12:31.435 --> 01:12:35.320
在另一个地方，这些模型实际上

01:12:35.320 --> 01:12:39.840
有用的是用编程语言做一些事情。

01:12:39.840 --> 01:12:42.020
我认为在某种程度上，

01:12:42.020 --> 01:12:46.545
这是因为应用程序在编程语言中更容易使用。

01:12:46.545 --> 01:12:51.150
所以不像在自然语言中，我们对什么是不确定的

01:12:51.150 --> 01:12:55.775
正确的解析树是因为自然语言中有很多歧义，

01:12:55.775 --> 01:12:58.295
在编程语言中，嗯，

01:12:58.295 --> 01:13:01.175
解析树实际上是很好的决定因素。

01:13:01.175 --> 01:13:07.560
嗯，伯克利的一群人，Dawn Song和她的学生都在努力

01:13:07.560 --> 01:13:10.870
程序设计语言建筑翻译

01:13:10.870 --> 01:13:14.490
树递归神经网络编码器解码器。

01:13:14.490 --> 01:13:17.375
所以你要建立一个树结构

01:13:17.375 --> 01:13:22.120
用一种语言表示程序的神经网络。

01:13:22.120 --> 01:13:26.345
这是一个咖啡脚本程序，然后你想建立一棵树

01:13:26.345 --> 01:13:31.760
然后将其转换为另一种语言的程序的树模型。

01:13:31.760 --> 01:13:35.150
他们能够做到这一点并取得良好的效果。

01:13:35.150 --> 01:13:38.760
嗯，我太懒了，没法再打这张桌子。

01:13:38.760 --> 01:13:40.610
所以这可能有点，

01:13:40.610 --> 01:13:42.010
有点难读。

01:13:42.010 --> 01:13:46.320
但与之形成对比的是，对于一些节目来说，这是

01:13:46.320 --> 01:13:51.650
coffeescript到javascript，um，um，translation。

01:13:51.650 --> 01:13:54.980
他们正在比较使用树对树模型。

01:13:54.980 --> 01:13:57.130
嗯，然后用序列来排序

01:13:57.130 --> 01:14:00.120
然后他们尝试了其他两种组合，

01:14:00.120 --> 01:14:03.345
按顺序到树，按顺序到树。

01:14:03.345 --> 01:14:06.215
嗯，他们发现你能得到最好的

01:14:06.215 --> 01:14:10.175
结果与树-树神经网络模型。

01:14:10.175 --> 01:14:13.665
尤其是这些树对树模型

01:14:13.665 --> 01:14:17.345
注意力增强，所以他们有我们所说的注意力。

01:14:17.345 --> 01:14:21.490
序列到序列模型，在那里你可以把注意力放回

01:14:21.490 --> 01:14:26.990
树结构中的节点，这是一种非常自然的翻译方法。

01:14:26.990 --> 01:14:31.320
事实上，这些结果表明，如果你没有-对，这些结果

01:14:31.320 --> 01:14:36.035
表演就是如果你没有注意力操作，它就根本不起作用。

01:14:36.035 --> 01:14:37.680
太难了，嗯，

01:14:37.680 --> 01:14:39.060
为了得到东西，嗯，

01:14:39.060 --> 01:14:41.050
如果你只是想创造

01:14:41.050 --> 01:14:45.810
一个单一的树表示，然后说生成翻译。

01:14:45.810 --> 01:14:48.245
但是如果你能用这种方式来做

01:14:48.245 --> 01:14:51.735
进入不同的模式，嗯，太好了。

01:14:51.735 --> 01:14:56.385
嗯，你可能-如果你知道什么是咖啡，你可能，嗯，

01:14:56.385 --> 01:14:58.740
感觉像是在等待，有点作弊，因为

01:14:58.740 --> 01:15:02.125
coffeescript与javascript有点太相似。

01:15:02.125 --> 01:15:03.725
嗯，但他们也，嗯，

01:15:03.725 --> 01:15:05.310
用其他语言完成。

01:15:05.310 --> 01:15:11.090
So this is going between Java and C# and this is a sort of

01:15:11.090 --> 01:15:14.490
handwritten Java to C# converter that you can

01:15:14.490 --> 01:15:18.820
如果你愿意，可以从Github下载，但它实际上不能很好地工作。

01:15:18.820 --> 01:15:20.570
嗯，他们能展示，

01:15:20.570 --> 01:15:23.120
他们能做得更好，嗯，

01:15:23.120 --> 01:15:25.580
Java to C# translator,

01:15:25.580 --> 01:15:28.080
嗯，这么做。

01:15:28.080 --> 01:15:30.390
嗯，那确实有点酷。

01:15:30.390 --> 01:15:33.110
很好地知道树结构的递归神经网络

01:15:33.110 --> 01:15:34.905
对某些事情有好处。

01:15:34.905 --> 01:15:37.820
嗯，我很高兴看到这样的工作。

01:15:37.820 --> 01:15:41.135
可以。我，我，我，差不多完成了，但我想，

01:15:41.135 --> 01:15:43.515
嗯，在完成之前，

01:15:43.515 --> 01:15:47.135
我只想提另外一件[噪音]的事，那是没什么可做的。

01:15:47.135 --> 01:15:50.850
与自然语言处理精确，但它是关于人工智能。

01:15:50.850 --> 01:15:54.570
嗯，但我想做点广告。

01:15:54.570 --> 01:15:57.335
嗯，我们很多人都是这样

01:15:57.335 --> 01:16:00.855
在过去一年左右的时间里努力工作，

01:16:00.855 --> 01:16:06.710
正在开发一个新的斯坦福以人为中心的人工智能研究所。

01:16:06.710 --> 01:16:11.880
实际上，这个研究所将在考试周的星期一启动，

01:16:11.880 --> 01:16:16.365
当你最大限度地专注于这样的事情时。

01:16:16.365 --> 01:16:19.680
但是我们希望

01:16:19.680 --> 01:16:23.495
围绕人工智能的新活动，

01:16:23.495 --> 01:16:27.510
从更广泛的角度来看人工智能，嗯，

01:16:27.510 --> 01:16:34.770
它是从人类的角度集中观察并计算出来的，嗯，

01:16:34.770 --> 01:16:38.080
我将探讨更广泛的问题，

01:16:38.080 --> 01:16:41.490
接受大学其他学生的很多兴趣

01:16:41.490 --> 01:16:45.100
它是社会科学和人文科学，或是各种各样的

01:16:45.100 --> 01:16:48.945
在像法学院和商学院这样的专业学校。

01:16:48.945 --> 01:16:52.760
嗯，让我们快说一句。

01:16:52.760 --> 01:16:58.980
嗯，那种激励的想法在我一生的大部分时间里都是这样的。

01:16:58.980 --> 01:17:01.270
人工智能似乎是一种

01:17:01.270 --> 01:17:03.670
有趣的智力探索

01:17:03.670 --> 01:17:06.420
你是否可以写一些能做任何事情的软件，

01:17:06.420 --> 01:17:10.245
嗯，半知半解，但显然不是这样，

01:17:10.245 --> 01:17:12.820
接下来的25年发生了什么。

01:17:12.820 --> 01:17:14.830
我们现在的处境

01:17:14.830 --> 01:17:19.680
人工智能系统正在向社会释放。

01:17:19.680 --> 01:17:23.730
嗯，希望他们能做些好事，但正如我们所做的

01:17:23.730 --> 01:17:26.120
越来越多的人看到

01:17:26.120 --> 01:17:29.070
还有很多机会让他们做坏事。

01:17:29.070 --> 01:17:32.510
即使我们没有想象终结者的场景，

01:17:32.510 --> 01:17:35.530
只有很多地方有人在使用

01:17:35.530 --> 01:17:39.545
用于决策的机器学习和人工智能算法。

01:17:39.545 --> 01:17:42.570
其中一些最糟糕的是像判决指南

01:17:42.570 --> 01:17:46.950
你有非常偏颇的算法做出错误决定的法庭

01:17:46.950 --> 01:17:51.425
人们开始越来越了解这些问题，因此

01:17:51.425 --> 01:17:54.075
实际上，我们希望有这样的研究所

01:17:54.075 --> 01:17:56.940
接受了许多社会科学家的工作，

01:17:56.940 --> 01:18:01.420
伦理学家和其他人实际探索如何拥有人工智能

01:18:01.420 --> 01:18:06.030
这确实改善了人类的生活，而不是产生相反的效果。

01:18:06.030 --> 01:18:07.860
所以这三个主题，

01:18:07.860 --> 01:18:09.390
嗯，我们是，嗯，

01:18:09.390 --> 01:18:15.870
主要是强调这所学院是左上角的第一所

01:18:15.870 --> 01:18:19.329
开发人工智能技术，但我们

01:18:19.329 --> 01:18:22.770
有兴趣与人类智能建立联系。

01:18:22.770 --> 01:18:25.575
所以认知科学和神经科学

01:18:25.575 --> 01:18:28.925
当人工智能的许多早期形成性工作

01:18:28.925 --> 01:18:31.410
完成，包括所有

01:18:31.410 --> 01:18:35.330
神经网络的早期工作，如反向传播的发展，

01:18:35.330 --> 01:18:38.400
它实际上主要是在认知科学的背景下完成的。

01:18:38.400 --> 01:18:42.090
正确的？这是一种倾向于迷失在

01:18:42.090 --> 01:18:47.140
90年代和2000年代的统计机器学习重点。

01:18:47.140 --> 01:18:49.030
我想更新一下会很好。

01:18:49.030 --> 01:18:51.240
嗯，右上角，嗯，

01:18:51.240 --> 01:18:53.910
人们越来越关注

01:18:53.910 --> 01:18:59.310
人工智能对人类和社会的影响，因此这是着眼于法律问题，

01:18:59.310 --> 01:19:01.920
经济问题，劳动力，

01:19:01.920 --> 01:19:05.905
伦理，嗯，绿色力量，政治，不管你是什么。

01:19:05.905 --> 01:19:09.520
但从底部看就像是

01:19:09.520 --> 01:19:13.725
有很多机会去做更多的事情，也就是说，

01:19:13.725 --> 01:19:18.825
嗯，我们怎样才能建造真正能增加人类生活的技术呢？

01:19:18.825 --> 01:19:25.860
在某种程度上，就像这里的技术一样，我们拥有人工智能技术，可以增强人类的生活。

01:19:25.860 --> 01:19:29.200
所以你所有的手机现在都有语音识别功能。

01:19:29.200 --> 01:19:31.235
所以你知道那是人工智能，嗯，

01:19:31.235 --> 01:19:34.055
这可以增加你的人类生活。

01:19:34.055 --> 01:19:37.205
但是有一种感觉

01:19:37.205 --> 01:19:43.055
人工智能实际上已经投入到了增强人类生活的服务中。

01:19:43.055 --> 01:19:45.840
就像手机上的大多数东西一样

01:19:45.840 --> 01:19:48.160
有点聪明可爱

01:19:48.160 --> 01:19:51.270
HCI的人和设计师，这是非常好的很多

01:19:51.270 --> 01:19:55.470
当你使用地图程序或其他东西，但我们没有

01:19:55.470 --> 01:20:00.455
这些设备中的人工智能有助于改善人们的生活。

01:20:00.455 --> 01:20:05.880
因此，我们不仅希望个人在申请医疗保健时，

01:20:05.880 --> 01:20:08.240
嗯，做更多的推杆

01:20:08.240 --> 01:20:12.420
人工智能应用于以人为中心的应用。

01:20:12.420 --> 01:20:14.755
嗯，不管怎样，那是我的简短广告。

01:20:14.755 --> 01:20:17.840
嗯，在你不为考试而学习的时候注意这个。

01:20:17.840 --> 01:20:20.700
我认为会有很多机会，嗯，

01:20:20.700 --> 01:20:24.315
让学生和其他人在接下来的几个月里更多地参与到这一活动中。

01:20:24.315 --> 01:20:26.290
可以。非常感谢。

01:20:26.290 --> 01:20:37.290
嗯，我稍后会在海报会上见到你。

01:20:37.290 --> 01:20:37.400
[APPLAUSE].

