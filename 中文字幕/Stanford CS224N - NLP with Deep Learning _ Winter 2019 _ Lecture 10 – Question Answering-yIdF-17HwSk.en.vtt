WEBVTT
Kind: captions
Language: en

00:00:05.480 --> 00:00:11.115
Okay. Hi, everyone. Um, so let's get started again today.

00:00:11.115 --> 00:00:14.610
So today's lecture what I'm going to do,

00:00:14.610 --> 00:00:16.710
is be talking about, um,

00:00:16.710 --> 00:00:19.060
question answering over text.

00:00:19.060 --> 00:00:22.025
Um, this is another of the big successes

00:00:22.025 --> 00:00:25.655
in using deep learning inside natural language processing,

00:00:25.655 --> 00:00:30.140
and it's also a technology that has some really obvious commercial uses.

00:00:30.140 --> 00:00:32.660
So it's an, it's an area that has attracted

00:00:32.660 --> 00:00:36.265
a lot of attention in the last couple of years.

00:00:36.265 --> 00:00:38.790
So this is the overall plan.

00:00:38.790 --> 00:00:43.970
Um, just a couple of reminders and things at the beginning about final project stuff,

00:00:43.970 --> 00:00:48.875
and then we'll, basically all of it is talking about question-answering starting with, um,

00:00:48.875 --> 00:00:53.000
motivation history, um, talking about the SQuAD data,

00:00:53.000 --> 00:00:56.390
uh, a particular simple model, our Stanford Attentive Reader.

00:00:56.390 --> 00:00:58.940
Then talking about some other more complex,

00:00:58.940 --> 00:01:02.460
um, stuff into the most modern stuff.

00:01:02.460 --> 00:01:05.815
Um, yeah, so in a census, um,

00:01:05.815 --> 00:01:09.365
lecture serves a double purpose because if you're going to do the,

00:01:09.365 --> 00:01:11.390
the default final project, well,

00:01:11.390 --> 00:01:13.410
it's about textual question-answering,

00:01:13.410 --> 00:01:17.855
and this is your chance to learn something about the area of textual question-answering,

00:01:17.855 --> 00:01:21.410
and the kinds of models you might want to be thinking about and building.

00:01:21.410 --> 00:01:24.890
Um but the content of this lecture pretty much is in

00:01:24.890 --> 00:01:28.915
no way specifically tied to the default final project,

00:01:28.915 --> 00:01:32.720
apart from by subject matter that really it's telling you about

00:01:32.720 --> 00:01:37.580
how people use neural nets to build question-answering systems.

00:01:37.580 --> 00:01:41.205
Okay. So first just quickly on the reminders,

00:01:41.205 --> 00:01:43.050
um, mid-quarter survey.

00:01:43.050 --> 00:01:45.149
I mean, a huge number of people,

00:01:45.149 --> 00:01:47.330
um, have actually filled this in already.

00:01:47.330 --> 00:01:51.140
Uh, we already had over 60 percent, um, um,

00:01:51.140 --> 00:01:54.170
filling-it-in rate by which by the standards of people

00:01:54.170 --> 00:01:57.245
who do surveys they come as a huge success already.

00:01:57.245 --> 00:01:59.510
But if you're not in that percent, um,

00:01:59.510 --> 00:02:03.480
we'd still love to have your feedback and now's the perfect time to do it.

00:02:03.480 --> 00:02:05.515
Um, yeah.

00:02:05.515 --> 00:02:09.455
I just wanted to sort of have a note on custom final projects.

00:02:09.455 --> 00:02:11.390
Um, so in general, um,

00:02:11.390 --> 00:02:14.645
it's great to get feedback on custom final projects.

00:02:14.645 --> 00:02:16.910
There's a formal mechanism for that which is

00:02:16.910 --> 00:02:19.625
the project proposal that I mentioned last time.

00:02:19.625 --> 00:02:22.330
It's also great to chat to people,

00:02:22.330 --> 00:02:25.930
um, informally about, um, final projects.

00:02:25.930 --> 00:02:28.685
And so I'm one of those people and I have

00:02:28.685 --> 00:02:31.610
been talking to lots of people about final projects,

00:02:31.610 --> 00:02:33.455
and, uh, very happy to do so.

00:02:33.455 --> 00:02:36.500
But there's sort of a problem that there's only one of me.

00:02:36.500 --> 00:02:38.630
Um, so I do also, um,

00:02:38.630 --> 00:02:42.080
encourage you to realize that among the various TAs that

00:02:42.080 --> 00:02:46.070
really lots of them have had experience of different deep learning projects,

00:02:46.070 --> 00:02:48.620
and in particular on the office hours page,

00:02:48.620 --> 00:02:53.420
there's a table that's like this but you can read it if you look at it on your own laptop,

00:02:53.420 --> 00:02:57.125
which talks about the experience of different TA's.

00:02:57.125 --> 00:02:59.930
And many of them have experience in different areas,

00:02:59.930 --> 00:03:04.930
and many of them are also good people to talk to about final projects.

00:03:04.930 --> 00:03:10.860
Okay. Um, so for the default final project, the textual question-answering.

00:03:10.860 --> 00:03:15.194
So um, draft materials for that app today,

00:03:15.194 --> 00:03:17.340
um, right now on the website actually.

00:03:17.340 --> 00:03:20.720
Um, we're calling them draft because we think that there are still

00:03:20.720 --> 00:03:24.230
probably a few things that are gonna get changed over the next week,

00:03:24.230 --> 00:03:29.840
so um, don't regard as completely final in terms of the code that,

00:03:29.840 --> 00:03:32.120
you know, it's sort of 90 percent final.

00:03:32.120 --> 00:03:35.405
So in terms of deciding whether you're going to do, um,

00:03:35.405 --> 00:03:38.495
a custom final project or a default final project,

00:03:38.495 --> 00:03:41.465
and working out what you're putting into your project proposal.

00:03:41.465 --> 00:03:42.755
Um, it should be, you know,

00:03:42.755 --> 00:03:44.270
well more than, um,

00:03:44.270 --> 00:03:46.470
what you need for this year.

00:03:46.470 --> 00:03:48.670
Okay. The one other, um,

00:03:48.670 --> 00:03:52.040
final bit I just wanted to say that I didn't get to

00:03:52.040 --> 00:03:55.520
last time is so for the final projects,

00:03:55.520 --> 00:03:58.055
regardless of which kind you're doing,

00:03:58.055 --> 00:04:00.750
um, well, part of it is, um,

00:04:00.750 --> 00:04:02.545
doing some experiments, of

00:04:02.545 --> 00:04:04.700
doing stuff with data and code,

00:04:04.700 --> 00:04:06.880
and getting some numbers and things like that.

00:04:06.880 --> 00:04:08.480
But I do really, um,

00:04:08.480 --> 00:04:11.630
encourage people to also remember that an important part of

00:04:11.630 --> 00:04:15.515
the final project is writing a final project report.

00:04:15.515 --> 00:04:20.900
And this is no different to any research project of the kinds that,

00:04:20.900 --> 00:04:25.590
um, students do for conferences or journals and things like that, right?

00:04:25.590 --> 00:04:30.020
You spend months commonly working over your code and experiments.

00:04:30.020 --> 00:04:31.970
But in most cases,

00:04:31.970 --> 00:04:36.365
the main evaluation of your work is from people reading,

00:04:36.365 --> 00:04:39.200
a written paper output version of things.

00:04:39.200 --> 00:04:41.420
So it's really important that,

00:04:41.420 --> 00:04:44.480
that paper version sort of reflects the work

00:04:44.480 --> 00:04:47.840
that you did and the interesting ideas that you came up with,

00:04:47.840 --> 00:04:50.720
and explains them well and present your experiments,

00:04:50.720 --> 00:04:52.100
and all of those things.

00:04:52.100 --> 00:04:56.670
And so we encourage you to sort of do a good job at writing up your projects.

00:04:56.670 --> 00:04:59.680
Um, here is just sort of a vague outline of, you know,

00:04:59.680 --> 00:05:03.320
what a typical project write-up is likely to look like.

00:05:03.320 --> 00:05:06.620
Now, there isn't really one size completely fits all

00:05:06.620 --> 00:05:09.950
because depending on what you've done different things might be appropriate.

00:05:09.950 --> 00:05:11.990
But, you know, typically the first page,

00:05:11.990 --> 00:05:15.905
you'll have an abstract for the paper and the introduction to the paper.

00:05:15.905 --> 00:05:19.220
You'll spend some time talking about related prior work.

00:05:19.220 --> 00:05:23.615
Um, you'll talk about what kind of models you built for a while.

00:05:23.615 --> 00:05:28.565
Um, there's probably some discussion of what data you are using for your projects.

00:05:28.565 --> 00:05:34.920
Um, experiments commonly with some tables and figures about the things that you're doing.

00:05:34.920 --> 00:05:39.740
Um, more tables and figures talking about the results as to how well your systems work.

00:05:39.740 --> 00:05:43.010
Um, it's great to have some error analysis to see

00:05:43.010 --> 00:05:46.290
what kind of things that you got right and wrong,

00:05:46.290 --> 00:05:48.500
and then maybe at the end there's sort of

00:05:48.500 --> 00:05:51.965
plans for the future, conclusions, or something like that.

00:05:51.965 --> 00:05:59.475
Okay. Um, that's sort of it for my extra administrative reminders.

00:05:59.475 --> 00:06:03.470
Um, are there any questions on final projects that people are dying to know?

00:06:03.470 --> 00:06:09.800
[NOISE] Okay. Good luck.

00:06:09.800 --> 00:06:10.925
I just meant to say good luck.

00:06:10.925 --> 00:06:13.470
Yeah. Good luck with your final projects. [LAUGHTER] Okay.

00:06:13.470 --> 00:06:15.375
So now moving into,

00:06:15.375 --> 00:06:18.550
um, yeah, the question answering.

00:06:18.550 --> 00:06:23.165
Okay. So, I mean- so question answering is

00:06:23.165 --> 00:06:28.610
a very direct application for something that human beings,

00:06:28.610 --> 00:06:30.095
um, want to do.

00:06:30.095 --> 00:06:33.620
Um, well, maybe human beings don't in general want to know this.

00:06:33.620 --> 00:06:37.355
Um, here's my query of "Who was Australia's third prime minister?".

00:06:37.355 --> 00:06:39.500
Um, maybe, yeah, that's not really the kind of

00:06:39.500 --> 00:06:41.645
thing you're gonna put into your queries but,

00:06:41.645 --> 00:06:43.145
you know, maybe you query,

00:06:43.145 --> 00:06:45.110
"Who was the lead singer of Big Thief?"

00:06:45.110 --> 00:06:46.745
or something like that. I don't know.

00:06:46.745 --> 00:06:48.050
Um, you're, uh, but you know,

00:06:48.050 --> 00:06:51.770
lots- a large percentage of stuff [NOISE] on the web

00:06:51.770 --> 00:06:56.090
is that people actually are asking for answers to questions.

00:06:56.090 --> 00:06:59.120
And so, if I put in this query into Google,

00:06:59.120 --> 00:07:00.530
it actually just works.

00:07:00.530 --> 00:07:03.920
It tells me the answer is John Christian Watson.

00:07:03.920 --> 00:07:08.915
And, um, so that's sort of question answering working in the real world.

00:07:08.915 --> 00:07:11.540
Um, if you try different kinds of questions in Google,

00:07:11.540 --> 00:07:14.585
you'll find that some of them work and lots of them don't work.

00:07:14.585 --> 00:07:15.770
And when they don't work,

00:07:15.770 --> 00:07:20.090
you're just sort of getting whatever kind of information retrieval, web search results.

00:07:20.090 --> 00:07:23.315
Um, there is one fine point that I just wanted,

00:07:23.315 --> 00:07:25.130
um, to mention down here.

00:07:25.130 --> 00:07:28.790
So another thing that Google has is the Google Knowledge Graph,

00:07:28.790 --> 00:07:32.225
which is a structured graph representation of knowledge.

00:07:32.225 --> 00:07:35.405
And some kinds of questions,

00:07:35.405 --> 00:07:39.080
um, being answered from that structured knowledge representation.

00:07:39.080 --> 00:07:40.430
And so, I mean,

00:07:40.430 --> 00:07:43.025
quite a lot of the time for things like movies,

00:07:43.025 --> 00:07:44.870
it's coming from that structured graph.

00:07:44.870 --> 00:07:47.690
If you're sort of saying, "Who's the director of a movie?"

00:07:47.690 --> 00:07:48.890
or something like that.

00:07:48.890 --> 00:07:51.050
But this answer isn't coming from that.

00:07:51.050 --> 00:07:53.000
This answer is a genuine,

00:07:53.000 --> 00:07:55.400
the kind of stuff we're gonna talk about today.

00:07:55.400 --> 00:07:59.360
It's textual question answering from a web page where

00:07:59.360 --> 00:08:01.580
Google's question and answering system has

00:08:01.580 --> 00:08:04.505
extracted the answer and is sticking it up there.

00:08:04.505 --> 00:08:06.365
Um, if you're, um,

00:08:06.365 --> 00:08:09.485
wanting to explore these things, um,

00:08:09.485 --> 00:08:14.735
if you get one of these boxes sort of down here where I've cut it off,

00:08:14.735 --> 00:08:16.340
there's a little bit of gray that says,

00:08:16.340 --> 00:08:17.990
"How did I get this result?".

00:08:17.990 --> 00:08:19.415
And if you click on that,

00:08:19.415 --> 00:08:23.300
it actually tells you what source it's getting it from and you can see if it's doing it

00:08:23.300 --> 00:08:28.130
from the textual question answering system or from something like the Knowledge Graph.

00:08:28.130 --> 00:08:31.040
Okay. Um, so the- in general,

00:08:31.040 --> 00:08:35.600
the motivation for question answering is that these days there's

00:08:35.600 --> 00:08:40.355
just these sort of massive collections of full text documents,

00:08:40.355 --> 00:08:42.110
i.e., there's the web.

00:08:42.110 --> 00:08:46.580
Um, so that there are sort of billions of documents of information.

00:08:46.580 --> 00:08:49.730
And traditionally, when people first started

00:08:49.730 --> 00:08:53.330
thinking about search information retrieval as a field,

00:08:53.330 --> 00:08:59.020
you know, nothing of that kind of quantity and size existed, right?

00:08:59.020 --> 00:09:02.320
That when people first started building search systems,

00:09:02.320 --> 00:09:05.200
it was sort of unthinkable to index

00:09:05.200 --> 00:09:09.340
whole documents because no one had hard disks big enough in those days, right?

00:09:09.340 --> 00:09:15.335
That really- they were indexing titles or titles and abstracts or something like that.

00:09:15.335 --> 00:09:19.925
And so, it seemed perfectly adequate in those days to say, "Okay.

00:09:19.925 --> 00:09:22.760
We're just gonna send you- give you your results."

00:09:22.760 --> 00:09:24.680
as to "Here's a list of documents."

00:09:24.680 --> 00:09:27.440
because the documents are only a hundred words long.

00:09:27.440 --> 00:09:31.010
But that's clearly not the case now when we have the sort of, you know,

00:09:31.010 --> 00:09:36.275
ten minute read, Medium posts um, which might have the answer to a question.

00:09:36.275 --> 00:09:39.080
And so, there's this need to sort of say, "Well,

00:09:39.080 --> 00:09:43.205
can we just have systems that will give us answers to questions?".

00:09:43.205 --> 00:09:49.730
And a lot of the recent changes in technology have hugely underlined that need.

00:09:49.730 --> 00:09:54.950
So, returning documents works okay if you're sitting at your laptop,

00:09:54.950 --> 00:09:59.150
but it works really terribly if you're on your phone and it works even more

00:09:59.150 --> 00:10:04.040
terribly if you're trying to work with speech on a digital assistant device,

00:10:04.040 --> 00:10:06.110
something like an Alexa system.

00:10:06.110 --> 00:10:08.840
And so, we really want to actually be able to produce

00:10:08.840 --> 00:10:12.260
systems that can give the answers to people's questions.

00:10:12.260 --> 00:10:16.865
And so typically, doing that is factored into two parts.

00:10:16.865 --> 00:10:21.500
That the first part of that is we still do information retrieval.

00:10:21.500 --> 00:10:26.270
We use stand- normally quite standard information retrieval techniques to

00:10:26.270 --> 00:10:32.150
find documents that quite likely to con- maintain- contain an answer.

00:10:32.150 --> 00:10:36.200
And the reason that this is normally done by quite traditional techniques is because

00:10:36.200 --> 00:10:41.390
the traditional techniques are extremely scalable over billions of documents,

00:10:41.390 --> 00:10:43.790
whereas current neural systems actually

00:10:43.790 --> 00:10:46.225
aren't really scalable over billions of documents.

00:10:46.225 --> 00:10:50.380
But that's an area in sort of which research is ongoing.

00:10:50.380 --> 00:10:53.920
But then once we have sort of some candidate likely documents,

00:10:53.920 --> 00:10:55.645
we want to find, uh,

00:10:55.645 --> 00:10:57.369
do they contain an answer,

00:10:57.369 --> 00:10:59.305
and if so, what is the answer?

00:10:59.305 --> 00:11:00.520
And so at that point,

00:11:00.520 --> 00:11:03.275
we have a document or a paragraph,

00:11:03.275 --> 00:11:07.445
and we're saying, "Can we answer this question from there?"

00:11:07.445 --> 00:11:11.345
And then that problem is often referred to as the Reading Comprehension problem.

00:11:11.345 --> 00:11:14.705
And so that's really what I'm gonna focus on today.

00:11:14.705 --> 00:11:19.535
Um, Reading Comprehension isn't a new problem.

00:11:19.535 --> 00:11:26.345
I mean it- you can trace it back into the early days of artificial intelligence and NLP.

00:11:26.345 --> 00:11:28.295
So, back in the 70's,

00:11:28.295 --> 00:11:31.520
a lot of NLP work was trying to do Reading Comprehension.

00:11:31.520 --> 00:11:35.420
I mean one of the famous strands of that, um, was, um,

00:11:35.420 --> 00:11:38.435
Sir Roger Shank was a famous,

00:11:38.435 --> 00:11:41.030
um, early NLP person.

00:11:41.030 --> 00:11:42.650
Though not a terribly nice man.

00:11:42.650 --> 00:11:43.985
I don't think, actually.

00:11:43.985 --> 00:11:48.440
Um, but the Yale School of AI was a very well-known,

00:11:48.440 --> 00:11:51.830
um, NLP approach and really,

00:11:51.830 --> 00:11:55.385
it was very focused on Reading Comprehension.

00:11:55.385 --> 00:11:58.205
Um, but it's sort of,

00:11:58.205 --> 00:12:01.070
you know, I think it was sort of the time, it was too early in any way.

00:12:01.070 --> 00:12:03.725
It sort of died out. Nothing much came out of that.

00:12:03.725 --> 00:12:07.670
Um, but then in- right just before the turn of the mil- millennium,

00:12:07.670 --> 00:12:11.150
Lynette Hirschman revived this idea and said, "Well,

00:12:11.150 --> 00:12:14.000
maybe a good challenge would be to find the kind of

00:12:14.000 --> 00:12:18.155
Reading Comprehension questions that elementary school kids do,

00:12:18.155 --> 00:12:19.700
and let's see if we could get,

00:12:19.700 --> 00:12:21.500
um, computers to do that.

00:12:21.500 --> 00:12:24.530
And some people tried that with fairly simple methods,

00:12:24.530 --> 00:12:26.690
which only work mediocrely.

00:12:26.690 --> 00:12:29.180
Then sort of somewhat after that, um,

00:12:29.180 --> 00:12:31.460
Chris Burges who was a guy who was at

00:12:31.460 --> 00:12:34.610
Microsoft Research and he wasn't really an NLP person at all.

00:12:34.610 --> 00:12:36.335
He was a machine learning person,

00:12:36.335 --> 00:12:39.065
but he got it into his head, um,

00:12:39.065 --> 00:12:43.820
that while really a big problem that should be being worked on is

00:12:43.820 --> 00:12:49.115
Machine Comprehension and he suggested that you sort of could codify it like this.

00:12:49.115 --> 00:12:52.715
And this is a particular clean codification

00:12:52.715 --> 00:12:55.340
that has lived on and we'll look at more today.

00:12:55.340 --> 00:12:58.880
All right. So, a machine comprehends a passage of text.

00:12:58.880 --> 00:13:01.640
If there's any question regarding that text that can be

00:13:01.640 --> 00:13:04.490
answered correctly by a majority of native speakers,

00:13:04.490 --> 00:13:06.890
that machine can provide a string,

00:13:06.890 --> 00:13:09.470
which those speakers would agree both answers

00:13:09.470 --> 00:13:13.565
that question and does not contain information irrelevant to that question.

00:13:13.565 --> 00:13:17.750
Um, and he sort of proposed this as sort of a challenge problem for

00:13:17.750 --> 00:13:21.980
artificial intelligence and set about collecting a corpus,

00:13:21.980 --> 00:13:27.410
the MCTest corpus, which was meant to be a simple Reading Comprehension challenge.

00:13:27.410 --> 00:13:29.855
Um, so they collected, um,

00:13:29.855 --> 00:13:32.840
stories, um, which, um,

00:13:32.840 --> 00:13:35.510
were meant to be kids' stories, you know.

00:13:35.510 --> 00:13:37.790
"Alyssa got to the beach after a long trip.

00:13:37.790 --> 00:13:40.010
She's from Charlotte. She traveled from Atlanta.

00:13:40.010 --> 00:13:41.570
She's now in Miami".

00:13:41.570 --> 00:13:43.505
Sort of pretty easy stuff.

00:13:43.505 --> 00:13:45.185
And then there were questions.

00:13:45.185 --> 00:13:47.795
"Why did Alyssa go to Miami?"

00:13:47.795 --> 00:13:49.895
Um, and then the answer is,

00:13:49.895 --> 00:13:51.320
"To visit some friends".

00:13:51.320 --> 00:13:55.130
And so you've got there this string that is coming from the passage.

00:13:55.130 --> 00:13:57.515
That's the answer to the question.

00:13:57.515 --> 00:14:00.950
Um, so the MCTest is a corpus of

00:14:00.950 --> 00:14:07.160
about 600 such stories and that challenge existed, and a few people worked on it.

00:14:07.160 --> 00:14:11.240
But that never really went very far either for the next couple of years.

00:14:11.240 --> 00:14:15.350
But what really changed things was that in 2015,

00:14:15.350 --> 00:14:18.515
and then with more stuff in 2016,

00:14:18.515 --> 00:14:23.000
um, deep learning people got interested in this idea of,

00:14:23.000 --> 00:14:27.620
"Could we perhaps build neural question answering systems?"

00:14:27.620 --> 00:14:30.965
And it seemed like if you wanted to do that, um,

00:14:30.965 --> 00:14:33.980
something like MCTest could only be a test set

00:14:33.980 --> 00:14:38.240
and the ways to make progress would be to do what had been done

00:14:38.240 --> 00:14:45.680
in other domains and to actually build just- hand build a large training set of passages,

00:14:45.680 --> 00:14:50.870
questions, and answers in such a way that would be able to train neural networks using

00:14:50.870 --> 00:14:53.600
the kind of supervised learning techniques that we've

00:14:53.600 --> 00:14:56.540
concentrated on so far in this class.

00:14:56.540 --> 00:15:00.445
And indeed, the kind of supervised neural network learning techniques,

00:15:00.445 --> 00:15:02.990
which is [NOISE] actually the successful stuff that

00:15:02.990 --> 00:15:06.500
powers nearly all the applications of deep learning,

00:15:06.500 --> 00:15:07.955
not only in NLP,

00:15:07.955 --> 00:15:10.200
but also in other fields like vision.

00:15:10.200 --> 00:15:15.680
Um, and so the first subs- the first such dataset was built by

00:15:15.680 --> 00:15:20.990
people at DeepMind over CNN and Daily Mail news stories.

00:15:20.990 --> 00:15:23.300
Um, but then the next year, um,

00:15:23.300 --> 00:15:26.270
Pranav Rajpurkar is a Stanford PhD student

00:15:26.270 --> 00:15:29.270
working with Percy Liang and a couple of other students, um,

00:15:29.270 --> 00:15:31.055
produced the SQuAD dataset,

00:15:31.055 --> 00:15:34.580
which was actually a much better designed dataset and proved to be

00:15:34.580 --> 00:15:38.120
sort of much more successful at driving this forward.

00:15:38.120 --> 00:15:39.830
And then following along from that,

00:15:39.830 --> 00:15:42.770
other people started to produce lots of other,

00:15:42.770 --> 00:15:45.590
um, question answering datasets which, you know,

00:15:45.590 --> 00:15:48.215
many of them have interesting advantages

00:15:48.215 --> 00:15:51.320
and disadvantages of their own including MS MARCO,

00:15:51.320 --> 00:15:53.810
TriviaQA, RACE, blah, blah, blah, lots of them.

00:15:53.810 --> 00:15:55.760
Um, but for today's class,

00:15:55.760 --> 00:15:58.100
I'm gonna concentrate on SQuAD,

00:15:58.100 --> 00:16:03.890
because SQuAD is actually the one that has been by far the most widely used.

00:16:03.890 --> 00:16:10.145
And because it - it was just a well-constructed clean dataset,

00:16:10.145 --> 00:16:13.670
that it sort of just proved a profitable one for people to work with.

00:16:13.670 --> 00:16:17.260
[NOISE]

00:16:17.260 --> 00:16:20.230
Okay. Um, so, that was reading comprehension.

00:16:20.230 --> 00:16:23.050
I'll also just quickly tell you the, um,

00:16:23.050 --> 00:16:26.485
the history of open domain question answering.

00:16:26.485 --> 00:16:29.080
So, the difference here for the- the field of

00:16:29.080 --> 00:16:33.305
Open-domain Question Answering that we're saying, okay,

00:16:33.305 --> 00:16:37.350
there's an encyclopedia or there's a web crawl,

00:16:37.350 --> 00:16:39.405
I'm just going to ask a question,

00:16:39.405 --> 00:16:40.560
can you answer it?

00:16:40.560 --> 00:16:43.555
So, it's this bigger task of question answering.

00:16:43.555 --> 00:16:46.570
And, you know, that was something that again was thought about,

00:16:46.570 --> 00:16:49.000
um, very early on.

00:16:49.000 --> 00:16:51.460
So, there's this kind of early, um,

00:16:51.460 --> 00:16:56.170
CACM paper by Simmons who sort of explores how you could

00:16:56.170 --> 00:17:00.940
do answering questions as textual question-answering, um, and yet, you know,

00:17:00.940 --> 00:17:03.010
he has the idea that what's going to

00:17:03.010 --> 00:17:05.755
happen is you're gonna dependency parse the question,

00:17:05.755 --> 00:17:08.470
and dependency parse sentences of the text,

00:17:08.470 --> 00:17:11.830
and then sort of do tree matching over the dependency parses,

00:17:11.830 --> 00:17:13.660
um, to get out the answers.

00:17:13.660 --> 00:17:15.865
And, you know, that's in some sense

00:17:15.865 --> 00:17:22.120
actually prefigured work that people actually were then attempting to do 35 years later.

00:17:22.120 --> 00:17:25.570
Um, getting a bit more modern, um, Julian Kupiec,

00:17:25.570 --> 00:17:28.000
she was working at Xerox PARC at the time,

00:17:28.000 --> 00:17:31.240
um, came up with this system called MURAX,

00:17:31.240 --> 00:17:35.890
and so at this stage in the 90s there started to be the first, um,

00:17:35.890 --> 00:17:38.770
digitally available encyclopedias available,

00:17:38.770 --> 00:17:41.275
so he was using the Grolier's Encyclopedia,

00:17:41.275 --> 00:17:44.560
and so he said about trying to build a system that could answer

00:17:44.560 --> 00:17:47.980
questions over that encyclopedia using,

00:17:47.980 --> 00:17:50.590
in general, fairly sort of shallow, um,

00:17:50.590 --> 00:17:55.435
linguistic processing methods, i.e, regular expressions.

00:17:55.435 --> 00:17:58.210
Um, for, after [LAUGHTER] having, um,

00:17:58.210 --> 00:18:01.555
done information retrieval search over that.

00:18:01.555 --> 00:18:05.515
But that started to evoke more interest from other people,

00:18:05.515 --> 00:18:13.135
and so in 1999 the US National Institutes of Standards and Technology, um,

00:18:13.135 --> 00:18:17.170
instituted a TREC question-answering track where the idea was,

00:18:17.170 --> 00:18:21.145
there was a large collection of News-wire documents,

00:18:21.145 --> 00:18:25.090
and you could be asked to provide the question of them,

00:18:25.090 --> 00:18:28.390
and lots of people started to build question answering systems.

00:18:28.390 --> 00:18:30.850
Indeed, if in some sense that was

00:18:30.850 --> 00:18:35.560
this competition which was where people at IBM started,

00:18:35.560 --> 00:18:38.320
um, working on textual question-answering,

00:18:38.320 --> 00:18:42.010
and then, um, sort of a decade later, um,

00:18:42.010 --> 00:18:47.305
IBM rejigged things into the sexier format of,

00:18:47.305 --> 00:18:52.975
um, let's build a Jeopardy contestant rather than let's answer questions from the news,

00:18:52.975 --> 00:18:56.620
and that then led to their DeepQA system in 2011.

00:18:56.620 --> 00:18:59.155
Which I presume quite a few of you saw,

00:18:59.155 --> 00:19:02.545
these people saw Jeopardy IBM?

00:19:02.545 --> 00:19:04.120
Yeah, some of you.

00:19:04.120 --> 00:19:07.195
Okay. So, that they were able to successfully, um,

00:19:07.195 --> 00:19:13.180
build a question answering system that could compete at Jeopardy, um, and win.

00:19:13.180 --> 00:19:17.710
Um, and, you know, like a lot of these demonstrations of

00:19:17.710 --> 00:19:23.950
technological success there are things you can quibble about the way it was set up,

00:19:23.950 --> 00:19:27.250
um, that really the kind of computer just had

00:19:27.250 --> 00:19:32.260
a speed advantage versus the human beings that had to buzz in to answer the question.

00:19:32.260 --> 00:19:34.945
But, you know, nevertheless, fundamentally,

00:19:34.945 --> 00:19:37.540
the textual question-answering had to work,

00:19:37.540 --> 00:19:42.895
that this was a system that was answering questions mainly based on textual passages,

00:19:42.895 --> 00:19:47.079
and it had to be able to find the answers to those questions correctly,

00:19:47.079 --> 00:19:48.790
for the system to work.

00:19:48.790 --> 00:19:52.090
Um, so then, more recently again, um,

00:19:52.090 --> 00:19:55.990
and really the first piece of work that did this with a neural system was,

00:19:55.990 --> 00:19:58.000
um, work that was, um,

00:19:58.000 --> 00:19:59.650
done by a Stanford PhD student,

00:19:59.650 --> 00:20:00.925
that I'll get to later,

00:20:00.925 --> 00:20:02.350
was then the idea of well,

00:20:02.350 --> 00:20:06.940
could we replace traditional complex question answering systems

00:20:06.940 --> 00:20:09.954
by using a neural reading comprehension system,

00:20:09.954 --> 00:20:12.280
and that's proved to be very successful.

00:20:12.280 --> 00:20:15.970
So, to, to explain that a little bit more, um,

00:20:15.970 --> 00:20:20.410
if you look at the kind of systems that were built for TREC question-answering,

00:20:20.410 --> 00:20:24.640
um, they were very complex multi-part systems.

00:20:24.640 --> 00:20:27.565
And really, if you then look at something like,

00:20:27.565 --> 00:20:31.510
IBM's Deep QA system it was sort of like this

00:20:31.510 --> 00:20:35.950
times 10 because it both had very complex systems like this,

00:20:35.950 --> 00:20:40.465
but it ensembled together sort of six different components in every place,

00:20:40.465 --> 00:20:41.860
and then did sort of,

00:20:41.860 --> 00:20:45.220
um, classify a combination on top of them.

00:20:45.220 --> 00:20:46.660
But so far, the current-.

00:20:46.660 --> 00:20:51.850
This is sort of around a sort of a 2003 question answering system,

00:20:51.850 --> 00:20:55.120
and so the kind of things that went through is,

00:20:55.120 --> 00:20:56.980
so when there was a question,

00:20:56.980 --> 00:20:59.470
it parsed the question with a parser

00:20:59.470 --> 00:21:02.380
kind of like the ones we saw with our dependency parsers.

00:21:02.380 --> 00:21:03.875
It did some sort of

00:21:03.875 --> 00:21:09.435
handwritten semantic normalization rules to try and get them into a better semantic form.

00:21:09.435 --> 00:21:13.140
It then had a question type classifier which tried to

00:21:13.140 --> 00:21:16.890
work out what kind of semantic type is this question looking for,

00:21:16.890 --> 00:21:18.780
is it looking for a person name,

00:21:18.780 --> 00:21:19.890
or a country name,

00:21:19.890 --> 00:21:22.860
or a temperature, or something like that.

00:21:22.860 --> 00:21:27.825
Um, it would, um, then, um,

00:21:27.825 --> 00:21:32.280
have an information retrieval system out of the document collection,

00:21:32.280 --> 00:21:37.565
um, which would find paragraphs that were likely to contain the answers.

00:21:37.565 --> 00:21:40.510
Um, and then it would have a method of ranking

00:21:40.510 --> 00:21:45.175
those paragraph choices to see which ones are likely to have the answers.

00:21:45.175 --> 00:21:47.740
Um, it would then,

00:21:47.740 --> 00:21:50.365
um, over there somewhere, um,

00:21:50.365 --> 00:21:56.320
run Named Entity Recognition on those passages to find entities that were in them.

00:21:56.320 --> 00:21:59.515
These systems depended strongly on the use of

00:21:59.515 --> 00:22:02.350
fine matching entities because then it could look for

00:22:02.350 --> 00:22:05.755
an entity which corresponded to the question type.

00:22:05.755 --> 00:22:09.970
Um, then once it had candidate entities,

00:22:09.970 --> 00:22:11.980
it had to actually try and determine whether

00:22:11.980 --> 00:22:14.980
these entities did or didn't answer the question.

00:22:14.980 --> 00:22:18.745
So, these people, this is the system from LCC by,

00:22:18.745 --> 00:22:21.100
um, Sanda Harabagiu and Dan Moldovan.

00:22:21.100 --> 00:22:23.605
They actually had some quite interesting stuff here,

00:22:23.605 --> 00:22:28.900
where they had a kind of a loose theorem prover that would try and prove that, um,

00:22:28.900 --> 00:22:31.510
the semantic form of a piece of text,

00:22:31.510 --> 00:22:34.120
um, gave an answer to what the question was.

00:22:34.120 --> 00:22:38.410
So, you know, that was kind of cool stuff with an Axiomatic Knowledge Base,

00:22:38.410 --> 00:22:41.275
um, and eventually out would come an answer.

00:22:41.275 --> 00:22:44.305
Um, so, you know, something that is,

00:22:44.305 --> 00:22:46.300
I do just want to emphasize, you know,

00:22:46.300 --> 00:22:50.050
sometimes with these deep learning courses you get these days,

00:22:50.050 --> 00:22:55.330
the impression you have is that absolutely nothing worked before 2014,

00:22:55.330 --> 00:22:57.445
uh, when we got back to deep learning,

00:22:57.445 --> 00:22:59.440
and that's not actually true.

00:22:59.440 --> 00:23:01.570
So, these kind of factoid question on,

00:23:01.570 --> 00:23:03.970
these kind of question answering systems within

00:23:03.970 --> 00:23:07.135
a certain domain actually really worked rather well.

00:23:07.135 --> 00:23:10.690
Um, so, I started saying the word Factoid Question Answering,

00:23:10.690 --> 00:23:13.120
and so let me explain that because that's the secret.

00:23:13.120 --> 00:23:14.860
So, people, at least in NLP,

00:23:14.860 --> 00:23:17.965
use the term "Factoid Question Answering" to mean

00:23:17.965 --> 00:23:21.790
the case that your answer is a named entity.

00:23:21.790 --> 00:23:23.890
So, it's sort of something like, you know,

00:23:23.890 --> 00:23:26.215
what year was Elvis Presley born,

00:23:26.215 --> 00:23:32.050
or what is the name of Beyonce's husband, or, um,

00:23:32.050 --> 00:23:35.320
you know, which state,

00:23:35.320 --> 00:23:38.740
um, has the most pork or something, I don't know.

00:23:38.740 --> 00:23:40.240
Right, anything that's got,

00:23:40.240 --> 00:23:45.205
anything that's sort of the answer is sort of some clear semantic type entity,

00:23:45.205 --> 00:23:46.735
and that's your answer.

00:23:46.735 --> 00:23:50.935
I mean, so, within the space of those kind of questions,

00:23:50.935 --> 00:23:55.195
which actually is a significant part of the questions you get in web search, right?

00:23:55.195 --> 00:23:58.630
Lots of web search is just, you know,

00:23:58.630 --> 00:24:01.120
who was the star of this movie,

00:24:01.120 --> 00:24:03.355
or what year was somebody born, right?

00:24:03.355 --> 00:24:05.785
There's zillions of those all the time.

00:24:05.785 --> 00:24:08.710
These systems actually really did work quite well

00:24:08.710 --> 00:24:12.070
that they could get about 70 percent of those questions right,

00:24:12.070 --> 00:24:14.110
um, which wasn't bad at all, um,

00:24:14.110 --> 00:24:16.270
though that they really sort of didn't really

00:24:16.270 --> 00:24:19.380
extend it out to other kinds of stuff beyond that.

00:24:19.380 --> 00:24:22.400
But whatever virtues they had, um,

00:24:22.400 --> 00:24:28.280
they were extremely complex systems that people spent years put togeth- putting together,

00:24:28.280 --> 00:24:32.885
which had many components with a huge amount of hand-built stuff.

00:24:32.885 --> 00:24:39.035
And most of the stuff was sort of built quite separately and tied together,

00:24:39.035 --> 00:24:41.120
and you just sort of hope that it worked,

00:24:41.120 --> 00:24:44.045
um, well, when put together in composite.

00:24:44.045 --> 00:24:47.690
And so we can contrast that to what we then see later,

00:24:47.690 --> 00:24:51.275
um, for neural network-style systems.

00:24:51.275 --> 00:24:57.350
Okay. Um, so let me now say some more stuff about, um,

00:24:57.350 --> 00:25:02.870
the Stanford Question Answering Dataset or SQuAD that I just mentioned a little bit ago,

00:25:02.870 --> 00:25:07.055
and as this is the data for the default final project as well.

00:25:07.055 --> 00:25:10.040
Um, so what SQuAD has is,

00:25:10.040 --> 00:25:13.490
questions in SQuAD have a passage,

00:25:13.490 --> 00:25:16.070
which is a paragraph from Wikipedia.

00:25:16.070 --> 00:25:18.425
And then there is a question,

00:25:18.425 --> 00:25:21.755
here it's, "Which team won Super Bowl 50?"

00:25:21.755 --> 00:25:27.270
And the goal of the system is to come up with the answer to this question.

00:25:27.270 --> 00:25:30.430
Um, human reading comprehension.

00:25:30.430 --> 00:25:32.350
What is the answer to the question?

00:25:32.350 --> 00:25:36.640
[NOISE]

00:25:36.640 --> 00:25:37.510
Broncos.

00:25:37.510 --> 00:25:39.130
Broncos. [LAUGHTER] Okay.

00:25:39.130 --> 00:25:42.730
Yeah. Um, so that's the answer to the question.

00:25:42.730 --> 00:25:47.060
Um, and so by construction for SQuAD,

00:25:47.060 --> 00:25:53.570
the answer to a question is always a sub-sequence of words from the passage which is,

00:25:53.570 --> 00:25:56.345
normally, it ends up being referred to as a span,

00:25:56.345 --> 00:25:58.580
a sub-sequence of words from the passage.

00:25:58.580 --> 00:26:01.670
So that's the only kind of questions you can have.

00:26:01.670 --> 00:26:04.639
You can't have questions that are counting questions,

00:26:04.639 --> 00:26:07.130
or yes, no questions, or anything like that.

00:26:07.130 --> 00:26:10.475
You can just pick out a sub-sequence.

00:26:10.475 --> 00:26:12.260
Um, okay.

00:26:12.260 --> 00:26:18.650
But, um, so they created in the first version about 100,000 examples.

00:26:18.650 --> 00:26:22.040
So there are a bunch of questions about each passage.

00:26:22.040 --> 00:26:24.200
So it's sort of something like, um,

00:26:24.200 --> 00:26:28.580
I think it's maybe sort of about five questions per passage,

00:26:28.580 --> 00:26:32.315
and there are 20,000 different bits that Wikipedia uses, used.

00:26:32.315 --> 00:26:34.910
Um, and this sort of must be a span form,

00:26:34.910 --> 00:26:39.260
as often referred to as extractive question answering.

00:26:39.260 --> 00:26:43.520
Okay. Um, here's just one more example

00:26:43.520 --> 00:26:47.540
that can give you some more sense of some of the things that are there,

00:26:47.540 --> 00:26:50.345
and it illustrates a couple of other factors.

00:26:50.345 --> 00:26:52.760
Um, so, you know,

00:26:52.760 --> 00:26:56.360
even this one, I guess the previous one wasn't, um,

00:26:56.360 --> 00:26:59.600
completely obvious what your answers should be because

00:26:59.600 --> 00:27:02.900
maybe you could say the answer should just have been Broncos,

00:27:02.900 --> 00:27:05.720
or you could have said it was Denver Broncos.

00:27:05.720 --> 00:27:07.340
Um, and in general,

00:27:07.340 --> 00:27:09.785
even if you're answering with a span,

00:27:09.785 --> 00:27:13.445
there's gonna be variation as to how long a span you choose.

00:27:13.445 --> 00:27:16.040
Um, so what they did, um,

00:27:16.040 --> 00:27:18.680
and so this was done with, on Mechanical Turk,

00:27:18.680 --> 00:27:21.170
gathering the data, or building questions,

00:27:21.170 --> 00:27:25.790
and getting answers, is that they got answers from three different people.

00:27:25.790 --> 00:27:26.900
So here's this question,

00:27:26.900 --> 00:27:29.810
"Along with non-governmental and non-state schools,

00:27:29.810 --> 00:27:32.029
what is another name for private schools?"

00:27:32.029 --> 00:27:35.585
And three human beings were asked the answer based on this passage.

00:27:35.585 --> 00:27:37.009
And one said independent,

00:27:37.009 --> 00:27:39.485
and two said independent schools.

00:27:39.485 --> 00:27:42.950
Um, this one, all three people gave the same answer.

00:27:42.950 --> 00:27:45.515
This one, again, you get two different answers,

00:27:45.515 --> 00:27:48.020
so that they sample three answers.

00:27:48.020 --> 00:27:52.670
And basically, then, you can be correct if you're going with any of the answers.

00:27:52.670 --> 00:27:59.330
And so that sort of at least gives you a bit of robustness to variation in human answers.

00:27:59.330 --> 00:28:04.460
Okay. And that starts me into the topic of evaluation.

00:28:04.460 --> 00:28:05.855
Um, yeah.

00:28:05.855 --> 00:28:08.450
And these slides here are entitled

00:28:08.450 --> 00:28:12.140
SQuAD version 1.1 because that means in five minutes time,

00:28:12.140 --> 00:28:14.600
I'm gonna tell you about SQuAD version 2,

00:28:14.600 --> 00:28:16.640
which adds a bit more stuff into it,

00:28:16.640 --> 00:28:19.535
but we'll just get 1.1 straight first.

00:28:19.535 --> 00:28:22.895
All right. So there are three answers that col- were collected.

00:28:22.895 --> 00:28:25.280
And so for evaluation metrics,

00:28:25.280 --> 00:28:28.145
they suggested two evaluation metrics.

00:28:28.145 --> 00:28:31.340
The first one is exact match.

00:28:31.340 --> 00:28:34.250
So you're going to return a span.

00:28:34.250 --> 00:28:37.970
If the span is one of these three,

00:28:37.970 --> 00:28:39.515
you get one point,

00:28:39.515 --> 00:28:40.820
and if the scan,

00:28:40.820 --> 00:28:42.980
span is not one of these three,

00:28:42.980 --> 00:28:45.185
you get zero for that question.

00:28:45.185 --> 00:28:48.560
And then your accuracy is just the percent correct,

00:28:48.560 --> 00:28:50.345
so that's extremely simple.

00:28:50.345 --> 00:28:52.910
But the second metric, and actually,

00:28:52.910 --> 00:28:55.984
the one that was favored as the primary metric,

00:28:55.984 --> 00:28:58.235
was an F1 metric.

00:28:58.235 --> 00:29:01.504
So what you do for this F1 metric

00:29:01.504 --> 00:29:05.105
is you're matching at the word level for the different answers.

00:29:05.105 --> 00:29:06.935
So you've treat each,

00:29:06.935 --> 00:29:12.275
you treat the system span and each gold answer as a bag of words,

00:29:12.275 --> 00:29:14.930
and then you work out a precision, which is,

00:29:14.930 --> 00:29:22.780
um, the percent of words in the system's answer that are actually in a span,

00:29:22.780 --> 00:29:25.765
i- in a gold span, the recall,

00:29:25.765 --> 00:29:31.615
which is the percent of words in a gold span that are in the system's span.

00:29:31.615 --> 00:29:34.720
And then you calculate the harmonic mean of those two numbers

00:29:34.720 --> 00:29:37.760
and the harmonic mean is sort of a very conservative average.

00:29:37.760 --> 00:29:40.460
So it's close to the mean of those two numbers,

00:29:40.460 --> 00:29:42.800
and that gives you a score.

00:29:42.800 --> 00:29:47.375
And what you then do is, for each question,

00:29:47.375 --> 00:29:50.090
you'd return, you say its score is

00:29:50.090 --> 00:29:55.355
the maximum F1 over the three different answers that were collected from human beings.

00:29:55.355 --> 00:29:58.850
And then for the whole, um, dataset,

00:29:58.850 --> 00:30:05.195
you then average those F1 scores across questions and that's then your final F1 result.

00:30:05.195 --> 00:30:08.345
So that's a more complicated thing to say.

00:30:08.345 --> 00:30:12.080
Um, and we provide there sort of a val code,

00:30:12.080 --> 00:30:13.970
um, for you that does that.

00:30:13.970 --> 00:30:18.230
Um, but it sort of seems that F1 is actually

00:30:18.230 --> 00:30:24.199
a more reliable and better measure because if you use exact match,

00:30:24.199 --> 00:30:25.850
you know, even though there's of,

00:30:25.850 --> 00:30:29.525
a bit of robustness that comes on three people's answers,

00:30:29.525 --> 00:30:31.940
three is not a very large sample,

00:30:31.940 --> 00:30:34.310
so there's sort of a bit of guessing as to whether you get

00:30:34.310 --> 00:30:37.760
exactly the same span some human being got,

00:30:37.760 --> 00:30:41.180
whereas you're sort of going to get a reasonable score

00:30:41.180 --> 00:30:44.330
in the F1 even if your boundaries are off by a little.

00:30:44.330 --> 00:30:47.345
So the F1 metric sort of, um,

00:30:47.345 --> 00:30:52.760
is more reliable and avoids various kinds of artifacts as to how big

00:30:52.760 --> 00:30:58.295
or small an answer human beings tend to choose in some circumstances.

00:30:58.295 --> 00:31:00.650
Um, and so that's sort of being used as

00:31:00.650 --> 00:31:04.955
the primary metric that people score people on in the leader boards.

00:31:04.955 --> 00:31:07.970
Um, final detail, both metrics, um,

00:31:07.970 --> 00:31:13.235
ignore punctuation and the English articles a, an, the.

00:31:13.235 --> 00:31:17.390
Okay. Um, so how did things work out?

00:31:17.390 --> 00:31:21.170
Um, so for SQuAD version 1.1, um.

00:31:21.170 --> 00:31:23.090
A long time ago,

00:31:23.090 --> 00:31:25.250
at the end of 2016,

00:31:25.250 --> 00:31:27.905
um, this is how the leaderboard looked.

00:31:27.905 --> 00:31:30.680
Um, this is the bottom of the leaderboard at this point in

00:31:30.680 --> 00:31:34.145
time because that allows me to show you a couple of things.

00:31:34.145 --> 00:31:36.890
So down at the bottom of the leaderboard, um,

00:31:36.890 --> 00:31:40.520
so they tested how well human beings did, um,

00:31:40.520 --> 00:31:42.830
at answering these questions because you know,

00:31:42.830 --> 00:31:45.875
human beings aren't perfect at answering questions either.

00:31:45.875 --> 00:31:49.145
Um, and so the human performance that they measured,

00:31:49.145 --> 00:31:52.895
um, had an F1 score of 91.2.

00:31:52.895 --> 00:31:56.285
And I'll come back to that again in a minute.

00:31:56.285 --> 00:31:59.015
Um, and so when they built the dataset,

00:31:59.015 --> 00:32:04.790
they built a logistic regression baseline which was sort of a conventional NLP system.

00:32:04.790 --> 00:32:09.320
So, they dependency parsed the question and sentences of the answer.

00:32:09.320 --> 00:32:12.200
They looked for dependency.

00:32:12.200 --> 00:32:14.780
So dependency link matches,

00:32:14.780 --> 00:32:18.350
so a word at both ends with the dependency relation in

00:32:18.350 --> 00:32:23.615
between and count and matches of those and sort of pointing to a likely answer.

00:32:23.615 --> 00:32:29.795
Um, so as sort of a fairly competently built traditional NLP system of it's

00:32:29.795 --> 00:32:32.150
not as complex as but it's sort of in

00:32:32.150 --> 00:32:36.110
the same vein of that early question answering system I mentioned.

00:32:36.110 --> 00:32:39.410
And it got an F1 of about 51.

00:32:39.410 --> 00:32:41.225
So not hopeless, um,

00:32:41.225 --> 00:32:43.985
but not that great compared to human beings.

00:32:43.985 --> 00:32:46.520
And so, very shortly after that, um,

00:32:46.520 --> 00:32:48.635
people then started building

00:32:48.635 --> 00:32:53.750
neural network systems to try and do better at this task on this dataset.

00:32:53.750 --> 00:32:58.040
And so, one of the first people to do this quite successfully,

00:32:58.040 --> 00:33:01.580
um, were these people from Singapore Management University,

00:33:01.580 --> 00:33:05.150
maybe not the first place you would have thought of but, um,

00:33:05.150 --> 00:33:08.870
they were really sort of the first people who showed that, yes,

00:33:08.870 --> 00:33:12.320
you could build an end-to-end trained neural network

00:33:12.320 --> 00:33:15.320
for this task and do rather better.

00:33:15.320 --> 00:33:18.935
And so, they got up to 67 F1.

00:33:18.935 --> 00:33:22.100
Um, and well, then they had a second system.

00:33:22.100 --> 00:33:24.995
They got 70 and then things started,

00:33:24.995 --> 00:33:28.145
um, to, um, go on.

00:33:28.145 --> 00:33:29.675
So that even by,

00:33:29.675 --> 00:33:32.570
um, the end of 2016,

00:33:32.570 --> 00:33:38.180
um, there started to be systems that really worked rather well on this task.

00:33:38.180 --> 00:33:40.985
Um, so here, this time was the,

00:33:40.985 --> 00:33:42.815
um, top of the leaderboard.

00:33:42.815 --> 00:33:46.455
So I'll talk later about this BiDAF system from, uh,

00:33:46.455 --> 00:33:48.380
the AI to,

00:33:48.380 --> 00:33:51.800
Allen Institute for Artificial Intelligence and the University of Washington.

00:33:51.800 --> 00:33:53.810
So, it was getting to 77 as

00:33:53.810 --> 00:33:57.770
a single system that like in just about all machine learning,

00:33:57.770 --> 00:34:00.260
people pretty soon noticed that if you made

00:34:00.260 --> 00:34:03.440
an ensemble of identically structured systems,

00:34:03.440 --> 00:34:06.830
you could push the number higher and so if you ensemble those,

00:34:06.830 --> 00:34:11.090
you could then get another sort of whatever it is about four points

00:34:11.090 --> 00:34:15.800
and get up to 81, um, F1.

00:34:15.800 --> 00:34:22.445
And so this was sort of around the situation when in the, uh, 2017, um,

00:34:22.445 --> 00:34:30.440
224N class, we first used SQuAD version one as jus- as a default final project.

00:34:30.440 --> 00:34:32.240
And at that point, you know,

00:34:32.240 --> 00:34:36.470
actually the best students got almost to the top of this leaderboard.

00:34:36.470 --> 00:34:38.180
So our best, um,

00:34:38.180 --> 00:34:44.239
CS224N Final Project in winter 2017 made it into,

00:34:44.239 --> 00:34:47.690
um, the equivalent of fourth place on this leaderboard,

00:34:47.690 --> 00:34:51.080
um, with 77.5 as their score.

00:34:51.080 --> 00:34:52.790
So that was really rather cool.

00:34:52.790 --> 00:34:56.105
Um, but that's a couple of years ago and since then,

00:34:56.105 --> 00:34:58.100
people have started building, um,

00:34:58.100 --> 00:35:02.780
bigger and bigger and more and more complex, um, systems.

00:35:02.780 --> 00:35:06.140
And, um, so essentially,

00:35:06.140 --> 00:35:10.790
you could sort of say that SQuAD version one is basically solved.

00:35:10.790 --> 00:35:13.970
So the very best systems are now getting

00:35:13.970 --> 00:35:18.470
F1 scores that are in the low 90s and in particular,

00:35:18.470 --> 00:35:22.910
you can see that the best couple of, um,

00:35:22.910 --> 00:35:25.895
systems have higher F1s and

00:35:25.895 --> 00:35:31.250
well higher exact matches than what was measured for human beings.

00:35:31.250 --> 00:35:34.145
Uh, but like a lot of the claims of

00:35:34.145 --> 00:35:37.310
deep learning being better and performing from human being,

00:35:37.310 --> 00:35:41.000
than human beings, there's sort of some asterisks you can put after that.

00:35:41.000 --> 00:35:43.520
I mean, in particular for this dataset,

00:35:43.520 --> 00:35:48.125
the way they measured human performance was a little bit

00:35:48.125 --> 00:35:53.870
unfair because they only actually collected three human beings' answers.

00:35:53.870 --> 00:35:58.340
So, to judge, um, the human performance,

00:35:58.340 --> 00:36:05.780
the hu- those hu- each of those humans was being scored versus only two other humans.

00:36:05.780 --> 00:36:08.780
And so, that means you only had two chances to match instead of three.

00:36:08.780 --> 00:36:13.820
So, there's actually sort of a systematic underscoring of the human performance.

00:36:13.820 --> 00:36:17.745
But whatever, systems got very good at doing this.

00:36:17.745 --> 00:36:20.960
Um, so the next step, um,

00:36:20.960 --> 00:36:22.520
was then to introduce, uh,

00:36:22.520 --> 00:36:25.445
the SQuAD vers- version 2 task.

00:36:25.445 --> 00:36:29.990
And so many people felt that a defect of SQuAD version

00:36:29.990 --> 00:36:34.985
1 was that in all cases, questions had answers.

00:36:34.985 --> 00:36:40.445
So, that you just had to find the answer in the paragraph,

00:36:40.445 --> 00:36:44.120
um, and so that's sort of turned into a kind of a ranking task.

00:36:44.120 --> 00:36:48.355
You just had to work out what seems the most likely answer.

00:36:48.355 --> 00:36:50.500
I'll return that without really having

00:36:50.500 --> 00:36:53.910
any idea whether it was an answer to the question or not.

00:36:53.910 --> 00:36:56.525
And so, for SQuAD version two,

00:36:56.525 --> 00:36:58.790
for the dev and test sets,

00:36:58.790 --> 00:37:01.760
half of the questions have answers and half of

00:37:01.760 --> 00:37:04.955
the questions just don't have an answer in the passage,

00:37:04.955 --> 00:37:08.015
um, it's slightly different distribution, the training data.

00:37:08.015 --> 00:37:12.785
Um, and the way it works for scoring is the sort of, like,

00:37:12.785 --> 00:37:18.920
the no answer kind of counts as like one word as a sort of a special token.

00:37:18.920 --> 00:37:23.690
So, if it's, if it should be a no answer and you say no answer,

00:37:23.690 --> 00:37:28.580
you get a score of one on the either exact match or the F-measure.

00:37:28.580 --> 00:37:30.560
And if you don't do that,

00:37:30.560 --> 00:37:32.210
you get a score of zero.

00:37:32.210 --> 00:37:38.690
Um, and so, the simplest way of approaching SQuAD 2.0 would be to say, well,

00:37:38.690 --> 00:37:42.274
rather than just always returning the best match in my system,

00:37:42.274 --> 00:37:47.075
I'll use some kind of threshold and only if the score is above a threshold,

00:37:47.075 --> 00:37:48.785
our counters and answer.

00:37:48.785 --> 00:37:51.050
You could do more sophisticated things.

00:37:51.050 --> 00:37:54.080
So another area that we've worked on quite a bit at Stanford is

00:37:54.080 --> 00:37:58.520
this natural language inference task that I'll talk about later in the course.

00:37:58.520 --> 00:38:02.840
Um, but that's really about saying whether one piece of,

00:38:02.840 --> 00:38:05.630
um, text is the conclusion of another,

00:38:05.630 --> 00:38:06.890
um, piece of text.

00:38:06.890 --> 00:38:10.670
And so that's sort of a way that you can try and see whether, uh,

00:38:10.670 --> 00:38:17.120
a piece of text actually gives you a justification and answer to what the question was.

00:38:17.120 --> 00:38:21.530
But at any rate, this trying to decide whether

00:38:21.530 --> 00:38:27.005
you've actually got an answer or not is a quite difficult problem in many cases.

00:38:27.005 --> 00:38:31.880
So here's an example from SQuAD, um, 2.0.

00:38:31.880 --> 00:38:35.120
So Genghis Khan united the Mongol and Turkic tribes of

00:38:35.120 --> 00:38:38.855
the steppes and became Great Khan in 1206.

00:38:38.855 --> 00:38:42.290
He and his successors expanded the Mongol Empire across Asia,

00:38:42.290 --> 00:38:43.940
blah, blah, blah, blah.

00:38:43.940 --> 00:38:45.635
And the question is,

00:38:45.635 --> 00:38:48.260
when did Genghis Khan kill Great Khan?

00:38:48.260 --> 00:38:50.480
And the answer to that is,

00:38:50.480 --> 00:38:53.525
you know, uh, there isn't an answer because actually,

00:38:53.525 --> 00:38:59.150
Genghis Khan was a person named Great Khan and he didn't kill a Great Khan.

00:38:59.150 --> 00:39:01.835
It's just not a question with an answer.

00:39:01.835 --> 00:39:07.985
Um, but it's precisely what happens with systems is, you know,

00:39:07.985 --> 00:39:11.645
even though these systems get high scores in terms of points,

00:39:11.645 --> 00:39:15.980
they don't actually understand human language that well.

00:39:15.980 --> 00:39:17.615
So they look at something that says,

00:39:17.615 --> 00:39:20.855
when did Genghis Khan kill Great Khan?

00:39:20.855 --> 00:39:23.930
Well, this is something that's looking for a date and there are

00:39:23.930 --> 00:39:27.740
some obvious dates in this passage there's 1206, 1234,

00:39:27.740 --> 00:39:31.835
1251 and well, there's kill,

00:39:31.835 --> 00:39:36.560
and kill looks a little bit similar to destroyed.

00:39:36.560 --> 00:39:38.645
I can see the word destroyed.

00:39:38.645 --> 00:39:41.345
So that probably kind of matches.

00:39:41.345 --> 00:39:43.400
And then we're talking about, um,

00:39:43.400 --> 00:39:45.560
Genghis Khan and there,

00:39:45.560 --> 00:39:48.395
I can see Genghis and Khan in this passage.

00:39:48.395 --> 00:39:50.960
And so it sort of puts that together and says

00:39:50.960 --> 00:39:55.175
1234 is the answer when that isn't the answer at all.

00:39:55.175 --> 00:39:59.870
And that's actually kind of pretty typical of the behavior of these systems.

00:39:59.870 --> 00:40:03.560
And so that, on the one hand, they work great.

00:40:03.560 --> 00:40:06.155
On the other hand, they don't actually understand that much,

00:40:06.155 --> 00:40:10.025
and effectively asking whether there's,

00:40:10.025 --> 00:40:14.930
this question is actually answered in the passage is a way of

00:40:14.930 --> 00:40:17.360
revealing the extent to which these models

00:40:17.360 --> 00:40:20.945
do or don't understand what's actually going on.

00:40:20.945 --> 00:40:23.915
Okay. So, at the time, um,

00:40:23.915 --> 00:40:27.095
they built SQuAD version 2.0.

00:40:27.095 --> 00:40:28.835
They took some of, um,

00:40:28.835 --> 00:40:32.090
the existing SQuAD version one's systems,

00:40:32.090 --> 00:40:36.725
and, um, modified them in a very simple way.

00:40:36.725 --> 00:40:39.275
I put in a threshold, um,

00:40:39.275 --> 00:40:43.175
score as to how good the final match was deemed to be,

00:40:43.175 --> 00:40:47.645
and said, Well, how well do you do on SQuAD 2.0?

00:40:47.645 --> 00:40:50.825
And the kind of systems that we saw doing well before,

00:40:50.825 --> 00:40:52.370
now didn't do that well,

00:40:52.370 --> 00:40:58.820
so something like the BiDAF system that we mentioned before was now scoring about 62 F1,

00:40:58.820 --> 00:41:01.370
so that that was sort of hugely lowering

00:41:01.370 --> 00:41:05.210
its performance and reflecting the limits of understanding.

00:41:05.210 --> 00:41:09.650
Um, but it turned out actually that this problem didn't prove to

00:41:09.650 --> 00:41:14.240
be q- quite as difficult as the dataset authors,

00:41:14.240 --> 00:41:16.820
um, maybe thought either.

00:41:16.820 --> 00:41:19.775
Um, because it turns out that um,

00:41:19.775 --> 00:41:23.375
here we are now in February 2019,

00:41:23.375 --> 00:41:26.285
and if you look at the top of the leaderboard,

00:41:26.285 --> 00:41:29.465
we're kind of getting close again to the point

00:41:29.465 --> 00:41:32.780
where the best systems are almost as good as human beings.

00:41:32.780 --> 00:41:39.080
So, um, the current top rate system there you can see is getting 87.6 F1,

00:41:39.080 --> 00:41:43.220
which is less than two points behind where the human beings are.

00:41:43.220 --> 00:41:47.510
Um, the SQuAD version 2 they also co- corrected the,

00:41:47.510 --> 00:41:49.400
um, scoring of human beings,

00:41:49.400 --> 00:41:52.805
so it's more of a fair evaluation this time, um,

00:41:52.805 --> 00:41:54.920
so there's still a bit of a gap but, you know,

00:41:54.920 --> 00:41:58.010
the systems are actually doing, um, really well.

00:41:58.010 --> 00:42:01.040
And the interesting thing there is,

00:42:01.040 --> 00:42:04.625
you know, on the one hand these systems are impressively good.

00:42:04.625 --> 00:42:06.890
Um, you can go on the SQuAD website and look

00:42:06.890 --> 00:42:09.275
at the output of several of the good systems,

00:42:09.275 --> 00:42:12.335
and you can see that there are just a ton of things that they get right.

00:42:12.335 --> 00:42:14.330
They're absolutely not bad systems.

00:42:14.330 --> 00:42:18.980
You have to be a good system to be getting five out of six of the questions right.

00:42:18.980 --> 00:42:21.860
Um, but, you know, on the other hand they still

00:42:21.860 --> 00:42:25.130
make quite elementary Natural Language Understanding Errors.

00:42:25.130 --> 00:42:28.295
And so here's an example of one of those.

00:42:28.295 --> 00:42:29.720
Okay, so this one,

00:42:29.720 --> 00:42:32.540
the Yuan dynasty is considered both a successor to

00:42:32.540 --> 00:42:36.155
the Mongol Empire and an imperial Chinese dynasty.

00:42:36.155 --> 00:42:38.840
It was the khanate ruled by the successors of

00:42:38.840 --> 00:42:42.665
Mongke Khan after the division of the Mongol Empire.

00:42:42.665 --> 00:42:46.730
In official Chinese histories the Yuan dynasty bore the Mandate of Heaven,

00:42:46.730 --> 00:42:50.480
following the Song dynasty and preceding the Ming dynasty.

00:42:50.480 --> 00:42:52.655
Okay. And then the question is,

00:42:52.655 --> 00:42:55.760
what dynasty came before the Yuan?

00:42:55.760 --> 00:42:58.490
And that's a pretty easy question,

00:42:58.490 --> 00:42:59.990
I'd hope, for a human being.

00:42:59.990 --> 00:43:02.790
Everyone can answer that question?

00:43:02.830 --> 00:43:08.480
Okay, um, yeah, so it says in official Chinese histories Yuan Dynast- uh,

00:43:08.480 --> 00:43:09.920
sorry the next sentence.

00:43:09.920 --> 00:43:12.560
Um, yeah followed- right the Yuan Dynasty following

00:43:12.560 --> 00:43:15.245
the Song dynasty and preceding the Ming dynasty.

00:43:15.245 --> 00:43:17.555
But, you know actually um,

00:43:17.555 --> 00:43:20.960
this sort of the leading um,

00:43:20.960 --> 00:43:25.310
Google BERT model says that it was the Ming dynasty that came before

00:43:25.310 --> 00:43:29.450
the Yuan Dynasty which you know is sort of elementarily

00:43:29.450 --> 00:43:33.320
wrong that reveals some of the same kind of it's

00:43:33.320 --> 00:43:38.240
not really understanding everything but it's doing a sort of a matching problem still.

00:43:38.240 --> 00:43:45.620
Okay. So, this SQuAD dataset has been useful and good.

00:43:45.620 --> 00:43:48.860
It still has some major limitations and I just thought I'd

00:43:48.860 --> 00:43:52.370
mentioned what a few of those are so you're aware of some of the issues.

00:43:52.370 --> 00:43:54.950
So one of them I've already mentioned, right,

00:43:54.950 --> 00:44:00.740
that you're in this space where all answers are a span from the passage.

00:44:00.740 --> 00:44:03.890
And that just limits the kind of questions you can

00:44:03.890 --> 00:44:07.025
ask and the kind of difficult situations there can be.

00:44:07.025 --> 00:44:10.370
So, there can't be yes-no questions counting

00:44:10.370 --> 00:44:15.785
questions or even any of the sort of more difficult implicit questions.

00:44:15.785 --> 00:44:21.185
So, if you think back to when you were in middle school and did reading comprehension,

00:44:21.185 --> 00:44:23.825
I mean, it wasn't typically um,

00:44:23.825 --> 00:44:27.440
the case um, that you're being asked

00:44:27.440 --> 00:44:31.400
questions that were just stated explicitly in the text of,

00:44:31.400 --> 00:44:34.880
you know, Sue is visiting her mother in Miami.

00:44:34.880 --> 00:44:36.335
And the question was,

00:44:36.335 --> 00:44:38.315
who was visiting in Miami?

00:44:38.315 --> 00:44:43.730
That wasn't the kind of questions you were asked you were normally asked questions um,

00:44:43.730 --> 00:44:46.310
like um, you know,

00:44:46.310 --> 00:44:52.505
um, Sue is going to a job interview this morning,

00:44:52.505 --> 00:44:56.360
um, it's a really important job interview for her future.

00:44:56.360 --> 00:44:59.435
At breakfast she um,

00:44:59.435 --> 00:45:03.395
starts buttering both sides of her piece of toast um,

00:45:03.395 --> 00:45:06.410
and you are asked a question like, um,

00:45:06.410 --> 00:45:11.320
why um, is Sue buttering both sides of her piece of toast?

00:45:11.320 --> 00:45:13.420
And you're meant to be able to answer,

00:45:13.420 --> 00:45:17.680
"She's distracted by her important job interview coming up later in the day."

00:45:17.680 --> 00:45:20.995
Which isn't the- something that you can answer um,

00:45:20.995 --> 00:45:23.505
by just picking out a sub span.

00:45:23.505 --> 00:45:31.055
Um, a second problem which is sort of actually a bigger problem is um,

00:45:31.055 --> 00:45:35.645
the way SQuAD was constructed for ease

00:45:35.645 --> 00:45:41.970
and not to be too expensive and various other reasons was um,

00:45:41.970 --> 00:45:46.235
paragraphs of Wikipedia were selected and then,

00:45:46.235 --> 00:45:48.680
Mechanical Turkers were hired to say,

00:45:48.680 --> 00:45:51.215
"Come up with some questions um,

00:45:51.215 --> 00:45:56.210
that can be answered by this this passage in version 1.1."

00:45:56.210 --> 00:45:59.315
And then in version two they were said- told,

00:45:59.315 --> 00:46:03.170
"Also come up with some questions that

00:46:03.170 --> 00:46:07.385
look like they're related to this passage but aren't actually answered in the passage."

00:46:07.385 --> 00:46:10.070
But, in all cases people were coming up with

00:46:10.070 --> 00:46:14.870
the questions staring at the passage and if you do that,

00:46:14.870 --> 00:46:18.260
it means that your questions are strongly

00:46:18.260 --> 00:46:21.905
overlapping with the passage both in terms of the,

00:46:21.905 --> 00:46:26.630
the words that are used and even the syntactic structures that are

00:46:26.630 --> 00:46:31.520
used for your questions tending to match the syntactic structures of the passage.

00:46:31.520 --> 00:46:37.085
And so that makes question answering um, naturally easy.

00:46:37.085 --> 00:46:39.125
What happens in the real world,

00:46:39.125 --> 00:46:42.260
is this human beings think up questions and

00:46:42.260 --> 00:46:46.010
type something into a search engine and the way

00:46:46.010 --> 00:46:49.355
that they type it in is completely distinct

00:46:49.355 --> 00:46:53.075
from the way something might be worded on a website.

00:46:53.075 --> 00:46:56.600
So that they might be saying something like,

00:46:56.600 --> 00:47:02.720
you know, "In what year did the price of hard disks drop below a dollar a megabyte?"

00:47:02.720 --> 00:47:07.220
Um, and the webpage will say something like

00:47:07.220 --> 00:47:12.050
the cost of hard disks has being dropping for many years um,

00:47:12.050 --> 00:47:18.470
in I know whenever it was 2004 prices eventually crossed um,

00:47:18.470 --> 00:47:20.870
the dollar megabyte barrier or something like that.

00:47:20.870 --> 00:47:24.785
But there's a quite different discussion of the ideas.

00:47:24.785 --> 00:47:28.220
And that kinda matching is much harder and that's one of

00:47:28.220 --> 00:47:32.270
the things that people have done other datasets have tried to do differently.

00:47:32.270 --> 00:47:35.960
Um, another limitation is that these questions and

00:47:35.960 --> 00:47:40.355
answers are very much, find the sentence that's addressing the fact,

00:47:40.355 --> 00:47:42.545
match your question to the sentence,

00:47:42.545 --> 00:47:45.080
return the right thing,

00:47:45.080 --> 00:47:49.400
that there's nothing sort of more difficult than involves multi sentence,

00:47:49.400 --> 00:47:53.210
combine facts together styles of inferencing,

00:47:53.210 --> 00:47:57.050
that the limits of cross sentence stuff there is pretty much limited to

00:47:57.050 --> 00:48:01.295
resolving co-reference which is something we'll talk about later in the class,

00:48:01.295 --> 00:48:04.310
that means that you see a he or she or an it,

00:48:04.310 --> 00:48:09.125
and you can work out who that refers to earlier in the, this course.

00:48:09.125 --> 00:48:12.590
Um, nevertheless, despite all those disadvantages,

00:48:12.590 --> 00:48:15.230
it sort of proved that SQuAD was, you know,

00:48:15.230 --> 00:48:20.180
well-targeted in terms of its level of difficulty, well-structured,

00:48:20.180 --> 00:48:22.910
clean dataset, and it's just been

00:48:22.910 --> 00:48:27.140
sort of everybody's favorite for a question answering dataset.

00:48:27.140 --> 00:48:30.080
It also seems to have proved that actually for

00:48:30.080 --> 00:48:33.530
people who work in industry and want to build a question answering system,

00:48:33.530 --> 00:48:36.005
starting off by training a model in SQuAD,

00:48:36.005 --> 00:48:39.230
actually turns out to work pretty well it turns out.

00:48:39.230 --> 00:48:41.420
I mean, it's not everything you want to do.

00:48:41.420 --> 00:48:46.250
You definitely wanna have relevant in domain data and be using that as well,

00:48:46.250 --> 00:48:50.450
but you know, it turns out that it seems to actually be a quite useful starting point.

00:48:50.450 --> 00:48:55.865
Okay. So, what I wanted to show you now was a- is a concrete,

00:48:55.865 --> 00:49:00.710
simple, neural question answering system.

00:49:00.710 --> 00:49:08.300
Um, and this is the model that was built by here and I guess she was

00:49:08.300 --> 00:49:15.860
sort of an Abby predecessor since she was the preceding head TA for CS 224N.

00:49:15.860 --> 00:49:18.650
Um, so this system,

00:49:18.650 --> 00:49:21.830
um, Stanford Attentive Reader it kind of gets called now.

00:49:21.830 --> 00:49:24.575
I mean, this is sort of essentially

00:49:24.575 --> 00:49:29.990
the simplest neural question answering system that works pretty well.

00:49:29.990 --> 00:49:32.780
So, it's not a bad thing to have in mind as

00:49:32.780 --> 00:49:36.320
a baseline and it's not the current state of the art by any means.

00:49:36.320 --> 00:49:40.790
But you know, if you're sort of wondering what's the simplest thing that I can build

00:49:40.790 --> 00:49:45.215
that basically works as a question answering system decently,

00:49:45.215 --> 00:49:47.315
this is basically it.

00:49:47.315 --> 00:49:50.390
Um, okay. So how does this work?

00:49:50.390 --> 00:49:52.595
So the way it works is like this.

00:49:52.595 --> 00:49:53.930
So, first of all,

00:49:53.930 --> 00:49:58.205
we have a question which team won Super Bowl 50?

00:49:58.205 --> 00:50:04.175
And what we're gonna wanna do is build a representation of a question as a vector.

00:50:04.175 --> 00:50:06.920
And the way we can do that is like this,

00:50:06.920 --> 00:50:09.035
for each word in the question,

00:50:09.035 --> 00:50:10.835
we look up a word embedding.

00:50:10.835 --> 00:50:15.440
So, in particular it used GloVe- GloVe 300 dimensional word embeddings.

00:50:15.440 --> 00:50:19.235
Um, we then run an LSTM

00:50:19.235 --> 00:50:23.330
forward through the question and then kind of like Abby talked about,

00:50:23.330 --> 00:50:25.295
we actually make it a bi-LSTM.

00:50:25.295 --> 00:50:29.030
So, we run a second LSTM backwards through the question.

00:50:29.030 --> 00:50:34.880
And so then, we grab the end state of both LSTMs

00:50:34.880 --> 00:50:40.760
and we simply concatenate them together into a vector of dimension 2D if,

00:50:40.760 --> 00:50:43.730
if our hidden states of the LSTM are dimension

00:50:43.730 --> 00:50:48.425
d and we say that is the representation of the question.

00:50:48.425 --> 00:50:51.245
Okay. So, once we have that,

00:50:51.245 --> 00:50:54.230
we then start looking at the passage.

00:50:54.230 --> 00:50:57.635
And so, for the start of dealing with the passage,

00:50:57.635 --> 00:50:59.180
we do the same thing.

00:50:59.180 --> 00:51:03.110
We, um, look up a word vector for every word in

00:51:03.110 --> 00:51:07.340
the passage and we run a bidirectional LSTM,

00:51:07.340 --> 00:51:12.200
now being represented a bit more compactly um, across the passage.

00:51:12.200 --> 00:51:15.710
But then we have to do a little bit more work because we actually

00:51:15.710 --> 00:51:19.040
have to find the answer in the passage.

00:51:19.040 --> 00:51:21.680
And so what we're gonna do is use

00:51:21.680 --> 00:51:28.175
the question representation to sort of work out where the answer is using attention.

00:51:28.175 --> 00:51:31.805
So this is a different use of attention to machine translation.

00:51:31.805 --> 00:51:35.105
That kind of attention equations are still exactly the same.

00:51:35.105 --> 00:51:39.170
But we've now got this sort of one question vector that we gonna be trying to

00:51:39.170 --> 00:51:43.385
match against to return the answer.

00:51:43.385 --> 00:51:47.150
So, what we do is we, um,

00:51:47.150 --> 00:51:51.125
work out an attention score between

00:51:51.125 --> 00:51:57.575
each word's bi-LSTM representation and the question.

00:51:57.575 --> 00:52:02.930
And so the way that's being done is we're using this bi-linear attention,

00:52:02.930 --> 00:52:07.370
um, that um, Abby briefly discussed and we'll see more of today.

00:52:07.370 --> 00:52:09.140
We've got the question vector,

00:52:09.140 --> 00:52:12.530
the vector for a particular position in the passage

00:52:12.530 --> 00:52:15.770
to the two concatenated LSTM hidden states.

00:52:15.770 --> 00:52:17.930
So they're the same dimensionality.

00:52:17.930 --> 00:52:21.020
We have this intervening learn W matrix.

00:52:21.020 --> 00:52:23.360
So, we work out that quantity,

00:52:23.360 --> 00:52:25.115
um, for each position,

00:52:25.115 --> 00:52:27.890
and then we put that through a softmax which will give us

00:52:27.890 --> 00:52:32.180
probabilities over the different words in the passage.

00:52:32.180 --> 00:52:34.220
Um, and those give us,

00:52:34.220 --> 00:52:36.665
um, our attention weights.

00:52:36.665 --> 00:52:39.350
And so at that point we have attention weights,

00:52:39.350 --> 00:52:42.140
um, for different positions, um,

00:52:42.140 --> 00:52:45.410
in the passage and we just declare that,

00:52:45.410 --> 00:52:47.030
um, that is where,

00:52:47.030 --> 00:52:49.610
um, the answer starts.

00:52:49.610 --> 00:52:53.270
Um, and then to get the end of the answer,

00:52:53.270 --> 00:53:01.310
we simply do exactly the same thing again apart from we train a different W matrix here,

00:53:01.310 --> 00:53:02.840
and we have that,

00:53:02.840 --> 00:53:04.940
um, predict the end token.

00:53:04.940 --> 00:53:07.490
And there's something a little bit subtle here.

00:53:07.490 --> 00:53:10.610
Um, because, you know, really we're asking it to sort

00:53:10.610 --> 00:53:13.685
of predict the starts and the ends of the answer,

00:53:13.685 --> 00:53:15.830
and you might think, but wait a minute.

00:53:15.830 --> 00:53:19.595
Surely, we need to look at the middle of the answer as well because maybe the,

00:53:19.595 --> 00:53:23.405
the most indicative words are actually going to be in the middle of the answer.

00:53:23.405 --> 00:53:27.710
Um, but, you know, really really what we're,

00:53:27.710 --> 00:53:32.960
we're sort of implicitly telling the model of well,

00:53:32.960 --> 00:53:37.055
when you're training, if there's stuff in the middle that's useful,

00:53:37.055 --> 00:53:42.440
it's the bi-LSTM's job to push it to the extremes of the span,

00:53:42.440 --> 00:53:47.075
so that this simple bi-linear attention

00:53:47.075 --> 00:53:51.950
will be able to get a big score at the start of the span.

00:53:51.950 --> 00:53:55.040
And you might also think there's something

00:53:55.040 --> 00:53:58.370
funny that this equation and that equation are exactly the same.

00:53:58.370 --> 00:54:02.270
So, how come one of them is meant to know it's picking up beginning, um,

00:54:02.270 --> 00:54:04.400
and the other at the end?

00:54:04.400 --> 00:54:07.475
And again, you know, we're not doing anything to impose that.

00:54:07.475 --> 00:54:09.890
We're just saying, neural network.

00:54:09.890 --> 00:54:11.915
It is your job to learn.

00:54:11.915 --> 00:54:16.115
Um, you have to learn a matrix here and a different one over there,

00:54:16.115 --> 00:54:20.240
so that one of them will pick out parts of the representation that

00:54:20.240 --> 00:54:25.175
indicate starts of answer spans and the other one ends of answer spans.

00:54:25.175 --> 00:54:28.160
And so, that will then again pressure

00:54:28.160 --> 00:54:31.550
the neural network to sort of self organize itself in

00:54:31.550 --> 00:54:34.100
such a way that there'll be some parts of

00:54:34.100 --> 00:54:38.270
this hidden representation that will be good at learning starts of spans.

00:54:38.270 --> 00:54:40.010
You know, maybe there'll be carried backwards by

00:54:40.010 --> 00:54:43.520
the backwards LSTM and and some parts of it will be good at

00:54:43.520 --> 00:54:45.980
learning where the spans end and then

00:54:45.980 --> 00:54:50.610
the W matrix will be able to pick out those parts of the representation.

00:54:50.610 --> 00:54:54.130
Um, but yeah, uh,

00:54:54.130 --> 00:54:58.360
that's the system. Um, yeah.

00:54:58.360 --> 00:55:00.640
So, um, so this is

00:55:00.640 --> 00:55:05.980
the basic Stanford Attentive Reader model and it's just no more complex than that.

00:55:05.980 --> 00:55:08.770
Um, and the interesting thing is, you know,

00:55:08.770 --> 00:55:14.245
that very simple model actually works nicely well.

00:55:14.245 --> 00:55:16.360
Um, so this is going back in time.

00:55:16.360 --> 00:55:23.230
Again, this was the February 2017 SQuAD version 1 leaderboard.

00:55:23.230 --> 00:55:28.690
Um, but at that time, that provide- like,

00:55:28.690 --> 00:55:32.680
it always in neural networks quite a bit of your success

00:55:32.680 --> 00:55:39.280
is training your hyperparameters and optimizing your model really well.

00:55:39.280 --> 00:55:41.260
And some time, you know,

00:55:41.260 --> 00:55:47.020
it's been repeatedly proven in neural network land that often you can get

00:55:47.020 --> 00:55:50.170
much better scores than you would think from

00:55:50.170 --> 00:55:53.845
very simple models if you optimize them really well.

00:55:53.845 --> 00:55:57.280
So there have been multiple cycles in sort of

00:55:57.280 --> 00:55:59.830
deep learning research where there

00:55:59.830 --> 00:56:02.950
was a paper that did something and then the next person says,

00:56:02.950 --> 00:56:04.960
"Here's a more- more- more complex model that

00:56:04.960 --> 00:56:07.540
works better," and then someone else published a paper saying,

00:56:07.540 --> 00:56:09.640
"Here's an even more complex than that model that works

00:56:09.640 --> 00:56:12.490
better," and then someone points out, "No.

00:56:12.490 --> 00:56:17.140
If you go back to the first model and just really train its hyperparameters well,

00:56:17.140 --> 00:56:19.375
you can beat both of those two models."

00:56:19.375 --> 00:56:21.880
And that was effectively the case about what

00:56:21.880 --> 00:56:24.610
was happening with the Stanford Attentive Reader.

00:56:24.610 --> 00:56:29.245
That, you know, back in- back in February 2017,

00:56:29.245 --> 00:56:32.920
if you just train this model really well,

00:56:32.920 --> 00:56:37.990
it could actually outperform most of the early SQuAD systems.

00:56:37.990 --> 00:56:39.235
I mean, in particular,

00:56:39.235 --> 00:56:41.875
it could outperform, um, the BiDAF,

00:56:41.875 --> 00:56:46.390
the version of BiDAF that was around in early 2017 and,

00:56:46.390 --> 00:56:49.315
you know, various of these other systems from other people.

00:56:49.315 --> 00:56:51.340
But it was actually, at that time,

00:56:51.340 --> 00:56:55.405
it was pretty close to the best system that anyone had built.

00:56:55.405 --> 00:56:57.970
Um, as I've already pointed out to you,

00:56:57.970 --> 00:57:00.280
um, the numbers have gone up a lot since then.

00:57:00.280 --> 00:57:02.500
So I'm not claiming that, um,

00:57:02.500 --> 00:57:08.785
this system is still as good as the best systems that you can build. But there you go.

00:57:08.785 --> 00:57:13.000
Um, so that's the simple system that already works pretty well,

00:57:13.000 --> 00:57:15.070
but of course you want this system to work better.

00:57:15.070 --> 00:57:19.690
Um, and so Danqi did quite a bit of work on that.

00:57:19.690 --> 00:57:23.305
And so here I'll just mention a few things for, um,

00:57:23.305 --> 00:57:26.125
Stanford Attentive Reader++ as to

00:57:26.125 --> 00:57:29.635
what kind of things can you do to make the model better.

00:57:29.635 --> 00:57:34.705
And so here's a sort of a picture of, um,

00:57:34.705 --> 00:57:37.960
the sort of the improved system and we'll go through

00:57:37.960 --> 00:57:41.290
some of the differences and what makes it better.

00:57:41.290 --> 00:57:45.190
Um, there's something I didn't have before that I should just mention, right?

00:57:45.190 --> 00:57:50.215
Sort of this whole model, all the parameters of this model are just trained end to end,

00:57:50.215 --> 00:57:53.980
where your training objective is simply, um,

00:57:53.980 --> 00:57:56.380
working out how accurately you're predicting

00:57:56.380 --> 00:57:59.050
the start position and how accurately you're predicting

00:57:59.050 --> 00:58:02.680
the end position so that the attention gives

00:58:02.680 --> 00:58:06.505
you a probability distribution over start positions and end positions.

00:58:06.505 --> 00:58:09.820
So you're just being asked what probability estimate

00:58:09.820 --> 00:58:13.330
are you giving to the true start position and the true end position.

00:58:13.330 --> 00:58:15.250
And to the extent that though,

00:58:15.250 --> 00:58:17.289
you know, those aren't one,

00:58:17.289 --> 00:58:22.375
you've then got loss that is then being sort of summed in terms of log probability.

00:58:22.375 --> 00:58:25.570
Okay. So how is this model, um,

00:58:25.570 --> 00:58:28.855
more complex now than what I showed before?

00:58:28.855 --> 00:58:31.945
Essentially in two main ways.

00:58:31.945 --> 00:58:36.370
So the first one is looking at the question,

00:58:36.370 --> 00:58:40.075
we still run the BiLSTM as before.

00:58:40.075 --> 00:58:44.530
Um, but now what we're going to do is it's a little bit crude

00:58:44.530 --> 00:58:48.850
just to take the end states of the LSTM and concatenate them together.

00:58:48.850 --> 00:58:54.280
It turns out that you can do better by making use of all states in an LSTM.

00:58:54.280 --> 00:58:57.880
And this is true for most tasks where you

00:58:57.880 --> 00:59:01.975
want some kind of sentence representation from a sequence model.

00:59:01.975 --> 00:59:04.585
It turns out you can generally gain by using

00:59:04.585 --> 00:59:07.510
all of them rather than just the endpoints or that.

00:59:07.510 --> 00:59:12.685
Um, so but this is just an interesting general thing to know again because, you know,

00:59:12.685 --> 00:59:18.415
this is actually another variant of how that- how you can use attention.

00:59:18.415 --> 00:59:25.525
There are, you know, a lot of sort of the last two years of neural NLP can be summed

00:59:25.525 --> 00:59:29.230
up as people have found a lot of clever ways to use

00:59:29.230 --> 00:59:33.220
attention and that's been pairing just about all the advances.

00:59:33.220 --> 00:59:41.890
Um, so what we wanna do is we want to have attention over the positions in this LSTM.

00:59:41.890 --> 00:59:46.255
But, you know, this- we're processing the query first.

00:59:46.255 --> 00:59:51.355
So it sort of seems like we've got nothing to calculate attention with respect to.

00:59:51.355 --> 00:59:55.150
So what we do is we just invent something.

00:59:55.150 --> 00:59:56.860
So we just sort of invent.

00:59:56.860 --> 01:00:01.660
Here is a vector and it's sometimes called a sentinel or some word like that,

01:00:01.660 --> 01:00:03.850
but, you know, we just in our PyTorch say,

01:00:03.850 --> 01:00:05.185
"Here is a vector.

01:00:05.185 --> 01:00:07.690
Um, we're going to calculate, um,

01:00:07.690 --> 01:00:09.460
we initialize it randomly,

01:00:09.460 --> 01:00:13.495
and we're gonna calculate attention with respect to that vector,

01:00:13.495 --> 01:00:20.950
and we're going to use those attention scores, um, to, um,

01:00:20.950 --> 01:00:24.250
work out where to pay attention, um,

01:00:24.250 --> 01:00:30.625
in this BiLSTM, and then we just sort of train that vector so it gets values.

01:00:30.625 --> 01:00:34.270
And so then we end up with a weighted sum of the time

01:00:34.270 --> 01:00:39.430
steps of that LSTM that uh, then form the question representation.

01:00:39.430 --> 01:00:42.370
Um, second change, uh,

01:00:42.370 --> 01:00:45.400
the pictures only show a shallow BiLSTM but, you know,

01:00:45.400 --> 01:00:48.940
it turns out you can do better if you have a deep BiLSTM and say

01:00:48.940 --> 01:00:53.005
use a three-layer deep BiLSTM rather than a single layer.

01:00:53.005 --> 01:00:56.200
Okay. Then the other changes in

01:00:56.200 --> 01:01:02.350
the passage representations and this part arguably gets a little bit more hacky,

01:01:02.350 --> 01:01:06.520
um, but there are things that you can do that make the numbers go up, I guess.

01:01:06.520 --> 01:01:07.810
Um, okay.

01:01:07.810 --> 01:01:13.840
So- so firstly for the representation of words rather than only using

01:01:13.840 --> 01:01:18.070
the GloVe representation that the input vectors are

01:01:18.070 --> 01:01:24.055
expanded so that- so a named entity recognizer and a part of speech tagger is run.

01:01:24.055 --> 01:01:28.615
And since those are sort of small sets of values,

01:01:28.615 --> 01:01:33.910
that the output of those is just one-hot encoded and concatenated onto

01:01:33.910 --> 01:01:36.490
the word vector, so it represents if it's

01:01:36.490 --> 01:01:40.195
a location or a person name and whether it's a noun or a verb.

01:01:40.195 --> 01:01:44.080
Um, word frequency proves to be a bit useful.

01:01:44.080 --> 01:01:52.165
So there's your concatenating on sort of a representation of the word frequency as,

01:01:52.165 --> 01:01:57.370
um, just sort of a float of the unigram probability.

01:01:57.370 --> 01:02:05.335
Um, and then this part is kind of key to getting some further advances which is, well,

01:02:05.335 --> 01:02:11.140
it turns out that we can do a better job by doing some sort

01:02:11.140 --> 01:02:16.945
of better understanding of the matching between the question and the passage.

01:02:16.945 --> 01:02:20.170
And, um, this feature seems like it's

01:02:20.170 --> 01:02:23.815
very simple but turns out to actually give you quite a lot of value.

01:02:23.815 --> 01:02:28.420
So you're simply saying for each word in the question,

01:02:28.420 --> 01:02:32.215
uh, so for each word- well,  I said that wrong.

01:02:32.215 --> 01:02:35.920
For each word in the passage,

01:02:35.920 --> 01:02:39.040
you were just saying, "Does this word appear in the question?"

01:02:39.040 --> 01:02:42.160
And if so you're setting a one bit into

01:02:42.160 --> 01:02:46.105
the input and that's done in three different ways: exact match,

01:02:46.105 --> 01:02:48.580
uncased match, and lemma match.

01:02:48.580 --> 01:02:51.655
So that means something like drive and driving, um,

01:02:51.655 --> 01:02:53.590
will match, and just that sort of

01:02:53.590 --> 01:02:56.755
indicator of here's where in the passage that's in the question.

01:02:56.755 --> 01:02:59.230
In theory, the system should be able to work that out

01:02:59.230 --> 01:03:03.115
anyway that explicitly indicate and it gives quite a bit of value.

01:03:03.115 --> 01:03:09.310
And then this last one does a sort of a softer version of that where it's using word

01:03:09.310 --> 01:03:12.550
embedding similarities to sort of calculate

01:03:12.550 --> 01:03:16.210
a kind of similarity between questions and answers,

01:03:16.210 --> 01:03:19.345
and that's a slightly complex equation that you can look up.

01:03:19.345 --> 01:03:26.035
But effectively, um, that you're getting the embedding of words and the question answers.

01:03:26.035 --> 01:03:30.085
Each of those, you're running through a single hidden layer,

01:03:30.085 --> 01:03:31.585
neural network, you know,

01:03:31.585 --> 01:03:35.245
dot producting it, and then putting all that through a Softmax,

01:03:35.245 --> 01:03:40.820
and that kind of gives you a sort of word similarity score and that helps as well.

01:03:41.040 --> 01:03:46.510
Okay. So here's the kind of just overall picture this gives you.

01:03:46.510 --> 01:03:49.435
So if you remember, um, um,

01:03:49.435 --> 01:03:52.540
there was the sort of the classical NLP

01:03:52.540 --> 01:03:55.825
with logistic regression baseline, there's around 51.

01:03:55.825 --> 01:03:58.810
So for sort of a fairly simple model,

01:03:58.810 --> 01:04:00.970
like the Stanford Attentive Reader,

01:04:00.970 --> 01:04:03.760
it gives you an enormous boost in performance, right?

01:04:03.760 --> 01:04:07.765
That's giving you close to 30 percent performance gain.

01:04:07.765 --> 01:04:10.180
And then, you know, from there,

01:04:10.180 --> 01:04:13.420
people have kept on pushing up neural systems.

01:04:13.420 --> 01:04:17.410
But, you know, so this gives you kind of in some sense three quarters of

01:04:17.410 --> 01:04:22.525
the value over the traditional NLP system and in the much more,

01:04:22.525 --> 01:04:26.080
um, complex, um, neural systems that come after it.

01:04:26.080 --> 01:04:27.145
Um, yeah.

01:04:27.145 --> 01:04:28.555
In terms of error reduction,

01:04:28.555 --> 01:04:31.780
they're huge but it's sort of more like they're giving you the sort of,

01:04:31.780 --> 01:04:34.880
um, 12 percent after that.

01:04:35.310 --> 01:04:43.030
Why did these systems work such a ton better um, than traditional systems?

01:04:43.030 --> 01:04:46.750
And so we actually did some error analysis of this and, you know,

01:04:46.750 --> 01:04:52.180
it turns out that most of their gains is because they can just do

01:04:52.180 --> 01:04:56.890
better semantic matching of word similarities

01:04:56.890 --> 01:05:02.080
or rephrasings that are semantically related but don't use the same words.

01:05:02.080 --> 01:05:10.675
So, to- to the extent that the question is where was Christopher Manning born?

01:05:10.675 --> 01:05:15.595
And the sentence says Christopher Manning was born in Australia,

01:05:15.595 --> 01:05:18.790
a traditional NLP system would get that right too.

01:05:18.790 --> 01:05:21.565
But that to the extent that you being able to get it right,

01:05:21.565 --> 01:05:23.980
depends on being able to match,

01:05:23.980 --> 01:05:29.575
sort of looser semantic matches so that we understand the sort of um,

01:05:29.575 --> 01:05:33.610
you know, the place of birth has to be matching was born or something.

01:05:33.610 --> 01:05:37.750
That's where the neural systems actually do work much much better.

01:05:37.750 --> 01:05:44.950
Okay. So, that's not the end of the story on question-answering systems.

01:05:44.950 --> 01:05:48.400
And I wanted to say just a little bit about um,

01:05:48.400 --> 01:05:51.670
more complex systems to give you some idea um,

01:05:51.670 --> 01:05:53.725
of what goes on after that.

01:05:53.725 --> 01:05:56.260
Um, but before I go further into that,

01:05:56.260 --> 01:05:59.980
are there any questions on uh,

01:05:59.980 --> 01:06:03.130
up until now, Stanford Attentive Reader?

01:06:03.130 --> 01:06:09.760
[NOISE] Yeah.

01:06:09.760 --> 01:06:12.925
I have a question about attention in general.

01:06:12.925 --> 01:06:18.550
Every example we've seen has been just linear mapping with a weight matrix.

01:06:18.550 --> 01:06:23.695
Has anybody tried to convert that to a deep neural network and see what happens?

01:06:23.695 --> 01:06:26.335
Um, so yes they have.

01:06:26.335 --> 01:06:30.040
Well, at least a shallow neural network.

01:06:30.040 --> 01:06:33.010
Um, I'll actually show an example of that in just a minute.

01:06:33.010 --> 01:06:35.800
So maybe I will um, save it till then.

01:06:35.800 --> 01:06:38.305
But yeah absolutely, um,

01:06:38.305 --> 01:06:43.520
yeah people have done that and that can be a good thing to um, play with.

01:06:45.030 --> 01:06:52.060
Anything else? Okay. Um, okay.

01:06:52.060 --> 01:06:57.970
So, this is a picture of the BiDAF system,

01:06:57.970 --> 01:07:00.730
so this is the one from AI2 UDub.

01:07:00.730 --> 01:07:03.490
And the BiDAF system is very well known.

01:07:03.490 --> 01:07:06.880
Um, it's another sort of classic version of

01:07:06.880 --> 01:07:11.140
question-answering system that lots of people have used and built off.

01:07:11.140 --> 01:07:14.260
Um, and, you know,

01:07:14.260 --> 01:07:20.260
some of it isn't completely different to what we saw before but it has various additions.

01:07:20.260 --> 01:07:23.980
So, there are word embeddings just like we had before,

01:07:23.980 --> 01:07:28.225
there's a biLSTM running just like what we had before,

01:07:28.225 --> 01:07:31.435
and that's being done for both the um,

01:07:31.435 --> 01:07:33.865
passage and the question.

01:07:33.865 --> 01:07:37.210
Um, but there are some different things that are happening as well.

01:07:37.210 --> 01:07:40.510
So one of them is rather than just having word embeddings,

01:07:40.510 --> 01:07:45.085
it also processes the questions and passages at the character level.

01:07:45.085 --> 01:07:48.730
And that's something that we're going to talk about coming up ahead in the class.

01:07:48.730 --> 01:07:54.204
There's been a lot of work at doing character level processing in recent neural NLP,

01:07:54.204 --> 01:07:56.365
but I don't want to talk about that now.

01:07:56.365 --> 01:08:00.460
Um, the main technical innovation of the BiDAF model

01:08:00.460 --> 01:08:06.175
is this attention flow layout because that's in its name bidirectional attention flow.

01:08:06.175 --> 01:08:10.300
And so, there was a model of attention flow where you have attention

01:08:10.300 --> 01:08:14.740
flowing in both directions between the query and the passage.

01:08:14.740 --> 01:08:18.985
And that was their main innovation and it was quite useful in their model.

01:08:18.985 --> 01:08:20.575
Um, but beyond that,

01:08:20.575 --> 01:08:23.500
there's you know, sort of more stuff to this model.

01:08:23.500 --> 01:08:27.324
So after the attention flow layer there's again

01:08:27.324 --> 01:08:31.675
multiple layers of bidirectional LSTMs running.

01:08:31.675 --> 01:08:35.770
And then on top of that their output layer is more

01:08:35.770 --> 01:08:41.230
complex than the sort of simple attention version that I showed previously.

01:08:41.230 --> 01:08:45.145
So let's just look at that in a bit more detail.

01:08:45.145 --> 01:08:47.935
Um so, for the attention flow layer.

01:08:47.935 --> 01:08:53.905
So, the motivation here was in the Stanford Attentive Reader,

01:08:53.905 --> 01:08:57.460
we used attention to map from

01:08:57.460 --> 01:09:03.175
the representation of the question onto the words of the passage.

01:09:03.175 --> 01:09:09.325
But, you know so as questions are whole mapping onto the words of the passage.

01:09:09.325 --> 01:09:11.950
Where their idea was well,

01:09:11.950 --> 01:09:18.760
presumably you could do better by mapping in both directions at the word level.

01:09:18.760 --> 01:09:23.890
So you should be sort of finding passage words that you can map onto question words,

01:09:23.890 --> 01:09:26.605
and question words that you can map onto passage words.

01:09:26.605 --> 01:09:29.965
And if you do that in both directions with attention flowing,

01:09:29.965 --> 01:09:34.315
and then run another round of sequence models on top of that,

01:09:34.315 --> 01:09:38.530
that you'll just be able to do much better matching between the two of them.

01:09:38.530 --> 01:09:42.940
And so the way they do that is, um,

01:09:42.940 --> 01:09:46.600
that they- they've got the bottom- so at

01:09:46.600 --> 01:09:50.800
the bottom layers they've sort of run these two LSTMs.

01:09:50.800 --> 01:09:57.480
So they have representations in the LSTM for each word and um,

01:09:57.480 --> 01:10:00.480
word and passage position.

01:10:00.480 --> 01:10:04.440
And at this point I have to put it in a slight apology because I just

01:10:04.440 --> 01:10:08.760
stole the equations and so the letters that are used change.

01:10:08.760 --> 01:10:12.845
Sorry. But, so these are the um,

01:10:12.845 --> 01:10:18.505
question individual words and these are the passage individual words.

01:10:18.505 --> 01:10:23.485
And so, what they're then wanting to do is to say for each passage word,

01:10:23.485 --> 01:10:28.105
and each question word, I want to work out a similarity score.

01:10:28.105 --> 01:10:34.570
And the way they work out that similarity score is they build a big concatenated vector.

01:10:34.570 --> 01:10:40.359
So there's the LSTM representation of the passage word, the question word,

01:10:40.359 --> 01:10:45.070
and then they throw in a third thing where they do a Hadamard product,

01:10:45.070 --> 01:10:49.855
so an element-wise product of the question word and the context word.

01:10:49.855 --> 01:10:53.590
Um, you know, for a neural net purist, throwing in

01:10:53.590 --> 01:10:57.580
these kind of Hadamard products is a little bit of a cheat because

01:10:57.580 --> 01:11:01.180
you kind of would hope that a neural net might just learn that

01:11:01.180 --> 01:11:05.635
this relation between the passage and the question was useful to look at.

01:11:05.635 --> 01:11:08.380
But you can find a lot of models that put in

01:11:08.380 --> 01:11:11.920
these kind of Hadamard product because it's sort of

01:11:11.920 --> 01:11:18.415
a very easy way of sort of having a model that knows that matching is a good idea.

01:11:18.415 --> 01:11:24.790
Because essentially this is sort of looking for each question and passage word pair.

01:11:24.790 --> 01:11:28.810
You know, do the vectors look similar in various dimensions?

01:11:28.810 --> 01:11:32.965
You can sort of access very well from looking at that Hadamard product.

01:11:32.965 --> 01:11:35.815
So that- so you take that big vector,

01:11:35.815 --> 01:11:40.765
and you then dot-product that with a learned weight matrix,

01:11:40.765 --> 01:11:43.389
and that gives you a similarity score

01:11:43.389 --> 01:11:47.050
between each position in the question and the context.

01:11:47.050 --> 01:11:50.395
And so then what you're gonna do is use that to

01:11:50.395 --> 01:11:55.325
define attentions that go in both directions. Um-

01:11:55.325 --> 01:11:58.989
So for the, um, context,

01:11:58.989 --> 01:12:02.415
the question attention, this one's completely straightforward.

01:12:02.415 --> 01:12:08.550
So, you put these similarity scores through a soft-max.

01:12:08.550 --> 01:12:13.515
So for each of the i positions in the passage or sort of,

01:12:13.515 --> 01:12:17.300
having a softmax which is giving you a probability distribution,

01:12:17.300 --> 01:12:20.375
over question words and then you're coming up with

01:12:20.375 --> 01:12:26.750
a new representation of the i-th position which is then the attention weighted,

01:12:26.750 --> 01:12:31.350
um, version, the attention weighted average of those question words.

01:12:31.350 --> 01:12:32.760
Um, so you're sort of,

01:12:32.760 --> 01:12:38.775
having attention weighted view of the question mapped onto each position in the passage.

01:12:38.775 --> 01:12:43.860
Um, you then want to do something in the reverse direction.

01:12:43.860 --> 01:12:49.815
Um, but the one in the reverse direction is done subtly differently.

01:12:49.815 --> 01:12:53.325
So you're again starting off, um,

01:12:53.325 --> 01:13:00.690
with the- the same similarity scores but this time they're sort of wanting to, sort of,

01:13:00.690 --> 01:13:04.875
really assign which position,

01:13:04.875 --> 01:13:12.120
in which position in the question is the one that's, sort of,

01:13:12.120 --> 01:13:16.980
aligning the most so that they're finding a max and so that they're finding

01:13:16.980 --> 01:13:22.545
which is the most aligned one and so then for each of,

01:13:22.545 --> 01:13:24.930
for each of the i's,

01:13:24.930 --> 01:13:27.885
they're finding the most aligned question word.

01:13:27.885 --> 01:13:33.670
And so then they're doing a softmax over these m scores and then those are being

01:13:33.670 --> 01:13:39.900
used to form a new representation of the passage by,

01:13:39.900 --> 01:13:43.110
sort of, summing over these attention weights.

01:13:43.110 --> 01:13:47.310
Okay. So you build these things up and this then

01:13:47.310 --> 01:13:51.330
gives you a new representation where you have,

01:13:51.330 --> 01:13:57.090
um, your original representations of the passage words.

01:13:57.090 --> 01:14:00.120
You'd have a new representation that you've built from

01:14:00.120 --> 01:14:02.585
this bidirectional attention flow and you

01:14:02.585 --> 01:14:05.310
look at these sort of Hadamard products of them and

01:14:05.310 --> 01:14:10.110
that then gives you kind of the output of the BiDAF layer and that output of

01:14:10.110 --> 01:14:12.990
the BiDAF layer is then what's sort of being fed as

01:14:12.990 --> 01:14:18.160
the input into these nick- next sequence of LSTM layers.

01:14:18.350 --> 01:14:22.240
Okay. Um, and so yeah,

01:14:22.240 --> 01:14:24.335
um, so then that's the modeling layer.

01:14:24.335 --> 01:14:29.085
You have another two BiLSTM layers and so the way they do the,

01:14:29.085 --> 01:14:32.400
um, suspense selection is a bit more complex as well.

01:14:32.400 --> 01:14:35.620
Um, so that they're then, um,

01:14:35.620 --> 01:14:40.020
sort of taking the output of the modeling layer and putting it through a sort of

01:14:40.020 --> 01:14:45.915
a dense feed-forward neural network layer and then softmaxing over that,

01:14:45.915 --> 01:14:49.020
um, and that's then getting a distribution of

01:14:49.020 --> 01:14:53.430
a start and you're running yet another LSTM kind of a distribution finish.

01:14:53.430 --> 01:14:58.020
Um, yeah. So, that gives you some idea of a more complex model.

01:14:58.020 --> 01:15:01.730
Um, you know, in some sense,

01:15:01.730 --> 01:15:05.895
um, the summary if you go further forward than here is that, sort of,

01:15:05.895 --> 01:15:08.835
most of the work in the last couple of years,

01:15:08.835 --> 01:15:14.220
people have been producing progressively more complex architectures with

01:15:14.220 --> 01:15:19.710
lots of variants of attention and effectively that has been giving good gains.

01:15:19.710 --> 01:15:23.010
Um, I think I'll skip since time is running,

01:15:23.010 --> 01:15:25.230
out, showing you that one.

01:15:25.230 --> 01:15:28.980
But, um, let me just mention this FusionNet model

01:15:28.980 --> 01:15:32.500
which was done by people at Microsoft because this relates to the answer,

01:15:32.500 --> 01:15:35.145
the attention question, right?

01:15:35.145 --> 01:15:40.740
So p- so people have definitely used different versions of attention, right?

01:15:40.740 --> 01:15:44.880
So that in some of the stuff that we've shown we tend to emphasize

01:15:44.880 --> 01:15:49.335
this bi-linear attention where you've got two vectors mediated by a matrix.

01:15:49.335 --> 01:15:51.825
And I guess traditionally at Stanford NLP,

01:15:51.825 --> 01:15:53.460
we've liked this, um,

01:15:53.460 --> 01:15:56.460
version of attention since it seems to very directly learn

01:15:56.460 --> 01:16:00.690
a similarity but other people have used a little neural net.

01:16:00.690 --> 01:16:03.000
So this is, sort of, a shallow neural net to

01:16:03.000 --> 01:16:05.340
work out attention scores and there's, sort of,

01:16:05.340 --> 01:16:07.740
no reason why you couldn't say, maybe it would be even better if I

01:16:07.740 --> 01:16:10.710
make that a deep neural net and add another layer.

01:16:10.710 --> 01:16:12.465
Um, and some of, you know,

01:16:12.465 --> 01:16:14.920
to be perfectly honest, um,

01:16:14.920 --> 01:16:18.425
some of the results that have been done by people including Google

01:16:18.425 --> 01:16:22.520
argue that actually that NLP version of attention is better.

01:16:22.520 --> 01:16:25.700
Um, so there's something to explore in that direction.

01:16:25.700 --> 01:16:31.635
But actually, um, the people in FusionNet didn't head that direction because they said,

01:16:31.635 --> 01:16:34.710
"Look, we want to use tons and tons of attention.

01:16:34.710 --> 01:16:37.740
So we want an attention computation that's pretty

01:16:37.740 --> 01:16:41.160
efficient and so it's bad news if you have to

01:16:41.160 --> 01:16:44.115
be evaluating a little dense neural net at

01:16:44.115 --> 01:16:47.880
every position every time that you do attention."

01:16:47.880 --> 01:16:51.630
So this bi-linear form is fairly appealing

01:16:51.630 --> 01:16:55.665
but they then did some playing with it so rather than having a W matrix

01:16:55.665 --> 01:16:59.700
you can reduce the rank and complexity of

01:16:59.700 --> 01:17:06.135
your W matrix by dividing it into the product of two lower rank matrices.

01:17:06.135 --> 01:17:08.985
So you can have a U and a V matrix.

01:17:08.985 --> 01:17:12.689
And if you make these rectangular matrices that are kind of skinny,

01:17:12.689 --> 01:17:16.455
you can then have a sort of a lower rank factorization and,

01:17:16.455 --> 01:17:18.420
that seems a good idea.

01:17:18.420 --> 01:17:19.680
And then they thought well,

01:17:19.680 --> 01:17:23.265
maybe really you want your attention distribution to be symmetric.

01:17:23.265 --> 01:17:26.460
So we can actually put in the middle here,

01:17:26.460 --> 01:17:29.100
we can have the U and the V, so to speak,

01:17:29.100 --> 01:17:32.160
be the same and just have a diagonal matrix in

01:17:32.160 --> 01:17:35.565
the middle and that might be a useful way to think of it.

01:17:35.565 --> 01:17:39.555
And that all makes sense from linear algebra terms but then they thought,

01:17:39.555 --> 01:17:43.055
"Oh, non-linearity is really good in deep learning.

01:17:43.055 --> 01:17:44.640
So why don't we, sort of,

01:17:44.640 --> 01:17:48.790
stick the left and right half through a ReLU and maybe that will help.

01:17:48.790 --> 01:17:52.380
[LAUGHTER] Which doesn't so much make sense in your linear algebra terms, um,

01:17:52.380 --> 01:17:56.850
but that's actually what they ended up using as their, um, attention forms.

01:17:56.850 --> 01:18:00.150
There are lots of things you can play with when doing your final project.

01:18:00.150 --> 01:18:02.085
Um, yeah.

01:18:02.085 --> 01:18:04.740
And, but, you know, their argument is still, you know,

01:18:04.740 --> 01:18:07.920
that doing attention this way is actually much much

01:18:07.920 --> 01:18:11.070
cheaper and so they can use a lot of attention.

01:18:11.070 --> 01:18:16.640
And so they build this very complex tons of attention model, um,

01:18:16.640 --> 01:18:19.155
which I'm not going to try and explain, um,

01:18:19.155 --> 01:18:21.555
all of now, um,

01:18:21.555 --> 01:18:24.750
but I will show you this picture.

01:18:24.750 --> 01:18:28.295
Um, so a point that they make is that a lot of

01:18:28.295 --> 01:18:32.340
the different models that people have explored in different years you,

01:18:32.340 --> 01:18:33.915
that, you know, they're sort of,

01:18:33.915 --> 01:18:36.305
doing different kinds of attention.

01:18:36.305 --> 01:18:39.180
That you could be doing attention right,

01:18:39.180 --> 01:18:42.240
lining up with the original LSTM,

01:18:42.240 --> 01:18:46.340
you could run both sides through some stuff and do attention,

01:18:46.340 --> 01:18:49.740
you can do self attention inside your layer that there are a lot of

01:18:49.740 --> 01:18:53.300
different attentions that different models have explored.

01:18:53.300 --> 01:18:55.710
And essentially what they are wanting to say is,

01:18:55.710 --> 01:18:59.980
let's do all of those and let's make it deep and do it all

01:18:59.980 --> 01:19:04.210
five times and the numbers will go up. And to some extent the answer is,

01:19:04.210 --> 01:19:09.405
yeah they do and the model ends up scoring very well.

01:19:09.405 --> 01:19:15.585
Okay, um, so the one last thing I just wanted to mention but not explain is,

01:19:15.585 --> 01:19:18.450
I mean in the last year there's then been

01:19:18.450 --> 01:19:22.955
a further revolution in how well people can do these tasks.

01:19:22.955 --> 01:19:29.795
And so people have developed algorithms which produce contextual word representation.

01:19:29.795 --> 01:19:32.790
So that means that rather than a traditional word vector,

01:19:32.790 --> 01:19:36.660
you have a representation for each word in a particular context.

01:19:36.660 --> 01:19:41.700
So here's the word frog in this particular context and the way people build

01:19:41.700 --> 01:19:44.490
those representations is using something

01:19:44.490 --> 01:19:47.580
like a language modeling tasks like Abby talked about,

01:19:47.580 --> 01:19:50.730
of saying putting probabilities of words in

01:19:50.730 --> 01:19:54.795
context to learn a context-specific word representation.

01:19:54.795 --> 01:19:57.870
And ELMo was the first well-known such model.

01:19:57.870 --> 01:20:00.410
And then people from Google came up with BERT,

01:20:00.410 --> 01:20:01.830
which worked even better.

01:20:01.830 --> 01:20:06.490
Um, and so BERT is really in some sense is

01:20:06.490 --> 01:20:11.235
super complex attention Architecture doing a language modeling like objective.

01:20:11.235 --> 01:20:13.680
We're going to talk about these later, um,

01:20:13.680 --> 01:20:16.580
I'm not going to talk about them now, um,

01:20:16.580 --> 01:20:22.260
but if you look at the current SQuAD 2.0 Leaderboard,

01:20:22.260 --> 01:20:24.090
um, you will quickly,

01:20:24.090 --> 01:20:28.485
um - sorry that's- oh I put the wrong slide and that was the bottom of the leaderboard.

01:20:28.485 --> 01:20:30.270
Oops, slipped at the last minute.

01:20:30.270 --> 01:20:34.785
If you go back to my slide which had the top of the leaderboard, um,

01:20:34.785 --> 01:20:38.805
you will have noticed that the top of the leaderboard,

01:20:38.805 --> 01:20:42.825
every single one of the top systems uses BERT.

01:20:42.825 --> 01:20:45.240
So that's something that you may want to

01:20:45.240 --> 01:20:47.820
consider but you may want to consider how you could

01:20:47.820 --> 01:20:52.800
use it as a sub-module which you could add other stuff too as many of these systems do.

01:20:52.800 --> 01:20:56.140
Okay. Done for today.

