WEBVTT
Kind: captions
Language: en

00:00:04.430 --> 00:00:07.410
可以。大家好。

00:00:07.410 --> 00:00:11.265
[笑声]好吧，我们该开始了。

00:00:11.265 --> 00:00:14.640
嗯，实际上还剩下不少座位。

00:00:14.640 --> 00:00:15.960
如果你真的想大胆，

00:00:15.960 --> 00:00:18.525
在我前面前排有几个座位。

00:00:18.525 --> 00:00:20.445
如果你再大胆一点的话。

00:00:20.445 --> 00:00:23.940
嗯，但他们也在一些排的中间位置。

00:00:23.940 --> 00:00:28.080
因此，如果人们想真正具有公民意识，有些人可以

00:00:28.080 --> 00:00:32.280
向边缘挤压，使其更容易接近，嗯，

00:00:32.280 --> 00:00:35.685
教室里还有一些座位。

00:00:35.685 --> 00:00:39.435
可以。嗯，那么，嗯，

00:00:39.435 --> 00:00:42.890
在这里见到这么多人真是太令人兴奋了。

00:00:42.890 --> 00:00:47.390
所以我非常欢迎来到CS224N，偶尔也会

00:00:47.390 --> 00:00:52.625
被称为凌284，这是自然语言处理与深入学习。

00:00:52.625 --> 00:00:55.420
嗯，作为一种个人轶事，

00:00:55.420 --> 00:00:59.720
这几天有那么多人来上这门课，我还是有点心烦意乱。

00:00:59.720 --> 00:01:03.980
所以，在我教NLP的头十年里，

00:01:03.980 --> 00:01:08.180
你知道我每年约有45人。

00:01:08.180 --> 00:01:11.240
所以它的数量级比

00:01:11.240 --> 00:01:14.360
现在是了，但我想这说明了很多

00:01:14.360 --> 00:01:17.450
关于革命的影响

00:01:17.450 --> 00:01:20.870
一般的人工智能和机器学习，

00:01:20.870 --> 00:01:25.600
深入学习，NLP开始在现代社会。

00:01:25.600 --> 00:01:28.860
可以。所以这是我们今天的计划。

00:01:28.860 --> 00:01:32.750
所以，嗯，嗯，我们今天真的要直接谈正事了。

00:01:32.750 --> 00:01:37.970
因此，他们将简要介绍一些物流课程，

00:01:37.970 --> 00:01:42.140
非常简短的讨论和谈论人类语言和

00:01:42.140 --> 00:01:46.370
字面意思，然后我们想直接谈论，嗯，

00:01:46.370 --> 00:01:50.540
我们要做的第一件事就是找出单词向量并寻找

00:01:50.540 --> 00:01:55.010
在word2vec算法中，这将填充类的其余部分。

00:01:55.010 --> 00:01:56.840
还有两个座位在里面

00:01:56.840 --> 00:01:59.480
前排是想坐在我前面的人，

00:01:59.480 --> 00:02:02.760
只是让你知道[笑声]。

00:02:02.760 --> 00:02:06.365
可以。可以。下面是课程物流的简要介绍。

00:02:06.365 --> 00:02:08.345
我叫克里斯托弗·曼宁，

00:02:08.345 --> 00:02:15.290
勇敢成为头目的人是阿比盖尔，看到就在那里。

00:02:15.290 --> 00:02:18.920
然后我们有很多很棒的助教。

00:02:18.920 --> 00:02:22.700
对那些很好的助教来说，只是站了一会儿。

00:02:22.700 --> 00:02:26.810
所以，嗯，[笑声]我们对很棒的助教有些感觉。

00:02:26.810 --> 00:02:28.900
[笑声]很好。

00:02:28.900 --> 00:02:31.320
嗯，好吧。

00:02:31.320 --> 00:02:33.260
所以你知道什么时候演讲是因为你成功了

00:02:33.260 --> 00:02:37.100
这里也欢迎SCPD的人。

00:02:37.100 --> 00:02:41.300
这也是一个SCPD类，您可以在视频中观看它。

00:02:41.300 --> 00:02:44.300
但我们喜欢斯坦福的学生

00:02:44.300 --> 00:02:47.300
在教室里展示他们美丽的面孔。

00:02:47.300 --> 00:02:52.810
可以。所以，嗯，网页上有所有关于教学大纲和其他课程的信息。

00:02:52.810 --> 00:02:56.175
可以。所以这节课我们希望教什么？

00:02:56.175 --> 00:02:59.240
所以，我们想教的一件事是，呃，你知道，

00:02:59.240 --> 00:03:02.495
对有效的现代深度学习方法的理解。

00:03:02.495 --> 00:03:05.090
从回顾一些基本知识开始，然后

00:03:05.090 --> 00:03:08.780
特别是说各种技术，包括嗯，

00:03:08.780 --> 00:03:11.450
经常性的网络和广泛的关注

00:03:11.450 --> 00:03:14.570
用于自然语言处理模型。

00:03:14.570 --> 00:03:18.770
我们要教的第二件事是了解

00:03:18.770 --> 00:03:23.075
人类语言和理解和产生它们的一些困难。

00:03:23.075 --> 00:03:25.490
当然，如果你想了解很多人类语言，

00:03:25.490 --> 00:03:29.060
有一个完整的语言学系，你可以做很多课程。

00:03:29.060 --> 00:03:33.590
嗯，但是我想至少给你点欣赏，这样你就知道什么是

00:03:33.590 --> 00:03:38.235
人类语言的挑战、困难和多样性。

00:03:38.235 --> 00:03:41.315
这也是一门实践课。

00:03:41.315 --> 00:03:44.960
就像我们真的想教你怎么做

00:03:44.960 --> 00:03:49.670
为NLP的一些主要部分构建实用的系统。

00:03:49.670 --> 00:03:53.750
所以如果你去一家科技公司找工作，他们会说“嘿，

00:03:53.750 --> 00:03:55.789
你能给我们建立一个命名的实体识别器吗？”

00:03:55.789 --> 00:03:58.130
你可以说“当然，我能做到。”

00:03:58.130 --> 00:04:00.530
所以对于一些问题，

00:04:00.530 --> 00:04:02.090
显然我们不能做任何事，

00:04:02.090 --> 00:04:03.230
我们要做单词的意思，

00:04:03.230 --> 00:04:07.579
依赖性分析、机器翻译，您可以选择回答问题，

00:04:07.579 --> 00:04:10.340
实际上，我正在为它们构建系统。

00:04:10.340 --> 00:04:15.080
如果你过去几年一直在和上过课的朋友聊天，

00:04:15.080 --> 00:04:18.860
嗯，这是今年的区别，只是为了把事情弄清楚。

00:04:18.860 --> 00:04:21.830
嗯，我们已经更新了课程的一些内容。

00:04:21.830 --> 00:04:26.285
所以，呃，在我和客座演讲之间有新的内容。

00:04:26.285 --> 00:04:28.470
好吧，那看起来很糟糕。

00:04:29.030 --> 00:04:32.505
不知道这会不会继续发生，我们会知道的。

00:04:32.505 --> 00:04:38.165
有新的内容和各种各样的主题，这些都是发展中的领域。

00:04:38.165 --> 00:04:41.300
这门课程的一个问题是在

00:04:41.300 --> 00:04:44.755
现在还只是发展得很快。

00:04:44.755 --> 00:04:47.480
所以，似乎一年前的内容已经

00:04:47.480 --> 00:04:51.290
我们正在尝试更新数据。

00:04:51.290 --> 00:04:54.140
我们今年要做的一个重大改变是

00:04:54.140 --> 00:04:56.930
有五个一周的任务而不是

00:04:56.930 --> 00:04:59.450
开始时三个两周的作业

00:04:59.450 --> 00:05:02.795
课程，我会在一分钟内多说一点。

00:05:02.795 --> 00:05:06.215
今年我们要用Pythorch而不是TensorFlow，

00:05:06.215 --> 00:05:08.860
我们稍后也会讨论这个问题。

00:05:08.860 --> 00:05:13.880
嗯，星期二或星期四的作业要在课前交。

00:05:13.880 --> 00:05:16.585
所以你不会分心，可以来上课。

00:05:16.585 --> 00:05:20.355
所以开始吧，嗯，是的。

00:05:20.355 --> 00:05:22.680
所以我们想让

00:05:22.680 --> 00:05:26.510
但另一方面，快速上升。

00:05:26.510 --> 00:05:29.555
所以我们有了第一个任务，有点简单，呃，

00:05:29.555 --> 00:05:34.040
但现在有空，下周二就到。

00:05:34.040 --> 00:05:37.460
最后一件事是我们今年没有期中考试。

00:05:37.460 --> 00:05:39.395
嗯，好吧。

00:05:39.395 --> 00:05:40.790
所以这就是我们要做的。

00:05:40.790 --> 00:05:44.300
所以我刚才提到的任务有五个。

00:05:44.300 --> 00:05:46.340
嗯，前一个是百分之六，

00:05:46.340 --> 00:05:49.090
其他的各占12%，

00:05:49.090 --> 00:05:52.185
嗯，我已经说过了。

00:05:52.185 --> 00:05:54.230
我们要用评分器评分。

00:05:54.230 --> 00:05:56.780
如果你能用的话，这对助教会有很大帮助

00:05:56.780 --> 00:06:01.010
您的Sunet ID作为您的GradeScope帐户ID。

00:06:01.010 --> 00:06:04.205
嗯，那么在课程的第二部分，

00:06:04.205 --> 00:06:08.795
人们做一个最终的项目，最终的项目有两种选择。

00:06:08.795 --> 00:06:12.080
你要么做我们默认的最终项目，

00:06:12.080 --> 00:06:14.030
对很多人来说这是一个很好的选择，

00:06:14.030 --> 00:06:15.890
或者你可以做一个定制的最终项目，我会

00:06:15.890 --> 00:06:19.010
在开始的时候多谈这个。

00:06:19.010 --> 00:06:21.200
这行不通。

00:06:21.200 --> 00:06:25.130
嗯，最后我们有了

00:06:25.130 --> 00:06:30.425
最后一次海报展示会，预计您会出席，

00:06:30.425 --> 00:06:34.580
我们将在晚上的那个星期三举行。

00:06:34.580 --> 00:06:37.460
可能还不到五个小时，但会在那个窗口里，

00:06:37.460 --> 00:06:39.485
我们会尽快解决细节问题。

00:06:39.485 --> 00:06:41.510
3%的参与率，

00:06:41.510 --> 00:06:43.390
有关详细信息，请访问网站。

00:06:43.390 --> 00:06:45.885
晚了六天，嗯，

00:06:45.885 --> 00:06:50.330
协作，就像在计算机科学课上一样，

00:06:50.330 --> 00:06:55.340
我们希望你自己做你自己的工作，不要从别人的GitHubs和

00:06:55.340 --> 00:06:57.650
所以我们确实强调你应该

00:06:57.650 --> 00:07:01.150
阅读并注意协作策略。

00:07:01.150 --> 00:07:04.700
可以。下面是问题集的高级计划。

00:07:04.700 --> 00:07:07.790
现在有家庭作业，

00:07:07.790 --> 00:07:10.130
是个很容易上坡的地方。

00:07:10.130 --> 00:07:11.720
在ipython笔记本上，

00:07:11.720 --> 00:07:13.565
帮助大家跟上进度。

00:07:13.565 --> 00:07:17.750
家庭作业二是纯Python加上numpy但那

00:07:17.750 --> 00:07:22.190
会开始教你更多关于潜在的东西，

00:07:22.190 --> 00:07:24.260
我们如何进行深度学习？

00:07:24.260 --> 00:07:29.430
如果你不是很好，有点生锈，或者从没见过，嗯，

00:07:29.430 --> 00:07:31.155
巨蟒还是麻木，嗯，

00:07:31.155 --> 00:07:34.730
星期五我们会有一个额外的部分。

00:07:34.730 --> 00:07:38.210
所以星期五从1:30到2:50嗯，

00:07:38.210 --> 00:07:42.710
在Skilling礼堂，我们将有一个关于python的评论部分。

00:07:42.710 --> 00:07:44.610
那是我们目前唯一的计划部门，

00:07:44.610 --> 00:07:46.595
我们不会有一个正常的部门。

00:07:46.595 --> 00:07:49.550
嗯，所以鼓励你去那，那也将是

00:07:49.550 --> 00:07:53.515
为SCPD录制，也可用于视频。

00:07:53.515 --> 00:07:56.790
嗯，然后是家庭作业三嗯，

00:07:56.790 --> 00:08:00.850
会让我们开始使用pytorch。

00:08:00.850 --> 00:08:04.760
然后我们将要使用的家庭作业4和5

00:08:04.760 --> 00:08:08.720
PY-PyTorch在GPU上，我们实际上将使用

00:08:08.720 --> 00:08:13.520
Microsoft Azure，非常感谢您对我们的支持

00:08:13.520 --> 00:08:19.165
赞助了我们过去三年的GPU计算。

00:08:19.165 --> 00:08:24.995
嗯，是的。所以基本上我的意思是所有的现代深度学习都已经开始使用了

00:08:24.995 --> 00:08:30.589
像Pythort TensorFlow这样的大型深度学习图书馆，

00:08:30.589 --> 00:08:32.210
链测器或MXnet Um，

00:08:32.210 --> 00:08:36.440
然后在GPU上进行计算。

00:08:36.440 --> 00:08:38.600
当然，既然我们在一栋楼里，

00:08:38.600 --> 00:08:40.450
我们当然应该用，嗯，

00:08:40.450 --> 00:08:42.620
GPU[笑声]但我的意思是总的来说

00:08:42.620 --> 00:08:48.830
GPU的可扩展性是现代深度学习的主要动力。

00:08:48.830 --> 00:08:50.720
可以。最后一个项目。

00:08:50.720 --> 00:08:55.460
所以对于最终的项目，有两件事你可以做。

00:08:55.460 --> 00:09:00.665
所以我们有一个默认的最终项目，它本质上是我们的最终项目。

00:09:00.665 --> 00:09:06.215
所以这是建立一个问答系统，我们通过班数据集来实现。

00:09:06.215 --> 00:09:11.450
所以，你建立什么以及如何提高你的表现完全取决于你自己。

00:09:11.450 --> 00:09:14.480
它是开放式的，但它有一个更容易的开始，

00:09:14.480 --> 00:09:16.910
一个明确的目标，我们可以

00:09:16.910 --> 00:09:19.775
有一个关于事情进展的排行榜。

00:09:19.775 --> 00:09:24.680
如果你没有明确的研究目标，那对你来说是个不错的选择

00:09:24.680 --> 00:09:29.600
或者你可以提出定制的最终项目，假设它是合理的，

00:09:29.600 --> 00:09:32.539
我们将批准您的自定义最终项目，

00:09:32.539 --> 00:09:34.190
我们会给你反馈，嗯，

00:09:34.190 --> 00:09:36.755
形成一个导师，嗯，

00:09:36.755 --> 00:09:42.410
无论哪种方式，对于最终的项目，我们都允许一个、两个或三个团队。

00:09:42.410 --> 00:09:45.200
因为家庭作业应该是你自己做的。

00:09:45.200 --> 00:09:50.020
当然，你可以用一种通用的方式和人们谈论这些问题。

00:09:50.020 --> 00:09:53.010
可以。所以这就是课程。

00:09:53.010 --> 00:09:55.700
一切都很好，甚至还没有落后计划。

00:09:55.700 --> 00:10:01.730
可以。所以下一部分是人类语言和词义。

00:10:01.730 --> 00:10:04.745
你知道，如果我是嗯，

00:10:04.745 --> 00:10:10.265
真的会告诉你很多关于人类语言的事情，这需要很多时间，嗯，

00:10:10.265 --> 00:10:12.110
我真的没有。

00:10:12.110 --> 00:10:14.015
所以我只想告诉你，

00:10:14.015 --> 00:10:16.655
关于人类语言的两件轶事。

00:10:16.655 --> 00:10:19.970
第一个是XKCD卡通。

00:10:19.970 --> 00:10:22.520
嗯，我的意思是这不是，

00:10:22.520 --> 00:10:25.110
我不知道为什么会这样。

00:10:26.050 --> 00:10:28.250
我不知道该怎么办。

00:10:28.250 --> 00:10:34.070
嗯，我真的很喜欢这个XKCD卡通。

00:10:34.070 --> 00:10:37.310
这不是你经常在这里看到的经典之作，

00:10:37.310 --> 00:10:42.140
但我认为它对语言有很大的影响，值得思考。

00:10:42.140 --> 00:10:45.650
就像我想很多时候都是为了那些来的人

00:10:45.650 --> 00:10:49.384
对于这个阶级来说，他们主要是像CS人一样的人，

00:10:49.384 --> 00:10:51.950
以及EE人员和随机其他人。

00:10:51.950 --> 00:10:55.250
我认识其他一些人，因为他们是语言学家等等。

00:10:55.250 --> 00:10:57.050
但对很多人来说，

00:10:57.050 --> 00:11:01.610
你已经花了一辈子的时间研究正式的语言和印象

00:11:01.610 --> 00:11:06.185
人类的语言是不是有点破坏形式语言？

00:11:06.185 --> 00:11:08.570
但其实还有很多事情要做，对吧？

00:11:08.570 --> 00:11:11.165
那语言真是太棒了，嗯，

00:11:11.165 --> 00:11:15.110
用于

00:11:15.110 --> 00:11:19.520
各种目的，并能适应各种目的。

00:11:19.520 --> 00:11:23.750
所以你可以做任何事情，从描述数学和人类语言

00:11:23.750 --> 00:11:28.520
嗯，跟你最好的朋友搭讪，让他们更好地了解你。

00:11:28.520 --> 00:11:31.910
所以人类语言确实有一个惊人的东西。不管怎样，我会读的。

00:11:31.910 --> 00:11:34.655
嗯，所以这是第一个人，

00:11:34.655 --> 00:11:36.185
黑发人说，

00:11:36.185 --> 00:11:38.105
“不管怎样，我不在乎。”

00:11:38.105 --> 00:11:40.010
她的朋友说，

00:11:40.010 --> 00:11:42.440
“我想你是说你不在乎。”

00:11:42.440 --> 00:11:46.490
说你可以少关心意味着你至少关心一些。

00:11:46.490 --> 00:11:49.775
黑发人说，“我不知道，

00:11:49.775 --> 00:11:54.590
我们是这些令人难以置信的复杂的大脑，在空虚中挣扎

00:11:54.590 --> 00:11:59.630
盲目地把话扔到黑暗中，彼此联系是徒劳的。”

00:11:59.630 --> 00:12:02.720
各式措辞、拼写和语气，

00:12:02.720 --> 00:12:07.775
时间携带着无数的信号、上下文和子文本等等。

00:12:07.775 --> 00:12:11.435
每个听众都用自己的方式解释这些信号。

00:12:11.435 --> 00:12:13.565
语言不是一个正式的系统，

00:12:13.565 --> 00:12:16.235
语言是光荣的混乱。

00:12:16.235 --> 00:12:20.750
你永远不知道任何一句话对任何人意味着什么。

00:12:20.750 --> 00:12:26.150
你所能做的就是更好地猜测你的话是如何影响人们的，所以

00:12:26.150 --> 00:12:28.790
你可以有机会找到那些

00:12:28.790 --> 00:12:31.790
他们的感觉就像你想让他们感受到的一样。

00:12:31.790 --> 00:12:34.235
其他一切都是毫无意义的。

00:12:34.235 --> 00:12:37.390
我想你是在给我提示你如何解释

00:12:37.390 --> 00:12:41.065
因为你想让我不那么孤单。

00:12:41.065 --> 00:12:43.510
如果是，谢谢。

00:12:43.510 --> 00:12:45.585
这意味着很多。

00:12:45.585 --> 00:12:48.440
但如果你只是把我的话说出来

00:12:48.440 --> 00:12:51.785
一些精神检查表，这样你就可以展示你对它的了解，

00:12:51.785 --> 00:12:53.180
那我就不在乎了。

00:12:53.180 --> 00:13:02.825
[噪音]嗯，我想嗯，

00:13:02.825 --> 00:13:07.790
我认为事实上，这有一些很好的信息，关于语言是如何不确定的

00:13:07.790 --> 00:13:13.340
发展了沟通系统，但不知何故，我们有足够的共识，你知道，

00:13:13.340 --> 00:13:15.500
我们可以进行很多沟通。

00:13:15.500 --> 00:13:16.865
但我们正在做一些你知道的事

00:13:16.865 --> 00:13:20.540
猜测人的意思的概率推理

00:13:20.540 --> 00:13:22.070
使用语言不仅仅是为了

00:13:22.070 --> 00:13:26.195
信息功能，但不包括社会功能等。

00:13:26.195 --> 00:13:32.310
可以。这是我的另一个想法，我复习过语言。

00:13:33.490 --> 00:13:40.565
所以，从本质上说，如果我们想拥有智能的人工智能，

00:13:40.565 --> 00:13:43.940
我们需要什么才能达到拥有的目的

00:13:43.940 --> 00:13:48.560
计算机-有人类知识的计算机，对吗？

00:13:48.560 --> 00:13:52.430
因为人类的知识给予他们智慧。

00:13:52.430 --> 00:13:55.460
如果你想想我们

00:13:55.460 --> 00:13:59.270
在我们人类世界的各个地方传播知识，

00:13:59.270 --> 00:14:04.025
我们主要是通过人类语言来完成的。

00:14:04.025 --> 00:14:06.410
你知道，你可以从某种程度上

00:14:06.410 --> 00:14:09.260
做正确的体育锻炼，

00:14:09.260 --> 00:14:11.900
我可以拿着这个放下它，我学到了一些东西。

00:14:11.900 --> 00:14:13.760
所以我必须在那里学到一些知识。

00:14:13.760 --> 00:14:17.180
但你头脑中的大部分知识以及你为什么坐在那里

00:14:17.180 --> 00:14:21.980
这间教室来自用人类语言与你交流的人们。

00:14:21.980 --> 00:14:24.260
嗯，一个著名的，

00:14:24.260 --> 00:14:26.990
最著名的陡坡学子延乐村，

00:14:26.990 --> 00:14:29.165
他喜欢说这句话，

00:14:29.165 --> 00:14:33.380
哦，你知道吗？我想你知道没什么区别

00:14:33.380 --> 00:14:37.965
在人类的智慧和猩猩之间。

00:14:37.965 --> 00:14:40.510
我真的认为他在这方面是错的。

00:14:40.510 --> 00:14:42.790
就像他说的那样，

00:14:42.790 --> 00:14:45.835
猩猩有一个非常好的视觉系统。

00:14:45.835 --> 00:14:48.610
猩猩有很好的控制力

00:14:48.610 --> 00:14:52.060
他们的手臂就像人类一样拿起东西。

00:14:52.060 --> 00:14:58.970
猩猩可以使用工具，所以猩猩可以制定计划

00:14:58.970 --> 00:15:02.270
如果你把食物放在他们必须移动的地方

00:15:02.270 --> 00:15:05.960
带着食物去岛上的木板，他们可以做这样的计划。

00:15:05.960 --> 00:15:09.890
所以是的，从某种意义上说，他们很聪明，但是你知道，

00:15:09.890 --> 00:15:13.385
有点猩猩不像人类。

00:15:13.385 --> 00:15:16.100
为什么他们不喜欢人类？

00:15:16.100 --> 00:15:21.605
我想向你们建议，这就是人类所取得的成就的原因，

00:15:21.605 --> 00:15:25.070
我们不是只有一台电脑

00:15:25.070 --> 00:15:29.825
A你知道在你妈妈的车库里有一台尘土飞扬的旧IBM电脑。

00:15:29.825 --> 00:15:33.740
我们拥有的是一个人类计算机网络。

00:15:33.740 --> 00:15:37.520
我们实现人类计算机网络的方法是，

00:15:37.520 --> 00:15:41.285
我们使用人类语言作为我们的网络语言。

00:15:41.285 --> 00:15:44.690
嗯，所以，当你想起来的时候，

00:15:44.690 --> 00:15:51.815
所以在任何一种进化尺度上，语言都是超超超最近的，对吧？

00:15:51.815 --> 00:15:57.470
嗯，生物对不太了解的人有远见，但你知道，

00:15:57.470 --> 00:16:00.980
也许是7500万年或者更长，对吧？

00:16:00.980 --> 00:16:03.845
一段很长的时间。

00:16:03.845 --> 00:16:07.295
人类有语言多久了？

00:16:07.295 --> 00:16:09.860
你知道人们也不知道，因为事实证明你知道，

00:16:09.860 --> 00:16:11.015
当你有化石的时候，

00:16:11.015 --> 00:16:13.490
你不能一边敲脑袋一边说，

00:16:13.490 --> 00:16:15.050
你没有语言吗？

00:16:15.050 --> 00:16:19.100
嗯，但是你知道，大多数人估计那种语言是

00:16:19.100 --> 00:16:25.985
一项最新的发明，在现在的人类离开非洲之前。

00:16:25.985 --> 00:16:28.550
所以很多人认为我们只有语言

00:16:28.550 --> 00:16:31.460
比如说10万年或者类似的。

00:16:31.460 --> 00:16:35.450
所以这就是你所知道的进化时间尺度上的转瞬即逝。

00:16:35.450 --> 00:16:39.740
但你知道，这是语言的发展[听不见]

00:16:39.740 --> 00:16:43.970
那种让人看不见的东西——无敌的噪音，对吧？

00:16:43.970 --> 00:16:46.475
不是这样，人类，嗯，

00:16:46.475 --> 00:16:51.410
毒牙发达或奔跑能力发达

00:16:51.410 --> 00:16:53.660
比任何其他生物都快，或者

00:16:53.660 --> 00:16:56.210
在他们的头上放一个大喇叭或类似的东西，对吗？

00:16:56.210 --> 00:16:59.060
你知道，人类基本上很弱小，嗯，

00:16:59.060 --> 00:17:01.190
但他们有这个嗯，

00:17:01.190 --> 00:17:04.310
他们能与之沟通的无与伦比的优势

00:17:04.310 --> 00:17:07.880
彼此之间，因此在团队中更有效地工作。

00:17:07.880 --> 00:17:11.495
这基本上使人类无敌。

00:17:11.495 --> 00:17:15.575
但你知道，即使在那时人类也是有限的，对吧？

00:17:15.575 --> 00:17:18.140
这让你了解石器时代的情况，对吧？

00:17:18.140 --> 00:17:20.390
在那里你可以敲击你的石头

00:17:20.390 --> 00:17:23.240
合适的石头可以做成锋利的东西。

00:17:23.240 --> 00:17:25.685
嗯，是什么让人类超越了这一点，

00:17:25.685 --> 00:17:28.100
是他们发明了文字。

00:17:28.100 --> 00:17:32.915
所以写作是一种你可以接受知识的能力

00:17:32.915 --> 00:17:37.730
不仅和你看到的人进行了口耳相传。

00:17:37.730 --> 00:17:41.660
你可以把它放在你的纸莎草纸上，这样你的泥板或者其他什么东西就可以了。

00:17:41.660 --> 00:17:45.620
这是一开始，然后知识可以被发送到地方。

00:17:45.620 --> 00:17:50.270
它可以被空间发送到世界各地，然后它可以

00:17:50.270 --> 00:17:55.430
在时间上是暂时的。

00:17:55.430 --> 00:17:57.290
好吧，写作年龄有多大？

00:17:57.290 --> 00:18:00.890
我的意思是，我们基本上知道写作有多古老，对吧？

00:18:00.890 --> 00:18:04.115
那篇文章大约有5000年的历史。

00:18:04.115 --> 00:18:09.740
这在进化史上是难以置信的最近，但是你知道，

00:18:09.740 --> 00:18:16.730
从本质上说，写作是一种拥有知识的方式，在那5000人中

00:18:16.730 --> 00:18:24.035
让人类从石器时代的锋利碎片或燧石到你知道的年代，

00:18:24.035 --> 00:18:26.240
有了iPhone和所有这些东西，

00:18:26.240 --> 00:18:28.790
所有这些难以置信的复杂设备。

00:18:28.790 --> 00:18:32.960
所以，语言是非常特别的东西，我想建议。

00:18:32.960 --> 00:18:37.910
嗯，但是你知道，如果我回到我的类比中，那就允许人类

00:18:37.910 --> 00:18:43.280
构建一个网络化的计算机，其功能远比um强大，

00:18:43.280 --> 00:18:47.600
就像猩猩一样聪明的个体生物。

00:18:47.600 --> 00:18:50.525
嗯，你把它和我们的电脑网络比较一下，

00:18:50.525 --> 00:18:53.045
这是一种非常有趣的网络，对吧？

00:18:53.045 --> 00:18:55.745
你知道这些天，嗯，

00:18:55.745 --> 00:19:01.805
我们的网络运行在我们有大网络带宽的地方，对吗？

00:19:01.805 --> 00:19:03.770
你知道，我们有时可能会因为

00:19:03.770 --> 00:19:06.530
我们的Netflix下载，但大体上你知道，

00:19:06.530 --> 00:19:09.755
我们可以轻松快速地下载数百兆字节。

00:19:09.755 --> 00:19:11.570
我们认为速度不够快，

00:19:11.570 --> 00:19:13.670
所以我们将推出5G网络。

00:19:13.670 --> 00:19:16.400
所以它又快了一个数量级。

00:19:16.400 --> 00:19:18.800
我是说，相比之下，我是说，

00:19:18.800 --> 00:19:23.540
人类语言是一个慢得可怜的网络，对吧？

00:19:23.540 --> 00:19:29.465
你能用人类语言传达的信息量是非常缓慢的。

00:19:29.465 --> 00:19:33.950
我是说，不管是什么，我一秒钟说15个字，对吧，

00:19:33.950 --> 00:19:35.420
你可以开始做，嗯，

00:19:35.420 --> 00:19:37.550
你的信息理论，如果你知道一些正确的？

00:19:37.550 --> 00:19:41.060
但是，嗯，实际上你没有太多的带宽。

00:19:41.060 --> 00:19:44.405
然后你就可以想到，

00:19:44.405 --> 00:19:45.980
那它是怎么工作的？

00:19:45.980 --> 00:19:47.570
所以，人类想出了

00:19:47.570 --> 00:19:53.390
这个令人印象深刻的系统，本质上是压缩的形式。

00:19:53.390 --> 00:19:56.120
一种非常适应的压缩形式，

00:19:56.120 --> 00:19:58.070
所以当我们与人交谈时，

00:19:58.070 --> 00:20:02.870
我们假设他们头脑中有大量的知识，

00:20:02.870 --> 00:20:07.640
我和你说话的时候是不是和我的不一样，但大体上和我的相似？

00:20:07.640 --> 00:20:10.565
你知道英语单词的意思，

00:20:10.565 --> 00:20:13.850
你对世界是如何运作的很了解。

00:20:13.850 --> 00:20:17.149
因此，我可以说一个简短的信息和沟通

00:20:17.149 --> 00:20:22.820
只有一个相对较短的位字符串，您实际上可以理解很多。好吧？

00:20:22.820 --> 00:20:26.030
所以，我可以随便说，

00:20:26.030 --> 00:20:28.850
想象一下一个繁忙的购物中心

00:20:28.850 --> 00:20:31.630
有两个人站在化妆台前，

00:20:31.630 --> 00:20:36.290
你知道，我只说了大概200比特的

00:20:36.290 --> 00:20:38.960
但这使你能够

00:20:38.960 --> 00:20:42.340
一个完整的视觉场景，我们把兆字节带到嗯，

00:20:42.340 --> 00:20:44.385
表示为图像。

00:20:44.385 --> 00:20:46.625
所以，这就是为什么语言是好的。

00:20:46.625 --> 00:20:49.100
嗯，从更权威的层面来说，

00:20:49.100 --> 00:20:51.425
我现在要回到具体的东西上。

00:20:51.425 --> 00:20:55.925
我们在这节课上要做的不是解决整个语言，

00:20:55.925 --> 00:20:57.950
但我们要代表，嗯，

00:20:57.950 --> 00:21:00.380
单词的意思，对吗？

00:21:00.380 --> 00:21:03.230
所以，很多语言都是用词和它们的含义来联系的

00:21:03.230 --> 00:21:06.200
单词可以有很丰富的含义，对吗？

00:21:06.200 --> 00:21:07.970
只要你说一句话，老师，

00:21:07.970 --> 00:21:12.530
这是相当丰富的意义，或者你可以有丰富的意义的行动。

00:21:12.530 --> 00:21:17.225
所以，如果我说预言之类的话，

00:21:17.225 --> 00:21:19.070
嗯，总而言之，或者你知道的，

00:21:19.070 --> 00:21:22.385
这些词有着丰富的含义和许多细微的差别。

00:21:22.385 --> 00:21:24.395
所以我们要代表意义。

00:21:24.395 --> 00:21:26.510
所以，问题是什么意思？

00:21:26.510 --> 00:21:29.360
所以，你当然可以——字典是用来告诉你意义的。

00:21:29.360 --> 00:21:31.490
所以，你可以查字典，嗯，

00:21:31.490 --> 00:21:35.720
韦伯斯特说，有点试图把意义和想法联系起来。

00:21:35.720 --> 00:21:39.515
一个词或短语所代表的思想。

00:21:39.515 --> 00:21:44.240
一个人想用文字符号等来表达的想法。

00:21:44.240 --> 00:21:46.190
我是说，你知道，

00:21:46.190 --> 00:21:49.730
你可以认为这些定义是一种逃避，因为看起来

00:21:49.730 --> 00:21:53.015
就像他们在用“想法”这个词改写意思，

00:21:53.015 --> 00:21:55.040
这真的让你得到了什么。

00:21:55.040 --> 00:21:58.370
嗯，语言学家是如何看待意义的？

00:21:58.370 --> 00:22:03.110
我是说，语言学家最常想到的方法

00:22:03.110 --> 00:22:05.660
意思是一个被称为外延的概念

00:22:05.660 --> 00:22:08.420
在编程语言中也使用的语义。

00:22:08.420 --> 00:22:14.810
所以，我们认为意义就是事物所代表的。

00:22:14.810 --> 00:22:16.955
所以，如果我说“椅子”这个词，

00:22:16.955 --> 00:22:21.140
“椅子”一词的外延包括这里的这个和那个，

00:22:21.140 --> 00:22:22.325
那个，那个，那个。

00:22:22.325 --> 00:22:24.919
所以“椅子”这个词有点代表

00:22:24.919 --> 00:22:28.580
所有的东西都是椅子，你可以，嗯，

00:22:28.580 --> 00:22:33.410
然后你也可以考虑跑步之类的事情，因为你知道有一套

00:22:33.410 --> 00:22:37.985
人们可以参与的行动——这就是他们的象征。

00:22:37.985 --> 00:22:42.200
这就是你在哲学或语言学中最常看到的外延。

00:22:42.200 --> 00:22:47.135
这是一件很难掌握的事情，嗯，从计算上来说。

00:22:47.135 --> 00:22:50.480
那么，嗯，什么类型的人最常见

00:22:50.480 --> 00:22:54.020
做或使用最常用的是我想我现在应该说

00:22:54.020 --> 00:22:57.530
在计算机上计算单词的意思

00:22:57.530 --> 00:23:01.115
通常情况下，这会变成有点像字典的东西。

00:23:01.115 --> 00:23:06.200
特别喜欢的在线词汇是这个叫做wordnet的在线词典。

00:23:06.200 --> 00:23:11.510
有点告诉你单词的意思和单词意思之间的关系。

00:23:11.510 --> 00:23:16.445
嗯，这只是给你一种切碎的感觉，

00:23:16.445 --> 00:23:19.820
嗯，在WordNet里。

00:23:19.820 --> 00:23:24.485
嗯，这是上面的一段真正的python代码，

00:23:24.485 --> 00:23:28.370
嗯，在你的电脑里输入，然后自己运行并做这个。

00:23:28.370 --> 00:23:31.040
嗯，所以这使用了一个叫做NLTK的东西。

00:23:31.040 --> 00:23:33.725
嗯，所以NLTK有点像

00:23:33.725 --> 00:23:39.364
“瑞士军刀NLP”意味着它对任何东西都没有太大的好处，

00:23:39.364 --> 00:23:41.570
但是它有很多基本的工具。

00:23:41.570 --> 00:23:46.460
所以，如果你想做一些事情，比如从WordNet中取出一些东西并展示出来，

00:23:46.460 --> 00:23:49.625
这是最好的选择。嗯，好吧。

00:23:49.625 --> 00:23:54.830
所以，嗯，从NLTK我要导入WordNet，然后我可以说，

00:23:54.830 --> 00:24:01.355
“好吧，嗯，对于good这个词，告诉我good参与的同义词集。”

00:24:01.355 --> 00:24:03.440
有一个名词叫善。

00:24:03.440 --> 00:24:04.760
有一个形容词good。

00:24:04.760 --> 00:24:08.330
有一个值得尊敬的好的，可敬的，可敬的。

00:24:08.330 --> 00:24:11.150
嗯，这看起来很复杂，很难理解。

00:24:11.150 --> 00:24:13.700
但是单词网的概念使

00:24:13.700 --> 00:24:18.080
一个词的感觉之间的细微差别。

00:24:18.080 --> 00:24:20.675
那么，什么样的说法是好的，嗯，

00:24:20.675 --> 00:24:23.570
有些传感器是名词，对吧？

00:24:23.570 --> 00:24:24.755
你就是这样的，

00:24:24.755 --> 00:24:27.200
我买了一些旅行用品，对吧？

00:24:27.200 --> 00:24:28.880
所以，有点，嗯，

00:24:28.880 --> 00:24:32.780
我想是这类名词传感器中的一个。

00:24:32.780 --> 00:24:35.480
嗯，还有形容词传感器，它试图

00:24:35.480 --> 00:24:38.840
区分-有一个基本的形容词，好的感觉是好的。

00:24:38.840 --> 00:24:41.270
然后在某些情况下，嗯，传感器，

00:24:41.270 --> 00:24:44.750
在不同的方向上有这些扩展的良好传感器。

00:24:44.750 --> 00:24:48.515
所以，我想这是有益的，嗯，

00:24:48.515 --> 00:24:52.925
这是一种值得尊敬的人。

00:24:52.925 --> 00:24:55.580
他是个好人，或者类似的人，对吧？

00:24:55.580 --> 00:24:56.855
所以，嗯，但是你知道，

00:24:56.855 --> 00:24:59.660
是什么让我们

00:24:59.660 --> 00:25:02.630
思考很有问题，练习使用它是否试图使

00:25:02.630 --> 00:25:06.850
所有这些人类传感器之间的细微差别

00:25:06.850 --> 00:25:11.410
几乎不了解他们之间的区别，嗯，和。

00:25:11.410 --> 00:25:13.690
嗯，这样你就可以用WordNet做其他事情了。

00:25:13.690 --> 00:25:18.460
所以，这段代码你可以很好地向上走，它是一种层次结构。

00:25:18.460 --> 00:25:21.635
所以，它有点像传统的，嗯，数据库。

00:25:21.635 --> 00:25:29.030
所以，如果我从熊猫开始，然后说-[噪音]如果我从熊猫开始。

00:25:29.030 --> 00:25:32.180
嗯，走上去，嗯，

00:25:32.180 --> 00:25:35.330
熊猫[听不见]。

00:25:35.330 --> 00:25:37.640
也许你们会把食肉动物比作生物，

00:25:37.640 --> 00:25:39.545
胎盘，哺乳动物，等等。

00:25:39.545 --> 00:25:44.135
好吧，那么，嗯，这就是你能从WordNet中得到的东西。

00:25:44.135 --> 00:25:47.105
嗯，你知道，实际上，WordNet已经。

00:25:47.105 --> 00:25:49.580
每个人都习惯用它，因为它

00:25:49.580 --> 00:25:51.995
你对这个词的意思有某种理解。

00:25:51.995 --> 00:25:54.125
但你也知道这是众所周知的。

00:25:54.125 --> 00:25:56.540
它从来没有那么好地工作过。

00:25:56.540 --> 00:26:02.720
嗯，所以你知道这类同义词集遗漏了很多细微差别。

00:26:02.720 --> 00:26:05.270
所以，你知道一个好的同义词集有

00:26:05.270 --> 00:26:08.240
精通IT，有点像精通

00:26:08.240 --> 00:26:11.495
但“精通”并没有更多的内涵和细微差别吗？

00:26:11.495 --> 00:26:13.250
我想是的。

00:26:13.250 --> 00:26:18.080
嗯，WordNet和大多数手工构建的资源一样，有点不完整。

00:26:18.080 --> 00:26:21.290
所以，一旦你开始了解单词的新含义，

00:26:21.290 --> 00:26:23.705
或是新词和俚语，

00:26:23.705 --> 00:26:25.310
好吧，那就什么也不给你了。

00:26:25.310 --> 00:26:28.985
嗯，它是用人工建造的，

00:26:28.985 --> 00:26:35.030
嗯，以你知道的方式，很难去创造和适应。

00:26:35.030 --> 00:26:37.670
特别是，我们要关注的是，

00:26:37.670 --> 00:26:41.870
似乎是你想用词做的一件基本的事情，事实上至少

00:26:41.870 --> 00:26:45.920
理解单词含义之间的相似性和关系。

00:26:45.920 --> 00:26:49.520
事实证明，你知道WordNet做得并不好

00:26:49.520 --> 00:26:53.600
因为它只有这些固定的离散同义词集。

00:26:53.600 --> 00:26:56.090
所以，如果你在同义词中有一个词说

00:26:56.090 --> 00:26:59.075
有点像同义词，可能含义不完全相同，

00:26:59.075 --> 00:27:00.800
它们不是同一组同义词，

00:27:00.800 --> 00:27:04.580
你不能真正地测量部分相似性作为他们的意义。

00:27:04.580 --> 00:27:08.435
所以，如果好的和神奇的东西不是在同一个同义词集中，

00:27:08.435 --> 00:27:11.960
但他们有一些共同点，你想代表。

00:27:11.960 --> 00:27:16.880
可以。所以，嗯，这有点让人觉得

00:27:16.880 --> 00:27:21.935
我们想做一些不同的更好的词义。

00:27:21.935 --> 00:27:25.730
而且，嗯，在到那里之前，我只是想再整理一下

00:27:25.730 --> 00:27:29.495
从传统的NLP构建一点。

00:27:29.495 --> 00:27:33.275
所以，在这门课的背景下，传统的NLP方法

00:27:33.275 --> 00:27:39.275
直到2012年左右的自然语言处理。

00:27:39.275 --> 00:27:43.640
有一些早期的先例，但基本上，嗯，

00:27:43.640 --> 00:27:47.600
2013年，情况真的开始改变

00:27:47.600 --> 00:27:53.060
人们开始使用神经网络风格的表达来处理自然语言。

00:27:53.060 --> 00:27:55.430
所以，到2012年，

00:27:55.430 --> 00:27:58.055
嗯，标准来说，你知道我们有话要说。

00:27:58.055 --> 00:28:02.210
它们只是文字而已。所以，我们开了酒店会议旅馆。

00:28:02.210 --> 00:28:06.650
它们是单词，我们会让你知道词汇，并把单词放到我们的模型中。

00:28:06.650 --> 00:28:12.290
在神经网络领域，这被称为局域表示。

00:28:12.290 --> 00:28:14.960
下次我再回到这些条款上来。

00:28:14.960 --> 00:28:20.015
但这就意味着，对于任何概念，都有一种特殊的，

00:28:20.015 --> 00:28:24.080
嗯，旅馆这个词还是汽车旅馆这个词。

00:28:24.080 --> 00:28:26.465
一种思考的方式是思考

00:28:26.465 --> 00:28:29.615
关于构建机器学习模型时会发生什么。

00:28:29.615 --> 00:28:34.759
所以，如果你有一个分类变量，就像你有选择单词的单词一样

00:28:34.759 --> 00:28:40.130
你想把它放到机器学习模型中的分类器里，

00:28:40.130 --> 00:28:42.905
不知何故，你必须对分类变量进行编码，

00:28:42.905 --> 00:28:46.550
标准的方法是通过

00:28:46.550 --> 00:28:51.275
变量的不同级别，这意味着你有一个向量，

00:28:51.275 --> 00:28:53.840
你知道，这就是“房子”这个词。

00:28:53.840 --> 00:28:55.670
这是“猫”这个词。这是“狗”这个词。

00:28:55.670 --> 00:28:57.020
这是“一些椅子”这个词。

00:28:57.020 --> 00:28:58.190
这是一个令人愉快的词。

00:28:58.190 --> 00:28:59.465
这是另外一个词。

00:28:59.465 --> 00:29:01.415
就是这个词，嗯，

00:29:01.415 --> 00:29:05.750
旅馆，嗯，这是另一个不同的词，对吧？

00:29:05.750 --> 00:29:08.075
所以你把一个放在这个位置上

00:29:08.075 --> 00:29:11.120
神经网络陆地，我们称之为热向量，

00:29:11.120 --> 00:29:12.470
所以这些可能是，啊，

00:29:12.470 --> 00:29:16.250
酒店和汽车旅馆的热门话题。

00:29:16.250 --> 00:29:19.040
所以，这里有一些不好的地方。

00:29:19.040 --> 00:29:21.005
嗯，就是那种，啊，

00:29:21.005 --> 00:29:27.140
实际的麻烦是你知道语言有很多单词。

00:29:27.140 --> 00:29:30.590
啊，所以，这是你可能还记得的那种字典

00:29:30.590 --> 00:29:35.450
学校里大概有25万个单词。

00:29:35.450 --> 00:29:37.400
但是你知道，如果你开始

00:29:37.400 --> 00:29:41.855
更多的技术和科学英语很容易达到一百万个单词。

00:29:41.855 --> 00:29:45.690
我的意思是，实际上你在一种语言中有多少个单词，嗯，

00:29:45.690 --> 00:29:48.620
就像英语实际上是无限的，因为我们

00:29:48.620 --> 00:29:52.220
这些过程被称为衍生形态学，

00:29:52.220 --> 00:29:56.930
嗯，在这里你可以通过在现有单词上添加词尾来生成更多的单词。

00:29:56.930 --> 00:29:59.660
所以，你知道你可以从家长主义开始，

00:29:59.660 --> 00:30:03.470
父爱，然后你可以从母亲那里说，

00:30:03.470 --> 00:30:06.275
你可以说家长主义，或者家长主义，

00:30:06.275 --> 00:30:10.070
家长主义和爸爸-我是家长主义的。

00:30:10.070 --> 00:30:14.255
正确的？现在，所有这些方法，你可以通过添加更多的东西来烘烤更大的单词。

00:30:14.255 --> 00:30:18.905
嗯，所以你最终会得到一个无限的单词空间。

00:30:18.905 --> 00:30:22.880
嗯，是的。这是个小问题，对吧？

00:30:22.880 --> 00:30:28.275
如果我们想表示一个合理大小的词汇表，我们有很大的向量。

00:30:28.275 --> 00:30:31.990
嗯，但还有一个比这更大的问题，那就是，

00:30:31.990 --> 00:30:35.200
我们一直想做的就是，

00:30:35.200 --> 00:30:38.590
有点，理解关系和单词的含义。

00:30:38.590 --> 00:30:42.380
所以，你知道，一个明显的例子就是网络搜索。

00:30:42.380 --> 00:30:45.350
所以，如果我搜索西雅图汽车旅馆，

00:30:45.350 --> 00:30:48.710
如果它也显示了

00:30:48.710 --> 00:30:52.655
西雅图酒店在页面上，反之亦然，因为，

00:30:52.655 --> 00:30:55.415
你知道，旅馆和汽车旅馆几乎是一样的。

00:30:55.415 --> 00:30:59.900
但是，你知道，如果我们有一个像以前一样的热向量

00:30:59.900 --> 00:31:04.250
他们之间没有S-相似关系，对吗？

00:31:04.250 --> 00:31:05.675
所以，用数学术语来说，

00:31:05.675 --> 00:31:07.775
这两个向量是正交的。

00:31:07.775 --> 00:31:10.865
他们之间没有相似关系。

00:31:10.865 --> 00:31:12.650
嗯，所以你，

00:31:12.650 --> 00:31:14.705
有点，什么也得不到。

00:31:14.705 --> 00:31:16.880
现在，你知道，有些事情你可以做，

00:31:16.880 --> 00:31:18.710
我-我刚给你看了WordNet的。

00:31:18.710 --> 00:31:20.840
WordNet为您显示了一些同义词和内容。

00:31:20.840 --> 00:31:22.610
所以这可能会有点帮助。

00:31:22.610 --> 00:31:24.035
还有其他事情你可以做。

00:31:24.035 --> 00:31:25.415
你可以说，等等，

00:31:25.415 --> 00:31:29.645
为什么不建一个大桌子，我们有一个大桌子，

00:31:29.645 --> 00:31:32.675
嗯，词的相似性，我们可以用它。

00:31:32.675 --> 00:31:34.910
而且，你知道，人们过去常常尝试这样做，对吗？

00:31:34.910 --> 00:31:39.770
你知道，这就是谷歌2005年的做法。

00:31:39.770 --> 00:31:42.080
你知道，它有单词相似度表。

00:31:42.080 --> 00:31:44.510
这样做的问题是，

00:31:44.510 --> 00:31:48.290
我们在谈论我们想要50万个词。

00:31:48.290 --> 00:31:52.040
如果你想建立一个单词相似度表

00:31:52.040 --> 00:31:56.060
从一个热门的表达中我们的成对单词，

00:31:56.060 --> 00:31:58.640
嗯，你-这意味着那张桌子的大小，

00:31:58.640 --> 00:32:00.380
因为我的数学很差，

00:32:00.380 --> 00:32:02.315
是2.5万亿吗？

00:32:02.315 --> 00:32:07.130
在你的相似度矩阵中有很多细胞。

00:32:07.130 --> 00:32:09.230
所以这几乎是不可能的。

00:32:09.230 --> 00:32:13.715
所以，我们要做的是探索一种方法，

00:32:13.715 --> 00:32:16.670
我们将把单词表示为向量，

00:32:16.670 --> 00:32:18.140
在某种程度上，我会告诉你，嗯，

00:32:18.140 --> 00:32:21.770
以这样的方式表示的一分钟

00:32:21.770 --> 00:32:26.480
一言以蔽之，相似点就没有了。

00:32:26.480 --> 00:32:30.635
可以。所以这会引出这些不同的想法。

00:32:30.635 --> 00:32:34.175
所以，我之前提到了表示语义。

00:32:34.175 --> 00:32:39.115
这里还有一个表达单词意思的方法，

00:32:39.115 --> 00:32:41.980
这就是所谓的分布语义。

00:32:41.980 --> 00:32:45.140
所以分布语义的概念是，

00:32:45.140 --> 00:32:50.900
我们要如何通过观察上下文来表示一个词的意思，

00:32:50.900 --> 00:32:52.925
嗯，在里面。

00:32:52.925 --> 00:32:56.510
这是一张英国语言学家小弗斯的照片。

00:32:56.510 --> 00:32:58.400
嗯，他以这句话出名，

00:32:58.400 --> 00:33:01.535
“你应该知道它所保存的公司的一个词。”

00:33:01.535 --> 00:33:06.950
嗯，但另一个以发展这种意义概念而出名的人是，嗯，

00:33:06.950 --> 00:33:10.670
哲学家路德维希-路德维希·维特根斯坦在他后来的著作中，

00:33:10.670 --> 00:33:13.445
他称之为“会议意义的使用理论”。

00:33:13.445 --> 00:33:16.070
嗯，事实上，他用了一个我不知道的德语大字，

00:33:16.070 --> 00:33:18.530
但是，我们称之为意义使用理论。

00:33:18.530 --> 00:33:22.535
你知道，关键是，嗯，你知道，

00:33:22.535 --> 00:33:26.780
如果你能解释的话

00:33:26.780 --> 00:33:31.160
解释使用某个词的正确语境，

00:33:31.160 --> 00:33:34.595
在什么语境下使用错误的词，

00:33:34.595 --> 00:33:38.135
这可能会让你回忆起高中时的英语，

00:33:38.135 --> 00:33:40.490
当人们说，“啊，这是个错误的词”时，

00:33:40.490 --> 00:33:43.205
嗯，那你就明白这个词的意思了，对吧？

00:33:43.205 --> 00:33:47.045
嗯，这就是分布语义学的概念。

00:33:47.045 --> 00:33:49.790
它一直是-所以最成功的想法之一

00:33:49.790 --> 00:33:54.005
现代统计学的NLP，因为它给你一个很好的方法来学习单词的意思。

00:33:54.005 --> 00:33:56.615
所以我们要做的是说，

00:33:56.615 --> 00:33:58.925
哈哈，我想知道银行这个词是什么意思。

00:33:58.925 --> 00:34:01.730
所以，我会抓取很多短信，

00:34:01.730 --> 00:34:04.520
当我们拥有万维网时，这很容易做到，

00:34:04.520 --> 00:34:07.955
我会找到很多使用“银行”这个词的句子，

00:34:07.955 --> 00:34:12.770
2009年发生的政府债务问题演变成银行危机。

00:34:12.770 --> 00:34:15.845
这两个-我只想说

00:34:15.845 --> 00:34:19.115
这是“银行”这个词的意思。

00:34:19.115 --> 00:34:23.750
嗯，这些就是使用“银行”这个词的上下文。

00:34:23.750 --> 00:34:29.495
这看起来很简单，甚至可能不是很正确的想法，

00:34:29.495 --> 00:34:34.880
但事实证明，这是一个非常有用的想法，在捕捉意义方面做得很好。

00:34:34.880 --> 00:34:38.300
所以我们要做的是说而不是说

00:34:38.300 --> 00:34:42.950
我们以前的地方代表我们现在要

00:34:42.950 --> 00:34:48.215
用我们称之为分布式表示的方式表示单词。

00:34:48.215 --> 00:34:51.830
因此，对于分布式表示，我们仍将继续

00:34:51.830 --> 00:34:55.655
to[噪声]表示一个词作为数字向量的意义。

00:34:55.655 --> 00:34:59.480
但现在我们要说的是每个词的意思是，

00:34:59.480 --> 00:35:01.520
小矢量，嗯，

00:35:01.520 --> 00:35:07.760
但它将是一个稠密的向量，所有的数字都是非零的。

00:35:07.760 --> 00:35:10.010
所以银行业的意义将是

00:35:10.010 --> 00:35:13.340
分布在这个向量的维上。

00:35:13.340 --> 00:35:19.190
现在，这里的向量是第九维度的，因为我想保持滑动，嗯，很好。

00:35:19.190 --> 00:35:23.195
嗯，生活在实践中不是很好。

00:35:23.195 --> 00:35:25.970
当我们这样做的时候，我们使用一个更大的维度，

00:35:25.970 --> 00:35:29.075
有点，固体，人们使用的最低限度是50。

00:35:29.075 --> 00:35:32.330
嗯，你在笔记本电脑上使用的一个典型数字是

00:35:32.330 --> 00:35:35.945
300如果你真的想最大限度地发挥性能，

00:35:35.945 --> 00:35:38.885
嗯，可能是1000，2000，4000。

00:35:38.885 --> 00:35:42.020
但是，你知道，尽管如此，（噪音）数量级是

00:35:42.020 --> 00:35:46.320
比长度为500000的向量小。

00:35:46.810 --> 00:35:51.890
可以。所以我们有了带有向量表示的单词。

00:35:51.890 --> 00:35:55.790
因为每个词都有一个向量，嗯，

00:35:55.790 --> 00:36:01.160
然后我们就有了一个向量空间，在这个空间中我们可以放置所有的单词。

00:36:01.160 --> 00:36:03.980
嗯，那是完全不可读的，嗯，

00:36:03.980 --> 00:36:08.135
但是如果你放大到向量空间，它仍然是完全不可读的。

00:36:08.135 --> 00:36:10.115
但是如果你再放大一点，

00:36:10.115 --> 00:36:13.100
嗯，你可以找到这个空间的不同部分。

00:36:13.100 --> 00:36:16.820
这就是各国参与的部分，

00:36:16.820 --> 00:36:18.950
嗯，有日语，德语，

00:36:18.950 --> 00:36:21.950
法语、俄语、英属澳大利亚裔美国人，

00:36:21.950 --> 00:36:25.130
嗯，法国，英国，德国等等。

00:36:25.130 --> 00:36:27.770
你可以转移到空间的另一部分。

00:36:27.770 --> 00:36:31.040
这是空间的一部分，这里有各种动词，

00:36:31.040 --> 00:36:33.485
过去，过去，过去，现在也是。

00:36:33.485 --> 00:36:40.880
哎呀。嗯，嗯，[听不见]总是在那里。

00:36:40.880 --> 00:36:43.970
你甚至可以看到一些形态在一起，

00:36:43.970 --> 00:36:46.100
像是说，

00:36:46.100 --> 00:36:48.770
想一想，期待那些能接受的东西，有点，恭维。

00:36:48.770 --> 00:36:50.795
他说或想了些什么。

00:36:50.795 --> 00:36:52.415
嗯，他们在一起。

00:36:52.415 --> 00:36:55.010
现在，我到底在给你看什么？

00:36:55.010 --> 00:36:57.755
嗯，你知道，这真的是从

00:36:57.755 --> 00:37:00.575
啊，100维字向量。

00:37:00.575 --> 00:37:05.630
而且这个问题很难想象100维的词向量。

00:37:05.630 --> 00:37:09.860
所以，这里实际发生的是这些，嗯，

00:37:09.860 --> 00:37:15.110
100维字向量正向下投射到两维中，

00:37:15.110 --> 00:37:17.990
你看到的是二维视图，

00:37:17.990 --> 00:37:19.790
我稍后再谈。

00:37:19.790 --> 00:37:22.400
嗯，一方面，嗯，

00:37:22.400 --> 00:37:24.410
每当你看到这些照片时，你应该紧紧抓住

00:37:24.410 --> 00:37:26.840
你的钱包因为里面有很多

00:37:26.840 --> 00:37:31.535
关于原始向量空间的细节被完全杀死然后消失了，嗯，

00:37:31.535 --> 00:37:32.839
在二维投影中，

00:37:32.839 --> 00:37:37.070
事实上，有些东西在二维空间中推动了事物的发展，

00:37:37.070 --> 00:37:39.875
嗯，投影可能真的，真的，

00:37:39.875 --> 00:37:42.590
真的歪曲了原始空间中的内容。

00:37:42.590 --> 00:37:45.740
嗯，但即使看这些二维表示，

00:37:45.740 --> 00:37:46.850
总体感觉是，

00:37:46.850 --> 00:37:48.920
天哪，这真是一种工作，不是吗？

00:37:48.920 --> 00:37:54.365
嗯，我们可以看出单词之间的相似之处。

00:37:54.365 --> 00:38:02.375
可以。所以，嗯，哈-那就是我们想做什么的想法。

00:38:02.375 --> 00:38:04.310
下一部分，嗯，

00:38:04.310 --> 00:38:07.940
那我们该怎么做呢？

00:38:07.940 --> 00:38:10.445
我会停下来呼吸半分钟。

00:38:10.445 --> 00:38:12.710
有人想问一个问题吗？

00:38:12.710 --> 00:38:20.300
[噪音]是的。

00:38:20.300 --> 00:38:26.720
向量在哪里，嗯，

00:38:26.720 --> 00:38:28.460
每个联系人的顺序不同，

00:38:28.460 --> 00:38:30.530
比如说，第一个十进制向量，

00:38:30.530 --> 00:38:32.840
第二个十进制向量，是标准向量吗？

00:38:32.840 --> 00:38:35.475
在所有的理论中，还是人们自己选择它们？

00:38:35.475 --> 00:38:42.340
嗯，它们不是整个NLP的标准，而且根本没有被选中。

00:38:42.340 --> 00:38:45.055
所以我们要展示的是一个学习算法。

00:38:45.055 --> 00:38:48.430
所以我们只是在大量文本中乱序排列

00:38:48.430 --> 00:38:51.970
奇迹般地，这些文字载体出现了。

00:38:51.970 --> 00:38:57.760
因此L-学习算法本身决定了维度。

00:38:57.760 --> 00:39:03.085
但这实际上让我想起了我想说的话，是的，

00:39:03.085 --> 00:39:05.425
因为这是一个向量空间，

00:39:05.425 --> 00:39:09.580
在某种意义上，任意权利的维度，

00:39:09.580 --> 00:39:12.570
因为你可以知道你的基向量

00:39:12.570 --> 00:39:15.945
任何不同的方向，你都可以重新表现，

00:39:15.945 --> 00:39:19.715
嗯，向量空间中的单词有不同的基础，

00:39:19.715 --> 00:39:22.930
基向量和它是完全相同的向量空间

00:39:22.930 --> 00:39:26.380
只是旋转到新的，向量。

00:39:26.380 --> 00:39:30.580
所以，你知道，你不应该对这些元素读太多。

00:39:30.580 --> 00:39:32.860
事实证明，这是因为

00:39:32.860 --> 00:39:36.070
深入学习UM操作工作，

00:39:36.070 --> 00:39:38.170
他们做的一些事情，都是因地制宜的。

00:39:38.170 --> 00:39:42.775
因此，维度确实趋向于对它们有某种意义，事实证明。

00:39:42.775 --> 00:39:46.900
不过，虽然我想说的是，

00:39:46.900 --> 00:39:52.240
你知道我们能想到的一件事就是事情有多接近

00:39:52.240 --> 00:39:54.250
在向量空间中，这是

00:39:54.250 --> 00:39:57.805
我们将要利用的意义相似的概念。

00:39:57.805 --> 00:40:00.640
但你可能希望你得到更多，

00:40:00.640 --> 00:40:03.010
你可能真的认为

00:40:03.010 --> 00:40:06.925
矢量空间中的不同维度和方向。

00:40:06.925 --> 00:40:11.335
答案是肯定的，我稍后再谈。

00:40:11.335 --> 00:40:17.770
可以。嗯，从某种意义上说这件事

00:40:17.770 --> 00:40:22.240
最大的影响，嗯，在某种程度上改变了世界

00:40:22.240 --> 00:40:27.625
神经网络方向的NLP就是这张图。

00:40:27.625 --> 00:40:32.260
嗯，这个算法是不是

00:40:32.260 --> 00:40:37.345
Thomas Mikolov于2013年提出了一种称为Word2vec算法的算法。

00:40:37.345 --> 00:40:43.210
所以这不是第一部作品，也不是分布式的文字表达。

00:40:43.210 --> 00:40:45.730
所以Yoshua Bengio的老作品

00:40:45.730 --> 00:40:48.370
回到千年之交，

00:40:48.370 --> 00:40:52.780
不知怎的，它不是真的击中了他们的头上的世界

00:40:52.780 --> 00:40:57.730
一个巨大的影响和托马斯·米科洛夫的表现非常简单，

00:40:57.730 --> 00:41:00.070
非常可扩展的学习方式

00:41:00.070 --> 00:41:05.005
用矢量表示的单词和这类单词真的打开了泄洪闸。

00:41:05.005 --> 00:41:08.650
这就是我现在要展示的算法。

00:41:08.650 --> 00:41:15.775
可以。所以这个算法的思想是从一大堆文本开始。

00:41:15.775 --> 00:41:20.650
嗯，所以无论你在什么地方发现报纸上的文章或什么东西，

00:41:20.650 --> 00:41:22.480
很多连续的文本，对吗？

00:41:22.480 --> 00:41:26.350
实际的句子，因为我们想学习两个词的意思上下文。

00:41:26.350 --> 00:41:32.470
嗯，NLP把一大堆文本称为语料库。

00:41:32.470 --> 00:41:35.890
我的意思是这只是身体的拉丁词，对吧？

00:41:35.890 --> 00:41:37.915
这是一个正文。

00:41:37.915 --> 00:41:43.224
重要的是，如果你想看起来受过真正的教育，就要注意拉丁语，

00:41:43.224 --> 00:41:46.690
这是第四个下倾名词。

00:41:46.690 --> 00:41:49.900
所以语料库的复数形式是语料库。

00:41:49.900 --> 00:41:51.190
如果你说

00:41:51.190 --> 00:41:55.390
核心派每个人都会知道你高中时没有学过拉丁语。

00:41:55.390 --> 00:42:00.490
[笑声]嗯，好的。

00:42:00.490 --> 00:42:06.460
嗯，对了-所以我们想说每一个词

00:42:06.460 --> 00:42:08.890
用固定的词汇

00:42:08.890 --> 00:42:12.445
语料库的词汇由矢量表示。

00:42:12.445 --> 00:42:16.615
我们把这些向量作为随机向量。

00:42:16.615 --> 00:42:18.340
所以我们要做的就是

00:42:18.340 --> 00:42:22.585
这是一个大的迭代算法，我们将遍历文本中的每个位置。

00:42:22.585 --> 00:42:24.715
我们说，课文里有一个词。

00:42:24.715 --> 00:42:30.520
让我们看看周围的文字，我们要做的就是说好，

00:42:30.520 --> 00:42:32.890
一个词的意思就是它的使用环境。

00:42:32.890 --> 00:42:35.290
所以我们想要这个词的表现形式

00:42:35.290 --> 00:42:37.870
在中间能够预测

00:42:37.870 --> 00:42:43.720
围绕着它，我们将通过移动矢量这个词的位置来达到这个目的。

00:42:43.720 --> 00:42:47.500
我们只是重复了十亿次

00:42:47.500 --> 00:42:51.190
不知怎的，奇迹发生了，结果是我们

00:42:51.190 --> 00:42:54.790
一个字向量空间，看起来像我展示的图片

00:42:54.790 --> 00:42:59.530
好的词义符合好的词义表达。

00:42:59.530 --> 00:43:03.090
所以稍微多一点，嗯，

00:43:03.090 --> 00:43:07.240
嗯，从图形上看，稍微有点。

00:43:07.240 --> 00:43:08.440
情况就是这样。

00:43:08.440 --> 00:43:12.835
所以我们的部分语料库问题变成了银行危机，

00:43:12.835 --> 00:43:14.290
所以我们想说的是，

00:43:14.290 --> 00:43:17.725
我们想知道单词into的意思，所以我们希望

00:43:17.725 --> 00:43:21.400
它的表示可以用一种

00:43:21.400 --> 00:43:24.820
精确预测单词的出现

00:43:24.820 --> 00:43:28.600
into的上下文，因为这就是into的含义。

00:43:28.600 --> 00:43:31.525
所以我们要尝试做出这些预测，

00:43:31.525 --> 00:43:34.855
看看我们能预测多少然后改变

00:43:34.855 --> 00:43:39.475
用一种我们可以做得更好的预测的方式来表示词的矢量。

00:43:39.475 --> 00:43:41.320
一旦我们处理好，

00:43:41.320 --> 00:43:43.765
接下来我们说，

00:43:43.765 --> 00:43:46.060
好吧，让我们以银行业为例。

00:43:46.060 --> 00:43:49.795
银行业的意义在于预测银行业发生的环境。

00:43:49.795 --> 00:43:51.265
这里有一个背景。

00:43:51.265 --> 00:43:54.550
让我们试着预测一下银行业和

00:43:54.550 --> 00:43:58.735
看看我们是怎么做的，然后我们会从那里继续前进。

00:43:58.735 --> 00:44:02.470
可以。嗯，听起来很简单。

00:44:02.470 --> 00:44:06.100
嗯，[噪音]现在我们继续做更多的事情。

00:44:06.100 --> 00:44:12.460
可以。所以总的来说，我们有一个很大的大写T字语料库。

00:44:12.460 --> 00:44:17.125
所以如果我们有很多文档，我们只是将它们连接在一起，然后说，

00:44:17.125 --> 00:44:19.014
好吧，这是10亿字，

00:44:19.014 --> 00:44:21.745
一大堆单词。

00:44:21.745 --> 00:44:23.305
所以我们要做的是，

00:44:23.305 --> 00:44:26.875
是我们要做的第一个嗯产品

00:44:26.875 --> 00:44:30.954
通读所有单词，然后再看第二个产品，

00:44:30.954 --> 00:44:34.630
我们会说-我们会选择固定大小的窗户，你知道，

00:44:34.630 --> 00:44:37.990
可能每边都有五个字，或者什么的，我们要试着

00:44:37.990 --> 00:44:42.010
预测围绕中心词的10个词。

00:44:42.010 --> 00:44:44.200
我们要预测的是

00:44:44.200 --> 00:44:46.780
根据中心词预测那个词。

00:44:46.780 --> 00:44:48.460
这就是我们的概率模型。

00:44:48.460 --> 00:44:51.175
所以如果我们把所有的东西都乘起来，

00:44:51.175 --> 00:44:54.610
这就是我们的模式，很可能是一份好工作。

00:44:54.610 --> 00:44:58.375
预测每一个词周围的词。

00:44:58.375 --> 00:45:01.600
这个模型的可能性将取决于

00:45:01.600 --> 00:45:05.185
关于我们模型的参数，我们写为θ。

00:45:05.185 --> 00:45:07.855
在这个特殊的模型中，

00:45:07.855 --> 00:45:10.690
其中唯一的参数实际上是

00:45:10.690 --> 00:45:13.810
我们给出的是向量表示。

00:45:13.810 --> 00:45:16.945
这个模型绝对没有其他参数。

00:45:16.945 --> 00:45:20.050
所以，我们只是说我们代表

00:45:20.050 --> 00:45:23.695
在向量空间中有向量的单词，以及

00:45:23.695 --> 00:45:27.880
它的代表性就是它的意义，然后我们将能够

00:45:27.880 --> 00:45:32.335
用它来预测我将要向你展示的其他单词的发生方式。

00:45:32.335 --> 00:45:37.240
可以。所以，嗯，这是我们的可能性，所以我们在所有

00:45:37.240 --> 00:45:42.280
这些模型是我们定义一个目标函数，然后我们将

00:45:42.280 --> 00:45:45.880
我想提出在

00:45:45.880 --> 00:45:50.740
使我们的目标函数最小化的方法。

00:45:50.740 --> 00:45:56.380
所以目标函数基本上和幻灯片上半部分的一样，

00:45:56.380 --> 00:45:58.045
但我们改变了一些事情。

00:45:58.045 --> 00:46:03.040
我们在它前面加一个负号，这样我们可以做最小化，而不是最大化。

00:46:03.040 --> 00:46:05.515
完全武断是没有区别的。

00:46:05.515 --> 00:46:08.125
嗯，我们把一个T放在它前面，

00:46:08.125 --> 00:46:11.800
所以我们计算出平均值

00:46:11.800 --> 00:46:16.150
对每个中心词的选择都有预测的好处。

00:46:16.150 --> 00:46:19.360
再说一次，这没什么区别，但它有点像

00:46:19.360 --> 00:46:23.095
事情啊，不取决于语料库的大小。

00:46:23.095 --> 00:46:27.235
嗯，最重要的是我们在

00:46:27.235 --> 00:46:31.690
上面的函数，因为一切都会变得很好。

00:46:31.690 --> 00:46:33.805
所以当你贴上原木找到产品的时候

00:46:33.805 --> 00:46:36.370
当你做优化之类的事情时。

00:46:36.370 --> 00:46:38.860
所以，当我们这样做的时候，我们得到了

00:46:38.860 --> 00:46:42.430
所有这些产品都能让我们改变你所知道的，

00:46:42.430 --> 00:46:46.300
这个概率的对数之和

00:46:46.300 --> 00:46:50.755
我们会在一分钟内再做一遍。

00:46:50.755 --> 00:46:55.420
可以。如果我们能改变的话

00:46:55.420 --> 00:47:00.865
我们用向量表示这些词，以使θ的j最小化，

00:47:00.865 --> 00:47:06.110
这意味着我们将擅长在另一个词的上下文中预测单词。

00:47:06.540 --> 00:47:10.450
那么，这一切听起来都不错，但都是

00:47:10.450 --> 00:47:13.960
依赖于你想要的概率函数

00:47:13.960 --> 00:47:17.020
预测一个词在

00:47:17.020 --> 00:47:20.635
中心词的上下文问题是，

00:47:20.635 --> 00:47:23.620
你怎么可能做到？

00:47:23.620 --> 00:47:28.390
嗯，嗯，记住我说的是我们的模特

00:47:28.390 --> 00:47:33.655
有单词的向量表示，这是模型的唯一参数。

00:47:33.655 --> 00:47:35.650
现在，这几乎是真的。

00:47:35.650 --> 00:47:37.105
这不完全正确。

00:47:37.105 --> 00:47:39.220
嗯，实际上我们有点作弊。

00:47:39.220 --> 00:47:42.400
因为我们实际上提出了

00:47:42.400 --> 00:47:46.600
每一个词，这使得做这件事更简单。

00:47:46.600 --> 00:47:48.070
嗯，你不能这样做，

00:47:48.070 --> 00:47:50.620
有很多方法可以绕过它，但这是最简单的方法。

00:47:50.620 --> 00:47:54.610
所以我们有一个向量，当它是预测的中心词时

00:47:54.610 --> 00:47:59.500
换言之，但当每个词是上下文词时，我们有第二个向量，

00:47:59.500 --> 00:48:01.225
所以这是上下文中的一个词。

00:48:01.225 --> 00:48:02.680
所以对于每种单词类型，

00:48:02.680 --> 00:48:06.850
我们把这两个向量作为中心词，作为上下文词。

00:48:06.850 --> 00:48:12.700
嗯，那么我们来计算一个词在上下文中的概率，

00:48:12.700 --> 00:48:14.574
考虑到中心词，

00:48:14.574 --> 00:48:22.105
纯粹根据这些向量，我们这样做，就是用这个方程，

00:48:22.105 --> 00:48:25.165
我一会儿再解释。

00:48:25.165 --> 00:48:29.650
所以我们仍然处于完全相同的情况，对吗？

00:48:29.650 --> 00:48:32.050
我们想算出

00:48:32.050 --> 00:48:35.665
出现在中心词上下文中的词。

00:48:35.665 --> 00:48:38.785
所以中心词是c，上下文词用

00:48:38.785 --> 00:48:42.370
这些[听不见]的幻灯片符号，但有点，

00:48:42.370 --> 00:48:44.890
我们基本上是说有一种

00:48:44.890 --> 00:48:47.590
中心词的矢量是另一种矢量

00:48:47.590 --> 00:48:53.665
对于上下文词，我们要计算出概率预测，嗯，

00:48:53.665 --> 00:48:56.470
就这些词向量而言。

00:48:56.470 --> 00:48:59.260
可以。那我们该怎么做呢？

00:48:59.260 --> 00:49:02.950
嗯，我们这样做是为了这个嗯，

00:49:02.950 --> 00:49:07.870
这里的公式是你一次又一次看到的形状，嗯，

00:49:07.870 --> 00:49:10.300
在深入学习与分类工作人员。

00:49:10.300 --> 00:49:12.670
所以对于它的中心部分，

00:49:12.670 --> 00:49:17.815
橙色的比特更多的是相同的事情发生在，嗯，分母。

00:49:17.815 --> 00:49:21.130
我们要做的是计算一个点积。

00:49:21.130 --> 00:49:24.460
所以，我们要通过向量的分量，我们要

00:49:24.460 --> 00:49:28.750
把它们相乘，这意味着如果嗯，

00:49:28.750 --> 00:49:32.800
不同的词有相同符号的B成分，

00:49:32.800 --> 00:49:35.620
加或减，在相同的位置，

00:49:35.620 --> 00:49:38.920
DOT产品会很大，如果

00:49:38.920 --> 00:49:42.460
他们有不同的标志，或者一个大一个小，

00:49:42.460 --> 00:49:44.410
DOT产品会小很多。

00:49:44.410 --> 00:49:48.100
所以橙色部分直接计算，

00:49:48.100 --> 00:49:51.670
有点像单词之间的相似之处

00:49:51.670 --> 00:49:55.345
相似性是指向量看起来是一样的，对吧？

00:49:55.345 --> 00:49:57.610
嗯，这就是它的核心，对吧？

00:49:57.610 --> 00:50:00.130
所以我们会有相似向量的单词，

00:50:00.130 --> 00:50:04.240
在向量空间中紧密相连具有相似的意义。

00:50:04.240 --> 00:50:06.580
嗯，剩下的部分-嗯，

00:50:06.580 --> 00:50:10.330
接下来我们要做的就是把这个数字加上一个x。

00:50:10.330 --> 00:50:12.100
所以，嗯，指数

00:50:12.100 --> 00:50:15.295
这是一个很好的属性，不管你坚持什么数字，

00:50:15.295 --> 00:50:17.845
因为点积可能是正的或负的，

00:50:17.845 --> 00:50:20.890
它会变成一个正数，如果

00:50:20.890 --> 00:50:24.160
我们最终想要得到一个概率，嗯，那真的很好。

00:50:24.160 --> 00:50:28.450
如果我们有正数而不是负数，那就好了。

00:50:28.450 --> 00:50:33.370
嗯，第三部分是蓝色的出价是我们想要的

00:50:33.370 --> 00:50:36.070
概率和概率的总和

00:50:36.070 --> 00:50:39.970
首先，我们用最标准、最愚蠢的方式来做。

00:50:39.970 --> 00:50:42.205
我们把这个数量加起来，

00:50:42.205 --> 00:50:47.080
我们词汇表中的每一个不同的词

00:50:47.080 --> 00:50:52.315
它使事物正常化，并把它们变成概率分布。

00:50:52.315 --> 00:50:54.685
是的，所以在实践中，

00:50:54.685 --> 00:50:55.990
有两部分。

00:50:55.990 --> 00:50:59.110
有橙色部分，这就是使用

00:50:59.110 --> 00:51:03.580
点积和向量空间作为词与词之间的相似度度量

00:51:03.580 --> 00:51:07.480
然后第二部分是我们喂养它的所有其他部分。

00:51:07.480 --> 00:51:11.665
通过我们一直将新闻称为SoftMax发行版。

00:51:11.665 --> 00:51:17.530
因此，expen规范化的两部分为您提供了一个softmax分布。

00:51:17.530 --> 00:51:22.120
嗯，SoftMax函数将把任何数字映射到

00:51:22.120 --> 00:51:26.950
概率分布总是因为我给出的两个原因，所以，

00:51:26.950 --> 00:51:30.000
它被称为SoftMax Um，

00:51:30.000 --> 00:51:33.525
因为它的工作原理就像是一个SoftMax，对吧？

00:51:33.525 --> 00:51:35.040
所以如果你有数字，

00:51:35.040 --> 00:51:39.735
你可以说这些数字的最大值是多少，嗯，

00:51:39.735 --> 00:51:46.810
你知道这有点热-如果你把你的原始数字映射到，

00:51:46.810 --> 00:51:49.390
如果它是最大值，其他都是零，

00:51:49.390 --> 00:51:51.160
这有点难。

00:51:51.160 --> 00:51:56.935
嗯，软-这是一个软最大值，因为指数i-你知道，

00:51:56.935 --> 00:52:00.310
如果你想象一下，但是-如果我们忽略了这个问题

00:52:00.310 --> 00:52:04.315
有一段时间是负数，你就不用计算经验了，嗯，

00:52:04.315 --> 00:52:06.220
那你就有点出来

00:52:06.220 --> 00:52:09.640
概率分布，但大体上是公平的。

00:52:09.640 --> 00:52:12.070
平的，不会特别挑出

00:52:12.070 --> 00:52:15.310
不同的XI数，而当你用幂指数表示它们时，

00:52:15.310 --> 00:52:18.670
这使得大数字变得更大，所以，

00:52:18.670 --> 00:52:25.990
这种软最大值主要把质量放在最大值或最大值对的地方。

00:52:25.990 --> 00:52:29.920
嗯，这是最大部分，而软部分则不是

00:52:29.920 --> 00:52:34.900
一个艰难的决定仍然会在其他地方传播一点概率质量。

00:52:34.900 --> 00:52:40.540
好了，现在我们有了，失去功能。

00:52:40.540 --> 00:52:45.160
我们有一个损失函数，里面有一个概率模型，我们可以

00:52:45.160 --> 00:52:50.230
所以我们想做的就是，

00:52:50.230 --> 00:52:55.690
移动单词的矢量表示

00:52:55.690 --> 00:53:01.075
这样他们就能够很好地预测在其他单词的上下文中会发生什么单词。

00:53:01.075 --> 00:53:06.400
嗯，现在我们要做的就是优化。

00:53:06.400 --> 00:53:10.465
所以，我们有不同单词的向量分量。

00:53:10.465 --> 00:53:13.180
我们又有了一个很高的空间，但是这里，

00:53:13.180 --> 00:53:16.270
我只有两张照片，我们要

00:53:16.270 --> 00:53:19.510
说怎么做-我们怎样才能最小化这个函数，我们要

00:53:19.510 --> 00:53:23.920
希望在中晃动单词表示中使用的数字

00:53:23.920 --> 00:53:28.990
我们正沿着这个空间的斜坡走下去。

00:53:28.990 --> 00:53:32.095
我沿着斜坡走，嗯，

00:53:32.095 --> 00:53:37.330
然后我们要最小化我们找到的单词的良好表示的函数。

00:53:37.330 --> 00:53:39.775
为了这个案子这么做，

00:53:39.775 --> 00:53:42.070
我们想在

00:53:42.070 --> 00:53:45.400
所有参数的高维向量空间

00:53:45.400 --> 00:53:48.730
我们的模型和这个模型唯一的参数

00:53:48.730 --> 00:53:53.095
has是单词的向量空间表示。

00:53:53.095 --> 00:53:56.170
所以如果有100维的单词表示，

00:53:56.170 --> 00:53:59.320
它们是aardvark和context的100个参数，

00:53:59.320 --> 00:54:03.400
100个参数用于单词a-in-context和cetera，

00:54:03.400 --> 00:54:08.020
100个参数，用于将单词aardvark[噪音]作为中心单词et cetera，

00:54:08.020 --> 00:54:12.520
等等，通过这给了我们一个很大的参数向量

00:54:12.520 --> 00:54:18.265
优化，我们将运行这个优化，然后，嗯，把它们下移。

00:54:18.265 --> 00:54:23.740
嗯，[噪音]是的，所以这基本上就是你要做的。

00:54:23.740 --> 00:54:26.365
嗯，我有点想通过嗯，

00:54:26.365 --> 00:54:28.990
这个的细节，嗯，

00:54:28.990 --> 00:54:32.440
只是为了让我们具体地经历一些事情

00:54:32.440 --> 00:54:36.070
确保所有人都在同一页。

00:54:36.070 --> 00:54:39.475
嗯，所以我怀疑，你知道，

00:54:39.475 --> 00:54:43.510
如果我尝试具体地做这件事，

00:54:43.510 --> 00:54:45.865
嗯，有很多人嗯，

00:54:45.865 --> 00:54:50.830
这会让人厌烦，有些人会非常讨厌，

00:54:50.830 --> 00:54:54.415
嗯，所以我为你道歉，

00:54:54.415 --> 00:54:55.810
但是你知道，

00:54:55.810 --> 00:54:59.140
我希望并认为

00:54:59.140 --> 00:55:02.650
有些人最近没做那么多的事情

00:55:02.650 --> 00:55:05.740
实际上，具体地做这件事可能是件好事

00:55:05.740 --> 00:55:09.760
让每个人在开始的时候都加快速度。是啊？

00:55:09.760 --> 00:55:14.680
[听不见]我们如何具体计算[听不见]？

00:55:14.680 --> 00:55:20.275
好吧，那么，我们-那么我们计算的方式，

00:55:20.275 --> 00:55:26.050
u和v向量是我们从一个随机向量开始

00:55:26.050 --> 00:55:33.010
每一个单词，然后我们在学习时迭代地改变这些向量。

00:55:33.010 --> 00:55:37.135
我们要解决如何改变他们的方法是说，

00:55:37.135 --> 00:55:42.400
“我想做优化，”这将被实现为可以的。

00:55:42.400 --> 00:55:44.830
我们有每个词的当前向量。

00:55:44.830 --> 00:55:51.550
让我做一些微积分来计算我如何改变向量这个词的意思，

00:55:51.550 --> 00:55:55.780
矢量这个词可以计算出

00:55:55.780 --> 00:56:00.160
在这个中心词的上下文中实际出现的词。

00:56:00.160 --> 00:56:01.855
我们会这样做的，

00:56:01.855 --> 00:56:03.925
我们会一次又一次地做，

00:56:03.925 --> 00:56:06.760
最终会得到好的词向量。

00:56:06.760 --> 00:56:08.260
谢谢你的提问，

00:56:08.260 --> 00:56:10.780
因为这是一个你应该理解的概念。

00:56:10.780 --> 00:56:13.330
是这样吗？也许我没有

00:56:13.330 --> 00:56:16.645
把那个高级菜谱解释清楚，是的。

00:56:16.645 --> 00:56:20.410
好吧，那么好吧，让我们来看看。我们已经看到了，对吧？

00:56:20.410 --> 00:56:24.070
所以，我们有一个我们想要最大化的公式，

00:56:24.070 --> 00:56:32.410
我们原来的函数，是t的乘积，等于t，

00:56:32.410 --> 00:56:35.995
然后是单词的产物，呃，

00:56:35.995 --> 00:56:40.720
位置减去m小于或等于j，

00:56:40.720 --> 00:56:42.460
小于或等于m，

00:56:42.460 --> 00:56:46.000
j不等于零，嗯，

00:56:46.000 --> 00:56:51.640
素数为t时w的概率

00:56:51.640 --> 00:56:57.700
加上j，根据我们模型的参数给出wt。

00:56:57.700 --> 00:57:01.330
好吧，然后我们已经看到我们要把它转换成

00:57:01.330 --> 00:57:05.515
我们要用到的函数中，θ的j，

00:57:05.515 --> 00:57:15.490
其中t和的负1等于m和的1到t，

00:57:15.490 --> 00:57:17.770
小于或等于j小于或等于m，

00:57:17.770 --> 00:57:27.400
j不等于w乘以t的概率对数的零，加上j，w，

00:57:27.400 --> 00:57:31.840
T.好吧，我们有了，然后我们有了

00:57:31.840 --> 00:57:36.490
这个公式，外边词的概率给定

00:57:36.490 --> 00:57:46.360
上下文词就是我们刚刚通过的这个公式

00:57:46.360 --> 00:57:56.770
w的和等于xu wt vc的词汇大小。

00:57:56.770 --> 00:57:59.530
好吧，这就是我们的模型。

00:57:59.530 --> 00:58:03.835
我们想把这个最小化。

00:58:03.835 --> 00:58:11.230
所以，我们想最小化这个，我们想通过改变这些参数来最小化这个。

00:58:11.230 --> 00:58:15.415
这些参数就是这些向量的内容。

00:58:15.415 --> 00:58:17.635
所以，我们现在想做的，

00:58:17.635 --> 00:58:23.560
是做微积分，我们想说，让我们来计算这些参数，它们是，

00:58:23.560 --> 00:58:25.960
u和v向量，嗯，

00:58:25.960 --> 00:58:30.115
对于我们随机初始化的参数的当前值。

00:58:30.115 --> 00:58:32.050
空间的坡度是多少？

00:58:32.050 --> 00:58:33.490
下坡在哪里？

00:58:33.490 --> 00:58:35.770
因为如果我们能走下坡路，

00:58:35.770 --> 00:58:39.115
我们只要走下坡路，我们的模型就会好起来。

00:58:39.115 --> 00:58:42.010
所以，我们要用衍生工具计算出

00:58:42.010 --> 00:58:45.610
下坡的方向是，然后我们想走那条路，是的。

00:58:45.610 --> 00:58:50.230
那么，为什么我们要最大化可能的优势，就像，

00:58:50.230 --> 00:58:51.805
就像通读每一个字，

00:58:51.805 --> 00:58:57.640
就像[听不见]给[听不见]

00:58:57.640 --> 00:58:59.665
所以，好吧，那么，那么，

00:58:59.665 --> 00:59:02.905
我想达到这个目的，嗯，

00:59:02.905 --> 00:59:08.395
对于我的意义分配概念，我想实现的是，

00:59:08.395 --> 00:59:11.500
我有一个有意义的词，一个向量。

00:59:11.500 --> 00:59:16.810
这个向量知道在

00:59:16.810 --> 00:59:19.525
嗯，一句话。

00:59:19.525 --> 00:59:23.020
并且知道在它的上下文中出现的单词意味着什么，

00:59:23.020 --> 00:59:24.790
它能准确地给出

00:59:24.790 --> 00:59:28.945
对上下文中出现的单词的高概率估计，

00:59:28.945 --> 00:59:32.319
它会给出低概率的估计

00:59:32.319 --> 00:59:35.050
对于上下文中通常不会出现的单词。

00:59:35.050 --> 00:59:37.240
所以，你知道，如果这个词是bank，

00:59:37.240 --> 00:59:39.549
我希望像树枝这样的词，

00:59:39.549 --> 00:59:41.575
开放和退出，

00:59:41.575 --> 00:59:43.360
很有可能，

00:59:43.360 --> 00:59:45.445
因为它们往往出现在单词bank中。

00:59:45.445 --> 00:59:49.950
我希望还有别的话，嗯，

00:59:49.950 --> 00:59:52.740
像神经网络之类的

00:59:52.740 --> 00:59:57.970
概率较低，因为它们不容易出现在单词库中。

00:59:58.290 --> 01:00:01.525
好吧，嗯，这有道理吗？

01:00:01.525 --> 01:00:01.780
是啊。

01:00:01.780 --> 01:00:03.730
是啊。另一件事是，

01:00:03.730 --> 01:00:06.865
我忘了要说的是，你知道，很明显，

01:00:06.865 --> 01:00:10.480
我们不能做得太好，否则就是做不好，

01:00:10.480 --> 01:00:13.180
我们可以说上下文中的所有单词

01:00:13.180 --> 01:00:15.880
这个词的概率是0.97，对吗？

01:00:15.880 --> 01:00:19.750
因为我们用的是一个简单的概率分布

01:00:19.750 --> 01:00:23.230
预测我们上下文中的所有单词。

01:00:23.230 --> 01:00:27.880
所以，特别是，我们用它来预测10个不同的词，对吗？

01:00:27.880 --> 01:00:32.425
所以，充其量，我们可以给其中一个百分之五的机会，对吧？

01:00:32.425 --> 01:00:33.820
我们不可能，

01:00:33.820 --> 01:00:35.950
所以每次都猜对了。

01:00:35.950 --> 01:00:37.390
嗯，嗯，你知道，

01:00:37.390 --> 01:00:40.255
它们将是不同的上下文，包含不同的单词。

01:00:40.255 --> 01:00:44.610
所以，你知道，这将是一个非常宽松的模型，

01:00:44.610 --> 01:00:48.660
但不管怎样，我们要抓住这个事实，你知道，

01:00:48.660 --> 01:00:51.330
退出的可能性更大，嗯，

01:00:51.330 --> 01:00:57.580
发生在世界银行附近，而不是足球。

01:00:57.580 --> 01:01:01.030
那就是，你知道，基本上我们的目标是什么。

01:01:01.030 --> 01:01:07.360
好的，嗯，是的，所以我们想最大化这个，

01:01:07.360 --> 01:01:12.610
通过最小化，这意味着我们要做一些微积分来解决这个问题。

01:01:12.610 --> 01:01:14.740
所以，我们接下来要做的是，

01:01:14.740 --> 01:01:16.720
我们要说的是，

01:01:16.720 --> 01:01:19.495
这些参数是我们的词向量

01:01:19.495 --> 01:01:22.630
我们要移动这些词向量，

01:01:22.630 --> 01:01:28.180
嗯，到，嗯，想办法，嗯，走下坡路。

01:01:28.180 --> 01:01:32.440
所以，我现在要做的是研究

01:01:32.440 --> 01:01:38.275
这个中心词vc，并计算出如何做与之相关的事情。

01:01:38.275 --> 01:01:40.750
嗯，现在，这不是你唯一想做的事，

01:01:40.750 --> 01:01:44.905
你还需要计算出相对于uo向量的斜率。

01:01:44.905 --> 01:01:47.965
嗯，但我不会那样做，因为上课时间快用完了。

01:01:47.965 --> 01:01:49.750
所以，如果你在

01:01:49.750 --> 01:01:51.715
回家后你会觉得更有能力。

01:01:51.715 --> 01:01:57.130
好吧，那么，嗯，我想让你做的是求偏导数

01:01:57.130 --> 01:02:03.205
关于这个量的vc向量表示，

01:02:03.205 --> 01:02:04.810
我们只是在看。

01:02:04.810 --> 01:02:08.290
也就是这里的数量，

01:02:08.290 --> 01:02:11.980
嗯，我们把那个数量的对数记在哪里。

01:02:11.980 --> 01:02:17.560
对，x的对数，

01:02:17.560 --> 01:02:20.140
O，T，V，C，

01:02:20.140 --> 01:02:26.830
在w的和上等于u的x的1到v，

01:02:26.830 --> 01:02:30.220
O，T，V，C。好的，

01:02:30.220 --> 01:02:33.219
所以这个，嗯，现在我们有了一个部门的记录，

01:02:33.219 --> 01:02:35.695
很容易重写，嗯，

01:02:35.695 --> 01:02:39.595
我们得到了对数的偏导数

01:02:39.595 --> 01:02:47.560
分子减号和

01:02:47.560 --> 01:02:49.690
我可以分布偏导数。

01:02:49.690 --> 01:02:53.395
所以，我可以得到负的偏导数，

01:02:53.395 --> 01:02:56.680
嗯，分母的，

01:02:56.680 --> 01:02:59.710
嗯，这是这件事的记录。

01:02:59.710 --> 01:03:08.865
[噪音]

01:03:08.865 --> 01:03:18.190
可以。嗯，这是分子，这是分母。

01:03:19.190 --> 01:03:27.060
可以。所以，嗯，分子的部分很容易。

01:03:27.060 --> 01:03:29.130
事实上，也许我可以把它放在这里。

01:03:29.130 --> 01:03:33.450
嗯，所以登录exp只是彼此的反比，

01:03:33.450 --> 01:03:34.800
所以他们取消了。

01:03:34.800 --> 01:03:43.650
所以，我们得到了u，t，v，c的偏导数。

01:03:43.650 --> 01:03:47.460
好吧，这一点我应该，嗯，只是，嗯，

01:03:47.460 --> 01:03:51.630
提醒人们，这里的向量是-um，

01:03:51.630 --> 01:03:56.130
它仍然是一个向量，因为我们有一个单词的100维表示。

01:03:56.130 --> 01:04:00.330
嗯，这就是多元微积分。

01:04:00.330 --> 01:04:02.790
嗯，所以你知道，如果你是，

01:04:02.790 --> 01:04:04.530
如果你能，嗯，

01:04:04.530 --> 01:04:06.105
记住这些东西，

01:04:06.105 --> 01:04:08.175
你可以说，“哈，这是微不足道的”。

01:04:08.175 --> 01:04:12.390
答案是你完成了，嗯，那太好了。

01:04:12.390 --> 01:04:14.955
但你知道，如果你，嗯，感觉，嗯，

01:04:14.955 --> 01:04:17.550
这些东西都不怎么好，嗯，

01:04:17.550 --> 01:04:19.125
你想，嗯，

01:04:19.125 --> 01:04:22.440
有点不小心，试着弄清楚它是什么，

01:04:22.440 --> 01:04:24.180
嗯，你可以说，

01:04:24.180 --> 01:04:25.980
“好吧，让我呃，

01:04:25.980 --> 01:04:28.380
求出偏导数，

01:04:28.380 --> 01:04:34.200
嗯，关于这个向量的一个元素，就像这个向量的第一个元素。

01:04:34.200 --> 01:04:42.869
好吧，我在这里得到的这个点积是，我有一个乘以一个V。

01:04:42.869 --> 01:04:49.560
加上u-o两次v-c二次加上点，点，

01:04:49.560 --> 01:04:56.910
圆点加上U_o 100乘以V_c 100，对吧，

01:04:56.910 --> 01:05:02.535
我找到了这个关于v_c one的偏导数，

01:05:02.535 --> 01:05:05.490
希望还记得高中的微积分

01:05:05.490 --> 01:05:09.135
这些术语中没有一个涉及到vu-c-one。

01:05:09.135 --> 01:05:12.660
唯一剩下的就是这个U-O，

01:05:12.660 --> 01:05:15.960
这就是我对这个维度的理解。

01:05:15.960 --> 01:05:17.850
所以，这个特殊的参数。

01:05:17.850 --> 01:05:23.265
但我不仅想做V_c向量的第一个分量，

01:05:23.265 --> 01:05:26.745
我还想做V_c矢量等的第二个分量，

01:05:26.745 --> 01:05:30.630
这意味着我最终会和他们所有人在一起

01:05:30.630 --> 01:05:35.685
出现在这些事情中的一个。

01:05:35.685 --> 01:05:41.190
最终的结果是我得到了向量u_。

01:05:41.190 --> 01:05:43.620
可以。但是你知道，

01:05:43.620 --> 01:05:47.220
如果你有点困惑，你的大脑也在崩溃，

01:05:47.220 --> 01:05:52.050
我认为把事情再简化到某种程度上是有用的，

01:05:52.050 --> 01:05:58.275
单维微积分，实际上有点玩弄实际发生的事情。

01:05:58.275 --> 01:06:00.840
嗯，不管怎样，这部分很简单。

01:06:00.840 --> 01:06:03.540
分子，我们得到了。

01:06:03.540 --> 01:06:08.085
嗯，所以当我们做分母的时候事情就不太好了。

01:06:08.085 --> 01:06:11.640
所以我们现在想要这个，嗯，B&D，

01:06:11.640 --> 01:06:17.010
w和的对数的v_c等于

01:06:17.010 --> 01:06:22.845
一个到U-O-T-V-C的P-X。

01:06:22.845 --> 01:06:25.800
可以。所以，现在在这一点上，

01:06:25.800 --> 01:06:27.450
我不太漂亮。

01:06:27.450 --> 01:06:31.035
我们有这个对数和x的组合，你会看到很多，

01:06:31.035 --> 01:06:35.640
所以在这一点上，你必须记住，有e，链式法则。

01:06:35.640 --> 01:06:38.520
可以。所以，我们可以说，这是你知道的，

01:06:38.520 --> 01:06:42.540
我们的函数f，这里是函数体，

01:06:42.540 --> 01:06:46.245
所以我们想做的是，

01:06:46.245 --> 01:06:48.630
分两个阶段进行。

01:06:48.630 --> 01:06:51.570
嗯，所以在一天结束的时候，

01:06:51.570 --> 01:06:53.430
我们最后有了这个V&U C。

01:06:53.430 --> 01:06:57.105
所以，我们这里有一些函数。

01:06:57.105 --> 01:06:59.910
最终有一个v_c的函数，

01:06:59.910 --> 01:07:02.220
所以我们要用链式法则。

01:07:02.220 --> 01:07:05.040
我们要说的是，连锁规则是我们首先

01:07:05.040 --> 01:07:09.135
这个外物的导数，放入这个身体，

01:07:09.135 --> 01:07:13.680
然后我们记得对数的导数是x上的。

01:07:13.680 --> 01:07:22.920
所以，我们有一个除以w的和等于u-o-t-v-c的exp的1-v

01:07:22.920 --> 01:07:26.640
然后我们需要把它乘以

01:07:26.640 --> 01:07:32.610
内部部分的导数，是。

01:07:32.610 --> 01:07:34.750
我们这里有什么。

01:07:40.490 --> 01:07:44.850
可以。乘以内部部分的导数

01:07:44.850 --> 01:07:48.600
重要的提醒是你需要改变变量，

01:07:48.600 --> 01:07:53.460
对于内部部分，使用您要求和的另一个变量。

01:07:53.460 --> 01:08:00.810
可以。所以，现在我们要求x和的导数。

01:08:00.810 --> 01:08:05.055
我们能做的第一件事是V-非常简单。

01:08:05.055 --> 01:08:08.865
我们可以把导数移到和里面。

01:08:08.865 --> 01:08:14.430
所以，我们可以重写它，得到x的和的第一等于

01:08:14.430 --> 01:08:20.430
关于[听不见]的偏导数的v。

01:08:20.430 --> 01:08:22.575
嗯，这有点进步。

01:08:22.575 --> 01:08:26.730
嗯，这一点，我们必须再做一次链式法则，对吧。

01:08:26.730 --> 01:08:33.210
所以，这是我们的函数，这是它里面的东西，它是v_c的函数。

01:08:33.210 --> 01:08:37.605
所以，我们再一次想，嗯，链式法则。

01:08:37.605 --> 01:08:41.340
所以我们就有了好的，

01:08:41.340 --> 01:08:45.720
x-um的导数是exp。

01:08:45.720 --> 01:08:54.630
那么，x的和等于u，x，v，c的exp的和，

01:08:54.630 --> 01:09:00.150
然后用偏导数乘以

01:09:00.150 --> 01:09:05.700
考虑到内部u x t v_c的t v_c。

01:09:05.700 --> 01:09:08.160
嗯，我们以前看到过，所以，

01:09:08.160 --> 01:09:13.200
它的导数是u-嗯，

01:09:13.200 --> 01:09:16.320
是的，因为我们是通过一个不同的X来做的，对吧。

01:09:16.320 --> 01:09:18.780
这就变成了u_x，

01:09:18.780 --> 01:09:23.850
所以x的和等于

01:09:23.850 --> 01:09:30.030
这个表达式的v x t b c乘以x的u。

01:09:30.030 --> 01:09:34.995
可以。所以，通过做两次链式法则，我们就得到了。

01:09:34.995 --> 01:09:38.190
所以，现在如果我们把它放在一起，你知道，

01:09:38.190 --> 01:09:43.050
V_c关于整体的导数，

01:09:43.050 --> 01:09:46.500
给定c的概率的对数，对吧。

01:09:46.500 --> 01:09:51.210
对于分子来说，它只是一个u o，

01:09:51.210 --> 01:09:54.030
然后减法，

01:09:54.030 --> 01:09:57.645
我们这里有这个词，嗯，

01:09:57.645 --> 01:09:59.730
这是一种分母，

01:09:59.730 --> 01:10:03.870
我们这里有一个术语，分子。

01:10:03.870 --> 01:10:07.725
所以，我们要减去分子，

01:10:07.725 --> 01:10:12.270
我们得到x的和等于

01:10:12.270 --> 01:10:18.765
u x t v_c的exp乘以u x，

01:10:18.765 --> 01:10:25.395
然后在分母中，我们有，嗯，

01:10:25.395 --> 01:10:35.920
w的和等于u-w-t-v-c的exp的1到v。

01:10:36.350 --> 01:10:40.035
嗯，好吧，我们有点明白了。

01:10:40.035 --> 01:10:44.025
哦，等等。是啊。是的，我明白了。

01:10:44.025 --> 01:10:45.900
是的，没错。嗯，好吧。

01:10:45.900 --> 01:10:52.170
我们得到了，然后我们可以重新安排一下。

01:10:52.170 --> 01:10:56.475
所以，我们可以直接得到这个总数，

01:10:56.475 --> 01:11:03.284
我们可以说这是一个很大的和，x等于1:v，

01:11:03.284 --> 01:11:09.870
我们可以把你的话拿出来说，好吧。

01:11:09.870 --> 01:11:13.155
让我们把它叫做“U”，

01:11:13.155 --> 01:11:15.090
如果我们这样做，

01:11:15.090 --> 01:11:20.295
发生了一件有趣的事情，因为你看这里，

01:11:20.295 --> 01:11:25.830
我们重新发现了完全相同的形式

01:11:25.830 --> 01:11:31.425
我们用它作为概率分布来预测单词的概率。

01:11:31.425 --> 01:11:37.860
所以，根据我们的模型，这是x给定c的概率。

01:11:37.860 --> 01:11:46.155
嗯，我们可以重写这个，然后说我们得到的是u减去

01:11:46.155 --> 01:11:54.795
x等于给定c乘以u x的x的概率的1到v。

01:11:54.795 --> 01:11:58.755
如果你仔细想想，这有一种有趣的意义。

01:11:58.755 --> 01:12:01.365
所以，这实际上是给我们，你知道，

01:12:01.365 --> 01:12:04.200
我们在这个多维空间的斜坡

01:12:04.200 --> 01:12:07.215
我们是如何得到这个斜坡的

01:12:07.215 --> 01:12:11.280
观察到的

01:12:11.280 --> 01:12:18.450
上下文词，我们从中减去我们的模型的想法，嗯，

01:12:18.450 --> 01:12:20.955
上下文应该是这样的。

01:12:20.955 --> 01:12:24.465
模型认为上下文应该是什么样的？

01:12:24.465 --> 01:12:27.330
这里的这一部分在预期中是正式的。

01:12:27.330 --> 01:12:31.395
所以，你要做的是找到加权平均数

01:12:31.395 --> 01:12:36.375
每一个词的表达模式，

01:12:36.375 --> 01:12:39.990
乘以当前模型中的概率。

01:12:39.990 --> 01:12:45.315
所以，根据我们当前的模型，这是一种预期的上下文词，

01:12:45.315 --> 01:12:47.460
所以我们要考虑

01:12:47.460 --> 01:12:52.170
出现的预期上下文词和实际上下文词，

01:12:52.170 --> 01:12:55.560
然后这一区别就证明了

01:12:55.560 --> 01:12:58.890
我们应该朝哪个方向倾斜

01:12:58.890 --> 01:13:01.050
走着换字

01:13:01.050 --> 01:13:06.715
为了提高模型的预测能力。

01:13:06.715 --> 01:13:11.565
可以。嗯，我们会的，

01:13:11.565 --> 01:13:14.100
嗯，任务二，嗯，是的。

01:13:14.100 --> 01:13:18.060
所以，嗯，这对你们来说是个很好的锻炼，

01:13:18.060 --> 01:13:20.115
嗯，到，嗯，

01:13:20.115 --> 01:13:22.830
要想为中央情报局做这件事，等等，

01:13:22.830 --> 01:13:26.640
嗯，我做的中心词也试图寻找上下文词

01:13:26.640 --> 01:13:31.125
向你展示你可以做同样的数学，然后让它出来。

01:13:31.125 --> 01:13:35.655
嗯，如果我最后还有几分钟的话。

01:13:35.655 --> 01:13:43.320
嗯，我只是想告诉你，如果我能把所有这些都做好的话。

01:13:43.320 --> 01:13:49.950
嗯，我们走这边[听不见]。

01:13:49.950 --> 01:13:53.590
好的，找到我的。

01:13:54.200 --> 01:14:00.075
可以。嗯，我只是想给你举个简单的例子。

01:14:00.075 --> 01:14:01.935
所以，对于第一个任务，

01:14:01.935 --> 01:14:04.170
嗯，又是一本ipython笔记本。

01:14:04.170 --> 01:14:09.015
所以，如果你都设置好了，你可以做一个Jupyter笔记本。

01:14:09.015 --> 01:14:12.945
嗯，你还有一些笔记本。

01:14:12.945 --> 01:14:16.630
嗯，这是我的小笔记本，我要给你看，

01:14:17.180 --> 01:14:23.620
嗯，诀窍是让这个足够大，人们可以看到它。

01:14:30.940 --> 01:14:35.530
可读吗？[笑声]好吧，嗯，

01:14:35.530 --> 01:14:39.210
所以对了，所以麻木是一种，

01:14:39.210 --> 01:14:41.930
嗯，用python做数学包。

01:14:41.930 --> 01:14:43.120
你会想知道的。

01:14:43.120 --> 01:14:44.445
如果你不知道的话。

01:14:44.445 --> 01:14:46.440
嗯，Matplotlib有点，

01:14:46.440 --> 01:14:49.040
最基本的绘图包之一

01:14:49.040 --> 01:14:51.755
如果你不知道，你会想知道的。

01:14:51.755 --> 01:14:55.900
这有点像伊普西顿或朱彼特的特色

01:14:55.900 --> 01:14:59.755
让你有一个互动的Matplotlib um，里面。

01:14:59.755 --> 01:15:03.675
如果你想得到幻想，你可以玩它-玩你的图形风格。

01:15:03.675 --> 01:15:06.615
嗯，就是这样。

01:15:06.615 --> 01:15:10.465
Scikit学习是一种通用的机器学习包。

01:15:10.465 --> 01:15:13.350
嗯，根西姆不是一个深入的学习包。

01:15:13.350 --> 01:15:17.590
gensim是一种单词相似度包，它从um开始，

01:15:17.590 --> 01:15:20.760
用隐式狄氏分析等方法。

01:15:20.760 --> 01:15:22.530
如果你从模特的话知道的话

01:15:22.530 --> 01:15:25.940
类似的是成长为一个好的包裹嗯，

01:15:25.940 --> 01:15:28.570
做，嗯，单词向量也是。

01:15:28.570 --> 01:15:31.650
所以，它经常用于单词向量和

01:15:31.650 --> 01:15:36.105
相似性这个词在大规模的工作中是有效的。

01:15:36.105 --> 01:15:37.720
嗯，是的。

01:15:37.720 --> 01:15:41.360
所以，我还没告诉你下次威尔的事

01:15:41.360 --> 01:15:46.400
我们自己的词汇载体形式，即手套式词汇载体。

01:15:46.400 --> 01:15:51.270
我使用它们不是因为它对我展示的东西很重要，但是你知道，

01:15:51.270 --> 01:15:55.745
这些向量很小。

01:15:55.745 --> 01:16:00.470
事实证明，Facebook和谷歌

01:16:00.470 --> 01:16:05.940
分布是非常大的词汇和非常高的维度。

01:16:05.940 --> 01:16:08.940
所以花我太长时间装进去

01:16:08.940 --> 01:16:12.860
在这节课的最后五分钟

01:16:12.860 --> 01:16:16.865
在我们的斯坦福向量中，我们有100个维向量，嗯，

01:16:16.865 --> 01:16:19.160
以及50个维度向量

01:16:19.160 --> 01:16:21.755
坦率地说，在笔记本电脑上做一些小事情很好。

01:16:21.755 --> 01:16:27.330
嗯，所以，我在这里做的是Gensim不支持本地人

01:16:27.330 --> 01:16:30.210
手套向量，但它们实际上提供了一个实用程序

01:16:30.210 --> 01:16:33.390
将手套文件格式转换为Word2vec文件格式。

01:16:33.390 --> 01:16:40.285
所以我做到了。然后我加载了一个预先训练过的单词向量模型。

01:16:40.285 --> 01:16:44.430
这就是他们所说的键控向量。

01:16:44.430 --> 01:16:46.890
所以，键控向量并不是什么稀奇的东西。

01:16:46.890 --> 01:16:51.660
只是你有土豆之类的词，每个词都有一个向量。

01:16:51.660 --> 01:16:55.435
所以它实际上就是一本大字典，每件事都有一个向量。

01:16:55.435 --> 01:16:58.690
但是，这个模型是一个经过训练的模型

01:16:58.690 --> 01:17:02.230
我们只是使用我们研究的算法，

01:17:02.230 --> 01:17:06.730
你知道，经过数十亿次的训练来摆弄我们的词汇载体。

01:17:06.730 --> 01:17:11.255
嗯，一旦我们有了，我们就可以，嗯，

01:17:11.255 --> 01:17:14.265
问一些问题，比如，我们可以说，

01:17:14.265 --> 01:17:17.110
与其他词最相似的词是什么？

01:17:17.110 --> 01:17:19.650
所以我们可以拿一些，嗯，

01:17:19.650 --> 01:17:23.175
我们来说说，与奥巴马最相似的词是什么？

01:17:23.175 --> 01:17:25.770
我们回到营房，布什，克林顿，

01:17:25.770 --> 01:17:29.040
麦凯恩、戈尔、希拉里·多尔、马丁、亨利。

01:17:29.040 --> 01:17:31.425
这似乎有点有趣。

01:17:31.425 --> 01:17:34.050
这些因素来自几年前。

01:17:34.050 --> 01:17:37.150
所以我们没有后奥巴马的工作人员。

01:17:37.150 --> 01:17:40.750
我的意思是如果你换个词，嗯，你知道，

01:17:40.750 --> 01:17:44.100
我们可以放进香蕉之类的东西，然后我们得到椰子，

01:17:44.100 --> 01:17:46.600
芒果，香蕉，土豆，菠萝。

01:17:46.600 --> 01:17:49.430
我们有热带食物。

01:17:49.430 --> 01:17:54.070
所以，你可以-你可以问呃，

01:17:54.070 --> 01:17:56.995
因为与言语不同。

01:17:56.995 --> 01:17:59.700
相异者本身并不是很有用。

01:17:59.700 --> 01:18:04.550
所以如果我问最相似的问题，我会说，

01:18:04.550 --> 01:18:09.290
负等于，嗯，香蕉，

01:18:09.290 --> 01:18:14.720
嗯，我不知道你对什么最不一样的概念，

01:18:14.720 --> 01:18:16.620
嗯，香蕉是，但你知道，

01:18:16.620 --> 01:18:22.655
事实上，你自己并没有从中得到任何有用的东西，嗯，

01:18:22.655 --> 01:18:28.000
因为，嗯，你只是得到这些奇怪的非常罕见的词嗯，

01:18:28.000 --> 01:18:31.440
当然，嗯，[笑声]不是那些想的人。

01:18:31.440 --> 01:18:37.570
嗯，但事实证明你可以用这个消极的想法做一些真正有用的事情

01:18:37.570 --> 01:18:39.000
它是

01:18:39.000 --> 01:18:44.175
词载体在其最初出现时的高度著名的结果。

01:18:44.175 --> 01:18:50.205
这就是这个概念，在这个空间里有意义的维度。

01:18:50.205 --> 01:18:54.820
所以这是最著名的例子，嗯，就是，

01:18:54.820 --> 01:18:59.985
我们能做的是从单词King开始，然后减去

01:18:59.985 --> 01:19:05.355
从中可以看出男人的意思，然后我们可以把女人的意思加上去。

01:19:05.355 --> 01:19:09.110
然后我们可以说，在我们的向量空间中，哪个词是

01:19:09.110 --> 01:19:13.060
意思和那个词最相似。

01:19:13.060 --> 01:19:15.810
这将是一种做类比的方式。

01:19:15.810 --> 01:19:18.640
可以做，嗯，类比，

01:19:18.640 --> 01:19:22.050
男人是国王，女人是对什么？

01:19:22.050 --> 01:19:26.500
所以，我们要做的就是说我们想和国王一样

01:19:26.500 --> 01:19:31.220
还有女人，因为她们都是积极的，离男人很远。

01:19:31.220 --> 01:19:35.185
所以，我们可以手动操作，

01:19:35.185 --> 01:19:36.950
这是手工说的，

01:19:36.950 --> 01:19:41.050
最相似的正面女性国王，负面男性。

01:19:41.050 --> 01:19:45.410
我们可以运行它，看到它产生皇后。

01:19:45.410 --> 01:19:48.570
为了让这更容易一点，我定义了这个类比，

01:19:48.570 --> 01:19:53.485
嗯，类比谓词，这样我可以运行其他谓词。

01:19:53.485 --> 01:19:59.095
所以我可以用另一个类似日本日语的例子，

01:19:59.095 --> 01:20:01.160
奥地利是奥地利人。

01:20:01.160 --> 01:20:03.125
嗯，你知道，

01:20:03.125 --> 01:20:07.150
我认为当人们首先

01:20:07.150 --> 01:20:10.950
看到你可以有一个简单的数学并运行它，

01:20:10.950 --> 01:20:12.950
学习单词的含义。

01:20:12.950 --> 01:20:18.465
我的意思是，这实际上有点让人们觉得这是多么有效。

01:20:18.465 --> 01:20:22.030
你知道的。就像那里-这里没有镜子和绳子，对吧？

01:20:22.030 --> 01:20:24.225
你知道不是我有一个单独的-

01:20:24.225 --> 01:20:28.330
在我的python中有一个特殊的列表，我在其中查找困难，

01:20:28.330 --> 01:20:30.240
呃，奥地利人，

01:20:30.240 --> 01:20:31.910
嗯，还有类似的事情。

01:20:31.910 --> 01:20:35.310
但是这些矢量表示

01:20:35.310 --> 01:20:38.760
这样，它实际上是对这些语义关系进行编码，

01:20:38.760 --> 01:20:40.920
你知道，所以你可以尝试不同的，

01:20:40.920 --> 01:20:43.360
你知道，不是只有这一个有效。

01:20:43.360 --> 01:20:46.185
我可以把法语说成法语。

01:20:46.185 --> 01:20:49.780
我可以放在德国，上面写着德语，

01:20:49.780 --> 01:20:54.590
我可以把它放在澳大利亚而不是奥地利，上面写着澳大利亚，

01:20:54.590 --> 01:20:59.480
你知道，如果你想用向量表示

01:20:59.480 --> 01:21:04.815
像理解单词之间的关系这样的想法，

01:21:04.815 --> 01:21:10.595
你只是在100维的数字上做向量空间操作，

01:21:10.595 --> 01:21:15.830
它实际上知道它们。这不仅是词义的相似性，而且

01:21:15.830 --> 01:21:18.255
实际上不同的语义关系

01:21:18.255 --> 01:21:21.635
在诸如国名和他们的民族等词之间。

01:21:21.635 --> 01:21:23.850
是的，这真是太神奇了。

01:21:23.850 --> 01:21:31.340
真的，你知道，在嗯上运行这样一个愚蠢的算法有点奇怪，

01:21:31.340 --> 01:21:35.100
数字矢量可以很好地捕捉单词的含义。

01:21:35.100 --> 01:21:38.160
这就成了很多种类的基础

01:21:38.160 --> 01:21:41.355
现代分布式语言的神经表示。

01:21:41.355 --> 01:21:42.630
好的，我会停在那儿。

01:21:42.630 --> 01:21:46.700
Thanks a lot guys and see you on Thursday. [NOISE]

