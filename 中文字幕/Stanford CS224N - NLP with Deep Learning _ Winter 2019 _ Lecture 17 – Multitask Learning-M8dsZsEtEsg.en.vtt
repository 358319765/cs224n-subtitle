WEBVTT
Kind: captions
Language: en

00:00:04.190 --> 00:00:08.790
所以今天，我们很高兴有第二个，嗯，

00:00:08.790 --> 00:00:10.980
受邀演讲人，理查德·索彻，

00:00:10.980 --> 00:00:14.085
他是Salesforce的首席科学家。

00:00:14.085 --> 00:00:18.040
嗯，理查德实际上也和这个班有更多的联系，

00:00:18.040 --> 00:00:21.840
嗯，因为，嗯，好几年了，嗯，

00:00:21.840 --> 00:00:24.970
理查德不是当教练就是，嗯，

00:00:24.970 --> 00:00:29.110
在斯坦福大学教授这一材料的共同导师，

00:00:29.110 --> 00:00:32.605
嗯，所以他对课程有点了解，嗯，相当了解。

00:00:32.605 --> 00:00:34.030
嗯，所以今天，

00:00:34.030 --> 00:00:38.830
他将要谈论一些挑战和最近的工作

00:00:38.830 --> 00:00:43.690
在自然语言处理中进行多任务学习。欢迎你，理查德。

00:00:43.690 --> 00:00:46.595
谢谢您。大家好。我很高兴来到这里。

00:00:46.595 --> 00:00:49.380
嗯，是的，我今天想和你谈谈我们，

00:00:49.380 --> 00:00:51.280
简而言之，被称为decalp。

00:00:51.280 --> 00:00:54.635
我想先对布莱恩·麦肯大吼一声。

00:00:54.635 --> 00:00:56.900
他是这篇论文的第一作者

00:00:56.900 --> 00:01:00.200
我把这个想法告诉了很多人，比如，

00:01:00.200 --> 00:01:01.280
三到四年，

00:01:01.280 --> 00:01:02.405
大多数人都喜欢，

00:01:02.405 --> 00:01:04.730
“这是太多的预处理，因为您试图

00:01:04.730 --> 00:01:07.290
在一个模型中执行10个不同的任务。”

00:01:07.290 --> 00:01:09.510
这就是十项全能，呃，

00:01:09.510 --> 00:01:11.805
措辞是这样的，但是他，

00:01:11.805 --> 00:01:13.305
他真的坚持住了，呃，

00:01:13.305 --> 00:01:16.730
做了所有的预处理和所有你现在知道的事情，比如标记化技术，

00:01:16.730 --> 00:01:18.500
结果是很多不同的数据集，

00:01:18.500 --> 00:01:20.270
对单词有不同的概念。

00:01:20.270 --> 00:01:21.710
这不是两个字，

00:01:21.710 --> 00:01:23.480
呃，或者说一句话，

00:01:23.480 --> 00:01:25.355
这样的事情改变了你

00:01:25.355 --> 00:01:27.470
写下你所有的评估脚本和所有这些。

00:01:27.470 --> 00:01:29.165
所以布莱恩，呃，是，

00:01:29.165 --> 00:01:30.770
是个非常出色的研究员，

00:01:30.770 --> 00:01:31.985
呃，和我们在一起，

00:01:31.985 --> 00:01:35.340
Nitish在优化方面帮助了我们很多，

00:01:35.340 --> 00:01:36.480
嗯，然后叫熊明明，

00:01:36.480 --> 00:01:38.415
研究部主任做了很多，

00:01:38.415 --> 00:01:41.735
非常出色的工作，对我们所有的项目都很有帮助。

00:01:41.735 --> 00:01:44.830
所以我要告诉你一些不同的，呃，

00:01:44.830 --> 00:01:48.560
导致我们

00:01:48.560 --> 00:01:50.525
嗯，这个多任务学习的想法。

00:01:50.525 --> 00:01:54.170
第一个是想退后一步看看场地，

00:01:54.170 --> 00:01:58.955
我注意到不像历史课那么多，基本上是在2010年之前，

00:01:58.955 --> 00:02:04.340
大多数自然语言处理都有一些手工设计的特性，

00:02:04.340 --> 00:02:05.660
我们基本上只是，

00:02:05.660 --> 00:02:08.710
嗯，机器学习的那种重量，

00:02:08.710 --> 00:02:12.680
嗯，在这些人为设计的特性的优化过程中。

00:02:12.680 --> 00:02:20.780
所以在2010年，克里斯和我以及其他人开始深入学习功能学习。

00:02:20.780 --> 00:02:22.150
所以一切都是一个词的载体，现在，

00:02:22.150 --> 00:02:25.910
我们可以反向传播到它们中，并实际学习这些表示。

00:02:25.910 --> 00:02:27.410
我想现在，

00:02:27.410 --> 00:02:28.880
我们处于这样一种状态，我们做了很多

00:02:28.880 --> 00:02:31.940
针对特定任务的深层建筑工程，

00:02:31.940 --> 00:02:33.110
你已经看到了。

00:02:33.110 --> 00:02:34.700
你就像一个NER模型，

00:02:34.700 --> 00:02:36.350
你有一个问答模式，

00:02:36.350 --> 00:02:37.745
你有一个翻译模型，

00:02:37.745 --> 00:02:39.110
我们现在基本上，

00:02:39.110 --> 00:02:41.990
每个社区至少，呃，

00:02:41.990 --> 00:02:44.660
收敛的可能是某种神经网络，

00:02:44.660 --> 00:02:47.570
但是仍然有很多不同类型的体系结构

00:02:47.570 --> 00:02:51.045
这些神经网络是你为每一个不同的任务而工作的。

00:02:51.045 --> 00:02:52.590
所以问题是，好吧，

00:02:52.590 --> 00:02:54.125
我们可能会这么做

00:02:54.125 --> 00:02:57.170
再过几年，因为我们正在取得进展，

00:02:57.170 --> 00:02:58.550
下一步怎么办？

00:02:58.550 --> 00:02:59.990
呃，在研究方面？

00:02:59.990 --> 00:03:02.480
我非常喜欢这门课

00:03:02.480 --> 00:03:05.000
你可能不太了解NLP

00:03:05.000 --> 00:03:07.715
对你来说基本上都能理解

00:03:07.715 --> 00:03:10.880
目前最先进的研究论文，

00:03:10.880 --> 00:03:12.950
嗯，这个，这是其中之一。

00:03:12.950 --> 00:03:15.480
嗯，那么[噪音]为什么，

00:03:15.480 --> 00:03:17.840
为什么不继续在这个多任务机制中工作呢？

00:03:17.840 --> 00:03:19.280
在某些方面，我觉得，

00:03:19.280 --> 00:03:20.960
社区有点，呃，

00:03:20.960 --> 00:03:22.700
就像这只可爱的狗，我们，有点，

00:03:22.700 --> 00:03:25.960
每次项目结束后随机重启。

00:03:25.960 --> 00:03:29.840
我很清楚如果你有很多训练数据，

00:03:29.840 --> 00:03:34.920
在数据集上定义一个特定的数据集和任务，

00:03:34.920 --> 00:03:39.080
你开始在你的模型中使用建筑学工程师在一个特定的度量标准上爬山，

00:03:39.080 --> 00:03:41.420
或排行榜或出版物，

00:03:41.420 --> 00:03:43.655
或者产品，或者不管是什么，呃，

00:03:43.655 --> 00:03:45.710
只要你的数据集

00:03:45.710 --> 00:03:48.090
大概是一套很有代表性的

00:03:48.090 --> 00:03:50.875
输出类数量的1000倍，

00:03:50.875 --> 00:03:56.210
你很可能会把它变成一个精确到80%到90%的区域系统，

00:03:56.210 --> 00:03:59.360
或者，如果有，你基本上做得很好。

00:03:59.360 --> 00:04:02.300
当然，现在当你看Imagenet上的趋势时，

00:04:02.300 --> 00:04:05.000
你有1000个不同的计算机视觉课程，

00:04:05.000 --> 00:04:08.640
1000个不同的类，每个类有1000个图像。

00:04:08.640 --> 00:04:11.460
所以如果你有大约一百万张图片，你做得很好。

00:04:11.460 --> 00:04:13.740
在机器翻译中，理想情况下，

00:04:13.740 --> 00:04:16.250
你知道，我还有很多，我有几十万个词，

00:04:16.250 --> 00:04:21.725
所以你需要数以百万计的例子，

00:04:21.725 --> 00:04:23.090
嗯，在他们的语境中。

00:04:23.090 --> 00:04:24.830
当然，你知道，警告是

00:04:24.830 --> 00:04:27.620
机器翻译不能达到人类的水平，

00:04:27.620 --> 00:04:30.110
但它的效果很好，至少在产品中是这样的，

00:04:30.110 --> 00:04:34.750
即使是最好的翻译人员也会把它当作一种预译，然后，

00:04:34.750 --> 00:04:37.030
呃，差不多，把它清理一下。

00:04:37.030 --> 00:04:39.985
我也很清楚在这个政权里，

00:04:39.985 --> 00:04:41.480
如果我们想去的话，

00:04:41.480 --> 00:04:43.550
更一般的人工智能功能，呃，

00:04:43.550 --> 00:04:47.360
我们需要对单个模型进行某种更为连续的学习。

00:04:47.360 --> 00:04:49.840
因为如果我们继续在每个项目中重新启动，

00:04:49.840 --> 00:04:51.830
我们永远不会得到一个单一的模型，

00:04:51.830 --> 00:04:55.715
包含了越来越多的自然语言的复杂性。

00:04:55.715 --> 00:04:59.115
当我说我们从随机开始，

00:04:59.115 --> 00:05:01.295
你当然知道那不是真的

00:05:01.295 --> 00:05:04.190
因为我们有一些东西需要预先训练，

00:05:04.190 --> 00:05:06.290
也就是说，在计算机视觉中，

00:05:06.290 --> 00:05:07.520
我们还有更多的东西。

00:05:07.520 --> 00:05:09.020
所以在某些方面，也就是说，啊，

00:05:09.020 --> 00:05:11.745
对NLP抱有抱负的理想，

00:05:11.745 --> 00:05:13.860
因为在计算机视觉中，你会

00:05:13.860 --> 00:05:15.590
疯狂地不使用某种

00:05:15.590 --> 00:05:19.610
卷积神经网络已经预先训练过-已经预先训练过某种

00:05:19.610 --> 00:05:22.520
当你开始你的项目和

00:05:22.520 --> 00:05:25.985
试着对物体进行分类，或者做物体检测等很多事情。

00:05:25.985 --> 00:05:29.750
从某种程度上说，整个社区可以很快地支持它，

00:05:29.750 --> 00:05:32.460
因为我的意思是，你知道，一旦成功，

00:05:32.460 --> 00:05:34.125
很好，因为

00:05:34.125 --> 00:05:35.990
计算机视觉中的单阻塞任务。

00:05:35.990 --> 00:05:38.610
如果你连狗和猫和房子都分不清，

00:05:38.610 --> 00:05:42.420
考虑更大的，呃，远景项目真的没有意义。

00:05:42.420 --> 00:05:45.210
在NLP中，我们在单词向量方面取得了很大的成功，

00:05:45.210 --> 00:05:46.650
你现在知道很多，

00:05:46.650 --> 00:05:48.750
开始的时候，只是一小部分，呃，

00:05:48.750 --> 00:05:51.780
基于窗口的方法或word2vec和glove，呃，

00:05:51.780 --> 00:05:55.020
然后，我们有，呃，上下文向量被训练，呃，

00:05:55.020 --> 00:05:57.295
机器翻译，但基本上，

00:05:57.295 --> 00:06:00.050
不是只有一组单词，

00:06:00.050 --> 00:06:04.455
我们实际上预先训练了一些在这些词向量之上的NLSTM，

00:06:04.455 --> 00:06:06.925
我们训练的方式，

00:06:06.925 --> 00:06:09.050
实际上也是布莱恩·麦肯的论文

00:06:09.050 --> 00:06:12.530
有机器翻译和ELMO的上下文向量，

00:06:12.530 --> 00:06:16.260
有点，用语言建模取代了机器翻译，

00:06:16.260 --> 00:06:18.570
哪一个当然更好，因为有更多的培训数据，

00:06:18.570 --> 00:06:20.340
它仍然告诉你很多，呃，

00:06:20.340 --> 00:06:23.210
以某种方式捕捉到更复杂版本的

00:06:23.210 --> 00:06:26.900
分布的假设，在简单的词向量中，

00:06:26.900 --> 00:06:29.640
伯特，不完全是一种语言模式，但也有点，

00:06:29.640 --> 00:06:31.610
试图在他们的语境中预测单词，

00:06:31.610 --> 00:06:34.400
但是，预培训有更多的层次和更深入的网络。

00:06:34.400 --> 00:06:39.700
所以我们看到了预先训练一组重量的成功。

00:06:39.700 --> 00:06:41.265
所以问题是，

00:06:41.265 --> 00:06:44.310
为什么不试着预先培训整个模型呢？

00:06:44.310 --> 00:06:46.650
包括你的输出，

00:06:46.650 --> 00:06:50.145
你的SoftMax，你的指针机制和一切，

00:06:50.145 --> 00:06:54.240
然后就拿一个完全预先训练过的模型做点什么，

00:06:54.240 --> 00:06:56.895
这就是我们的目标。

00:06:56.895 --> 00:06:58.890
所以，呃，我们，有点，

00:06:58.890 --> 00:07:00.525
问问我们自己为什么没有发生这种事？

00:07:00.525 --> 00:07:01.740
为什么我们，你知道，

00:07:01.740 --> 00:07:03.430
第一个想到的，比如，

00:07:03.430 --> 00:07:05.810
尝试预先训练整个模型，

00:07:05.810 --> 00:07:07.370
编码器和解码器，

00:07:07.370 --> 00:07:08.420
输出和一切。

00:07:08.420 --> 00:07:12.740
嗯，我认为部分原因是NLP需要很多不同类型的推理。

00:07:12.740 --> 00:07:14.420
你已经看过很多了。

00:07:14.420 --> 00:07:18.290
你有一些逻辑推理，比如这个房间里有550个人，

00:07:18.290 --> 00:07:20.300
25离开，房间里还有人吗？

00:07:20.300 --> 00:07:22.790
你逻辑上可以回答这个问题，

00:07:22.790 --> 00:07:25.930
你有很多不同种类的语言和情感推理，

00:07:25.930 --> 00:07:27.470
情绪分析，你知道，

00:07:27.470 --> 00:07:30.140
这是一部典型的尼古拉斯·凯奇电影，然后你需要知道

00:07:30.140 --> 00:07:33.590
可能是负面评论，除非你喜欢尼古拉斯凯奇的电影。

00:07:33.590 --> 00:07:36.470
嗯，没有判断力。而且，呃，

00:07:36.470 --> 00:07:38.180
你知道，视觉推理的类型等等。

00:07:38.180 --> 00:07:41.450
所以我想部分是因为开始感觉到的复杂性，

00:07:41.450 --> 00:07:46.580
并没有取得太大的进展，时不时地把它分开。

00:07:46.580 --> 00:07:50.675
我认为在某些情况下，人工地将这些任务分开，

00:07:50.675 --> 00:07:52.340
就像你命名了实体识别，

00:07:52.340 --> 00:07:55.795
词性标注、语义角色标注等。

00:07:55.795 --> 00:07:58.560
而且，在某些方面-听起来有点难听，但是，

00:07:58.560 --> 00:07:59.990
你知道，当时这很有道理，

00:07:59.990 --> 00:08:02.540
它让我们在社区里取得了很大的进步，

00:08:02.540 --> 00:08:04.850
但基本上我们开始追求这些基准，

00:08:04.850 --> 00:08:06.290
所有这些不同的社区，

00:08:06.290 --> 00:08:08.610
开始以自己的方式出发。

00:08:08.610 --> 00:08:10.320
我们甚至有一些社区说，

00:08:10.320 --> 00:08:11.950
“我们一般都会回答问题，

00:08:11.950 --> 00:08:14.990
还有一些关于回答一般问题的讲习班，当我问的时候，

00:08:14.990 --> 00:08:18.350
嗯，组织者，“我能问一下你的模特，这条推特有什么感想吗？”

00:08:18.350 --> 00:08:21.240
他们说，“不，那是情绪分析。去另一个车间。

00:08:21.240 --> 00:08:22.515
就在楼下，大厅那边。”

00:08:22.515 --> 00:08:24.270
但我想，“这是个问题。

00:08:24.270 --> 00:08:27.335
为什么你不能在回答问题的一般研讨会上回答呢？”

00:08:27.335 --> 00:08:29.940
很多人说，

00:08:29.940 --> 00:08:31.540
“好吧，如果你想做更一般的事情，

00:08:31.540 --> 00:08:33.860
它必须是一种无监督的，有点，

00:08:33.860 --> 00:08:36.695
任务和，将不监视该功能。”

00:08:36.695 --> 00:08:40.490
我不认为全国人民党会完全无人监督，

00:08:40.490 --> 00:08:42.830
我们不会解决的，呃，完全没有监督，

00:08:42.830 --> 00:08:45.410
因为最终，语言对人有很大的监督作用，

00:08:45.410 --> 00:08:49.020
嗯，还有，嗯，我想，对于系统也是。

00:08:49.020 --> 00:08:52.620
呃，你不会的，你知道，

00:08:52.620 --> 00:08:54.600
如果你有-有个孩子在丛林里，

00:08:54.600 --> 00:08:57.290
它本身可能会发展出一个相当好的视觉皮层，

00:08:57.290 --> 00:08:59.365
但它不会独自开发语言。

00:08:59.365 --> 00:09:01.230
然后-然后，就像，

00:09:01.230 --> 00:09:03.720
我想如果你允许人工智能彼此交谈，

00:09:03.720 --> 00:09:06.200
对于他们来说，试图提出

00:09:06.200 --> 00:09:09.140
像人类一样，通信协议效率低下，

00:09:09.140 --> 00:09:13.970
语言的顺序处理，因为算法和计算机可以，

00:09:13.970 --> 00:09:16.070
如果没有对人类语言的监督，

00:09:16.070 --> 00:09:19.455
他们可以用更有效的方式彼此交流。

00:09:19.455 --> 00:09:21.055
所以我觉得很清楚，

00:09:21.055 --> 00:09:24.490
我们需要很多的监督，呃，在国家公园。

00:09:24.490 --> 00:09:27.840
所以基本上，所有这些都让我们，呃，

00:09:27.840 --> 00:09:34.340
尝试为许多不同的NLP任务考虑一个统一的多任务模型。

00:09:34.340 --> 00:09:36.515
顺便问一下，如果你有什么问题，请举手。

00:09:36.515 --> 00:09:39.110
好吧，让我们让这个非常互动。

00:09:39.110 --> 00:09:42.555
嗯，基本上，我们想要这个统一的模型，呃，

00:09:42.555 --> 00:09:45.570
决定如何转移知识，

00:09:45.570 --> 00:09:47.885
嗯，而且不是手动分配的。

00:09:47.885 --> 00:09:49.280
就像大多数情况一样，

00:09:49.280 --> 00:09:50.870
当你分配你的项目时，你会说，“哦，

00:09:50.870 --> 00:09:55.030
我知道语音标记的命名实体识别部分互相帮助。

00:09:55.030 --> 00:09:56.870
因为一旦你知道什么是名词，

00:09:56.870 --> 00:10:00.730
那么它更有可能也是一个命名实体。”

00:10:00.730 --> 00:10:05.090
在这种情况下，我们希望基本上允许单一的统一模型

00:10:05.090 --> 00:10:09.890
了解如何进行领域适应以及如何分享权重，

00:10:09.890 --> 00:10:12.650
希望这会带来很多，

00:10:12.650 --> 00:10:15.935
嗯，转移学习和零镜头学习能力。

00:10:15.935 --> 00:10:19.100
我也认为如果我们能做到这一点，

00:10:19.100 --> 00:10:23.265
实现单一FA-单一统一多任务模型的硬目标，

00:10:23.265 --> 00:10:27.140
然后我们就可以轻松地适应

00:10:27.140 --> 00:10:31.085
新的任务，我们也可以更快地在生产中部署它。

00:10:31.085 --> 00:10:32.405
如果现在你想建造

00:10:32.405 --> 00:10:35.570
一个小松鼠探测器，把它连接到你的洒水系统上，

00:10:35.570 --> 00:10:37.895
你可以下载一些现成的软件，

00:10:37.895 --> 00:10:40.200
基本上，这是可行的。

00:10:40.200 --> 00:10:42.170
如果你试着这么做，情况就不是这样了。

00:10:42.170 --> 00:10:44.390
一个相当复杂的语言项目

00:10:44.390 --> 00:10:46.955
想要翻译成某种全新的语言，或者，

00:10:46.955 --> 00:10:50.240
你知道，先分析一些网站，然后再做其他的事情。

00:10:50.240 --> 00:10:51.890
所以，呃，你也，

00:10:51.890 --> 00:10:56.370
当您实际尝试部署和使用这些工具和公司时，

00:10:56.370 --> 00:10:59.075
你会发现有很多不同的群体。

00:10:59.075 --> 00:11:00.200
有搜索组，

00:11:00.200 --> 00:11:01.310
还有聊天机器人团队，

00:11:01.310 --> 00:11:02.540
以及翻译团队，

00:11:02.540 --> 00:11:05.930
还有，呃，还有社会情绪分析小组，

00:11:05.930 --> 00:11:07.100
他们都使用不同的型号，

00:11:07.100 --> 00:11:08.390
他们都部署了不同的模型，

00:11:08.390 --> 00:11:10.850
他们都必须在

00:11:10.850 --> 00:11:15.150
人工智能模型的核心或围绕着这个核心。

00:11:15.150 --> 00:11:18.240
所以基本上，嗯，最后，

00:11:18.240 --> 00:11:20.435
这是，某种程度上，我们和这只狗的关系。

00:11:20.435 --> 00:11:22.170
我认为一旦我们有了这个统一的模型，

00:11:22.170 --> 00:11:24.380
这也是实现

00:11:24.380 --> 00:11:26.870
然后不断学习这个，只有一个模型

00:11:26.870 --> 00:11:28.880
随着时间的推移变得越来越好，然后开始

00:11:28.880 --> 00:11:32.030
以捕捉越来越多的语言复杂性。

00:11:32.030 --> 00:11:33.980
好吧，有什么问题吗？

00:11:33.980 --> 00:11:37.560
高水平的动机？

00:11:41.700 --> 00:11:44.860
好吧。那么，呃，

00:11:44.860 --> 00:11:48.370
这是一个问题，我们如何真正做到这一点？

00:11:48.370 --> 00:11:52.135
然后我们——我先坐下来，看着，

00:11:52.135 --> 00:11:56.560
您可能经历的所有任务的一般格式

00:11:56.560 --> 00:11:58.510
这个类和那个NLP在

00:11:58.510 --> 00:12:01.000
将军和我认为他们可以大致分类，

00:12:01.000 --> 00:12:03.100
可分为三类。

00:12:03.100 --> 00:12:04.900
序列标记，你已经知道了。

00:12:04.900 --> 00:12:07.840
像NER或方面特定的情绪或

00:12:07.840 --> 00:12:12.250
一个特定的上下文，如果一个词是正的或是负的，我们要进行分类。

00:12:12.250 --> 00:12:14.380
然后是文本分类，

00:12:14.380 --> 00:12:17.290
整个文本只有一个标签

00:12:17.290 --> 00:12:20.335
然后对序列进行很多不同的排序，

00:12:20.335 --> 00:12:23.575
问题就在于此，我个人很喜欢，

00:12:23.575 --> 00:12:27.490
这三个特别的任务：机器翻译，总结，回答问题。

00:12:27.490 --> 00:12:31.450
因为它们很有用，你不用向别人解释，

00:12:31.450 --> 00:12:34.195
“哦，但是为什么您需要语义角色labeller或parser？”

00:12:34.195 --> 00:12:36.490
如果你是外行，你知道，

00:12:36.490 --> 00:12:38.620
在互联网上，你马上就能理解为什么

00:12:38.620 --> 00:12:41.140
有助于做总结，回答问题，

00:12:41.140 --> 00:12:43.240
或翻译和改进

00:12:43.240 --> 00:12:46.840
这些任务可以立即转化为更好的产品，

00:12:46.840 --> 00:12:51.430
嗯，人们能够用语言更好、更有效地交流。

00:12:51.430 --> 00:12:57.400
所以，这种分析让我们想到，

00:12:57.400 --> 00:13:01.030
关于这些，我称之为NLP的三个等价的超任务。

00:13:01.030 --> 00:13:03.910
嗯，基本上他们是

00:13:03.910 --> 00:13:07.780
语言建模，问答-问答和对话系统。

00:13:07.780 --> 00:13:11.410
语言建模，基本上是为了预测下一个单词，

00:13:11.410 --> 00:13:12.430
你已经在努力了。

00:13:12.430 --> 00:13:18.775
嗯，而且通常这几天只是用来重新订票或者基本上是为了预先培训。

00:13:18.775 --> 00:13:22.645
但如果你问我一个问题，然后试着预测接下来的几个词，

00:13:22.645 --> 00:13:25.435
这也是语言建模

00:13:25.435 --> 00:13:28.810
如果你能在一个问题后预测接下来的几个词，比如，

00:13:28.810 --> 00:13:32.350
句子中的命名实体是什么，然后你就生成了，

00:13:32.350 --> 00:13:34.120
德累斯顿是个地方，

00:13:34.120 --> 00:13:36.430
理查德是个什么都不是的人。

00:13:36.430 --> 00:13:41.140
嗯，然后你可以把几乎所有这些任务都转换成语言建模。

00:13:41.140 --> 00:13:42.580
呃，同样的问题回答，

00:13:42.580 --> 00:13:44.080
你可以问任何问题，

00:13:44.080 --> 00:13:45.430
翻译是什么？

00:13:45.430 --> 00:13:48.115
总结是什么，嗯，等等，

00:13:48.115 --> 00:13:50.770
而现在的对话有点棘手，因为

00:13:50.770 --> 00:13:55.930
没有很好的对话数据集，很多时候你想要一些互动，

00:13:55.930 --> 00:14:00.010
您必须运行用户研究，并且大多数现有的NLP任务

00:14:00.010 --> 00:14:04.360
基本上是很短的一步对话，比如什么是命名实体标签，

00:14:04.360 --> 00:14:05.560
你给他们，就这样。

00:14:05.560 --> 00:14:09.850
所以有点过分了，因为我们基本上是一致的，

00:14:09.850 --> 00:14:13.525
呃，回答问题是我们的主要形式。

00:14:13.525 --> 00:14:18.355
下面是我们10项不同任务的概述，

00:14:18.355 --> 00:14:21.610
嗯，我们把所有的问题都当作回答问题。

00:14:21.610 --> 00:14:25.120
这些都是训练，

00:14:25.120 --> 00:14:27.700
培训数据集的格式，

00:14:27.700 --> 00:14:30.880
最终也是我们的方式

00:14:30.880 --> 00:14:35.530
测试集，你基本上可以看到每一个任务，

00:14:35.530 --> 00:14:38.605
您有一个上下文作为某种文档。

00:14:38.605 --> 00:14:39.700
可能是维基百科的文章，

00:14:39.700 --> 00:14:41.500
可能是推特，可能是更长的文档，

00:14:41.500 --> 00:14:45.550
不管怎样，你问一个关于它的问题，你想得到一个答案。

00:14:45.550 --> 00:14:49.090
实际上，我很好奇你是否能想到NLP中的任何任务

00:14:49.090 --> 00:14:52.795
不能用这种结构来表述。

00:14:52.795 --> 00:14:55.720
嗯，那么，我们来看看这些。

00:14:55.720 --> 00:14:57.865
嗯，第一种是标准的，

00:14:57.865 --> 00:15:00.145
呃，任务就是——你们现在都熟悉了。

00:15:00.145 --> 00:15:02.440
斯坦福问答小组。

00:15:02.440 --> 00:15:06.880
呃，答案基本上是上下文中的某个短语。

00:15:06.880 --> 00:15:12.265
但是，嗯，第二个是你绝大多数人看不到的东西，

00:15:12.265 --> 00:15:16.900
呃，一般的，呃，问答研讨会，也就是说，

00:15:16.900 --> 00:15:20.560
有一个单句的上下文，问翻译是什么意思

00:15:20.560 --> 00:15:25.090
英语译成德语，输出的也是一系列单词，但在这种情况下，

00:15:25.090 --> 00:15:26.500
我们在这里给它们涂上不同的颜色。

00:15:26.500 --> 00:15:31.870
嗯，这是蓝色的，因为这些词基本上不在上下文中，也不在

00:15:31.870 --> 00:15:35.110
这个问题，我们将生成它们

00:15:35.110 --> 00:15:39.280
用标准的SoftMax来回答这个问题。

00:15:39.280 --> 00:15:43.390
我们也可以问一下总结是什么，你可以看到那些

00:15:43.390 --> 00:15:47.290
在某些方面，有两种方法是人为的，使它们成为自然语言问题。

00:15:47.290 --> 00:15:51.250
你可以说翻译或总结，这就像

00:15:51.250 --> 00:15:56.140
网络中的一种任务标记，但实际上是这些任务的一半。

00:15:56.140 --> 00:16:02.305
这是有道理的，因为每个例子的问题都有不同的交流。

00:16:02.305 --> 00:16:06.040
这是自然语言推理，nli，呃，

00:16:06.040 --> 00:16:10.920
她还谈到了我们想问的问题，两个句子是否相互牵连，

00:16:10.920 --> 00:16:14.810
相互矛盾，或者他们之间有某种中立的关系。

00:16:14.810 --> 00:16:16.900
你已经看到很多感情用事了。

00:16:16.900 --> 00:16:18.580
这很重要。

00:16:18.580 --> 00:16:22.600
我们实际上问的是，这句话是正面的还是负面的，而不是情绪

00:16:22.600 --> 00:16:27.745
为什么重要的是你在这里看到绿色，

00:16:27.745 --> 00:16:30.760
这个答案实际上来自

00:16:30.760 --> 00:16:34.380
一个有问题的词，如果我们这样表述，

00:16:34.380 --> 00:16:39.330
我们最终可以在我们提出新问题的地方进行零镜头学习。

00:16:39.330 --> 00:16:44.155
以前从未要求过新的标签，在某些情况下，很神奇，

00:16:44.155 --> 00:16:46.180
它仍然有效，我们会，你知道，

00:16:46.180 --> 00:16:50.500
问阙-我们可以问这样的问题，这个故事是快乐还是悲伤，它仍然会

00:16:50.500 --> 00:16:52.120
尽管我们从未给出答案

00:16:52.120 --> 00:16:55.195
它是一组经过训练的快乐和悲伤的故事集。

00:16:55.195 --> 00:16:59.740
所以，这是一种零镜头分类

00:16:59.740 --> 00:17:02.230
有些情况下，如果你以某种方式提出问题

00:17:02.230 --> 00:17:05.270
答案是问题的一部分。

00:17:05.270 --> 00:17:08.340
然后我们在这里有语义角色标签。

00:17:08.340 --> 00:17:15.540
有什么经验，一个随机的奇怪的问题。

00:17:15.540 --> 00:17:18.450
然后我们有一个零镜头的关系提取谁是

00:17:18.450 --> 00:17:22.255
狼人循环的插图画家，

00:17:22.255 --> 00:17:24.580
我们还有一些对话状态跟踪。

00:17:24.580 --> 00:17:28.615
当前的状态是什么？在对话中，背景是什么？

00:17:28.615 --> 00:17:33.985
随着对话的发展，我们还有SQL，

00:17:33.985 --> 00:17:37.690
wiki SQL翻译任务，但不翻译为

00:17:37.690 --> 00:17:42.025
转换为SQL数据库查询的另一种自然语言。

00:17:42.025 --> 00:17:43.720
这实际上是一项非常有益的任务。

00:17:43.720 --> 00:17:47.830
你知道，有很多数据存储在数据库中。

00:17:47.830 --> 00:17:50.440
如果你不用问就可以进入

00:17:50.440 --> 00:17:53.380
知道如何编程SQL的人

00:17:53.380 --> 00:17:56.200
这些数据对很多人都是可用的，所以

00:17:56.200 --> 00:17:59.260
他们可以分析IT，喜欢业务分析等。

00:17:59.260 --> 00:18:02.740
然后在这里，winograd模式和回指解析。

00:18:02.740 --> 00:18:06.100
嗯，有些人把这种常识性推理叫做

00:18:06.100 --> 00:18:10.225
你知道，在这种情况下，大多只是回指决议试图理解。

00:18:10.225 --> 00:18:12.385
呃，什么——谁，你知道，

00:18:12.385 --> 00:18:15.550
呃，这个词就像是谁给予了帮助，

00:18:15.550 --> 00:18:19.030
是苏珊还是乔安妮，然后根据这个背景，

00:18:19.030 --> 00:18:22.900
你应该能够在这里一次又一次的发现，

00:18:22.900 --> 00:18:26.860
每个例子的问题都不同。好吧，是吗？

00:18:26.860 --> 00:18:29.890
当你测试它的时候——比如当你问的时候，

00:18:29.890 --> 00:18:31.795
这句话是肯定的还是否定的，

00:18:31.795 --> 00:18:35.290
有时会像[听不见]？

00:18:35.290 --> 00:18:37.765
好问题。所以，问题是当我问，

00:18:37.765 --> 00:18:40.510
这句话是肯定的还是否定的

00:18:40.510 --> 00:18:43.915
不小心切换到另一个任务，呃，

00:18:43.915 --> 00:18:47.110
我们实际上有一张幻灯片，答案是它非常擅长

00:18:47.110 --> 00:18:52.780
知道如何完成任务，以及从何处获得答案。

00:18:52.780 --> 00:18:56.860
嗯，是的，一旦我们看了模型，它们在几张幻灯片中会更有意义。

00:18:56.860 --> 00:18:58.555
还有其他问题吗？

00:18:58.555 --> 00:19:00.820
嗯，回答形式主义的问题？

00:19:00.820 --> 00:19:04.930
您是否也能够以问答格式制定文本生成？

00:19:04.930 --> 00:19:06.685
比如，给我讲个故事。

00:19:06.685 --> 00:19:10.195
好问题。我们能做文本生成吗？

00:19:10.195 --> 00:19:11.800
就像给我讲个故事，

00:19:11.800 --> 00:19:14.590
从一种随机的——或者在这种形式主义中。

00:19:14.590 --> 00:19:19.450
嗯，我们不把这当作一项任务，因为在很大程度上，这很难评估。

00:19:19.450 --> 00:19:22.120
它会告诉你一些随机的事情，然后这是一个好故事还是不好的，

00:19:22.120 --> 00:19:24.325
它符合语法吗，你必须想出很多，

00:19:24.325 --> 00:19:25.750
呃，有点，呃，

00:19:25.750 --> 00:19:28.420
我们实际正在做的评估指标

00:19:28.420 --> 00:19:31.330
一些对话系统，如果是对话，

00:19:31.330 --> 00:19:33.280
为什么——为什么它们是等效的，因为

00:19:33.280 --> 00:19:36.160
环境会不断发展，每次，呃，

00:19:36.160 --> 00:19:38.395
用户说了什么，呃，

00:19:38.395 --> 00:19:43.525
你基本上试着在对话中预测下一个答案。

00:19:43.525 --> 00:19:48.700
所以我认为你可以很容易地用它来产生文本。

00:19:48.700 --> 00:19:51.220
呃，你基本上只是问——像这样说，你知道，

00:19:51.220 --> 00:19:54.490
这个故事的好结局是什么，你可能会从类似的内容开始

00:19:54.490 --> 00:19:58.420
两个或三个单词，然后要求模型生成越来越多的单词，

00:19:58.420 --> 00:20:01.975
嗯，以这个网络的形式，我将在稍后描述。是啊？

00:20:01.975 --> 00:20:04.720
我在想，呃，你训练的时候

00:20:04.720 --> 00:20:07.795
它和你试图像一个新的任务一样研究。

00:20:07.795 --> 00:20:11.470
嗯，喜欢用更少的数据学习吗？

00:20:11.470 --> 00:20:14.320
这是一个令人惊奇的深思熟虑的问题

00:20:14.320 --> 00:20:16.930
这是非常重要的，我们会在上面放一堆幻灯片。

00:20:16.930 --> 00:20:20.980
所以也许我们会-我们会去-我们会继续，我们会解决这个问题，呃，

00:20:20.980 --> 00:20:25.075
在很多细节上，因为这就是我们为什么要这样做的原因，简短的回答是肯定的。

00:20:25.075 --> 00:20:27.855
但我们会了解更多细节。好吧。

00:20:27.855 --> 00:20:30.210
所以这基本上就是10个任务。

00:20:30.210 --> 00:20:33.970
嗯，这也是它的实际格式。

00:20:33.970 --> 00:20:35.890
所以如果你有问题，

00:20:35.890 --> 00:20:37.810
你可以用这种格式，呃，

00:20:37.810 --> 00:20:40.630
你只需要，呃，打开源代码并运行它，

00:20:40.630 --> 00:20:42.025
嗯，它会-它会起作用的。

00:20:42.025 --> 00:20:45.010
所以当你分析和思考我们在这里所做的事情时。

00:20:45.010 --> 00:20:47.680
在某些方面，我们接受了

00:20:47.680 --> 00:20:50.950
通常是在你的头脑中，但它不给模型。

00:20:50.950 --> 00:20:54.730
模型的输入x和输出y几乎都是

00:20:54.730 --> 00:21:00.760
被监视的系统，而实际上我们把任务包括在输入中，

00:21:00.760 --> 00:21:05.950
呃，在模型的输入集合中。所以你可以称之为元监督学习。

00:21:05.950 --> 00:21:08.260
所以这个问题，呃，

00:21:08.260 --> 00:21:11.140
是我们对这些不同任务的任务定义。

00:21:11.140 --> 00:21:13.570
这个模型必须弄清楚什么时候提问

00:21:13.570 --> 00:21:16.180
这样，它也可以自己决定何时

00:21:16.180 --> 00:21:21.565
从这些其他任务中转移知识，而Y又是答案。

00:21:21.565 --> 00:21:25.330
所以，在某些方面，它是元监督学习，我很兴奋

00:21:25.330 --> 00:21:29.560
因为一旦您允许将任务作为输入提供给模型，

00:21:29.560 --> 00:21:32.170
它可以自己决定如何进行

00:21:32.170 --> 00:21:35.020
解决那个特定的任务，现在你可以学习，

00:21:35.020 --> 00:21:36.835
嗯，很多更强大的型号。

00:21:36.835 --> 00:21:39.310
所以一旦我们有了数据集，

00:21:39.310 --> 00:21:42.265
我们想，“好吧，我们现在怎么解决这个问题？”

00:21:42.265 --> 00:21:43.960
最简单的方法就是你可以说，“好吧，

00:21:43.960 --> 00:21:45.010
我有一个很大的假设陈述，

00:21:45.010 --> 00:21:47.260
我在开始时有一个分类器，然后进行分类。

00:21:47.260 --> 00:21:49.225
如果这是机器翻译任务，

00:21:49.225 --> 00:21:51.015
然后运行我的机器翻译模型。”

00:21:51.015 --> 00:21:54.300
总的来说，在Python中，它仍然像一条大蟒蛇，

00:21:54.300 --> 00:21:56.430
嗯，用一堆if语句做模型，对吗？

00:21:56.430 --> 00:21:58.770
这不是我们的目标，因为那样我们就无法达到

00:21:58.770 --> 00:22:02.195
我们希望的转移学习和零射击能力。

00:22:02.195 --> 00:22:07.630
所以[噪音]我们想要想要的模型

00:22:07.630 --> 00:22:10.105
具备内部调整能力

00:22:10.105 --> 00:22:13.880
去完成这些不同的任务，自己做决定。

00:22:15.360 --> 00:22:18.490
基本上，所有这些考虑

00:22:18.490 --> 00:22:20.620
这些想法让我们，呃，找到了这个模型。

00:22:20.620 --> 00:22:22.120
所以在我走之前，呃，

00:22:22.120 --> 00:22:23.455
更详细一点。

00:22:23.455 --> 00:22:25.825
我想给你一个高层次的概述。

00:22:25.825 --> 00:22:27.925
同样，从上下文开始。

00:22:27.925 --> 00:22:30.715
嗯，你开始-你问一个问题，呃，

00:22:30.715 --> 00:22:33.700
上下文文档，然后我们将生成，

00:22:33.700 --> 00:22:38.560
嗯，一次回答一个词，要么指向上下文，

00:22:38.560 --> 00:22:40.045
你已经有了指针，对吧？

00:22:40.045 --> 00:22:44.035
指针网络，所有这些？伟大的。嗯，指着一个疑问词，

00:22:44.035 --> 00:22:48.190
或者使用标准的SoftMax分类器从外部词汇表中选择一个单词。

00:22:48.190 --> 00:22:52.630
嗯，我们会有一个指针开关机构

00:22:52.630 --> 00:22:57.415
选择这三种生成机制中每一种的权重（噪声）。

00:22:57.415 --> 00:23:00.760
所以，呃，让我们深入研究一下这个模型。

00:23:00.760 --> 00:23:04.600
幸运的是，在某种程度上，它只是在发挥最好的作用，

00:23:04.600 --> 00:23:09.160
最先进的技术，并以某种方式组合在一起，

00:23:09.160 --> 00:23:11.560
呃，那-概括得很好。

00:23:11.560 --> 00:23:14.140
呃，你可以看看decalp.com上的所有代码，

00:23:14.140 --> 00:23:16.870
[噪音]有几千个，呃，

00:23:16.870 --> 00:23:20.400
星星和叉子之类的东西结合在一起，

00:23:20.400 --> 00:23:21.795
你可以，你知道，

00:23:21.795 --> 00:23:24.180
基本上运行所有的东西，呃，

00:23:24.180 --> 00:23:29.755
在这个实验中，只有一个命令。

00:23:29.755 --> 00:23:33.610
它会加倍，你得到所有的数据集和所有的东西，并且运行所有的东西，

00:23:33.610 --> 00:23:36.340
你真的可以探索它的样子，但是让我们-让我们

00:23:36.340 --> 00:23:39.370
深入了解一下这个模型告诉我们的细节。

00:23:39.370 --> 00:23:41.065
在某些方面，它只是需要

00:23:41.065 --> 00:23:43.870
所有来自深度学习[噪音]NLP的最佳成分，

00:23:43.870 --> 00:23:48.490
其中大部分你已经了解并以合理的方式将它们组合在一起。

00:23:48.490 --> 00:23:50.470
所以我们从固定手套嵌入开始。

00:23:50.470 --> 00:23:52.630
最终，我们会-我们更新，呃，

00:23:52.630 --> 00:23:54.730
嵌入到凹形的嵌入，呃，

00:23:54.730 --> 00:23:57.715
如果你把它们更新到bert embeddings，可能会更好。

00:23:57.715 --> 00:24:00.820
嗯，但在某种程度上，我们必须继续做其他的事情。

00:24:00.820 --> 00:24:03.460
但是基本上，你有一组固定的词向量，

00:24:03.460 --> 00:24:05.860
这一点很重要，因为在其中一些方面，

00:24:05.860 --> 00:24:08.545
嗯，数据集，它们比其他的小得多。

00:24:08.545 --> 00:24:10.360
呃，你从队里知道的，

00:24:10.360 --> 00:24:12.580
如果你真的反向传播到矢量这个词中，

00:24:12.580 --> 00:24:14.680
你只是在训练数据集上做得非常非常好，

00:24:14.680 --> 00:24:18.310
但是你不会因为大多数[噪音]文本而概括，

00:24:18.310 --> 00:24:21.430
呃，测试文档中会包含你以前从未见过的单词。

00:24:21.430 --> 00:24:24.640
所以如果你在训练过程中改变了所有的矢量词，

00:24:24.640 --> 00:24:28.300
它不会——它在测试时不会很好地工作，也不会概括那些看不见的词。

00:24:28.300 --> 00:24:30.355
所以，呃，固定手套嵌入，

00:24:30.355 --> 00:24:31.990
如果你没有矢量词，呃，

00:24:31.990 --> 00:24:35.140
对于看不见的单词，我们也有字符n-gram嵌入。

00:24:35.140 --> 00:24:37.870
然后我们把它们通过一个简单的线性层，

00:24:37.870 --> 00:24:39.250
然后我们有一个共享的，呃，

00:24:39.250 --> 00:24:42.535
具有跳过连接的双向LSTM。

00:24:42.535 --> 00:24:46.255
所以，呃，这是一个很深的层次，所以你跳到更高的层次，

00:24:46.255 --> 00:24:49.090
它在上下文和问题之间共享。

00:24:49.090 --> 00:24:51.850
所以它们有基本相同的（噪音）权重集。

00:24:51.850 --> 00:24:56.440
[噪音]那么，呃，我们有一个共同关注层。

00:24:56.440 --> 00:24:58.840
呃，我们基本上只有外部产品，

00:24:58.840 --> 00:25:03.400
在这两个序列的所有隐藏状态之间，

00:25:03.400 --> 00:25:06.070
还有，跳过连接，呃，

00:25:06.070 --> 00:25:08.050
为了规避，呃，那些。

00:25:08.050 --> 00:25:11.200
所以现在你有了某种背景或问题依赖性，呃，

00:25:11.200 --> 00:25:15.460
上下文表示[噪音]或-或该上下文的表示。

00:25:15.460 --> 00:25:18.970
[噪音]嗯，然后我们把这些输入变压器层，

00:25:18.970 --> 00:25:23.575
嗯，实际上我们试着用变压器来做所有的事情，

00:25:23.575 --> 00:25:25.765
没有任何LSTM或其他。

00:25:25.765 --> 00:25:28.735
呃，不幸的是，变压器层仍然，呃，

00:25:28.735 --> 00:25:32.590
非常，呃，挑剔，很难优化，

00:25:32.590 --> 00:25:35.020
在学习率方面有很多诡计，

00:25:35.020 --> 00:25:38.515
我们不能让他们表现得很好，

00:25:38.515 --> 00:25:41.755
嗯，在这10个不同的任务上。

00:25:41.755 --> 00:25:45.760
呃，[噪音]有时候你有一个变压器层，一个变压器网络，

00:25:45.760 --> 00:25:46.930
在一项任务中效果很好，

00:25:46.930 --> 00:25:49.330
但唯一一个运行良好的变压器网络

00:25:49.330 --> 00:25:51.895
第二个任务有一半的层次。

00:25:51.895 --> 00:25:55.150
一旦你试图拥有一个具有相同层数的网络，

00:25:55.150 --> 00:25:57.715
这两项任务都不管用了。

00:25:57.715 --> 00:26:00.640
嗯，所以-是的，不幸的是和他们一样好

00:26:00.640 --> 00:26:03.579
因为它们在GPU中很容易瘫痪，

00:26:03.579 --> 00:26:05.110
呃，它们还不够结实，

00:26:05.110 --> 00:26:06.820
呃，用来做这个。

00:26:06.820 --> 00:26:09.280
[噪音]所以我们必须有这些LSTM，

00:26:09.280 --> 00:26:11.200
呃，变压器层前后。

00:26:11.200 --> 00:26:15.295
[噪音]然后我们基本上有一种标准的自回归，呃，

00:26:15.295 --> 00:26:17.770
最后一个状态的解码器，

00:26:17.770 --> 00:26:19.720
嗯，我们生成下一个词。

00:26:19.720 --> 00:26:22.090
然后我们有三个指针机制。

00:26:22.090 --> 00:26:24.460
呃，它们非常类似于指针的NE-机制，你已经知道了。

00:26:24.460 --> 00:26:28.405
但是现在在这些非常上下文化的表达之上，呃，

00:26:28.405 --> 00:26:30.580
在编码器的末端，呃，

00:26:30.580 --> 00:26:33.640
它基本上学会了用任何一个点来提问，

00:26:33.640 --> 00:26:35.770
基于隐藏状态的上下文词，

00:26:35.770 --> 00:26:38.125
或者还有一个标准的SoftMax，

00:26:38.125 --> 00:26:41.395
然后我们基本上有一个加权和，

00:26:41.395 --> 00:26:45.490
凸和，这三种不同分布的输出词。

00:26:45.490 --> 00:26:48.115
[噪音]好的。

00:26:48.115 --> 00:26:52.690
所以我认为这些大部分都是你已经看到的标准组件，

00:26:52.690 --> 00:26:54.610
呃，对你来说-已经看到了他们所有的细节。

00:26:54.610 --> 00:26:55.944
但是如果你有任何问题，

00:26:55.944 --> 00:26:58.690
嗯，关于我们如何组合？是啊？

00:26:58.690 --> 00:27:02.920
[噪音]所以输出-输出必须是一个字。

00:27:02.920 --> 00:27:06.610
这是正确的。输出必须是一个单词，并且始终是上下文中的一个单词，

00:27:06.610 --> 00:27:08.470
问题中的一个词或SoftMax中的一个词。

00:27:08.470 --> 00:27:11.050
[噪音]

00:27:11.050 --> 00:27:15.610
那就是-数据预处理，我想每个任务都是不同的。

00:27:15.610 --> 00:27:18.220
所以每个任务的数据预处理是不同的，

00:27:18.220 --> 00:27:20.950
但我们基本上要把所有的东西都正常化

00:27:20.950 --> 00:27:23.710
同样的标记化技术，以及所有这些。[噪音]

00:27:23.710 --> 00:27:29.770
嗯，那么编码中的双箭头是否只是表示有一个双向的？

00:27:29.770 --> 00:27:30.125
是啊。

00:27:30.125 --> 00:27:30.775
可以。

00:27:30.775 --> 00:27:32.395
是啊。但是双箭头，

00:27:32.395 --> 00:27:34.000
呃，这里只是双向的。

00:27:34.000 --> 00:27:38.080
所以从左到右和从右到左是LSTM。好吧。

00:27:38.080 --> 00:27:41.050
那么，我们在使用什么数据集？

00:27:41.050 --> 00:27:44.125
嗯，我刚开始提到那是个大头痛。

00:27:44.125 --> 00:27:46.540
嗯，我们肯定想包括很多序列

00:27:46.540 --> 00:27:49.720
我们觉得顺序任务非常，

00:27:49.720 --> 00:27:54.055
嗯，有点高水平，我-马上就有用了，呃，

00:27:54.055 --> 00:27:57.955
在某些方面，这也表明了

00:27:57.955 --> 00:28:03.310
现在，您不必在一些中间表示上做太多的工作，

00:28:03.310 --> 00:28:05.275
呃，在NLP里。

00:28:05.275 --> 00:28:09.490
嗯，你可以直接去做真正用户可能关心的最终任务，

00:28:09.490 --> 00:28:12.340
然后有这些端到端的训练系统，

00:28:12.340 --> 00:28:14.695
嗯，那真的很好。

00:28:14.695 --> 00:28:17.290
而且，呃，我自己在解析方面做了很多工作。

00:28:17.290 --> 00:28:18.415
所以我不想，你知道，

00:28:18.415 --> 00:28:19.540
说我们-我们不需要它。

00:28:19.540 --> 00:28:21.580
当然还有一些任务是你需要的，

00:28:21.580 --> 00:28:26.095
但是你可以直接去翻译或总结，这有点奇怪

00:28:26.095 --> 00:28:28.870
没有中间陈述

00:28:28.870 --> 00:28:32.035
有点特别的手工设计。

00:28:32.035 --> 00:28:36.310
嗯，所以我们有这三个非常有趣的任务。

00:28:36.310 --> 00:28:38.380
答疑、机器翻译、总结。

00:28:38.380 --> 00:28:41.260
它们实际上也有三个最大的数据集，

00:28:41.260 --> 00:28:42.820
呃，所有这些。

00:28:42.820 --> 00:28:46.960
嗯，然后我们有了NLI，基本上，嗯，

00:28:46.960 --> 00:28:52.195
所有这些，呃，10个数据集[噪音]是，呃，

00:28:52.195 --> 00:28:56.875
公开提供，嗯，在一些情况下，特别是翻译，

00:28:56.875 --> 00:29:01.030
你可以找到更大的，呃，翻译数据集，

00:29:01.030 --> 00:29:03.790
但我们也试着保留它，呃，

00:29:03.790 --> 00:29:08.530
到了一个普通人不在大公司工作的规模

00:29:08.530 --> 00:29:13.540
GPU基础设施仍然可以运行实验，[噪音]呃，他们自己。

00:29:13.540 --> 00:29:16.630
所以大学和人们，呃，仍然可以运行它。

00:29:16.630 --> 00:29:18.985
基本上，如果你只有一个GPU，

00:29:18.985 --> 00:29:21.385
大概要一个星期左右，呃，

00:29:21.385 --> 00:29:23.680
进行实验。

00:29:23.680 --> 00:29:26.635
如果一台大型AWS机器上有多个GPU，

00:29:26.635 --> 00:29:29.560
你可以在一两天内做一个实验。

00:29:29.560 --> 00:29:31.750
尤其是翻译，对吧，

00:29:31.750 --> 00:29:35.605
你可以得到比我更多的数据。

00:29:35.605 --> 00:29:38.470
每一个，呃，

00:29:38.470 --> 00:29:42.100
社区、数据集和任务都有自己的度量标准。

00:29:42.100 --> 00:29:44.050
我们一开始就试图

00:29:44.050 --> 00:29:46.330
关于我们应该如何做，我们进行了很多讨论

00:29:46.330 --> 00:29:49.870
定义此项目的成功度量。

00:29:49.870 --> 00:29:51.565
呃，这没道理，呃，

00:29:51.565 --> 00:29:55.300
对于基本上所有不同的任务，要有一个标准化的F1分数，

00:29:55.300 --> 00:29:57.310
但我们基本上意识到

00:29:57.310 --> 00:30:00.250
出于某种原因，这些不同的社区有不同的度量标准。

00:30:00.250 --> 00:30:05.005
不幸的是，理论上至少所有这些指标都是0-100。

00:30:05.005 --> 00:30:07.405
当然，在实践中，你很少看到，

00:30:07.405 --> 00:30:10.270
一个100的翻译系统，

00:30:10.270 --> 00:30:12.280
甚至是高达90秒的布鲁分数，

00:30:12.280 --> 00:30:14.935
呃，或者这些真的，真的很高的胭脂分数。

00:30:14.935 --> 00:30:18.550
但是，你知道，理论上，它们从0到100，所以，呃，

00:30:18.550 --> 00:30:24.039
我们基本上保持了这些社区的不同评估指标的完整性，

00:30:24.039 --> 00:30:26.440
我们只是说要总结一下。

00:30:26.440 --> 00:30:29.380
当我们第一次谈到这个的时候，

00:30:29.380 --> 00:30:31.150
我们进行了很多讨论，

00:30:31.150 --> 00:30:32.890
呃，和其他人一起，比如，哦，

00:30:32.890 --> 00:30:35.530
但是翻译更重要，因为它非常重要

00:30:35.530 --> 00:30:38.245
比你现在更大更有用的任务，

00:30:38.245 --> 00:30:40.630
你知道，愚蠢的像代词解析的Winograd模式

00:30:40.630 --> 00:30:43.150
只有几百个训练样本。

00:30:43.150 --> 00:30:45.730
所以你应该有更多的加权翻译

00:30:45.730 --> 00:30:48.310
然后从字面上说五个问题，后来有人说，

00:30:48.310 --> 00:30:50.140
“你为什么不更重视代词的分辨力呢？

00:30:50.140 --> 00:30:54.370
这是一项非常艰巨的任务，它捕捉到了一些常识性的推理，你知道，

00:30:54.370 --> 00:30:56.590
语言和语义的复杂性，

00:30:56.590 --> 00:31:00.340
与所有这些不同，比如统计模式匹配（噪声），你在翻译中做的。”

00:31:00.340 --> 00:31:03.190
我就好像，我曾经和那个家伙[笑声]聊过，

00:31:03.190 --> 00:31:04.510
希望最后，

00:31:04.510 --> 00:31:08.050
我们都同意把它们总结起来是合理的，

00:31:08.050 --> 00:31:13.645
当然，当你在这里进行实验的时候，你也必须解决这个问题。

00:31:13.645 --> 00:31:17.845
呃，你在机器学习中有很多复杂性，

00:31:17.845 --> 00:31:21.625
你知道，很少有人谈论的东西，比如有非常偏斜的分布。

00:31:21.625 --> 00:31:24.610
所以你有翻译，呃，

00:31:24.610 --> 00:31:26.620
成千上万的例子，

00:31:26.620 --> 00:31:27.730
你有Winograd模式，

00:31:27.730 --> 00:31:29.920
嗯，只有几百个。

00:31:29.920 --> 00:31:34.750
如何训练这样的数据集，使您不只是完全忽略较小的数据集。

00:31:34.750 --> 00:31:38.350
嗯，那么我们来看看优化策略，

00:31:38.350 --> 00:31:42.010
嗯，那只狗花了几个月的时间。

00:31:42.010 --> 00:31:45.310
但我首先想给你第一组实验。

00:31:45.310 --> 00:31:46.960
你可以从所有的数字中看到，

00:31:46.960 --> 00:31:48.565
有很多实验，呃，

00:31:48.565 --> 00:31:50.695
我们跑过去就是为了达到这个目的，

00:31:50.695 --> 00:31:52.960
所以我们会非常小心地通过这个。

00:31:52.960 --> 00:31:56.110
我想希望你也能得到一些关于消融的想法，

00:31:56.110 --> 00:31:59.800
或者你可能想在你的，嗯，

00:31:59.800 --> 00:32:01.210
在你的实验和

00:32:01.210 --> 00:32:03.670
呃，问题-最终-最终项目。

00:32:03.670 --> 00:32:05.290
我们在这里看到什么？

00:32:05.290 --> 00:32:07.405
所以基本上，呃，在左边，

00:32:07.405 --> 00:32:08.770
我们有单一的任务执行。

00:32:08.770 --> 00:32:13.475
在这里，每一个数字都来自不同的训练模型，

00:32:13.475 --> 00:32:16.330
嗯，单独完成一项任务。

00:32:16.330 --> 00:32:22.540
嗯，每一行-这里的每一列都是相同的架构，呃，

00:32:22.540 --> 00:32:23.935
右边的[噪音]

00:32:23.935 --> 00:32:25.435
我们基本上，呃，

00:32:25.435 --> 00:32:31.165
对于每一列，基本上都是相同的体系结构和相同的精确模型。

00:32:31.165 --> 00:32:34.675
所以这里，我们有四种不同的型号，这里，呃，

00:32:34.675 --> 00:32:37.165
我们有40种不同的型号，

00:32:37.165 --> 00:32:40.105
每一列都是相同的架构。

00:32:40.105 --> 00:32:41.725
所以最简单的，呃，

00:32:41.725 --> 00:32:44.620
这里的第一列只是一个标准的序列

00:32:44.620 --> 00:32:48.280
只有很少的铃铛、口哨和一些指针，

00:32:48.280 --> 00:32:49.960
但没有什么大不了的。

00:32:49.960 --> 00:32:51.270
很深，你知道，

00:32:51.270 --> 00:32:53.550
堆栈双向LSTM跳过连接，

00:32:53.550 --> 00:32:57.780
对于序列到序列模型，所有标准的好调优的东西。

00:32:57.780 --> 00:33:00.945
然后，我们增加了自我关注。

00:33:00.945 --> 00:33:03.405
嗯，这个-这种，呃，

00:33:03.405 --> 00:33:06.310
基本上，呃，变压器层。

00:33:06.310 --> 00:33:08.110
[噪音]那么我们的共同关注层

00:33:08.110 --> 00:33:10.225
我们一开始提到的外部产品，

00:33:10.225 --> 00:33:12.715
然后我们还添加了问题指针。

00:33:12.715 --> 00:33:17.060
所以有能力指出问题中的一个词。

00:33:18.330 --> 00:33:21.670
好吧。这张桌子有问题吗？

00:33:21.670 --> 00:33:23.320
我们将深入研究一些细节。

00:33:23.320 --> 00:33:25.090
嗯，好吧。好吧，我们会深入研究

00:33:25.090 --> 00:33:27.760
先是细节问题，然后再考虑一些问题。

00:33:27.760 --> 00:33:29.830
我们来分析一下，呃，

00:33:29.830 --> 00:33:32.740
这张桌子上发生了什么因为有很多数字，呃，

00:33:32.740 --> 00:33:36.510
你真的想仔细分析和区分。

00:33:36.510 --> 00:33:37.890
我想我的第一个，呃，

00:33:37.890 --> 00:33:40.590
观察发现，哇，我们可以有一个单一的建筑。

00:33:40.590 --> 00:33:43.170
就像，即使这样，这也不是我们想要的，对吧？

00:33:43.170 --> 00:33:44.535
我们想要一个单一的型号。

00:33:44.535 --> 00:33:46.140
但即使是这样的表演，哇，

00:33:46.140 --> 00:33:51.429
你可以有一个非常好的、随机的单一体系结构，

00:33:51.429 --> 00:33:53.920
在某些情况下，它实际上得到了最先进的结果。

00:33:53.920 --> 00:33:56.020
例如，wiki-sql，

00:33:56.020 --> 00:33:59.200
这个建筑有最好的模型

00:33:59.200 --> 00:34:02.245
要将自然语言的英语问题转换为SQL查询，

00:34:02.245 --> 00:34:05.530
这对我们来说是个惊喜，因为它是第九个数据集。

00:34:05.530 --> 00:34:08.950
这对我们来说真的不是优先考虑的事情，当我们设计的时候

00:34:08.950 --> 00:34:12.970
该模型考虑了如何生成单词和指针机制等。

00:34:12.970 --> 00:34:16.390
我们有点像SQL单词的标准上下文

00:34:16.390 --> 00:34:19.990
我们问这个问题，SQL的翻译是什么，然后，呃，

00:34:19.990 --> 00:34:24.790
让我们有点惊讶的是，这种特殊的建筑有着最先进的技术，

00:34:24.790 --> 00:34:27.820
关于SQL的生成和这类社区中的一群人

00:34:27.820 --> 00:34:30.865
因为它有最先进的技术，所以能更快地把它捡起来。

00:34:30.865 --> 00:34:32.590
那是-呃，不幸的是，

00:34:32.590 --> 00:34:34.915
它没有那么多其他最先进的数字，呃，

00:34:34.915 --> 00:34:36.400
这就是为什么它更难，呃，

00:34:36.400 --> 00:34:37.750
这实际上是一项更艰巨的任务。

00:34:37.750 --> 00:34:40.195
你也观察到，

00:34:40.195 --> 00:34:42.325
嗯，在一些情况下，呃，

00:34:42.325 --> 00:34:44.080
使用多任务模型，

00:34:44.080 --> 00:34:46.645
所以对于所有10个任务都有一个单一的模型，

00:34:46.645 --> 00:34:48.880
嗯，一开始真的很伤人。

00:34:48.880 --> 00:34:52.120
这也是你很少在报纸上看到的，因为报纸

00:34:52.120 --> 00:34:55.210
有很强的选择倾向，只发布积极的结果。

00:34:55.210 --> 00:35:00.310
嗯，当你看大多数转移学习和多任务学习论文时，

00:35:00.310 --> 00:35:04.660
它们有点超出了实际模型的考虑范围，

00:35:04.660 --> 00:35:09.100
好吧，让我们只结合我们知道彼此会很好工作的任务。

00:35:09.100 --> 00:35:11.050
如果他们不工作，影响表现，

00:35:11.050 --> 00:35:13.285
然后我们就把它们排除在实验之外。

00:35:13.285 --> 00:35:16.615
所以你不会看到很多负面的任务结果，呃，

00:35:16.615 --> 00:35:20.215
在文献中，有几篇论文，在这里和那里，呃，

00:35:20.215 --> 00:35:24.910
学习转移学习的另一面，也就是说，

00:35:24.910 --> 00:35:28.315
呃，灾难性的干扰和灾难性的遗忘。

00:35:28.315 --> 00:35:32.110
所以当你在同一个模型中训练两个不同的任务时，

00:35:32.110 --> 00:35:35.155
为了互相干扰，你们伤害了对方的表演。

00:35:35.155 --> 00:35:37.960
灾难性的遗忘是如果你不断的训练

00:35:37.960 --> 00:35:41.305
你的第一次训练是在一个任务中，然后你的第二个任务中，

00:35:41.305 --> 00:35:42.895
人们曾经认为，

00:35:42.895 --> 00:35:44.080
“哦，好吧，你知道，

00:35:44.080 --> 00:35:45.790
基本上第一个任务是

00:35:45.790 --> 00:35:48.970
忘了，”你在第二个任务上做得很好。

00:35:48.970 --> 00:35:52.750
如果你按顺序训练神经网络，一个任务，然后

00:35:52.750 --> 00:35:56.850
又一个，有点奇怪，呃，

00:35:56.850 --> 00:35:59.160
我们发现事实并非如此

00:35:59.160 --> 00:36:01.935
灾难性的被遗忘在这些模型中，

00:36:01.935 --> 00:36:04.410
如果你按顺序训练它们，

00:36:04.410 --> 00:36:07.065
你在第一个任务中添加了一点原始元素，

00:36:07.065 --> 00:36:08.760
它很快就回来了。

00:36:08.760 --> 00:36:10.655
所以，虽然表现很差，

00:36:10.655 --> 00:36:12.910
你可以得到非常好的表现，

00:36:12.910 --> 00:36:14.470
在很少的迭代中非常快。

00:36:14.470 --> 00:36:18.115
但这是我们发现的许多有趣的小道消息之一，

00:36:18.115 --> 00:36:20.905
嗯，在这个过程中，我们还没有发表。好吧。

00:36:20.905 --> 00:36:24.055
所以，呃，专注于，呃，

00:36:24.055 --> 00:36:26.560
这里的变压器层，我们基本上找到变压器

00:36:26.560 --> 00:36:29.275
帮助原始序列对模型进行大量排序。

00:36:29.275 --> 00:36:33.415
所以如果你仔细地调整它们，把它们和，呃，

00:36:33.415 --> 00:36:36.235
一些双向LSTM等等，呃，

00:36:36.235 --> 00:36:38.410
他们很有帮助，而且进步了，呃，

00:36:38.410 --> 00:36:41.800
跨越一系列不同的数据集，在某些情况下非常显著。

00:36:41.800 --> 00:36:46.390
另一个观察是问答和语义角色标记，

00:36:46.390 --> 00:36:49.660
嗯，实际上可以很好地预测对方的表现。

00:36:49.660 --> 00:36:51.670
如果一个工作得好，另一个工作得好，

00:36:51.670 --> 00:36:53.140
嗯，反之亦然。

00:36:53.140 --> 00:36:54.400
如果他们工作不好，

00:36:54.400 --> 00:36:56.590
嗯，两个都不太好用。

00:36:56.590 --> 00:37:00.849
嗯，这也很有趣，因为这两项任务都有不同的问题，

00:37:00.849 --> 00:37:04.075
呃，每个训练例子。

00:37:04.075 --> 00:37:07.780
磨尖。呃，所以问题是指，

00:37:07.780 --> 00:37:09.520
嗯，非常重要。

00:37:09.520 --> 00:37:11.695
呃，实际上在某些情况下，呃，

00:37:11.695 --> 00:37:13.915
两倍的性能，甚至，

00:37:13.915 --> 00:37:15.565
这让我们很惊讶，

00:37:15.565 --> 00:37:18.700
一个简单的分类任务，您可以只使用一个标准的SoftMax。

00:37:18.700 --> 00:37:22.645
但不是说你有一个包含、矛盾等的最温和的条件，

00:37:22.645 --> 00:37:25.015
你基本上，呃，

00:37:25.015 --> 00:37:28.015
指出问题中的“蕴涵”一词。

00:37:28.015 --> 00:37:32.050
这也是Winograd模式的情况，它也受益匪浅，

00:37:32.050 --> 00:37:34.000
嗯，从这个指针机制。

00:37:34.000 --> 00:37:36.190
[噪音]

00:37:36.190 --> 00:37:36.880
你能解释一下吗？

00:37:36.880 --> 00:37:39.490
当然。嗯，我们能解释一下吗？为什么—

00:37:39.490 --> 00:37:41.470
[听不见]

00:37:41.470 --> 00:37:42.760
为什么会有这么多帮助？

00:37:42.760 --> 00:37:44.980
嗯，在某些方面，

00:37:44.980 --> 00:37:47.860
我想部分是整个建筑

00:37:47.860 --> 00:37:51.160
已经得到了-越来越擅长指点。

00:37:51.160 --> 00:37:53.320
部分原因是我们确实做了很多，

00:37:53.320 --> 00:37:54.730
翻译很差，

00:37:54.730 --> 00:37:59.020
这是在我们的第一次实验中唯一受伤的任务，呃，

00:37:59.020 --> 00:38:02.500
在多任务设置中，这是唯一必须生成的任务，

00:38:02.500 --> 00:38:05.440
嗯，是完全分离的SoftMax的结果，

00:38:05.440 --> 00:38:07.660
而建筑的其他部分

00:38:07.660 --> 00:38:12.535
很擅长指点事物来回答问题，任何类型的问题。

00:38:12.535 --> 00:38:15.550
嗯，所以，但在某些方面，

00:38:15.550 --> 00:38:17.560
我认为这是一个解释，

00:38:17.560 --> 00:38:19.720
但我-我不认为是-这就是全部。

00:38:19.720 --> 00:38:29.005
我想我们还是需要弄清楚为什么会这样。好吧。

00:38:29.005 --> 00:38:32.200
现在，多任务学习是最重要的

00:38:32.200 --> 00:38:35.470
对于零距离射击很有帮助，我真的很兴奋。

00:38:35.470 --> 00:38:39.835
这是一个零镜头关系提取，你有不同种类的，呃，

00:38:39.835 --> 00:38:42.430
你可能想要得到的关系，你可能从来没有

00:38:42.430 --> 00:38:45.550
就像你在尝试的师生关系一样

00:38:45.550 --> 00:38:47.860
在特定的上下文中确定

00:38:47.860 --> 00:38:51.745
一种产品公司关系或类似的关系。

00:38:51.745 --> 00:38:55.480
所以，呃，那个实际上，呃，

00:38:55.480 --> 00:38:58.180
受益匪浅，几乎得到两次，呃，

00:38:58.180 --> 00:39:00.280
准确度很高，

00:39:00.280 --> 00:39:02.380
当你和其他东西一起学习的时候。

00:39:02.380 --> 00:39:04.360
所以这些都是问题，以前从来没有见过，

00:39:04.360 --> 00:39:06.265
以前从未见过的关系，

00:39:06.265 --> 00:39:08.725
它有两倍的好，呃，

00:39:08.725 --> 00:39:13.210
尤其是在看到其他问题后受益匪浅。

00:39:13.210 --> 00:39:16.870
在某些方面，我们也必须给球队很多荣誉，

00:39:16.870 --> 00:39:18.895
呃，因为小队是一个数据集，

00:39:18.895 --> 00:39:24.760
呃，有点强迫人们把指针作为产生答案的机制。

00:39:24.760 --> 00:39:28.750
指针，我们有点把它们看作是一种给予，它们没有得到那么多的信任，

00:39:28.750 --> 00:39:33.535
但它们让你能够预测你在训练时从未见过的答案。

00:39:33.535 --> 00:39:36.040
为了创造词汇，你在训练时从未见过，

00:39:36.040 --> 00:39:39.850
这真的是相当-相当惊人。好吧。

00:39:39.850 --> 00:39:43.090
现在，主要的观察结果是

00:39:43.090 --> 00:39:46.810
如果你有神谕可以告诉你

00:39:46.810 --> 00:39:50.275
确切地说，你现在在做什么工作

00:39:50.275 --> 00:39:54.685
你完全可以把它们分成10种不同的型号，

00:39:54.685 --> 00:39:58.945
也许它们都是同一个架构，但仍然有10种不同的模型，那么，呃，

00:39:58.945 --> 00:40:02.410
实际上你还是会做得更好，

00:40:02.410 --> 00:40:06.535
呃，比这个多任务学习模型的第一个版本还要好。

00:40:06.535 --> 00:40:09.070
这主要是因为我们

00:40:09.070 --> 00:40:12.430
选择包含一堆不同的任务，这些任务与

00:40:12.430 --> 00:40:15.130
我们希望社区能够开始

00:40:15.130 --> 00:40:18.310
正在考虑解决灾难性的干扰，对吗？

00:40:18.310 --> 00:40:21.685
如果你学得像一门新语言，或者，你知道，

00:40:21.685 --> 00:40:24.670
你学会了如何理解Twitter上的社交媒体，

00:40:24.670 --> 00:40:26.860
你不会取代你所有的语言，

00:40:26.860 --> 00:40:28.825
呃，你知道，在你的大脑里。

00:40:28.825 --> 00:40:30.820
你只有一个大脑，它变得越来越聪明，

00:40:30.820 --> 00:40:32.065
你不断学习新技能，

00:40:32.065 --> 00:40:35.140
即使对你来说新的技能非常，

00:40:35.140 --> 00:40:36.520
和以前的技能大不相同。

00:40:36.520 --> 00:40:40.420
所以在某些方面，我们可能让我们的生活变得过于艰难，

00:40:40.420 --> 00:40:41.770
现在我们在想，好吧，

00:40:41.770 --> 00:40:44.620
如果你想发表一篇关于多任务学习的更好的论文，

00:40:44.620 --> 00:40:46.810
我们只看看所有能互相帮助的任务，

00:40:46.810 --> 00:40:48.880
然后我们就有了一组任务，

00:40:48.880 --> 00:40:51.445
然后我可以很快发表，

00:40:51.445 --> 00:40:54.010
嗯，一些，一些一流的报纸。

00:40:54.010 --> 00:40:57.370
但基本上我们还是

00:40:57.370 --> 00:41:03.910
在10个不同的模型和一个单一的模型之间的十分显著地远离。

00:41:03.910 --> 00:41:06.280
现在，这当然是一个甲骨文分数，

00:41:06.280 --> 00:41:09.805
这就是为什么我们把它放在括号里，因为你实际上没有这个神谕。

00:41:09.805 --> 00:41:11.260
在某些情况下，

00:41:11.260 --> 00:41:13.780
构建一个几乎完美的分类器是很容易的。

00:41:13.780 --> 00:41:16.615
所以，你知道，分开摘要是什么

00:41:16.615 --> 00:41:19.810
基于这个问题，从英语到德语的翻译是什么？

00:41:19.810 --> 00:41:21.610
你可以做到几乎100%的准确度。

00:41:21.610 --> 00:41:25.090
呃，但是，呃，小组，回答问题，

00:41:25.090 --> 00:41:26.665
零炮点关系提取，

00:41:26.665 --> 00:41:29.575
问答作为一种语义角色标记，

00:41:29.575 --> 00:41:33.220
事实上，他们很容易就如何

00:41:33.220 --> 00:41:37.330
如果你不知道答案，

00:41:37.330 --> 00:41:40.870
呃，哪个型号的车要走，呃，这个。

00:41:40.870 --> 00:41:44.935
所以在某种意义上，这是理论上的。好吧。

00:41:44.935 --> 00:41:47.710
现在，我提到我们有这个问题-这个

00:41:47.710 --> 00:41:51.730
优化策略的复杂性，这是其中之一，

00:41:51.730 --> 00:41:55.795
嗯，有点问题没有得到那么多，呃，覆盖范围。

00:41:55.795 --> 00:41:57.535
但是当你有一个非常，

00:41:57.535 --> 00:41:59.785
呃，不平衡或扭曲的数据集，

00:41:59.785 --> 00:42:05.005
它很容易失去跟踪，基本上超过了较小的数据集任务。

00:42:05.005 --> 00:42:07.510
所以，呃，第一个，呃，

00:42:07.510 --> 00:42:10.780
最简单的培训——我们尝试了大量不同的培训策略，

00:42:10.780 --> 00:42:13.600
但最后，这个完全联合的工作非常好。

00:42:13.600 --> 00:42:18.160
但事实上承诺会在这张桌子上问“去吧，等等”的问题。

00:42:18.160 --> 00:42:20.680
到目前为止，对所有这些结果有什么疑问吗？是啊？

00:42:20.680 --> 00:42:24.550
所以，呃，[噪音]自从你提到

00:42:24.550 --> 00:42:26.740
一个预言家会告诉你它是什么任务，以及

00:42:26.740 --> 00:42:29.215
你有两种更好的方法，10种不同的方法。

00:42:29.215 --> 00:42:32.440
所以真的试着训练一个模特

00:42:32.440 --> 00:42:35.710
像数据一样，意味着什么任务对这个特定版本感兴趣？

00:42:35.710 --> 00:42:38.310
我们做到了。所以-它很困惑，你知道，

00:42:38.310 --> 00:42:42.240
小队，还有那些任务，另一个，基本上是另一个，

00:42:42.240 --> 00:42:47.265
嗯，两种类型的问题，也被演员，问问题回答。

00:42:47.265 --> 00:42:49.350
所以它混淆了这些。

00:42:49.350 --> 00:42:53.490
嗯，但后来很多其他人，都很喜欢，非常完美地做到了。

00:42:53.490 --> 00:42:56.190
但你基本上，一旦你，

00:42:56.190 --> 00:43:01.105
嗯，我们要试着做一个完整的模型然后得到一个十分的分数，

00:43:01.105 --> 00:43:05.395
如果你的分类器准确度甚至达到90%，

00:43:05.395 --> 00:43:08.530
你基本上把这个乘以0.9，然后

00:43:08.530 --> 00:43:11.680
你被打得太惨了-已经没有竞争力了。

00:43:11.680 --> 00:43:14.350
所以如果你试着做

00:43:14.350 --> 00:43:17.080
整个系统不断添加if-then-else语句，

00:43:17.080 --> 00:43:18.880
呃，为了做到这一点，呃，

00:43:18.880 --> 00:43:20.885
一种单一的系统。是啊？

00:43:20.885 --> 00:43:24.090
你试过告诉模特这是在做什么工作吗？

00:43:24.090 --> 00:43:27.330
只是快速给出这类任务的指标？

00:43:27.330 --> 00:43:29.010
我是说，在某些方面，

00:43:29.010 --> 00:43:30.120
在这个案子中我们做到了，

00:43:30.120 --> 00:43:33.360
因为我们只对每个模型分别进行培训。

00:43:33.360 --> 00:43:34.280
[听不见]

00:43:34.280 --> 00:43:36.905
嗯，只有通过这个问题。

00:43:36.905 --> 00:43:39.185
是啊。因为我在想

00:43:39.185 --> 00:43:42.760
嗯，也许模型找出我们想要它做什么并不重要

00:43:42.760 --> 00:43:44.965
实际[噪音]应用

00:43:44.965 --> 00:43:47.560
如果我们能告诉它我们现在想要它做什么？

00:43:47.560 --> 00:43:49.420
在某些情况下，你可以看出。

00:43:49.420 --> 00:43:51.430
呃，所以问题是，

00:43:51.430 --> 00:43:53.260
呃，即使在多任务环境下，

00:43:53.260 --> 00:43:56.095
你可以用一种额外的象征来表示，

00:43:56.095 --> 00:43:58.150
“现在，你在做总结。

00:43:58.150 --> 00:43:59.950
所以，这是另一个输入。”

00:43:59.950 --> 00:44:01.255
呃，在某些方面，

00:44:01.255 --> 00:44:03.610
不管你有没有总结记号，

00:44:03.610 --> 00:44:05.650
呃，或者你问一下总结是什么？

00:44:05.650 --> 00:44:08.125
事实上，我认为没有什么大的区别。

00:44:08.125 --> 00:44:11.190
现在您可以在

00:44:11.190 --> 00:44:13.140
非常自然的语言，而不是必须知道

00:44:13.140 --> 00:44:15.600
一种特殊的标记，用于查询模型。

00:44:15.600 --> 00:44:19.710
嗯，我们会在几张幻灯片中看到这个模型没有混淆，

00:44:19.710 --> 00:44:22.860
呃，说到如何生成答案。

00:44:22.860 --> 00:44:24.710
所以，对于每一项任务，

00:44:24.710 --> 00:44:28.660
它非常清楚地知道如何生成正确的单词，

00:44:28.660 --> 00:44:30.700
为了得到一个相当准确的答案。

00:44:30.700 --> 00:44:36.520
[噪音]嗯，模型在-[听不见]中

00:44:36.520 --> 00:44:42.580
查看所有数据，然后[听不见]该类，或者它只包含[听不见]？

00:44:42.580 --> 00:44:45.400
哦，好问题。那么，我们如何训练单任务模型呢？

00:44:45.400 --> 00:44:47.980
他们只接受过这个数据集的训练。

00:44:47.980 --> 00:44:51.700
所以，这里的队号只是一个单一的模式，只见过队训练。

00:44:51.700 --> 00:44:57.250
[噪音]所以，你的观点是，

00:44:57.250 --> 00:44:59.050
嗯，指针例外，呃，

00:44:59.050 --> 00:45:02.310
[听不见]通常比[听不见]更有用？

00:45:02.310 --> 00:45:04.830
有点奇怪，甚至，啊，

00:45:04.830 --> 00:45:06.315
在这里，呃，

00:45:06.315 --> 00:45:09.065
我们在那里，嗯，这是munli，

00:45:09.065 --> 00:45:10.690
我的意思是，这个特殊的模型，

00:45:10.690 --> 00:45:12.550
如果你只有标准的序列来排序，

00:45:12.550 --> 00:45:14.035
它只是产生，你知道，

00:45:14.035 --> 00:45:16.660
还有一个SoftMax，呃，那个标签。

00:45:16.660 --> 00:45:18.640
所以在这个意义上，它是非常相似的。

00:45:18.640 --> 00:45:23.650
嗯，但是，事实上，它能更好地指出，这实际上导致了我们，呃，

00:45:23.650 --> 00:45:27.730
想了一会儿，也许我们应该有一个项目，我们只需要说一点

00:45:27.730 --> 00:45:32.125
所有的事情，只是永远摆脱SoftMax分类器。

00:45:32.125 --> 00:45:35.890
嗯，问题是当你尝试翻译的时候，

00:45:35.890 --> 00:45:37.210
好像没事的哇，

00:45:37.210 --> 00:45:38.395
你指的是什么？

00:45:38.395 --> 00:45:40.420
然后你就可以预先训练它

00:45:40.420 --> 00:45:43.750
一些排列，它变得非常大，你指出很多不同的地方，比如，

00:45:43.750 --> 00:45:46.360
你可能有成千上万的潜在候选人。

00:45:46.360 --> 00:45:49.540
所以我们有点抛弃它，把它当作所有事物的统一模型，

00:45:49.540 --> 00:45:51.895
但你可以指出很多不同的地方，

00:45:51.895 --> 00:45:52.990
像很多这样的任务，

00:45:52.990 --> 00:45:54.280
你实际上可以指向和

00:45:54.280 --> 00:45:59.030
我认为这是另一个有趣的项目，可以从中产生，是的。

00:46:01.440 --> 00:46:03.745
只是一个简单的问题，

00:46:03.745 --> 00:46:06.910
多敏感[听不见]多敏感，呃，

00:46:06.910 --> 00:46:09.850
个别成分[听不见]是当你

00:46:09.850 --> 00:46:13.240
稍微干扰它们在损失函数中的相对权重？

00:46:13.240 --> 00:46:16.855
所以，我们——问题是，呃，怎么，嗯，

00:46:16.855 --> 00:46:19.795
敏感的是我们要做的任务，

00:46:19.795 --> 00:46:22.825
嗯，给不同的任务增加权重？

00:46:22.825 --> 00:46:27.490
我们[噪音]在优化方面做了很多小把戏

00:46:27.490 --> 00:46:32.080
如何训练它，但我们从来没有说过这个任务只有0.5之类的。

00:46:32.080 --> 00:46:34.930
所以，我们没有做那个分析。是啊？

00:46:34.930 --> 00:46:37.990
共同关注似乎有点负担。

00:46:37.990 --> 00:46:39.070
在某些情况下，是的。

00:46:39.070 --> 00:46:44.425
这是[听不见]的共同关注和秩序，但没有共同关注，还是类似的，

00:46:44.425 --> 00:46:47.320
“哦，您已经看到了测试数据，因此，您不能使用这些数据。”

00:46:47.320 --> 00:46:49.045
我是说，这些都是DEP套装。

00:46:49.045 --> 00:46:53.560
嗯，但你肯定可以做更多的建筑工程。

00:46:53.560 --> 00:46:55.900
事实上，整个领域我都不认为

00:46:55.900 --> 00:46:58.690
你开始了，对吧，神经架构搜索？

00:46:58.690 --> 00:47:02.515
是啊。就像你可以把强化学习结合起来，

00:47:02.515 --> 00:47:05.695
你说强化学习代理的行动空间

00:47:05.695 --> 00:47:07.360
想喝两杯

00:47:07.360 --> 00:47:09.580
不同的神经网络模块，比如你想要的

00:47:09.580 --> 00:47:11.185
就像CNN的一层然后像

00:47:11.185 --> 00:47:14.320
一个内存层，然后一个LSTM层，可能是双向的

00:47:14.320 --> 00:47:19.465
基本上，让一个强化学习代理计算出所有这些决策。

00:47:19.465 --> 00:47:22.855
嗯，所以我认为尝试申请是很了不起的

00:47:22.855 --> 00:47:25.210
神经架构搜索不到什么

00:47:25.210 --> 00:47:27.790
通常我们已经知道如何进行图像分类，

00:47:27.790 --> 00:47:30.715
我们只需要用NAS，神经架构搜索做得更好一点。

00:47:30.715 --> 00:47:31.930
但实际上我们试图找到

00:47:31.930 --> 00:47:34.810
一个单一的多任务学习架构，我们不知道。

00:47:34.810 --> 00:47:38.620
当然，问题是已经到了这个地步。

00:47:38.620 --> 00:47:41.470
所有这些数字花费了大量的计算时间和

00:47:41.470 --> 00:47:44.875
摆弄东西，我可以，

00:47:44.875 --> 00:47:48.985
我只能给你一个大概的概念，我们会说多少次，

00:47:48.985 --> 00:47:50.890
“哦，伙计，我们得到了这样一个非常惊人的结果。

00:47:50.890 --> 00:47:53.110
在这项任务中，但它需要这种学习速度。”

00:47:53.110 --> 00:47:55.000
结果是同一个模型，

00:47:55.000 --> 00:47:57.100
所有的超参数都是一样的，

00:47:57.100 --> 00:48:01.555
但另一项要取得良好成绩的任务需要更高的学习率。

00:48:01.555 --> 00:48:05.650
现在，你只想把这两个任务结合起来，

00:48:05.650 --> 00:48:07.345
“好吧，你现在如何选择学习率？”

00:48:07.345 --> 00:48:09.070
你选择，你知道，

00:48:09.070 --> 00:48:11.650
如果你选择了任务，从任务中的学习率，你知道，

00:48:11.650 --> 00:48:13.780
比小任务更大根本不起作用

00:48:13.780 --> 00:48:15.970
因为它需要更高的学习率。

00:48:15.970 --> 00:48:19.405
如果你使用的学习率越高，任务越小，数据集越小，

00:48:19.405 --> 00:48:23.995
嗯，那件大的做得很好，只是过了头，也不太好用。

00:48:23.995 --> 00:48:25.960
如果你试着做平均数，两个都不做。

00:48:25.960 --> 00:48:29.560
就像尝试多任务学习有很多复杂性。

00:48:29.560 --> 00:48:33.860
这就是为什么，这就是为什么它是如此有趣，我想，呃，研究挑战。

00:48:35.100 --> 00:48:38.415
好吧，关于第一组结果还有什么问题吗？

00:48:38.415 --> 00:48:39.780
他们会变好的。

00:48:39.780 --> 00:48:42.270
我们，我们，我们已经有了一些想法，

00:48:42.270 --> 00:48:45.370
呃，关于如何改进它们。

00:48:47.250 --> 00:48:49.780
好吧。所以，呃，

00:48:49.780 --> 00:48:51.775
我们是怎么训练这整件事的？

00:48:51.775 --> 00:48:54.895
嗯，我们试过很多不同的方法，但最后，呃，

00:48:54.895 --> 00:48:58.990
这个非常简单的完全联合训练策略实际上效果最好。

00:48:58.990 --> 00:49:02.800
嗯，那就是你基本上从每一个

00:49:02.800 --> 00:49:07.540
不同的任务，你只需要从那个任务中训练小批量。

00:49:07.540 --> 00:49:11.470
所以基本上只需完成所有的10个任务然后循环，

00:49:11.470 --> 00:49:13.690
呃，检查一下。

00:49:13.690 --> 00:49:16.825
嗯，现在看来，啊，

00:49:16.825 --> 00:49:19.090
不起作用的，

00:49:19.090 --> 00:49:21.460
呃，当然，呃，

00:49:21.460 --> 00:49:26.050
作为另一种培训策略，如果你研究优化，

00:49:26.050 --> 00:49:27.685
呃，神经网络的策略，呃，

00:49:27.685 --> 00:49:29.170
实际上有几篇论文

00:49:29.170 --> 00:49:31.720
所谓的课程学习，其理念是，

00:49:31.720 --> 00:49:36.430
你首先用简单的问题实例来训练你的模型。

00:49:36.430 --> 00:49:38.830
因此，在翻译中，例如，你开始用

00:49:38.830 --> 00:49:41.995
很短的句子，然后你会变得越来越大，

00:49:41.995 --> 00:49:44.560
呃，句子，或者更长的句子。

00:49:44.560 --> 00:49:47.545
嗯，现在它变成了多任务学习，

00:49:47.545 --> 00:49:49.285
你其实想做相反的事情。

00:49:49.285 --> 00:49:52.045
你想做反课程学习。

00:49:52.045 --> 00:49:55.330
嗯，那就是你从最困难的任务开始，然后重复

00:49:55.330 --> 00:49:58.930
一段时间后再添加简单的任务。

00:49:58.930 --> 00:50:02.050
在某种程度上，我认为这是直觉，因为

00:50:02.050 --> 00:50:07.780
你训练了这个巨大而强大的模型，

00:50:07.780 --> 00:50:11.020
呃，在一个非常简单的任务上

00:50:11.020 --> 00:50:14.515
情绪，你只需要把每件事分类为积极的或消极的。

00:50:14.515 --> 00:50:18.220
你训练了所有的重量，然后你到达某种程度上，呃，

00:50:18.220 --> 00:50:20.710
局部最优，非常深

00:50:20.710 --> 00:50:24.370
具体来说就是生成这两个单词，如果你想摆脱它，

00:50:24.370 --> 00:50:27.430
为了完成这个非常简单的任务

00:50:27.430 --> 00:50:30.655
然后尝试生成所有其他类型的单词并指向不同的单词，

00:50:30.655 --> 00:50:33.925
你知道，以前从来没有见过的话，

00:50:33.925 --> 00:50:36.940
很难从这个局部最优状态中得出结论。

00:50:36.940 --> 00:50:40.975
这就是我的直觉，为什么说起来更有意义，

00:50:40.975 --> 00:50:44.935
“让我们从小组和机器翻译以及一些更困难的任务开始。

00:50:44.935 --> 00:50:47.020
我们将使模型非常通用。

00:50:47.020 --> 00:50:48.910
它必须产生很多不同的东西，

00:50:48.910 --> 00:50:52.240
创建一个SoftMax，德语单词，

00:50:52.240 --> 00:50:54.460
它必须指向所有类型

00:50:54.460 --> 00:50:57.895
不同的单词，能够解析各种不同的维基百科段落。”

00:50:57.895 --> 00:51:01.315
你做了几次，然后一旦你完成了，

00:51:01.315 --> 00:51:03.190
呃，这种预培训，呃，

00:51:03.190 --> 00:51:09.220
阶段或反课程，然后你继续并添加一些简单的小任务。

00:51:09.220 --> 00:51:11.590
所以（噪音），呃，

00:51:11.590 --> 00:51:15.085
相对简单的改变确实带着我们，

00:51:15.085 --> 00:51:17.455
呃，有很多不同的实验要做。

00:51:17.455 --> 00:51:20.200
嗯，我们实际上，呃，

00:51:20.200 --> 00:51:22.045
关闭，或者，呃，嗯，

00:51:22.045 --> 00:51:25.570
更接近于缩小差距，现在，嗯，

00:51:25.570 --> 00:51:30.330
我们只有一点，嗯，14，呃，离开了。

00:51:30.330 --> 00:51:32.780
是的，是的，呃，14岁左右。

00:51:32.780 --> 00:51:35.180
呃，但还有，呃，

00:51:35.180 --> 00:51:37.700
一个大缺口和最大的，呃，

00:51:37.700 --> 00:51:40.880
我们遇到的麻烦和问题是翻译。

00:51:40.880 --> 00:51:42.845
基本上，如果你看所有这些，

00:51:42.845 --> 00:51:44.914
大多数情况都差不多，

00:51:44.914 --> 00:51:49.160
稍微好一点，嗯，这有点让人不知所措，但是，

00:51:49.160 --> 00:51:52.130
大致相似，但翻译很糟糕。

00:51:52.130 --> 00:51:53.450
差不多只有一半，呃，

00:51:53.450 --> 00:51:56.420
多任务学习设置中的性能，

00:51:56.420 --> 00:52:00.110
部分原因是翻译是唯一的任务

00:52:00.110 --> 00:52:05.960
一个非常大的SoftMax词汇表，其中的单词不在其他任务中。

00:52:05.960 --> 00:52:08.075
其他大部分任务，

00:52:08.075 --> 00:52:10.430
实际上在指点方面做得很好。

00:52:10.430 --> 00:52:14.570
所以，呃，我的解释是中间层，

00:52:14.570 --> 00:52:16.550
我们学习到的所有这些表述

00:52:16.550 --> 00:52:19.520
双向LSTM和变压器，他们真的，

00:52:19.520 --> 00:52:21.875
很擅长被人指指点点，

00:52:21.875 --> 00:52:27.560
就像创建隐藏的表示，答案模块可以非常准确地指向。

00:52:27.560 --> 00:52:29.465
然后你有一个任务，就像，

00:52:29.465 --> 00:52:31.085
我什么都没指，

00:52:31.085 --> 00:52:34.235
我基本上只生成其他单词，然后生成不同的词汇。

00:52:34.235 --> 00:52:37.610
因此，这些隐藏的表示对于这项任务就变得不那么有用了。

00:52:37.610 --> 00:52:41.360
所以，这是其中的一个见解

00:52:41.360 --> 00:52:45.020
尝试改善这一点的方法之一。

00:52:45.020 --> 00:52:47.615
现在，我们遇到的一个有趣的问题是，

00:52:47.615 --> 00:52:49.040
当我们改进模型时，

00:52:49.040 --> 00:52:51.500
所有10个任务的多单模型，

00:52:51.500 --> 00:52:53.090
我们说过很多次，

00:52:53.090 --> 00:52:55.280
但现在我们也得回去跑了

00:52:55.280 --> 00:52:59.060
再做10个实验，对所有的单一任务进行适当的比较，对吗？

00:52:59.060 --> 00:53:01.280
因为如果你调整你关心的事情，

00:53:01.280 --> 00:53:04.790
你不再调整你想展示的东西，你可以做得更好，

00:53:04.790 --> 00:53:06.275
那就不公平了。

00:53:06.275 --> 00:53:09.470
呃，所以你总是想付出同样多，呃，

00:53:09.470 --> 00:53:13.655
TLC和焦点和实验时间到你的基线。

00:53:13.655 --> 00:53:17.789
所以，在某些情况下，我们实际上，

00:53:18.670 --> 00:53:22.415
嗯，改进了一些-改进了一些。

00:53:22.415 --> 00:53:26.495
但是，我们改进了10个单独的模型和我们的模型，

00:53:26.495 --> 00:53:29.090
有些情况下，比如10个独立的模型得到了改进，甚至更多。

00:53:29.090 --> 00:53:30.485
所以差距变得更大了。

00:53:30.485 --> 00:53:32.720
这和我们想展示的有点相反，但总的来说，

00:53:32.720 --> 00:53:34.220
两种测试都比较好，

00:53:34.220 --> 00:53:36.530
嗯，从总体上来说。

00:53:36.530 --> 00:53:37.970
所以基本上，我们开始，呃，

00:53:37.970 --> 00:53:40.220
通过这次全面的联合训练，我们

00:53:40.220 --> 00:53:42.515
这种单一的模式，我们可以，

00:53:42.515 --> 00:53:44.150
从理论上讲，对于一些先知来说，

00:53:44.150 --> 00:53:45.335
总而言之，呃，

00:53:45.335 --> 00:53:47.015
在他们的分数中，得到一个十分。

00:53:47.015 --> 00:53:49.115
所以差距从23开始。

00:53:49.115 --> 00:53:53.030
然后，呃，我们基本上做了反课程培训，

00:53:53.030 --> 00:53:55.790
呃，它，呃，把空隙缩小到15。

00:53:55.790 --> 00:53:57.380
所以我们很兴奋，

00:53:57.380 --> 00:53:58.760
呃，进展顺利。

00:53:58.760 --> 00:53:59.930
然后我们换了，呃，

00:53:59.930 --> 00:54:01.880
从手套和使用湾。

00:54:01.880 --> 00:54:04.055
所以上下文向量，嗯，

00:54:04.055 --> 00:54:06.320
这实际上又大大增加了差距。

00:54:06.320 --> 00:54:09.325
所以一切都变好了，但10个不同的型号

00:54:09.325 --> 00:54:13.000
甚至比一个完成10项任务的单一模型更好。

00:54:13.000 --> 00:54:14.650
嗯，所以差距变大了，

00:54:14.650 --> 00:54:17.140
但每个人的表现都有所提高。

00:54:17.140 --> 00:54:19.510
所以总体来说还是一件好事。

00:54:19.510 --> 00:54:22.780
呃，然后，呃，我们基本上认为，

00:54:22.780 --> 00:54:24.610
尤其是这个机器翻译问题，

00:54:24.610 --> 00:54:26.470
我们不应该只是在班上预先训练，

00:54:26.470 --> 00:54:30.100
但是我们也应该把机器翻译包括在

00:54:30.100 --> 00:54:34.845
这是一开始的预培训，所以模型不只是开始学习点。

00:54:34.845 --> 00:54:37.625
嗯，这帮了我们，呃，

00:54:37.625 --> 00:54:40.160
为了缩小10个不同型号之间的差距，

00:54:40.160 --> 00:54:43.085
甲骨文，单台型号要到五点左右。

00:54:43.085 --> 00:54:44.690
然后，呃，我们基本上说，

00:54:44.690 --> 00:54:46.640
好吧，翻译还是不太好。

00:54:46.640 --> 00:54:47.780
我们只是一直过采样。

00:54:47.780 --> 00:54:52.760
所以，每次我们通过一个循环小批量设置，

00:54:52.760 --> 00:54:54.740
我们总是包括机器翻译。

00:54:54.740 --> 00:54:59.270
这基本上让我们可以缩小差距，

00:54:59.270 --> 00:55:01.025
呃，就一点而言。

00:55:01.025 --> 00:55:03.590
所以现在，呃，我们开始，呃，

00:55:03.590 --> 00:55:06.650
几个月前，嗯，586。

00:55:06.650 --> 00:55:08.960
现在单曲，呃，

00:55:08.960 --> 00:55:11.330
Oracle有10种不同型号，

00:55:11.330 --> 00:55:12.560
如果你总结一下，

00:55:12.560 --> 00:55:16.100
拿618，呃，还有，你知道的，

00:55:16.100 --> 00:55:19.985
更好的上下文向量和调优，增加更多的翻译，

00:55:19.985 --> 00:55:23.210
翻译仍然不如我们希望的那么好，呃，

00:55:23.210 --> 00:55:26.525
但现在，其他几个任务使一帮人受益。

00:55:26.525 --> 00:55:30.140
现在我们基本上离

00:55:30.140 --> 00:55:33.740
有一个单一的模型，也有10个不同的模型。

00:55:33.740 --> 00:55:36.395
你基本上可以，

00:55:36.395 --> 00:55:38.525
你可以做更多的实验，

00:55:38.525 --> 00:55:41.930
在某些方面，你可以在这里花费数百万美元，

00:55:41.930 --> 00:55:47.180
因为大多数时候我们保持这些不同模型的超参数不变。

00:55:47.180 --> 00:55:49.385
就像这些，你也可以说，

00:55:49.385 --> 00:55:52.010
也许这个多任务模型还需要50层，

00:55:52.010 --> 00:55:53.720
或者是19层以上，

00:55:53.720 --> 00:55:56.225
或者可能还有5层，也许应该有1000层，

00:55:56.225 --> 00:55:57.860
你知道，在它们隐藏的维度上更宽。

00:55:57.860 --> 00:56:01.310
你基本上可以做更多的实验。

00:56:01.310 --> 00:56:03.830
也许，希望，最终，社区会共同这样做，

00:56:03.830 --> 00:56:06.170
然后我们可以移动，朝着那个方向移动。

00:56:06.170 --> 00:56:08.480
但我们想，好吧，我们已经很接近了，

00:56:08.480 --> 00:56:13.850
所以我们继续做一些其他的事情，也许明年我会告诉你。

00:56:13.850 --> 00:56:16.715
[笑声]但基本上，嗯，

00:56:16.715 --> 00:56:18.980
让我们分析一下这个项目中发生了什么。

00:56:18.980 --> 00:56:22.235
这是一种，我想我也会鼓励你们所有人去做。

00:56:22.235 --> 00:56:25.460
像你一样，你可以花一段时间，在某些方面，

00:56:25.460 --> 00:56:28.385
你应该对自己的评价持怀疑态度。

00:56:28.385 --> 00:56:29.780
在某些情况下，

00:56:29.780 --> 00:56:33.230
你已经看到了-我们在全国人民党社区看到了

00:56:33.230 --> 00:56:36.935
像基本上只优化布鲁分数翻译多年。

00:56:36.935 --> 00:56:38.690
然后有人拿着一张纸出来说，

00:56:38.690 --> 00:56:44.510
事实证明，布鲁的衡量标准和人类对翻译有多好的评价，

00:56:44.510 --> 00:56:46.175
不是真的有关联吗？

00:56:46.175 --> 00:56:48.320
你就像，啊，那太糟糕了，

00:56:48.320 --> 00:56:53.000
我们只是花了几年时间来调整这个指标，并发表了一堆论文。

00:56:53.000 --> 00:56:57.290
嗯，所以在某些方面，所有这些指标都有缺陷，呃，你知道，

00:56:57.290 --> 00:57:00.140
根分数汇总是一个超级，

00:57:00.140 --> 00:57:03.380
嗯，主观的任务。

00:57:03.380 --> 00:57:05.465
例如，总结，

00:57:05.465 --> 00:57:07.730
当你分析错误时，

00:57:07.730 --> 00:57:10.595
你经常意识到单词向量也有问题。

00:57:10.595 --> 00:57:12.920
例如，jason，john，

00:57:12.920 --> 00:57:15.290
杰里米也一样，对吧？

00:57:15.290 --> 00:57:16.940
他们都有类似的，呃，

00:57:16.940 --> 00:57:20.045
分布、类似上下文、窗口等。

00:57:20.045 --> 00:57:22.610
所以名字的词向量非常相似。

00:57:22.610 --> 00:57:25.835
所以在总结错误时，你会意识到，哦，

00:57:25.835 --> 00:57:29.300
好吧，你知道，这篇文章，新闻文章谈到杰里米被绑架。

00:57:29.300 --> 00:57:31.160
但总结说杰森被绑架了。

00:57:31.160 --> 00:57:33.650
你喜欢，嗯，你知道，在评估指标中

00:57:33.650 --> 00:57:36.320
只有一个词是关的，其余的都是对的，

00:57:36.320 --> 00:57:38.000
但这是一个非常重要的词。

00:57:38.000 --> 00:57:40.970
因此，词汇载体也有类似的问题

00:57:40.970 --> 00:57:44.075
总结一下，这是非常基本的，我不认为，

00:57:44.075 --> 00:57:46.835
嗯，现在谁的防守都很好。

00:57:46.835 --> 00:57:48.875
呃，所有这些指标都有问题。

00:57:48.875 --> 00:57:51.620
我想说的是，把这10个事实结合起来

00:57:51.620 --> 00:57:54.440
使问题更少，更有意义，

00:57:54.440 --> 00:57:56.630
而不是分别看每一个。

00:57:56.630 --> 00:58:00.725
因为现在你不能利用

00:58:00.725 --> 00:58:04.970
一个特别的评估指标，让你的分数更高一点。

00:58:04.970 --> 00:58:09.740
嗯，因为如果你只是在脑海里想一想，

00:58:09.740 --> 00:58:13.370
它会伤害到其他任务，而你也无法找到将军，

00:58:13.370 --> 00:58:15.950
嗯，NLP模式更容易。

00:58:15.950 --> 00:58:18.605
好吧。现在，让我们做一些分析，呃，

00:58:18.605 --> 00:58:20.645
这个模型，呃，

00:58:20.645 --> 00:58:24.140
看看，这是一个问题的答案。

00:58:24.140 --> 00:58:28.295
嗯，这个模型能为正确的任务生成正确的单词吗？

00:58:28.295 --> 00:58:31.775
在这里，我们基本上研究了频率分布，

00:58:31.775 --> 00:58:37.100
模型在这些不同的机制中生成单词，

00:58:37.100 --> 00:58:40.370
SoftMax词汇表、上下文指针或问题指针。

00:58:40.370 --> 00:58:42.515
而且，呃，如你所见，

00:58:42.515 --> 00:58:45.500
在大多数情况下，它确切知道如何生成。

00:58:45.500 --> 00:58:47.915
所以，呃，因为，呃，

00:58:47.915 --> 00:58:51.110
问答和语义角色标记，

00:58:51.110 --> 00:58:55.355
以及Squad和wiki SQL，

00:58:55.355 --> 00:58:59.150
嗯，总结一下，它基本上使用上下文指针。

00:58:59.150 --> 00:59:01.565
所以它只是指向上下文文档。

00:59:01.565 --> 00:59:02.795
我们知道球队，

00:59:02.795 --> 00:59:05.990
这基本上就是数据集的生成方式。

00:59:05.990 --> 00:59:08.600
所以这是唯一真正有意义的事情。

00:59:08.600 --> 00:59:11.930
嗯，有点酷的是，在某些情况下，比如总结，

00:59:11.930 --> 00:59:14.240
它有时会创造新单词，或者，你知道，

00:59:14.240 --> 00:59:17.330
这并没有在文中提到。

00:59:17.330 --> 00:59:19.910
对于零炮点关系提取，

00:59:19.910 --> 00:59:21.455
有时也会用到，呃，

00:59:21.455 --> 00:59:24.050
这个外部词汇表以及在某些情况下的上下文指针。

00:59:24.050 --> 00:59:26.210
所以大部分时间，呃，

00:59:26.210 --> 00:59:31.970
这个模型不会-不会混淆如何在给定的任务上执行，呃，

00:59:31.970 --> 00:59:35.180
这个问题是形式主义而不是，

00:59:35.180 --> 00:59:37.370
嗯，这就是任务的形式，

00:59:37.370 --> 00:59:39.840
做这个特殊的测试。

00:59:41.200 --> 00:59:44.030
现在，嗯，你可能会争辩，

00:59:44.030 --> 00:59:45.830
好吧，你知道，我没那么印象，

00:59:45.830 --> 00:59:48.500
一个模型的性能与

00:59:48.500 --> 00:59:51.590
10个独立的型号，即使你想正确部署它也很好，

00:59:51.590 --> 00:59:53.255
比如，使用更少的RAM，所有这些，

00:59:53.255 --> 00:59:54.965
假设它们的大小相同，

00:59:54.965 --> 00:59:57.080
嗯，虽然，你知道，只有十分之一大。

00:59:57.080 --> 01:00:00.710
但我最兴奋的是接下来的两个结果。

01:00:00.710 --> 01:00:02.750
也就是说，这种转移学习，

01:00:02.750 --> 01:00:04.550
领域适应和零镜头，

01:00:04.550 --> 01:00:06.020
呃，这些能力。

01:00:06.020 --> 01:00:11.630
所以这里，呃，我们选择了两个数据集，它们没有包含在原来的10中。

01:00:11.630 --> 01:00:17.795
我们基本上训练了一个预先训练过的模型和一个随机模型。

01:00:17.795 --> 01:00:20.510
然后，呃，随机再来一次，

01:00:20.510 --> 01:00:21.859
他们是同一个建筑，

01:00:21.859 --> 01:00:25.295
而预训练意味着整个模型都是预训练的。

01:00:25.295 --> 01:00:26.945
所有的，你知道的，

01:00:26.945 --> 01:00:31.325
编码器，包括SoftMax中的解码器和所有东西，呃，

01:00:31.325 --> 01:00:36.140
以及另外两个IWSLT语言配对的任务，即，

01:00:36.140 --> 01:00:37.685
从英语翻译到捷克语，

01:00:37.685 --> 01:00:40.880
以及你们都非常熟悉的命名实体识别任务。

01:00:40.880 --> 01:00:43.460
所以基本上我们发现的是，

01:00:43.460 --> 01:00:45.935
嗯，它收敛得更快，

01:00:45.935 --> 01:00:47.810
开始的时候，然后，

01:00:47.810 --> 01:00:51.200
仍然有一个巨大的，但不是巨大的差距。

01:00:51.200 --> 01:00:55.595
因此，对这些完全独立的任务进行的预先培训有帮助。

01:00:55.595 --> 01:00:58.745
而且，呃，我想那是，

01:00:58.745 --> 01:01:00.365
这很令人兴奋，嗯，

01:01:00.365 --> 01:01:02.420
尤其是更快的融合，比如，

01:01:02.420 --> 01:01:04.165
学得更快，呃，

01:01:04.165 --> 01:01:06.310
不管你有什么新任务，你都会想出，

01:01:06.310 --> 01:01:09.010
这也意味着在某些情况下你可以摆脱

01:01:09.010 --> 01:01:11.950
关于这些新任务的培训数据更少。

01:01:11.950 --> 01:01:15.970
嗯，现在领域适应是一种简单的转移学习形式，

01:01:15.970 --> 01:01:19.280
基本上你只是有一个不同的，

01:01:19.280 --> 01:01:21.410
呃，类型，呃，

01:01:21.410 --> 01:01:23.060
你知道，你的话的分布。

01:01:23.060 --> 01:01:26.750
嗯，我们提到我们有斯坦福情感树库进行情感分析。

01:01:26.750 --> 01:01:29.780
嗯，然后我们分析这个

01:01:29.780 --> 01:01:31.610
嗯，情绪数据集，

01:01:31.610 --> 01:01:34.505
即亚马逊产品评论和Yelp餐厅评论，

01:01:34.505 --> 01:01:36.605
在没有任何训练的情况下，

01:01:36.605 --> 01:01:39.965
模型在这两个数据集上的精确度都达到了80%。

01:01:39.965 --> 01:01:42.319
嗯，我认为对于从业者来说，

01:01:42.319 --> 01:01:45.140
这很令人兴奋，因为你基本上不需要训练任何东西，

01:01:45.140 --> 01:01:46.610
有点像开箱即用，

01:01:46.610 --> 01:01:48.830
从Github下载并运行它。

01:01:48.830 --> 01:01:51.620
呃，斯尼，那有点不同。

01:01:51.620 --> 01:01:53.330
它也不太管用。

01:01:53.330 --> 01:01:55.280
这是另一个自然语言推理数据集，

01:01:55.280 --> 01:01:59.135
但是有很大的不同-一个非常不同的分布，不同的，呃，

01:01:59.135 --> 01:02:01.040
各种领域，呃，那，

01:02:01.040 --> 01:02:03.290
呃，这些问题都问完了。

01:02:03.290 --> 01:02:06.980
嗯，在这里，开箱即用，它达到了62。

01:02:06.980 --> 01:02:10.200
呃，但是，一旦你把它调好了，

01:02:10.200 --> 01:02:14.230
与这里的实验类似，继续对这个数据集进行实际训练，

01:02:14.230 --> 01:02:17.680
它很快就收敛到87

01:02:17.680 --> 01:02:21.625
与Randomlyor初始化的McCann模型相比，仍然有2%的收益。是啊。

01:02:21.625 --> 01:02:29.075
在那个实验中，你是否评估了你能少得到多少数据？

01:02:29.075 --> 01:02:32.900
我们是否评估了我们能少得到多少数据？我们没有。

01:02:32.900 --> 01:02:35.510
在某些方面，无论何时你要做这个实验，

01:02:35.510 --> 01:02:38.000
你基本上是这样的，你还是不会做的那么好。

01:02:38.000 --> 01:02:41.555
比如说，所有的这些模型在训练数据越多的情况下都会做得更好。

01:02:41.555 --> 01:02:43.640
所以你只是一种模糊的说法，

01:02:43.640 --> 01:02:46.220
比如，切割模糊的结果，对吗？

01:02:46.220 --> 01:02:48.140
你说的话，好吧，只要我们能得到十分之一

01:02:48.140 --> 01:02:50.885
到50，另一个型号可能只有40，

01:02:50.885 --> 01:02:52.160
做类似的事情。

01:02:52.160 --> 01:02:54.830
嗯，我们没有-我没有那些号码。

01:02:54.830 --> 01:02:57.380
它实际上也有点整洁，整洁，呃，

01:02:57.380 --> 01:02:59.750
要做的分析。是啊。

01:02:59.750 --> 01:03:06.835
所以如果你想接受新任务的训练[听不见]。

01:03:06.835 --> 01:03:07.935
是啊。

01:03:07.935 --> 01:03:10.165
[听不见]。

01:03:10.165 --> 01:03:13.105
那么，我们有训练新任务的代码吗？是的，我们有。

01:03:13.105 --> 01:03:14.695
嗯，你可以，呃，编辑，

01:03:14.695 --> 01:03:16.795
使用上下文将其转换成这种格式。

01:03:16.795 --> 01:03:19.465
这是一个简单的问题，像csv类型的格式，

01:03:19.465 --> 01:03:24.160
然后你添加它，你可以像训练预先训练的模型自己。

01:03:24.160 --> 01:03:28.690
你可以下载一个预先培训过的模型，然后添加它。所以我会查的，是的。

01:03:28.690 --> 01:03:34.795
你知道这与使用其他类型的预先训练的表示相比有什么不同吗，比如伯特？

01:03:34.795 --> 01:03:37.330
所以，嗯，这是个很好的问题。

01:03:37.330 --> 01:03:40.120
那么，与其他像伯特这样的预先培训过的表示法相比，这是怎样的呢？

01:03:40.120 --> 01:03:41.935
所以，在某些方面，

01:03:41.935 --> 01:03:44.200
人们说伯特是那种无所不能的人，

01:03:44.200 --> 01:03:46.690
但是当你读到报纸的时候，你会意识到，

01:03:46.690 --> 01:03:49.930
对于这些不同的任务，它是一个单独的模型，对吗？

01:03:49.930 --> 01:03:52.375
如果你想有一个分类任务，

01:03:52.375 --> 01:03:54.070
一开始你有一个小小的象征，

01:03:54.070 --> 01:03:55.330
你有一个不同的顶层。

01:03:55.330 --> 01:03:57.400
如果你想做一个序列标记任务，

01:03:57.400 --> 01:03:58.450
你有一个不同的顶层。

01:03:58.450 --> 01:04:00.400
如果你想做一个序列提取任务，

01:04:00.400 --> 01:04:01.765
你有一个不同的顶层。

01:04:01.765 --> 01:04:06.220
所以，对于所有这些不同的任务，伯特实际上不是一个单一的模型。

01:04:06.220 --> 01:04:08.410
啊，然后，在所有的结果上，

01:04:08.410 --> 01:04:11.800
每个数据集都有很多额外的调优，

01:04:11.800 --> 01:04:13.765
任务，呃，你知道，

01:04:13.765 --> 01:04:16.030
这项任务的学习率不同，

01:04:16.030 --> 01:04:19.120
不同的大小，或不同的伯特集，等等。

01:04:19.120 --> 01:04:21.670
所以，我们也非常兴奋，我们可能就是这样，

01:04:21.670 --> 01:04:23.590
我们会把一切都放在伯特身上，

01:04:23.590 --> 01:04:25.180
然后我们研究了所有的细节，

01:04:25.180 --> 01:04:26.920
开始的时候很兴奋。

01:04:26.920 --> 01:04:29.020
我们越是深入了解细节，

01:04:29.020 --> 01:04:31.795
当我们像这样回答的时候，我们变得不那么兴奋了，

01:04:31.795 --> 01:04:33.580
因为它不是一个单一的模型。

01:04:33.580 --> 01:04:36.880
嗯，从某种程度上来说，参加预训可能更好。

01:04:36.880 --> 01:04:38.290
所以不是海湾，

01:04:38.290 --> 01:04:41.140
你可以从一开始就有伯特这样的人，

01:04:41.140 --> 01:04:43.450
我的预感是一切都会好一点，

01:04:43.450 --> 01:04:46.075
但你仍然需要，嗯，

01:04:46.075 --> 01:04:52.120
在它上面有很多其他类型的建模体系结构。

01:04:52.120 --> 01:04:56.050
呃，可悲的是，要真正取得最先进的成果，

01:04:56.050 --> 01:05:00.355
最后一个顶层有很多特定于任务的调优。

01:05:00.355 --> 01:05:04.525
所以，如果你试图统一特定于任务的调优，

01:05:04.525 --> 01:05:06.700
你失去了伯特的出色表现。

01:05:06.700 --> 01:05:10.495
嗯，所以，不幸的是，这不是那种，

01:05:10.495 --> 01:05:12.175
“哦，就用伯特吧，

01:05:12.175 --> 01:05:15.220
您将拥有最先进的数字和所有东西。”

01:05:15.220 --> 01:05:18.565
嗯，我可能会更喜欢谈论它，但是，呃，

01:05:18.565 --> 01:05:21.295
我觉得想想还是有意义的，嗯，

01:05:21.295 --> 01:05:23.065
伯特的一些想法，

01:05:23.065 --> 01:05:26.355
基本上，添加为任务语言建模之一。

01:05:26.355 --> 01:05:30.990
这很可能是对所有其他任务帮助最大的任务，

01:05:30.990 --> 01:05:33.480
我们应该包括这个，呃，

01:05:33.480 --> 01:05:37.535
现在有一个更快的型号也不错。

01:05:37.535 --> 01:05:40.270
嗯，很难做语言建模非常非常大，

01:05:40.270 --> 01:05:41.740
它从中获益更多，

01:05:41.740 --> 01:05:43.840
你知道，几十亿字。

01:05:43.840 --> 01:05:45.670
很难训练麦肯模型，

01:05:45.670 --> 01:05:48.940
当前问题共同关注机制的问答模式

01:05:48.940 --> 01:05:52.029
像一个越来越大的背景。

01:05:52.029 --> 01:05:54.970
所以你得像伯特一样把它分开，

01:05:54.970 --> 01:05:59.020
也相当好地工作，只是最多我想500字左右，

01:05:59.020 --> 01:06:02.050
如果你想做总结的话，基本上你必须减少

01:06:02.050 --> 01:06:06.490
原始文档只有500个字，然后尝试对其进行总结。

01:06:06.490 --> 01:06:09.820
所以，在细节上有很多类似魔鬼的东西，他们不必去想，

01:06:09.820 --> 01:06:12.520
因为他们说，“好吧，我们会像矢量一样，

01:06:12.520 --> 01:06:16.420
我们可以接受他们，然后我们做很多其他的特定任务的事情，

01:06:16.420 --> 01:06:18.775
嗯，有了这些词载体，

01:06:18.775 --> 01:06:20.350
或者用伯特的架构。”

01:06:20.350 --> 01:06:22.720
我仍然-我不想-这个伯特显然很了不起，

01:06:22.720 --> 01:06:25.120
我们正在努力利用它的想法。

01:06:25.120 --> 01:06:27.400
但不幸的是，这不仅仅是一个银弹

01:06:27.400 --> 01:06:33.355
解决多任务学习。嗯，嗯？

01:06:33.355 --> 01:06:35.515
要考虑的培训前流程，呃，

01:06:35.515 --> 01:06:40.990
根据小组人数减少多少、损失多少确定优先抽样？

01:06:40.990 --> 01:06:42.670
对不起，我们再说一遍了吗？

01:06:42.670 --> 01:06:46.390
你会考虑优先抽样[听不见]？

01:06:46.390 --> 01:06:48.370
那么，我们是否考虑过对抽样进行优先级排序？

01:06:48.370 --> 01:06:51.760
所以在某种程度上，通过这种预先培训的策略，嗯，

01:06:51.760 --> 01:06:56.500
这就是我们所做的，主要集中在这些非常困难的任务上。

01:06:56.500 --> 01:07:02.140
而且，呃，很多人都喜欢最后的差距是通过真正的等待来改善的，

01:07:02.140 --> 01:07:04.555
就像最后的四项任务，

01:07:04.555 --> 01:07:05.995
呃，在-直到-你知道，呃，

01:07:05.995 --> 01:07:08.560
直到你经历之后，呃，

01:07:08.560 --> 01:07:10.750
有点过采样了，

01:07:10.750 --> 01:07:11.800
呃，任务真的很艰巨。

01:07:11.800 --> 01:07:16.375
在过去的10分钟里，呃，基本上，呃，

01:07:16.375 --> 01:07:18.400
最令人兴奋的事，呃，

01:07:18.400 --> 01:07:22.540
最后，尽管我认为你也可以在这个方向上做更多的工作。

01:07:22.540 --> 01:07:24.460
呃，我提到了唯一的问号

01:07:24.460 --> 01:07:26.380
一开始就没有短期学习，而且，呃，

01:07:26.380 --> 01:07:29.965
我们基本上只是试着和它玩一点，嗯，

01:07:29.965 --> 01:07:32.185
发现在某些情况下，

01:07:32.185 --> 01:07:35.080
它实际上有点神奇。

01:07:35.080 --> 01:07:37.060
呃，所以在这里，我们试过，呃，

01:07:37.060 --> 01:07:38.725
约翰有个聚会，

01:07:38.725 --> 01:07:40.855
但没有人来，他一个人。

01:07:40.855 --> 01:07:43.960
然后我们问，“这个故事是悲伤的还是快乐的？”

01:07:43.960 --> 01:07:46.120
你知道，虽然模特可能

01:07:46.120 --> 01:07:47.920
生成一些随机的德语单词，

01:07:47.920 --> 01:07:49.570
或者一些随机的SQL单词，

01:07:49.570 --> 01:07:51.235
或者只是随便说，

01:07:51.235 --> 01:07:54.490
它实际上指的是，在所有的单词中，

01:07:54.490 --> 01:07:56.440
你可以在上下文或问题中指出

01:07:56.440 --> 01:07:58.825
指的是“悲伤”，这很酷。

01:07:58.825 --> 01:08:01.750
就像一个小样本，

01:08:01.750 --> 01:08:03.580
而且，你知道，你可以做的更多，

01:08:03.580 --> 01:08:08.905
你可以试着提出一个非常大的零镜头分类数据集，

01:08:08.905 --> 01:08:10.300
其实也有点难。

01:08:10.300 --> 01:08:12.550
你必须非常有创造力，这不像你可以说，“哦，

01:08:12.550 --> 01:08:13.750
只需要这些评论，

01:08:13.750 --> 01:08:15.700
把它们标为正-负。

01:08:15.700 --> 01:08:19.810
啊，但是，我认为我们-我们需要在这个方向上做更多的工作。

01:08:19.810 --> 01:08:23.230
希望有人能创建一个零镜头类型的任务数据集，

01:08:23.230 --> 01:08:25.570
你知道，这不仅仅是零投，

01:08:25.570 --> 01:08:29.050
一种新的发行版，或者具有完全不同的输出的发行版。

01:08:29.050 --> 01:08:31.810
呃，但是我们-我们试了两次，

01:08:31.810 --> 01:08:32.950
它并不总是有效的，对吧。

01:08:32.950 --> 01:08:34.510
你可以对它持敌对态度，

01:08:34.510 --> 01:08:38.470
你可以让这个看起来很像，

01:08:38.470 --> 01:08:40.510
这种情绪是积极的还是消极的？

01:08:40.510 --> 01:08:42.805
呃，这句话是肯定的还是否定的？

01:08:42.805 --> 01:08:45.955
这就是我们对情感分析的形式主义。

01:08:45.955 --> 01:08:47.665
所以你可以，

01:08:47.665 --> 01:08:50.380
如果你的问题越来越不一样，

01:08:50.380 --> 01:08:52.000
最终，它会被绊倒。

01:08:52.000 --> 01:08:55.015
啊，很明显，它受益匪浅，

01:08:55.015 --> 01:08:57.010
从单词vectors，

01:08:57.010 --> 01:08:59.020
悲伤接近消极，

01:08:59.020 --> 01:09:01.360
然后通过这些理解，

01:09:01.360 --> 01:09:03.715
呃，相关性，和-和，呃，

01:09:03.715 --> 01:09:08.920
深层次的表象，在这种背景下，还有其他的悲伤的话语，

01:09:08.920 --> 01:09:10.120
或者，不管是什么。

01:09:10.120 --> 01:09:12.370
嗯，所以，它能指出这一点。

01:09:12.370 --> 01:09:14.740
但你可能是敌对的，这并不总是有效的。

01:09:14.740 --> 01:09:16.780
但事实上，呃，

01:09:16.780 --> 01:09:20.335
这是一种基于矢量词的零镜头分类，

01:09:20.335 --> 01:09:22.150
对于新的问题，

01:09:22.150 --> 01:09:24.070
呃，就我个人而言，我很兴奋。

01:09:24.070 --> 01:09:26.170
我们还尝试了其他一些方法，比如，

01:09:26.170 --> 01:09:28.615
嗯，布莱恩说了一句话，没人鼓掌。

01:09:28.615 --> 01:09:29.650
布莱恩是快乐还是悲伤？

01:09:29.650 --> 01:09:30.670
它也把它弄好了。

01:09:30.670 --> 01:09:33.295
所以，嗯，有两个-两个，

01:09:33.295 --> 01:09:36.190
这些例子至少和快乐或悲伤的事情一样有效。

01:09:36.190 --> 01:09:39.295
然后，呃，一些其他类型的形容词问题，

01:09:39.295 --> 01:09:40.780
我们试过了，但是，嗯，

01:09:40.780 --> 01:09:43.690
我最兴奋的是最终

01:09:43.690 --> 01:09:47.755
尝试进行零镜头分类任务，

01:09:47.755 --> 01:09:49.675
嗯，这也结合了不同的任务。

01:09:49.675 --> 01:09:52.540
所以，呃，不幸的是，这方面没有数据集，

01:09:52.540 --> 01:09:54.460
所以我们没有训练它，所以它不会发生在模型上。

01:09:54.460 --> 01:09:57.729
但是从理论上讲，如果你问这个总数是多少，你可以总结一下，

01:09:57.729 --> 01:09:59.995
你可以把英语翻译成德语，

01:09:59.995 --> 01:10:02.515
为什么你不能向模型要一份德国摘要？

01:10:02.515 --> 01:10:04.240
如果成功了，最终，

01:10:04.240 --> 01:10:05.650
那会更令人惊讶，

01:10:05.650 --> 01:10:07.390
但现在不行，

01:10:07.390 --> 01:10:09.190
因为我们从来没有要求过这些

01:10:09.190 --> 01:10:12.310
合成任务-这些合成任务问题。

01:10:12.310 --> 01:10:15.490
但这是另一个有趣的研究方向，我认为可以从中衍生出来。

01:10:15.490 --> 01:10:16.675
嗯，好吧。

01:10:16.675 --> 01:10:19.150
所以，我希望我能告诉你

01:10:19.150 --> 01:10:24.130
decalp框架是广义NLP的一个有趣的新基准。

01:10:24.130 --> 01:10:27.160
嗯，我认为这是一个相当好的框架

01:10:27.160 --> 01:10:30.310
for tackling a bunch of the really hard questions in the field.

01:10:30.310 --> 01:10:32.259
Uh, more general language understanding,

01:10:32.259 --> 01:10:33.550
and question answering of course,

01:10:33.550 --> 01:10:37.180
uh, multitask learning, domain adaptation, uh,

01:10:37.180 --> 01:10:39.790
which we sort of analyzed a little bit with the sentiment,

01:10:39.790 --> 01:10:41.815
and SNLI versus multi NLI,

01:10:41.815 --> 01:10:44.710
um, transfer learning, and then weight sharing.

01:10:44.710 --> 01:10:46.780
I think it's clear, everybody loves weight sharing,

01:10:46.780 --> 01:10:48.850
you wanna share as many weights as possible.

01:10:48.850 --> 01:10:52.375
Uh, word vector started at, uh, ELMo,

01:10:52.375 --> 01:10:55.300
CoVe, and now BERT basically share more and more,

01:10:55.300 --> 01:10:56.545
deeper and deeper layers.

01:10:56.545 --> 01:10:59.560
It would be great if we can unify that last bit also, uh,

01:10:59.560 --> 01:11:02.575
and then share basically the entirety of the networks,

01:11:02.575 --> 01:11:05.200
and then eventually hopefully get to zero-shot learning.

01:11:05.200 --> 01:11:07.330
Now, there's a bunch of related work.

01:11:07.330 --> 01:11:09.220
The original paper has over 100,

01:11:09.220 --> 01:11:11.725
um, citations in it, uh, of,

01:11:11.725 --> 01:11:13.525
of, you know, papers to other,

01:11:13.525 --> 01:11:16.405
other, um, lines of, uh, work.

01:11:16.405 --> 01:11:18.490
But, uh, this is actually zero- at least some of

01:11:18.490 --> 01:11:21.670
the models and papers that influenced us the most,

01:11:21.670 --> 01:11:23.920
uh, in, in our thinking and modelling.

01:11:23.920 --> 01:11:25.465
Uh, one of them actually comes from,

01:11:25.465 --> 01:11:27.550
uh, the two instructors of the class.

01:11:27.550 --> 01:11:31.165
And so, um, hopefully, uh, we can,

01:11:31.165 --> 01:11:35.050
you know, sort of think about what- what's next after all this architecture engineering.

01:11:35.050 --> 01:11:38.125
And, uh, I think one potential answer to that, uh,

01:11:38.125 --> 01:11:42.400
is single multitask learning for more generalized NLP models.

01:11:42.400 --> 01:11:53.620
[NOISE] All right. Thank you. [APPLAUSE]

