WEBVTT
Kind: captions
Language: en

00:00:04.280 --> 00:00:07.620
今天的计划就是我要讲的00:00:07.620 --> 00:00:10.710
是卷积神经网络的主题。00:00:10.710 --> 00:00:13.920
实际上，有很多00:00:13.920 --> 00:00:17.700
这堂课的内容有很多，00:00:17.700 --> 00:00:20.760
因为本质上这是要学习的00:00:20.760 --> 00:00:24.840
卷积神经网络在一个大的咬入为NLP。00:00:24.840 --> 00:00:27.465
那么，嗯，关于公告，00:00:27.465 --> 00:00:30.945
解释卷积神经网络的一般概念，00:00:30.945 --> 00:00:33.270
在相当长的一段时间里，00:00:33.270 --> 00:00:38.490
我想对一些论文做一些详细的介绍00:00:38.490 --> 00:00:40.950
使用卷积神经网络00:00:40.950 --> 00:00:44.235
文本分类，句子分类任务。00:00:44.235 --> 00:00:47.040
第一个很简单，00:00:47.040 --> 00:00:50.370
CNN那是2014年做的，00:00:50.370 --> 00:00:52.365
第二个是a00:00:52.365 --> 00:00:58.435
更复杂的CNN是在2017年做的。00:00:58.435 --> 00:01:01.275
好。首先，我要宣布几件事。00:01:01.275 --> 00:01:06.360
首先，关于季度中期反馈调查的最后一个提醒。00:01:06.360 --> 00:01:08.700
你们很多人已经做过这个了。00:01:08.700 --> 00:01:10.470
谢谢，非常感谢。00:01:10.470 --> 00:01:14.340
但如果你还是要拖到最后一分钟00:01:14.340 --> 00:01:17.330
今晚午夜是你最后的机会00:01:17.330 --> 00:01:20.045
要填写季度中期调查，00:01:20.045 --> 00:01:23.275
给我们反馈并得到你的半分。00:01:23.275 --> 00:01:26.990
嗯,好吧。你应该考虑的另一件事是，00:01:26.990 --> 00:01:29.510
我知道你们很多人都在想00:01:29.510 --> 00:01:32.495
因为我昨天花了三个小时和人们聊天，00:01:32.495 --> 00:01:35.195
是关于期末项目的。00:01:35.195 --> 00:01:39.060
所以一定要有个计划00:01:39.060 --> 00:01:40.725
为了，嗯，00:01:40.725 --> 00:01:44.640
内p。星期四下午4点30分。00:01:44.640 --> 00:01:47.535
我的意思是，特别是我们已经讨论过的，00:01:47.535 --> 00:01:52.745
你今年要做的就是找到一些研究论文，00:01:52.745 --> 00:01:57.730
读过它，总结一下它是如何影响你的工作的。00:01:57.730 --> 00:02:01.550
然后确保你的日历上有00:02:01.550 --> 00:02:05.735
CS224n的最终项目海报会议，00:02:05.735 --> 00:02:09.320
时间是3月20日星期三晚上00:02:09.320 --> 00:02:12.480
我们在校友中心举行。00:02:12.640 --> 00:02:19.940
好。还有一件事要宣布，或者是一些需要考虑的一般事情。00:02:19.940 --> 00:02:23.060
我们现在正式进入下半节课。00:02:23.060 --> 00:02:24.545
祝贺你。00:02:24.545 --> 00:02:26.630
你知道，00:02:26.630 --> 00:02:31.880
我们还想教你一些基本的东西，00:02:31.880 --> 00:02:34.700
卷积神经网络就是其中之一。00:02:34.700 --> 00:02:39.950
但是，我的意思是，尽管如此，在下半节课，00:02:39.950 --> 00:02:44.480
事情开始改变了，我们希望更多，00:02:44.480 --> 00:02:49.970
为你成为真正的深度学习NLP研究者或实践者做好准备。00:02:49.970 --> 00:02:52.395
具体是什么意思呢?00:02:52.395 --> 00:02:55.745
嗯，讲座开始少了00:02:55.745 --> 00:02:59.660
给出了如何建立一个非常基础的东西的每一个细节，00:02:59.660 --> 00:03:02.629
更多的给你一些想法00:03:02.629 --> 00:03:05.880
在不同领域所做的一些工作。00:03:05.880 --> 00:03:08.510
所以在某种程度上，这是有趣的00:03:08.510 --> 00:03:11.375
rele-与项目或类似的事情相关。00:03:11.375 --> 00:03:14.360
希望你能采取一些主动00:03:14.360 --> 00:03:17.915
了解更多正在谈论的事情。00:03:17.915 --> 00:03:22.100
我也非常欢迎大家提出任何关于人类的问题，00:03:22.100 --> 00:03:24.440
想知道更多00:03:24.440 --> 00:03:26.420
还有一件事你应该知道00:03:26.420 --> 00:03:30.440
深度学习是指一旦我们掌握了基本知识，00:03:30.440 --> 00:03:33.350
我们教的很多东西都不是00:03:33.350 --> 00:03:38.120
真正的科学或人们确信的事情，00:03:38.120 --> 00:03:41.870
你知道，我在这门课的后半部分教的大部分内容都很不错00:03:41.870 --> 00:03:46.175
很多人认为2019年的做法很好。00:03:46.175 --> 00:03:49.370
但是，你知道，事实是人们怎么想的00:03:49.370 --> 00:03:53.390
深度学习的良好实践正在迅速发生变化。00:03:53.390 --> 00:03:58.330
所以如果你回到两年或者肯定是四年，对吧?00:03:58.330 --> 00:04:01.640
人们曾经相信很多不同的事情，00:04:01.640 --> 00:04:04.850
现在人们对什么是最好的有不同的看法。00:04:04.850 --> 00:04:09.530
很明显，2021年或2023年，00:04:09.530 --> 00:04:12.350
关于什么又会有一些不同的观点，00:04:12.350 --> 00:04:14.090
人们认为是最好的。00:04:14.090 --> 00:04:17.750
所以你不得不接受这是，嗯，00:04:17.750 --> 00:04:20.630
一个新兴的快速发展的领域00:04:20.630 --> 00:04:24.125
了解基本原理和事物是如何组合在一起的是很好的。00:04:24.125 --> 00:04:27.740
但在那之后，相当多的知识是这是什么人00:04:27.740 --> 00:04:31.280
思考是当下的好东西，它会随着时间不断进化。00:04:31.280 --> 00:04:34.745
如果你想留在这个领域，或者做一些有深度学习的事情，00:04:34.745 --> 00:04:37.505
你还是要跟上它的变化。00:04:37.505 --> 00:04:39.710
现在被称为终身学习。00:04:39.710 --> 00:04:41.810
这是一个非常流行的概念。00:04:41.810 --> 00:04:45.200
除了讲课，00:04:45.200 --> 00:04:49.735
作业也是如此。00:04:49.735 --> 00:04:51.720
你知道，00:04:51.720 --> 00:04:57.050
我们一直在做作业，所以他们一开始都很介绍性，00:04:57.050 --> 00:05:01.340
逐渐减少脚手架的使用，00:05:01.340 --> 00:05:03.395
我们希望，00:05:03.395 --> 00:05:10.530
继续，嗯，在第五项作业中少举手。00:05:10.530 --> 00:05:13.910
你知道，我猜我们想做的就是让你做好准备00:05:13.910 --> 00:05:17.495
无论是期末项目还是现实生活。00:05:17.495 --> 00:05:21.005
我想我今天早上打了个比方，00:05:21.005 --> 00:05:25.370
把这个和CS介绍序列比较一下，00:05:25.370 --> 00:05:29.135
所以当CS106A和B有很多脚手架的时候，00:05:29.135 --> 00:05:31.025
在CS107中，00:05:31.025 --> 00:05:34.850
你应该学习如何诊断和解决问题00:05:34.850 --> 00:05:38.910
对于你自己来说，在调试器中也是一样的，00:05:38.910 --> 00:05:41.010
对于神经网络来说，00:05:41.010 --> 00:05:43.770
对于早期的作业，00:05:43.770 --> 00:05:46.610
我们已经给了你所有的握手00:05:46.610 --> 00:05:49.490
这些测试是为了确保每一点都没问题，00:05:49.490 --> 00:05:51.815
下面就是具体的结构。00:05:51.815 --> 00:05:54.305
但是，在现实世界中，00:05:54.305 --> 00:05:57.695
你只能建立和使用神经网络。00:05:57.695 --> 00:06:00.259
如果你能弄明白为什么它们不工作00:06:00.259 --> 00:06:02.990
你必须改变什么才能让它们发挥作用。00:06:02.990 --> 00:06:06.785
事实上，正如我上周所说的，00:06:06.785 --> 00:06:11.210
这往往是超过一半的工作，似乎很容易坚持下来。00:06:11.210 --> 00:06:14.270
这是我的神经网络和对我有意义的部分，00:06:14.270 --> 00:06:17.660
然后你就可以把剩下的80%的时间花在这上面00:06:17.660 --> 00:06:21.230
挠头想知道为什么它不能很好地工作，00:06:21.230 --> 00:06:24.335
以及你如何改变它使它工作得更好。00:06:24.335 --> 00:06:29.810
我承认调试神经网络很困难，但是，00:06:29.810 --> 00:06:34.190
我们的目标是你应该从中学到一些东西，00:06:34.190 --> 00:06:38.600
这也是这门课的学习目标之一。00:06:38.600 --> 00:06:41.045
最后一个小广告。00:06:41.045 --> 00:06:43.370
如果你想看书，00:06:43.370 --> 00:06:45.155
嗯，就在这周，00:06:45.155 --> 00:06:48.305
有一本关于使用PyTorch进行自然语言处理的新书00:06:48.305 --> 00:06:51.575
作者:Delip Rao和Brian McMahan。00:06:51.575 --> 00:06:53.990
Delip实际上住在旧金山。00:06:53.990 --> 00:06:56.660
如果你愿意00:06:56.660 --> 00:06:58.310
当然，你可以买一本。00:06:58.310 --> 00:07:00.230
但如果你不想00:07:00.230 --> 00:07:03.230
买了它，你会想要仔细看看，00:07:03.230 --> 00:07:09.110
斯坦福图书馆实际上拥有O'Reilly的Safari图书收藏许可。00:07:09.110 --> 00:07:14.945
你可以从library。stanford。edu开始免费阅读。00:07:14.945 --> 00:07:18.230
这里有一个陷阱，那就是图书馆只有一个00:07:18.230 --> 00:07:21.710
Safari Books的16个同步许可。00:07:21.710 --> 00:07:25.450
所以，如果你也想让你的同学能够免费阅读这本书，00:07:25.450 --> 00:07:29.944
如果你记得在网上登录Safari图书，00:07:29.944 --> 00:07:32.270
当你看完之后。00:07:32.270 --> 00:07:34.785
嗯，是的，所以这是一种，00:07:34.785 --> 00:07:36.420
我的意思是，在某种意义上，00:07:36.420 --> 00:07:39.025
我希望你们看这本书的时候能感觉到，00:07:39.025 --> 00:07:41.610
“天哪，那些东西我大部分都已经知道了。00:07:41.610 --> 00:07:43.740
这不是一本超级高级的书。00:07:43.740 --> 00:07:49.780
但这是一个编写良好的教程，介绍了如何使用PyTorch和NLP。”00:07:49.780 --> 00:07:52.620
如果你觉得这本书里的大部分内容你都不懂，00:07:52.620 --> 00:07:56.250
你可以让我知道，但我会有点难过。00:07:56.250 --> 00:08:01.030
嗯，好吧，嗯，是的。00:08:01.030 --> 00:08:03.760
从今天开始。00:08:03.760 --> 00:08:06.430
我们花了很多时间00:08:06.430 --> 00:08:10.630
递归神经网络对很多东西都很有用。00:08:10.630 --> 00:08:15.670
但是有些事情他们并不擅长。00:08:15.670 --> 00:08:21.280
所以，你知道，我们可能想知道像我的出生这样的短语，00:08:21.280 --> 00:08:23.800
或者一个更大的短语，比如我的出生，00:08:23.800 --> 00:08:27.550
没有独立的，00:08:27.550 --> 00:08:31.480
这些跨度在递归神经网络中的表示。00:08:31.480 --> 00:08:35.365
我们得到了整个句子的前缀。00:08:35.365 --> 00:08:38.815
当我们这样做的时候，双向的，00:08:38.815 --> 00:08:42.100
递归神经网络，你可能会说，00:08:42.100 --> 00:08:45.670
等一下，你可以在两个方向上使用它，在某种程度上这是正确的。00:08:45.670 --> 00:08:49.120
我们可以从这个方向得到东西，从这个方向得到东西，00:08:49.120 --> 00:08:51.265
但是我们仍然有00:08:51.265 --> 00:08:54.730
到句末或句末的整个序列。00:08:54.730 --> 00:08:57.790
我们不只是有几句话。00:08:57.790 --> 00:09:03.595
通常，我们想算出句子的意思，00:09:03.595 --> 00:09:06.280
这里有两个问题。00:09:06.280 --> 00:09:09.835
我们只有初始和最终的子序列。00:09:09.835 --> 00:09:14.230
而且，如果你看这些表述，比如你说，00:09:14.230 --> 00:09:18.820
将最后一种状态作为文本含义的表示。00:09:18.820 --> 00:09:20.080
你会发现，00:09:20.080 --> 00:09:22.360
它是由"的"这个词的意思决定的吗00:09:22.360 --> 00:09:27.640
最新的词汇，以及他们试图预测接下来会发生什么，00:09:27.640 --> 00:09:30.085
这也是我提到的原因之一00:09:30.085 --> 00:09:33.280
上次的问题回答课，00:09:33.280 --> 00:09:37.060
你可以通过哨兵和训练做得更好00:09:37.060 --> 00:09:41.755
它关注于整个LSTM结构。00:09:41.755 --> 00:09:44.560
好。但是今天我们要看00:09:44.560 --> 00:09:48.565
另一种方法是卷积神经网络，00:09:48.565 --> 00:09:53.485
通常缩写为CNN或ConvNets。00:09:53.485 --> 00:09:57.385
这个想法是，00:09:57.385 --> 00:09:59.920
听着，也许我们可以00:09:59.920 --> 00:10:06.910
每个特定长度的子序列并计算它的表示形式，00:10:06.910 --> 00:10:10.090
所以，如果我们有一段文字，00:10:10.090 --> 00:10:12.684
为保持政府开放而达成的临时协议，00:10:12.684 --> 00:10:14.320
我们可以说，00:10:14.320 --> 00:10:17.110
我们取这三个单词的序列，00:10:17.110 --> 00:10:19.765
暂时达成协议，达成协议，00:10:19.765 --> 00:10:21.385
伸手去抓，等等，00:10:21.385 --> 00:10:26.470
我们要计算这些序列的某种表示形式。00:10:26.470 --> 00:10:30.250
所以，这不是一个强烈的语言概念。00:10:30.250 --> 00:10:33.430
对吧?我们不担心它是否是一个连贯的短语，00:10:33.430 --> 00:10:36.310
这在语法上是有效的，00:10:36.310 --> 00:10:41.125
从认知上讲，我们只是取一定长度的每个子序列。00:10:41.125 --> 00:10:45.370
然后，一旦我们计算了它们的表示，00:10:45.370 --> 00:10:48.025
我们来看看如何把它们分组。00:10:48.025 --> 00:10:55.900
好。所以，让我们更详细地了解一下CNN是什么以及它们是如何工作的。00:10:55.900 --> 00:11:01.900
这就是卷积的一般概念00:11:01.900 --> 00:11:07.855
没有在数学或电气工程课上见过。00:11:07.855 --> 00:11:12.010
然后是卷积的特殊形式，00:11:12.010 --> 00:11:15.310
离散卷积，你可以这样理解00:11:15.310 --> 00:11:18.910
你可以使用友好的求和符号而不是积分。00:11:18.910 --> 00:11:20.905
这是，00:11:20.905 --> 00:11:22.480
这是一个离散卷积。00:11:22.480 --> 00:11:25.495
我发现这个符号毫无用处。00:11:25.495 --> 00:11:27.040
我就不解释了。00:11:27.040 --> 00:11:28.690
但我有很多例子，00:11:28.690 --> 00:11:34.075
卷积对于神经网络来说是很简单的就它们的作用而言。00:11:34.075 --> 00:11:38.590
好的，卷积神经网络的经典例子，00:11:38.590 --> 00:11:40.270
是在视觉应用。00:11:40.270 --> 00:11:44.605
如果你下个季度做CS231N，00:11:44.605 --> 00:11:47.770
你知道，最开始的四周就是这样00:11:47.770 --> 00:11:51.715
卷积神经网络的所有变种和荣耀。00:11:51.715 --> 00:11:55.540
我的基本想法是，00:11:55.540 --> 00:11:57.894
视觉的卷积，00:11:57.894 --> 00:12:02.410
你想要识别事物无论它们出现在图像的什么地方。00:12:02.410 --> 00:12:05.620
所以你有一种平移和方差的性质，00:12:05.620 --> 00:12:08.230
卷积的概念00:12:08.230 --> 00:12:10.810
在图像的不同地方找到一些东西，00:12:10.810 --> 00:12:12.670
不管它出现在哪里。00:12:12.670 --> 00:12:19.360
这是我从Andrew Ng的UFLDL网站上偷来的视觉例子。00:12:19.360 --> 00:12:21.925
卷积是什么，00:12:21.925 --> 00:12:24.130
这是一个补丁，00:12:24.130 --> 00:12:26.815
但是你可以把它看成一个向量，00:12:26.815 --> 00:12:31.450
而patch的权值就是这些红色的小数字，00:12:31.450 --> 00:12:32.995
你要做的是00:12:32.995 --> 00:12:40.345
是像这个动画一样在图像上滑动那个补丁。00:12:40.345 --> 00:12:43.075
所以在每个位置，00:12:43.075 --> 00:12:47.905
你要把每个红色的数乘以那个位置的黑色数，00:12:47.905 --> 00:12:50.080
然后把它们加起来。00:12:50.080 --> 00:12:53.245
这就是离散卷积的作用，00:12:53.245 --> 00:12:55.180
这就是上面那个符号的意思，00:12:55.180 --> 00:12:58.495
对吧?你把它们相乘然后相加，00:12:58.495 --> 00:13:00.235
所以你这样做，00:13:00.235 --> 00:13:04.240
然后用粉色填充生成物，00:13:04.240 --> 00:13:05.710
和乘积。00:13:05.710 --> 00:13:07.855
所以，这有点像，你拿这些00:13:07.855 --> 00:13:12.400
将点积放到粉色矩阵中，00:13:12.400 --> 00:13:14.815
这就是卷积的特征。00:13:14.815 --> 00:13:17.350
这是一个二维卷积，00:13:17.350 --> 00:13:18.760
今天剩下的时间里，00:13:18.760 --> 00:13:20.470
我们不会再看了。00:13:20.470 --> 00:13:23.215
所以，这就是你学习视觉的全部内容。00:13:23.215 --> 00:13:28.390
现在我们回头来看一维卷积，00:13:28.390 --> 00:13:32.995
这就是人们在使用卷积神经网络处理文本时所使用的。00:13:32.995 --> 00:13:36.610
那么，文本卷积神经网络的起点，00:13:36.610 --> 00:13:38.410
我们有一个输入。00:13:38.410 --> 00:13:42.190
这是我的句子和每个单词00:13:42.190 --> 00:13:45.970
在这个句子中，我有一个密集的向量，00:13:45.970 --> 00:13:51.325
我把它画成4D，想让它在我的例子中保持小但是通常你们知道，它更大。00:13:51.325 --> 00:13:54.580
我们的起点是输入，00:13:54.580 --> 00:13:58.060
输入可以是一个热编码，这里没有禁止，00:13:58.060 --> 00:14:01.795
但通常我们会有这些密集的词向量。00:14:01.795 --> 00:14:06.310
所以它和3D和2D是一样的，00:14:06.310 --> 00:14:08.185
除了我们只有一个维度。00:14:08.185 --> 00:14:10.510
我们有一个过滤器。00:14:10.510 --> 00:14:14.410
这是我们的过滤器，00:14:14.410 --> 00:14:21.680
我们的过滤器会做三个步骤和时间，三个单词。00:14:21.750 --> 00:14:25.930
这在所有维度上都是成立的。00:14:25.930 --> 00:14:28.240
这些不同的维度00:14:28.240 --> 00:14:32.500
卷积神经网络通常被称为信道。00:14:32.500 --> 00:14:35.665
我们在输入通道上工作，00:14:35.665 --> 00:14:37.990
我们有一个这样的补丁。00:14:37.990 --> 00:14:45.430
我们要把这个补丁放在前三个单词的上面。00:14:45.430 --> 00:14:47.980
我的动画没有上一张那么好。00:14:47.980 --> 00:14:51.610
对不起。我们要算出点积，00:14:51.610 --> 00:14:56.410
在这两者之间，我在家里用Excel做的。00:14:56.410 --> 00:14:58.015
答案(笑声)是，00:14:58.015 --> 00:15:01.255
乘积是- 1。0。00:15:01.255 --> 00:15:05.495
然后在这一点，我们滑动，00:15:05.495 --> 00:15:08.345
我们把这个幻灯片，00:15:08.345 --> 00:15:11.410
矩阵，它被称为核或者00:15:11.410 --> 00:15:16.305
过滤器是卷积神经网络的补丁。00:15:16.305 --> 00:15:21.520
我们把它滑下来，然后再做这些项的点积。00:15:21.520 --> 00:15:28.955
结果是- 1 / 2我们继续往下滑，得到，00:15:28.955 --> 00:15:33.095
得到右边显示的输出。00:15:33.095 --> 00:15:34.265
所以在这一点上，00:15:34.265 --> 00:15:36.690
我们只是简化了句子，00:15:36.690 --> 00:15:39.105
对一个向量。00:15:39.105 --> 00:15:44.740
看来我们要做的不止这些。00:15:44.740 --> 00:15:48.455
但你会注意到的另一件事是00:15:48.455 --> 00:15:52.500
我们的句子缩小了，因为之前，00:15:52.500 --> 00:15:57.710
我们有一个7个单词的句子，但是因为我刚刚滑过了这个3个单词，00:15:57.710 --> 00:15:59.615
恩，内核在这里，00:15:59.615 --> 00:16:03.015
最后我只有五个位置可以放进去。00:16:03.015 --> 00:16:05.825
所以它变成了一个五个字的东西。00:16:05.825 --> 00:16:08.960
首先要解决这个问题，00:16:08.960 --> 00:16:14.030
通常当人们使用卷积神经网络时，他们会添加填充。00:16:14.030 --> 00:16:18.790
我能做的就是在这里加上零填充00:16:18.790 --> 00:16:25.805
两端都做同样的操作然后对它进行卷积。00:16:25.805 --> 00:16:31.355
现在，我可以把我的3号滤镜放到7个不同的位置00:16:31.355 --> 00:16:37.835
把它滑下来，就得到了一个与输入长度相同的向量。00:16:37.835 --> 00:16:40.650
嗯，你知道，有不同的方法，00:16:40.650 --> 00:16:43.200
这是最常见的方法。00:16:43.200 --> 00:16:46.765
这看起来很合理，因为它保持了大小。00:16:46.765 --> 00:16:50.460
我的意思是，你知道，总有不止一种方法。00:16:50.460 --> 00:16:52.310
如果你真的想00:16:52.310 --> 00:16:54.390
你，噢，我不想要你，是的，00:16:54.390 --> 00:16:59.560
哦，我做了，00:16:59.560 --> 00:17:05.855
我在幻灯片上犯了一个小错误00:17:05.855 --> 00:17:08.405
我马上就会讲到00:17:08.405 --> 00:17:12.790
但我还是要解释一下(笑声)。00:17:12.790 --> 00:17:15.450
如果你想的话00:17:15.450 --> 00:17:19.740
你可以在这两端有两步填充。00:17:19.740 --> 00:17:24.290
第一个卷积我们将会看到，00:17:24.290 --> 00:17:30.585
10的^()次方然后卷积就会增大输入的大小。00:17:30.585 --> 00:17:35.915
是的。但,是的。我的意思是,00:17:35.915 --> 00:17:38.565
到目前为止，00:17:38.565 --> 00:17:41.380
我们从这些向量开始00:17:41.380 --> 00:17:46.335
卷积神经网络项的长度为4。00:17:46.335 --> 00:17:49.475
我们的输入有四个通道。00:17:49.475 --> 00:17:53.030
但当我们回到这里时00:17:53.030 --> 00:17:56.515
我们只是用这个来生产，00:17:56.515 --> 00:17:59.690
内核，输出的一列。00:17:59.690 --> 00:18:02.560
所以我们的输出只有一个通道。00:18:02.560 --> 00:18:08.690
所以我们把列方向从4缩小到1。00:18:08.690 --> 00:18:11.490
这似乎很糟糕。00:18:11.490 --> 00:18:14.105
从很多方面来说，这都是不好的。00:18:14.105 --> 00:18:16.714
所以很多时候，00:18:16.714 --> 00:18:21.164
你要做的是说，00:18:21.164 --> 00:18:25.325
与其只有一个过滤器，00:18:25.325 --> 00:18:29.260
相反，为什么我没有几个过滤器呢?00:18:29.260 --> 00:18:32.680
这里我有三个不同的过滤器00:18:32.680 --> 00:18:36.620
这些过滤器的大小都差不多，00:18:36.620 --> 00:18:41.825
3倍的大小，内核大小乘以输入，00:18:41.825 --> 00:18:46.145
矩阵的通道数。00:18:46.145 --> 00:18:49.550
我有三个不同的过滤器，我要运行00:18:49.550 --> 00:18:53.380
每一个都沿着文本向下，在这里得到一列。00:18:53.380 --> 00:18:56.510
现在，我得到了三列输出。00:18:56.510 --> 00:18:59.675
我有这样一个三通道输出。00:18:59.675 --> 00:19:04.940
直观地思考这个问题的方法是，00:19:04.940 --> 00:19:07.505
你知道，对于我们在神经网络中所做的，00:19:07.505 --> 00:19:11.040
我们将通过反向传播来学习它们。00:19:11.040 --> 00:19:16.760
但我们希望这些过滤器能以某种方式专门处理不同的事情。00:19:16.760 --> 00:19:20.480
也许这个过滤器可以专门，00:19:20.480 --> 00:19:22.355
这种语言礼貌吗?00:19:22.355 --> 00:19:26.725
当它看到礼貌的词语时，就会产生很高的价值。00:19:26.725 --> 00:19:29.850
也许，嗯，这个，嗯，00:19:29.850 --> 00:19:35.605
过滤器可以专门，我不知道，00:19:35.605 --> 00:19:38.795
吃，它将有一个很高的价值，每当它看到的话00:19:38.795 --> 00:19:42.430
关于食物，你知道这个过滤器会做第三件事。00:19:42.430 --> 00:19:49.235
这就是人们有时谈论的意义，00:19:49.235 --> 00:19:53.075
你得到的是不同功能的输出，因为你的希望就是这样00:19:53.075 --> 00:19:57.515
你会从文本中获得不同的潜在特征。00:19:57.515 --> 00:20:02.555
好。这给了我们一个表示00:20:02.555 --> 00:20:07.540
在我们的课本中找到了一些有用的学习功能。00:20:07.540 --> 00:20:11.290
我们经常想做的是00:20:11.290 --> 00:20:15.610
根据这些特点总结全文。00:20:15.610 --> 00:20:18.030
所以你可能会问，00:20:18.030 --> 00:20:20.045
在这段文字中，00:20:20.045 --> 00:20:23.435
它有礼貌吗?它谈论食物吗?00:20:23.435 --> 00:20:26.555
我们会经常做的另一个手术00:20:26.555 --> 00:20:30.410
要做的是总结卷积网络的输出。00:20:30.410 --> 00:20:32.750
最简单的方法是，00:20:32.750 --> 00:20:35.110
对于一维卷积，00:20:35.110 --> 00:20:37.635
称为最大时间池。00:20:37.635 --> 00:20:40.080
所以如果我们让池子最大，00:20:40.080 --> 00:20:43.935
每个频道或其他被称为功能，00:20:43.935 --> 00:20:53.865
我们只要往下看它的最大值是多少，0。3 1。6 1。4。00:20:53.865 --> 00:20:55.775
所以，你知道，00:20:55.775 --> 00:20:58.730
如果我用前两个故事，00:20:58.730 --> 00:21:00.700
过滤器，它的意思是，00:21:00.700 --> 00:21:04.600
这不是很礼貌的短信，但它确实是关于食物的，对吧?00:21:04.600 --> 00:21:06.300
我们在总结，00:21:06.300 --> 00:21:08.460
我们在那里发现了什么。00:21:08.460 --> 00:21:14.400
在某种意义上，最大汇聚的概念，00:21:14.400 --> 00:21:18.640
这个东西在什么地方被激活了吗?00:21:18.640 --> 00:21:22.180
所以如果我们有礼貌和食物方面的东西，00:21:22.180 --> 00:21:25.510
最大池的输出将具有较高的值。00:21:25.510 --> 00:21:28.600
如果句子中的某个地方有一个明确的标记00:21:28.600 --> 00:21:32.035
礼貌或食物方面的一些明显的东西。00:21:32.035 --> 00:21:37.210
这是一个很有用的概念因为你想知道的是，00:21:37.210 --> 00:21:42.260
你知道，这句话里有没有关于食物的讨论?00:21:42.260 --> 00:21:46.150
还有一件事，你可以做其他的事。00:21:46.150 --> 00:21:48.635
而不是max pooling，00:21:48.635 --> 00:21:51.210
你可以做平均池。00:21:51.210 --> 00:21:55.405
这里你只需要取这些数然后求它们的平均值。00:21:55.405 --> 00:21:58.910
这就有了不同的语义00:21:58.910 --> 00:22:02.595
这种礼貌的平均程度是多少，00:22:02.595 --> 00:22:05.855
或者平均有多少，00:22:05.855 --> 00:22:10.265
这句话有多少是关于食物之类的。00:22:10.265 --> 00:22:12.190
出于某些目的，00:22:12.190 --> 00:22:13.680
这样更好，因为，00:22:13.680 --> 00:22:16.960
它把所有重要的构建都平均起来。00:22:16.960 --> 00:22:18.900
很多时候，00:22:18.900 --> 00:22:22.890
人们发现最大汇聚更好，因为，00:22:22.890 --> 00:22:27.490
很多自然语言中的信号都是稀疏的。00:22:27.490 --> 00:22:30.630
不管你有多礼貌00:22:30.630 --> 00:22:32.940
你不可能每句话都彬彬有礼。00:22:32.940 --> 00:22:37.430
你会说名词和冠词，00:22:37.430 --> 00:22:40.390
介词和连词，00:22:40.390 --> 00:22:42.635
这些都不是天生的礼貌，对吧?00:22:42.635 --> 00:22:46.325
所以如果有一些明显的礼貌，00:22:46.325 --> 00:22:51.470
然后这句话就变得有礼貌了而max pooling实际上更适合捕捉这一点。00:22:51.470 --> 00:22:54.430
当然还有另一种方法00:22:54.430 --> 00:22:58.115
min集中起来，发现最不活跃的东西(笑声)。00:22:58.115 --> 00:23:01.135
它不常被使用，但你也可以这么做。00:23:01.135 --> 00:23:04.380
好。如果你在PyTorch，00:23:04.380 --> 00:23:07.365
这都是很容易做到的。00:23:07.365 --> 00:23:10.005
所以就有了一个便利的时髦对流。00:23:10.005 --> 00:23:13.025
你可能会猜到，视觉上也有一个Conv2d。00:23:13.025 --> 00:23:15.005
但是有一个对流，00:23:15.005 --> 00:23:18.790
指定有多少输入通道。00:23:18.790 --> 00:23:20.725
这就是我们的单词嵌入大小。00:23:20.725 --> 00:23:22.735
有多少输出通道?00:23:22.735 --> 00:23:24.365
我们有三个。00:23:24.365 --> 00:23:27.820
卷积核的大小是多少?00:23:27.820 --> 00:23:29.525
我们展示的这些也是00:23:29.525 --> 00:23:32.380
三个，然后还有其他各种参数。00:23:32.380 --> 00:23:35.990
你可以说你想要一个填充1之类的东西。00:23:35.990 --> 00:23:38.075
一旦你有了其中一个，00:23:38.075 --> 00:23:39.695
你可以随便跑00:23:39.695 --> 00:23:44.355
对输入进行卷积滤波，得到一个新的隐藏状态。00:23:44.355 --> 00:23:46.220
如果你想要最大限度地利用资源，00:23:46.220 --> 00:23:47.570
你可以max，00:23:47.570 --> 00:23:51.750
通过它的输出然后你就得到了一个最大的合并输出。00:23:51.750 --> 00:23:58.869
好。这给了我们构建卷积神经网络的基础，00:23:58.869 --> 00:24:01.150
嗯，为了，嗯，NLP。00:24:01.150 --> 00:24:04.280
在那之前，这有意义吗?00:24:06.000 --> 00:24:10.570
是的。好。下一点是展示00:24:10.570 --> 00:24:15.265
你可以做三到四件事。00:24:15.265 --> 00:24:18.325
我开始打这些幻灯片00:24:18.325 --> 00:24:20.920
其他不太有用的概念，因为我00:24:20.920 --> 00:24:23.590
有点想，至少它们在NLP中不常出现。00:24:23.590 --> 00:24:28.090
但是，实际上，当我开始写第二篇论文的时候，00:24:28.090 --> 00:24:32.740
当我说复杂卷积神经网络时，00:24:32.740 --> 00:24:37.750
在那篇论文中，他们尝试了所有这些我说没人用过的东西。00:24:37.750 --> 00:24:42.145
所以知道它们是用来看各种各样的论文的很好。00:24:42.145 --> 00:24:49.795
到目前为止，当我们计算这些卷积时，00:24:49.795 --> 00:24:52.660
我们在每个位置都尝试过。00:24:52.660 --> 00:24:55.285
所以我们有一个零的临时协议。00:24:55.285 --> 00:24:58.420
然后就暂时达成协议，然后再达成协议。00:24:58.420 --> 00:25:00.970
所以我们只是走下一个台阶00:25:00.970 --> 00:25:04.765
一段时间，被称为一个人的跨步。00:25:04.765 --> 00:25:08.095
这是目前最常见的做法。00:25:08.095 --> 00:25:09.595
但是你可以观察到，00:25:09.595 --> 00:25:10.825
等一下，00:25:10.825 --> 00:25:15.655
因为第一个卷积是关于零假设的。00:25:15.655 --> 00:25:18.085
我把这三个字都写在里面了。00:25:18.085 --> 00:25:25.225
即使我跳至next did，交易达成，然后我继续保持政府，00:25:25.225 --> 00:25:30.460
我仍然会在一个或另一个卷积中保留句子中的每个单词00:25:30.460 --> 00:25:32.950
所以我可以做一半的计算00:25:32.950 --> 00:25:35.635
在某种意义上，所有的东西都在里面。00:25:35.635 --> 00:25:38.425
这就是所谓的2步。00:25:38.425 --> 00:25:42.130
然后我得到了一半的行数。00:25:42.130 --> 00:25:46.840
这是一种压缩表示和生成的方法00:25:46.840 --> 00:25:52.855
更短的句子，我们稍后会看到它的用法。00:25:52.855 --> 00:25:59.890
还有其他的方法来压缩从句子中提取的切分表示。00:25:59.890 --> 00:26:05.710
所以有一个不同的池的概念就是局部池。00:26:05.710 --> 00:26:09.640
如果你们见过00:26:09.640 --> 00:26:13.510
当人们谈论最大汇聚和愿景时，00:26:13.510 --> 00:26:16.960
它们通常指的是地方统筹，而不是00:26:16.960 --> 00:26:21.400
我先给你们看的时间最大汇聚。00:26:21.400 --> 00:26:27.070
这就回到了我们开始的地方00:26:27.070 --> 00:26:33.535
我们的大小为3步1卷积和之前一样产生输出。00:26:33.535 --> 00:26:39.310
但现在，我要做的是当地的游泳池，以两步的速度。00:26:39.310 --> 00:26:44.650
也就是说，我要把每两行集合起来00:26:44.650 --> 00:26:47.110
第一行，我可以再做一次00:26:47.110 --> 00:26:50.680
要么是最大值，要么是平均值，或者任何对我有吸引力的东西。00:26:50.680 --> 00:26:53.200
我取前两行，00:26:53.200 --> 00:26:54.970
我最大限度地集中他们，我明白了。00:26:54.970 --> 00:26:56.800
我取后面两行，00:26:56.800 --> 00:26:58.555
我最大限度地集中他们，我明白了。00:26:58.555 --> 00:27:01.420
下两个，下两个，我把它垫起来00:27:01.420 --> 00:27:04.285
底部有两行。00:27:04.285 --> 00:27:09.415
然后给我一个2步的局部最大值。00:27:09.415 --> 00:27:13.300
从某种意义上说，这也有同样的效果00:27:13.300 --> 00:27:16.990
以不同的结果作为使用两步进00:27:16.990 --> 00:27:20.530
我的卷积因为我又把它化简成00:27:20.530 --> 00:27:26.090
原来是八行，现在是四行。00:27:26.970 --> 00:27:29.935
是的,照片。00:27:29.935 --> 00:27:33.640
好的，就是这个。00:27:33.640 --> 00:27:35.410
你还能做什么?00:27:35.410 --> 00:27:38.080
你可以做更多的事情来使它变得复杂。00:27:38.080 --> 00:27:43.765
人们有时做的另一件事是k-max池。00:27:43.765 --> 00:27:49.510
这是一个更复杂的问题，00:27:49.510 --> 00:27:53.530
而不是随着时间的推移保持最大值，00:27:53.530 --> 00:28:00.325
如果一个特征在句子中被激活了两三次，00:28:00.325 --> 00:28:03.640
也许把所有的时间都记录下来会很好00:28:03.640 --> 00:28:07.375
在句子中激活，同时丢弃其余的。00:28:07.375 --> 00:28:09.070
在k-max池中，00:28:09.070 --> 00:28:10.870
这里取最大值为2，00:28:10.870 --> 00:28:17.335
向下看这一列，你会发现这一列的两个最大值。00:28:17.335 --> 00:28:23.665
然后你把两个最高值按从高到低的顺序排列，00:28:23.665 --> 00:28:26.620
而是它们在这些列中的顺序。00:28:26.620 --> 00:28:28.840
所以是- 0。2，00:28:28.840 --> 00:28:32.230
这个是0。3，这个是1。6，00:28:32.230 --> 00:28:38.065
0。6，因为它反映了上面列的顺序。00:28:38.065 --> 00:28:43.210
好。差不多完成了，还有一个概念。00:28:43.210 --> 00:28:52.285
这是另一种压缩数据的方法即膨胀卷积。00:28:52.285 --> 00:28:55.315
如果你有一个膨胀的卷积，00:28:55.315 --> 00:29:01.870
所以在这里做膨胀卷积实际上没有意义但是你可以在哪里使用它00:29:01.870 --> 00:29:08.440
膨胀卷积就是如果我把这个放到另一个卷积层，00:29:08.440 --> 00:29:13.540
我们可以有深度卷积网络它有多个卷积层。00:29:13.540 --> 00:29:20.560
膨胀卷积的概念是你要跳过一些行。00:29:20.560 --> 00:29:24.295
所以如果你从顶部开始用两个膨胀，00:29:24.295 --> 00:29:27.460
你要选第一个，第三个，00:29:27.460 --> 00:29:31.870
第五行乘以我的费尔，抱歉，00:29:31.870 --> 00:29:32.980
我有不同的过滤器。00:29:32.980 --> 00:29:38.305
将它们乘以过滤器，然后得到这里的值。00:29:38.305 --> 00:29:40.480
然后如果像这样大步走，00:29:40.480 --> 00:29:46.900
然后你会使用，你会继续做下一个展开行。00:29:46.900 --> 00:29:51.025
这样就可以得到卷积了00:29:51.025 --> 00:29:56.680
没有很多参数的句子的更大的扩展。00:29:56.680 --> 00:29:59.065
所以你不必这样做。00:29:59.065 --> 00:30:00.670
你可以说，00:30:00.670 --> 00:30:07.015
我可以用内核大小为5的卷积代替。00:30:07.015 --> 00:30:08.470
然后他们会说5，00:30:08.470 --> 00:30:11.500
连续看五个单词，然后我就有了00:30:11.500 --> 00:30:17.230
更大的矩阵来指定我的特征。00:30:17.230 --> 00:30:20.770
然而，这样我就可以保持矩阵很小，但是仍然00:30:20.770 --> 00:30:25.105
查看一个操作中句子的更大范围。00:30:25.105 --> 00:30:30.670
是啊，还有那句话你说了多少00:30:30.670 --> 00:30:36.490
see是卷积神经网络中的一个重要概念。00:30:36.490 --> 00:30:39.940
因为，你知道，如果你从一个句子的开头开始00:30:39.940 --> 00:30:43.780
你只是在做3×3的卷积，00:30:43.780 --> 00:30:47.995
你看到的是句子的这三个单词。00:30:47.995 --> 00:30:50.350
事实证明，在自然语言中00:30:50.350 --> 00:30:53.305
实际上已经是一个很有用的表示了。00:30:53.305 --> 00:30:56.920
因为有这些n克的特征00:30:56.920 --> 00:31:01.165
非常适合用于多种用途，包括文本分类。00:31:01.165 --> 00:31:05.680
但是如果你想了解更多句子的语义，00:31:05.680 --> 00:31:08.575
不知何故，你想马上看到更多。00:31:08.575 --> 00:31:13.780
你有一些工具可以用来查看更多一次，00:31:13.780 --> 00:31:15.730
你可以使用更大的过滤器，00:31:15.730 --> 00:31:16.870
你可以用00:31:16.870 --> 00:31:18.460
内核大小5 7，00:31:18.460 --> 00:31:20.650
9或者其他卷积。00:31:20.650 --> 00:31:25.585
你可以做一些类似于展开卷积的事情这样你就可以看到分散的图像。00:31:25.585 --> 00:31:28.120
你能做的第三件事就是00:31:28.120 --> 00:31:30.835
可以有深度的卷积神经网络。00:31:30.835 --> 00:31:35.605
因为卷积神经网络的深度越大，你看到的就越多。00:31:35.605 --> 00:31:37.690
所以在第一层，00:31:37.690 --> 00:31:43.150
现在行中有关于三个单词的信息。00:31:43.150 --> 00:31:46.630
如果你把第二层粘上00:31:46.630 --> 00:31:48.280
卷积神经网络00:31:48.280 --> 00:31:51.670
同样的基本性质，你可以00:31:51.670 --> 00:31:55.450
前三行，然后再进行卷积00:31:55.450 --> 00:32:00.940
然后是下一个，这些人知道你原来输入句子的五个单词。00:32:00.940 --> 00:32:03.700
当你有一个更深的卷积网络栈时00:32:03.700 --> 00:32:07.495
开始了解越来越大的句子。00:32:07.495 --> 00:32:09.970
好。所有好吗?00:32:09.970 --> 00:32:12.530
有什么问题吗?00:32:14.760 --> 00:32:22.899
不，很好。所以，下一段基本上是再次向你们展示这个东西，00:32:22.899 --> 00:32:26.560
在特定论文的背景下。00:32:26.560 --> 00:32:27.850
所以这是，嗯，00:32:27.850 --> 00:32:32.125
哈佛学生Yoon Kim的一篇论文，00:32:32.125 --> 00:32:36.460
也许还是个哈佛学生，嗯，2014年。00:32:36.460 --> 00:32:39.790
这是一篇相当早期的论文。00:32:39.790 --> 00:32:45.520
他想证明卷积神经网络可以做到这一点00:32:45.520 --> 00:32:47.500
干得好00:32:47.500 --> 00:32:52.240
文本分类当您想要分类的是一个句子时。00:32:52.240 --> 00:32:55.750
你可能想做的是00:32:55.750 --> 00:33:00.400
你在烂番茄网站上看到的电影评论片段，00:33:00.400 --> 00:33:04.900
“这是一个肯定的还是否定的句子描述?”00:33:04.900 --> 00:33:08.155
他建立的模型实际上是类似的00:33:08.155 --> 00:33:11.695
对于卷积神经网络，00:33:11.695 --> 00:33:14.980
在他们2011年的论文中00:33:14.980 --> 00:33:18.100
在我们讨论基于窗口的分类器之前提到过。00:33:18.100 --> 00:33:20.500
所以，在他们的论文中他们实际上使用00:33:20.500 --> 00:33:25.600
基于窗口的分类器和卷积分类器。00:33:25.600 --> 00:33:28.570
好。嗯,是的,00:33:28.570 --> 00:33:29.800
我已经说过了。00:33:29.800 --> 00:33:34.210
所以他们的任务是句子分类，可能是感情。00:33:34.210 --> 00:33:35.875
也可以是其他的，00:33:35.875 --> 00:33:39.100
这个句子是主观的还是客观的?00:33:39.100 --> 00:33:42.040
所以客观是主要新闻文章的意思00:33:42.040 --> 00:33:45.295
主观是意见的组成部分。00:33:45.295 --> 00:33:48.970
然后是其他的东西，比如问题分类。00:33:48.970 --> 00:33:51.220
这是一个关于一个人的问题吗，00:33:51.220 --> 00:33:53.200
地点，电话号码，还是别的什么?00:33:53.200 --> 00:33:57.400
这就是他所做的。00:33:57.400 --> 00:34:01.495
这些幻灯片，00:34:01.495 --> 00:34:06.880
使用他论文的符号，有点不同00:34:06.880 --> 00:34:09.310
数学运算的方式和我刚才展示的一样00:34:09.310 --> 00:34:12.160
你，它实际上在做完全一样的事情。00:34:12.160 --> 00:34:16.930
我们从长度为k的词向量开始，00:34:16.930 --> 00:34:24.610
这个句子是由把所有的词向量连接在一起组成的，00:34:24.610 --> 00:34:27.280
我们有一系列的单词，00:34:27.280 --> 00:34:30.190
它是句子向量的一部分。00:34:30.190 --> 00:34:36.310
卷积滤波器就是用向量表示的，因为00:34:36.310 --> 00:34:42.100
这里他把所有的东西都平展成一个很长的向量，00:34:42.100 --> 00:34:44.515
而我却走进了一个矩阵。00:34:44.515 --> 00:34:51.070
所以大小为3的卷积就是长度为hk的实向量，00:34:51.070 --> 00:34:56.350
卷积滤波器的大小乘以单词的维数。00:34:56.350 --> 00:35:01.210
那么，他要做什么来建造00:35:01.210 --> 00:35:07.450
他的文本分类器使用不同大小的卷积。00:35:07.450 --> 00:35:10.765
所以可以有大小为2的卷积，00:35:10.765 --> 00:35:16.000
如图所示，大小为三个卷积，以及更大的卷积。00:35:16.000 --> 00:35:23.140
所以，嗯，为了计算CNN的一个特色频道，我们00:35:23.140 --> 00:35:26.620
然后在权重向量之间做点积00:35:26.620 --> 00:35:30.415
特征乘以相同项的子序列，00:35:30.415 --> 00:35:35.035
他还提出了一种偏见，我忽略了。00:35:35.035 --> 00:35:41.110
然后把它变成非线性，00:35:41.110 --> 00:35:43.390
我也没这么做00:35:43.390 --> 00:35:46.045
但是我们已经看到了很多。00:35:46.045 --> 00:35:49.810
我们想做的是，00:35:49.810 --> 00:35:53.410
我们想要，00:35:53.410 --> 00:35:58.150
完成所有这些――对于内核大小为3的特性，00:35:58.150 --> 00:36:00.880
我们要把这句话从头到尾讲一遍。00:36:00.880 --> 00:36:04.735
他做的另一件事有点滑稽，00:36:04.735 --> 00:36:08.920
他的窗户在符号上是不对称的。00:36:08.920 --> 00:36:11.695
有一个词，00:36:11.695 --> 00:36:15.355
右边是h - 1个单词。00:36:15.355 --> 00:36:20.095
所以他在右边有内边距00:36:20.095 --> 00:36:25.810
大多数人绕着物体在两个方向对称地做卷积。00:36:25.810 --> 00:36:31.630
好。我们会对一些特征做这个00:36:31.630 --> 00:36:34.480
通道Ci，因此计算00:36:34.480 --> 00:36:38.680
我们之前讲过的卷积表示。00:36:38.680 --> 00:36:43.435
好。然后他就像我们说的那样做了。00:36:43.435 --> 00:36:48.370
随着时间的推移，池层中需要捕获的池最多00:36:48.370 --> 00:36:53.650
最相关的是给我们每个频道一个数字。00:36:53.650 --> 00:37:01.465
我们有一些不同的特性，它们有不同的内核大小。00:37:01.465 --> 00:37:08.230
这是他使用的另一个观点，可能是一个很好的观点。00:37:08.230 --> 00:37:13.659
他知道一件事你甚至可以用不同的方式来思考，00:37:13.659 --> 00:37:17.350
比如说一个问答系统。00:37:17.350 --> 00:37:21.605
所以他使用了预先训练好的单词向量。00:37:21.605 --> 00:37:28.980
但是他实际上是把向量这个词翻倍了。00:37:28.980 --> 00:37:32.475
所以，对于每个单词，他都有两个向量的拷贝，00:37:32.475 --> 00:37:37.290
你有两个信道集和一个he00:37:37.290 --> 00:37:42.375
他僵住了，另一只在训练中调整得很好。00:37:42.375 --> 00:37:46.590
所以他试图在两方面都达到最好的微调效果00:37:46.590 --> 00:37:51.770
而不是微调所有这些都进入了最大池操作。00:37:51.770 --> 00:38:01.614
好。那么，在最大池之后，我们为每个频道取出一个数字，00:38:01.614 --> 00:38:06.760
他有三个大小的卷积，00:38:06.760 --> 00:38:10.390
每个尺寸有4、5、100个功能。00:38:10.390 --> 00:38:13.435
所以我们得到了一个大小的向量，00:38:13.435 --> 00:38:15.670
当时是300人，00:38:15.670 --> 00:38:19.810
在这一点，你取最后一个向量并把它粘在上面00:38:19.810 --> 00:38:24.595
通过softmax，然后给出类的分类。00:38:24.595 --> 00:38:31.495
所有这些都可以在这张图中总结出来如果它足够大，可以阅读。00:38:31.495 --> 00:38:32.800
这是我们的句子。00:38:32.800 --> 00:38:34.855
我非常喜欢这部电影，00:38:34.855 --> 00:38:39.310
我们的嵌入维数是5，00:38:39.310 --> 00:38:42.265
在这个例子中，00:38:42.265 --> 00:38:46.930
对于每个内核大小，我们都有两个通道00:38:46.930 --> 00:38:52.030
我们考虑大小为2、3和4的内核。00:38:52.030 --> 00:38:57.205
然后我们得到两个不同的。00:38:57.205 --> 00:39:01.615
我们得到6个。00:39:01.615 --> 00:39:04.405
这是我们的六个过滤器。00:39:04.405 --> 00:39:07.180
我们应用这些。00:39:07.180 --> 00:39:10.975
当我们-当我们应用那些没有填充的过滤器时，00:39:10.975 --> 00:39:15.880
然后我们得到这些滤波器的输出大小为4，00:39:15.880 --> 00:39:18.985
五个，六个。00:39:18.985 --> 00:39:23.065
一旦我们有了这些00:39:23.065 --> 00:39:27.265
对于每一组数，我们都在做一个最大池。00:39:27.265 --> 00:39:30.880
我们取每一个的最大值，00:39:30.880 --> 00:39:36.715
输出特征给出了这六个数字。00:39:36.715 --> 00:39:43.060
我们可以把它们连接成一个矢量，00:39:43.060 --> 00:39:50.120
嗯，对于情绪是积极的还是消极的，两门课以上的学生给出了一个软限制。00:39:52.580 --> 00:39:55.590
这就是基本的模型。00:39:55.590 --> 00:40:01.200
这其实很简单，00:40:01.200 --> 00:40:03.630
计算效率很高，00:40:03.630 --> 00:40:06.780
为如何构建文本分类器建模。00:40:06.780 --> 00:40:13.155
[噪音]嗯，是的，还有几件事要处理，00:40:13.155 --> 00:40:15.210
在其中一项作业中，00:40:15.210 --> 00:40:17.700
我们讨论了辍学[噪音]，你使用了它。00:40:17.700 --> 00:40:19.065
所以，嗯，你知道，00:40:19.065 --> 00:40:21.705
希望你们都是中途退学的大师。00:40:21.705 --> 00:40:24.720
所以他使用了辍学法00:40:24.720 --> 00:40:28.185
这是2014年，00:40:28.185 --> 00:40:31.820
辍学论文只在2014年发表。00:40:31.820 --> 00:40:34.895
我想，早在几年前就有了一个更早的版本。00:40:34.895 --> 00:40:37.160
这还是相当早的，00:40:37.160 --> 00:40:39.425
利用辍学的机会00:40:39.425 --> 00:40:41.135
所以在训练的时候，00:40:41.135 --> 00:40:44.105
你有这样一个辍学向量，00:40:44.105 --> 00:40:49.010
对伯努利随机变量抽样，00:40:49.010 --> 00:40:54.825
某种程度上，设计是为了在你每次做事情的时候去掉一些功能。00:40:54.825 --> 00:40:58.200
在测试的时候，你不会退出，00:40:58.200 --> 00:41:02.130
但因为在你放弃很多东西之前，00:41:02.130 --> 00:41:07.455
你把你的权重矩阵乘以你退出的概率，00:41:07.455 --> 00:41:09.000
所以你会得到，00:41:09.000 --> 00:41:12.000
与之前相同尺度的向量。00:41:12.000 --> 00:41:15.075
正如我们在作业中讨论过的，00:41:15.075 --> 00:41:18.420
辍学是一种非常有效的正规化形式，00:41:18.420 --> 00:41:20.580
广泛应用于神经网络。00:41:20.580 --> 00:41:23.700
他不仅这么做了，他真的这么做了00:41:23.700 --> 00:41:27.600
这是另一种时髦的正则化形式。00:41:27.600 --> 00:41:31.425
这就是软最大权向量，00:41:31.425 --> 00:41:35.280
他限制了L2规范，00:41:35.280 --> 00:41:41.100
所以权重向量的平方范数和softmax，[噪音]嗯，00:41:41.100 --> 00:41:45.405
矩阵，一个固定的数S，00:41:45.405 --> 00:41:47.460
这是一组超参数，00:41:47.460 --> 00:41:49.515
设置为3。00:41:49.515 --> 00:41:53.055
如果你的体重过大00:41:53.055 --> 00:41:55.590
他们被重新调整了等级，00:41:55.590 --> 00:41:57.345
所以他们没有爆炸。00:41:57.345 --> 00:42:00.210
这不是一件很常见的事。00:42:00.210 --> 00:42:03.690
我不确定是不是很有必要，但是，00:42:03.690 --> 00:42:05.850
我想它给了你一些――我的意思是，00:42:05.850 --> 00:42:09.045
我想通过向你们展示这个的一些细节，00:42:09.045 --> 00:42:10.590
我的希望是，00:42:10.590 --> 00:42:13.680
给你一些关于你可以玩很多东西的想法00:42:13.680 --> 00:42:17.025
如果你想尝试不同的东西，00:42:17.025 --> 00:42:19.020
你的期末作业。00:42:19.020 --> 00:42:21.000
嗯,好吧。00:42:21.000 --> 00:42:24.120
这是他最后的一些超参数。00:42:24.120 --> 00:42:27.360
他用的是ReLU非线性，00:42:27.360 --> 00:42:30.765
窗户大小有三，四，五，00:42:30.765 --> 00:42:35.790
卷积，每个大小的100个特征或通道，00:42:35.790 --> 00:42:38.535
像往常一样中途退学。00:42:38.535 --> 00:42:41.865
你从辍学中得到了几个百分比的进步，00:42:41.865 --> 00:42:43.860
这其实很常见。00:42:43.860 --> 00:42:47.835
L2约束，s = 3，00:42:47.835 --> 00:42:50.175
小批量50个，00:42:50.175 --> 00:42:52.635
300维向量，00:42:52.635 --> 00:42:55.755
培训以最大化开发集性能。00:42:55.755 --> 00:42:58.830
好。这是一张大桌子，00:42:58.830 --> 00:43:00.690
我太懒了00:43:00.690 --> 00:43:06.570
重做这些不同文本分类数据集的性能。00:43:06.570 --> 00:43:08.460
嗯，有很多不同的。00:43:08.460 --> 00:43:11.820
这两个都是斯坦福情感树银行。00:43:11.820 --> 00:43:14.565
这是主客观语言。00:43:14.565 --> 00:43:19.650
这是问题分类，问的是一个人的名字和位置，00:43:19.650 --> 00:43:20.790
公司什么的。00:43:20.790 --> 00:43:24.150
这是，00:43:24.150 --> 00:43:26.280
从某种角度来说，00:43:26.280 --> 00:43:28.335
这是另一个分类。00:43:28.335 --> 00:43:30.885
消费者报告是另一种观点。00:43:30.885 --> 00:43:36.210
有很多数据集，还有很多模型。00:43:36.210 --> 00:43:41.580
下面的模型，00:43:41.580 --> 00:43:46.020
是传统的基于特征的分类器。00:43:46.020 --> 00:43:48.000
特别地，00:43:48.000 --> 00:43:52.230
有点像2012年的王和我00:43:52.230 --> 00:43:56.025
通过采取特定的步骤00:43:56.025 --> 00:44:00.720
用n-g特征和其他形式的归一化，00:44:00.720 --> 00:44:03.420
你可以得到很好的结果00:44:03.420 --> 00:44:06.960
只是传统的基于分类器的特征。00:44:06.960 --> 00:44:12.045
很多人以此为基准来证明你可以做得更好。00:44:12.045 --> 00:44:14.360
上面这些，00:44:14.360 --> 00:44:18.200
我的小组很喜欢树状神经网络吗00:44:18.200 --> 00:44:22.805
在2010年代早期，在最顶端，00:44:22.805 --> 00:44:24.695
他的CNN模特。00:44:24.695 --> 00:44:26.510
正如你所看到的，00:44:26.510 --> 00:44:27.875
这是一种混合。00:44:27.875 --> 00:44:30.870
有时CNN模式会赢，00:44:30.870 --> 00:44:33.015
比如这一列和这一列，00:44:33.015 --> 00:44:36.015
有时候它不像在这些专栏里赢。00:44:36.015 --> 00:44:38.010
但总的来说，00:44:38.010 --> 00:44:40.260
你没有看到的是，00:44:40.260 --> 00:44:43.140
这非常简单，00:44:43.140 --> 00:44:46.335
卷积神经网络模型，00:44:46.335 --> 00:44:48.720
嗯，这个系统还不错。00:44:48.720 --> 00:44:54.720
你可以对结果表吹毛求疵00:44:54.720 --> 00:45:01.285
就像写你的项目建议书一样，00:45:01.285 --> 00:45:07.250
你应该做的一件事是想想你在读什么，00:45:07.250 --> 00:45:10.100
因为，你知道，很多论文并不完美00:45:10.100 --> 00:45:13.130
对他们的说法吹毛求疵是有理由的。00:45:13.130 --> 00:45:17.785
有时候，如果你考虑他们的主张是否合理，00:45:17.785 --> 00:45:20.895
这是有原因的，或者有想法00:45:20.895 --> 00:45:24.405
你如何做不同的事情或展示不同的东西。00:45:24.405 --> 00:45:27.315
我的意思是，你可以吹毛求疵的主要原因，00:45:27.315 --> 00:45:31.365
Yoon Kim的结果表是，00:45:31.365 --> 00:45:35.385
他已经说过，我之前有几张幻灯片，00:45:35.385 --> 00:45:37.980
Dropout给出的语句00:45:37.980 --> 00:45:41.220
神经网络的准确率提高了2%到4%。00:45:41.220 --> 00:45:45.210
[噪音]嗯，但是这些系统中的大多数是因为它们00:45:45.210 --> 00:45:49.365
都是在辍学之前完成的，00:45:49.365 --> 00:45:51.390
没有利用辍学。00:45:51.390 --> 00:45:55.170
但是，你知道，上面的任何神经网络系统00:45:55.170 --> 00:45:59.445
可以用Dropout，它可能会给他们一些，00:45:59.445 --> 00:46:01.140
也增加了百分之几。00:46:01.140 --> 00:46:05.385
所以可以说，这是一种有偏见的，不公平的比较。00:46:05.385 --> 00:46:10.635
正确的做法是比较所有的系统，使用Dropout。00:46:10.635 --> 00:46:12.120
但是，你知道，00:46:12.120 --> 00:46:13.890
尽管如此，00:46:13.890 --> 00:46:16.980
这仍然是一个prett-很多人注意到00:46:16.980 --> 00:46:20.820
这篇论文因为它表明使用这种非常简单的方法，00:46:20.820 --> 00:46:23.190
非常快的卷积架构，00:46:23.190 --> 00:46:27.070
可以为您提供强大的文本分类结果。00:46:28.250 --> 00:46:31.005
嗯,就是这样。00:46:31.005 --> 00:46:33.765
是的。所以总的来说,00:46:33.765 --> 00:46:38.475
你知道，你应该在项目中考虑一些事情，00:46:38.475 --> 00:46:44.370
我们正在有效地建立一个更大的工具包，里面有你可以使用的不同工具，00:46:44.370 --> 00:46:48.135
项目，未来的工作等等。00:46:48.135 --> 00:46:49.635
首先，00:46:49.635 --> 00:46:53.250
我们有单词向量，然后我们可以构建一个袋子00:46:53.250 --> 00:46:57.105
向量模型就是取向量这个词，然后求它们的平均值。00:46:57.105 --> 00:47:01.080
你知道，这实际上是一个非常好的基线。00:47:01.080 --> 00:47:03.960
在很多情况下，我们会向你建议，00:47:03.960 --> 00:47:05.085
你应该用这个。00:47:05.085 --> 00:47:06.270
看看效果如何，00:47:06.270 --> 00:47:07.965
确保你工作得更好。00:47:07.965 --> 00:47:10.605
我的意思是，特别是，你可以做得更好，00:47:10.605 --> 00:47:14.490
如果你在上面添加一些额外的ReLU图层，00:47:14.490 --> 00:47:18.015
这是一个已经在深度平均网络中得到探索的想法。00:47:18.015 --> 00:47:22.290
然后我们看了一些非常简单的橱窗模型。00:47:22.290 --> 00:47:23.850
你只是拿这些00:47:23.850 --> 00:47:27.585
五个单词窗口，并在上面计算一个前馈网络，00:47:27.585 --> 00:47:32.835
而且它们对于只需要局部上下文的单词分类问题非常有效。00:47:32.835 --> 00:47:36.045
比如词性标注或NER。00:47:36.045 --> 00:47:39.390
然后我们继续看一些其他的模型。00:47:39.390 --> 00:47:45.405
CNN非常适合文本分类，00:47:45.405 --> 00:47:49.590
它们非常好，因为它们在gpu上并行化得非常好，00:47:49.590 --> 00:47:51.840
这个我稍后会再讲。00:47:51.840 --> 00:47:57.510
所以他们，他们只是一般的代表句子的意思。00:47:57.510 --> 00:47:59.099
它们实际上是有效的，00:47:59.099 --> 00:48:02.295
通用性强，方法好，已被广泛应用。00:48:02.295 --> 00:48:05.460
然后它们和递归神经网络形成对比。00:48:05.460 --> 00:48:07.800
递归神经网络具有一定的优势。00:48:07.800 --> 00:48:10.080
它们在认知上更可信，00:48:10.080 --> 00:48:12.120
因为你在通读课文，00:48:12.120 --> 00:48:14.145
明白它的意思了。00:48:14.145 --> 00:48:16.830
递归神经网络很有用00:48:16.830 --> 00:48:19.800
比如序列标记和分类，00:48:19.800 --> 00:48:23.385
建立语言模型来预测接下来会发生什么。00:48:23.385 --> 00:48:26.910
嗯，当它们和注意力结合在一起的时候，它们可以做得很好。00:48:26.910 --> 00:48:29.565
但是它们也有一些缺点。00:48:29.565 --> 00:48:33.870
它们比卷积神经网络慢得多00:48:33.870 --> 00:48:38.300
我们要做的是找出一个句子的整体意思表示，00:48:38.300 --> 00:48:39.845
“这是什么意思?00:48:39.845 --> 00:48:41.375
这两个，00:48:41.375 --> 00:48:43.850
短语互相转述?”00:48:43.850 --> 00:48:46.730
现在有很多研究结果表明00:48:46.730 --> 00:48:49.805
用递归神经网络不能得到更好的结果。00:48:49.805 --> 00:48:55.440
使用卷积神经网络等技术可以得到更好的结果。00:48:55.550 --> 00:49:05.010
好。[噪音]所以下一步[噪音]是，00:49:05.010 --> 00:49:09.675
某种程度上，朝向我们的com-我们的复合体，00:49:09.675 --> 00:49:12.375
卷积架构的例子。00:49:12.375 --> 00:49:14.010
在讲这个之前，00:49:14.010 --> 00:49:18.525
我只是想介绍一些我们没见过的概念，00:49:18.525 --> 00:49:22.625
当我们这样做的时候，所有这些都开始出现。00:49:22.625 --> 00:49:26.360
我们在序列模型部分花了很多时间，00:49:26.360 --> 00:49:32.345
讨论门控模型或门控递归单元和LSTM单元。00:49:32.345 --> 00:49:36.080
但是门的概念是一般的，我们可以00:49:36.080 --> 00:49:40.130
我们可以计算一些东西，00:49:40.130 --> 00:49:42.175
把它穿过去00:49:42.175 --> 00:49:47.370
一个sigmoid非线性函数，得到一个介于0和1之间的值，00:49:47.370 --> 00:49:50.385
或者一个值在0和1之间的向量。00:49:50.385 --> 00:49:52.980
然后对向量做哈达玛积00:49:52.980 --> 00:49:55.860
在它的值和0之间进行门运算。00:49:55.860 --> 00:49:59.490
这说明你也可以应用00:49:59.490 --> 00:50:04.110
当你建立多层网络时，垂直门。00:50:04.110 --> 00:50:07.845
在连续的LSTMs被证明之后，00:50:07.845 --> 00:50:11.780
这是一个很受欢迎的想法，00:50:11.780 --> 00:50:13.730
人们开始探索，00:50:13.730 --> 00:50:19.445
我们如何使用这些跳过连接和门控的思想，00:50:19.445 --> 00:50:21.425
垂直方向?00:50:21.425 --> 00:50:23.480
这里有两个版本。00:50:23.480 --> 00:50:26.450
这个很简单，00:50:26.450 --> 00:50:30.965
但是一个非常成功的例子基本上就是一个跳转连接。00:50:30.965 --> 00:50:36.890
这被称为残差块，它在残差网络中使用，00:50:36.890 --> 00:50:38.690
也称为ResNets。00:50:38.690 --> 00:50:42.470
所以在剩余块中，对于每个块，00:50:42.470 --> 00:50:48.440
你允许一个值直接跳到下一层。00:50:48.440 --> 00:50:52.535
或者你可以通过conv模块，00:50:52.535 --> 00:50:56.825
典型的conv块是通过卷积层，00:50:56.825 --> 00:50:59.600
然后通过一个相关的非线性，00:50:59.600 --> 00:51:03.250
另一个卷积层，当你出来的时候，00:51:03.250 --> 00:51:05.430
把这两个值相加。00:51:05.430 --> 00:51:07.710
这是相同的思路00:51:07.710 --> 00:51:11.820
和LSTM一样，值的求和也很神奇。00:51:11.820 --> 00:51:15.165
然后把它的输出通过另一个ReLU，00:51:15.165 --> 00:51:18.705
这个东西叫做残差块00:51:18.705 --> 00:51:22.950
通常你会把剩余的块叠在一起。00:51:22.950 --> 00:51:25.230
这里有个小技巧，00:51:25.230 --> 00:51:28.320
你需要使用填充，对吧?00:51:28.320 --> 00:51:33.000
因为在一天结束的时候因为你想要总结这两条路径，00:51:33.000 --> 00:51:35.355
你希望它们大小相同。00:51:35.355 --> 00:51:36.585
如果你，00:51:36.585 --> 00:51:40.200
让它们在conv块中收缩你就不能对它们求和了。00:51:40.200 --> 00:51:45.120
所以你想要，在每个阶段都有一个填充所以它们在这里保持相同的大小，00:51:45.120 --> 00:51:47.437
这样你就可以把它们加起来。00:51:47.437 --> 00:51:54.500
这是块的另一个版本00:51:54.500 --> 00:51:57.470
有点像lstm00:51:57.470 --> 00:52:01.710
这一块是由J眉和学生的rgen。施密德胡贝尔表示,00:52:01.710 --> 00:52:06.000
谁是LSTMs背后的同一个人，你可以看到同样的想法。00:52:06.000 --> 00:52:08.150
它被称为高速公路街区。00:52:08.150 --> 00:52:10.800
所以在某种程度上是相似的。00:52:10.800 --> 00:52:16.080
你需要，你知道，考虑移动一个跳过的恒等式x00:52:16.080 --> 00:52:23.085
一个非线性块或者你可以让它经历完全相同的东西conv, relu, conv。00:52:23.085 --> 00:52:26.480
不同之处在于，00:52:26.480 --> 00:52:29.165
这次是显式门，00:52:29.165 --> 00:52:33.290
还有这个t形门和这个c形门。00:52:33.290 --> 00:52:39.230
所以这条路径和这条路径相乘00:52:39.230 --> 00:52:42.280
通过一个门，就像那种00:52:42.280 --> 00:52:47.130
我们之前看到的get输入门然后把它们加起来。00:52:47.130 --> 00:52:50.670
所以那种感觉更强烈00:52:50.670 --> 00:52:56.285
很强大，但并不清楚它是否更强大。00:52:56.285 --> 00:52:59.460
我的意思是，这个其实很简单00:52:59.460 --> 00:53:03.070
语义的，因为如果你想想这个的语义00:53:03.070 --> 00:53:05.930
默认的是你走了吗00:53:05.930 --> 00:53:11.015
这样，你就只是在某种程度上发扬你的价值，什么也不做。00:53:11.015 --> 00:53:14.900
这个block的作用是，00:53:14.900 --> 00:53:18.155
是为了学习一个要学习的delta吗00:53:18.155 --> 00:53:21.750
你对什么都不做有什么偏离。00:53:21.750 --> 00:53:25.210
这是一个很简单的语义，00:53:25.210 --> 00:53:28.680
似乎在神经网络中学习很有效。00:53:28.680 --> 00:53:31.390
嗯，这有点00:53:31.390 --> 00:53:36.500
更复杂的语义，00:53:36.500 --> 00:53:43.005
恒等式的某些部分乘以哈达玛乘积中的这种门00:53:43.005 --> 00:53:49.880
这个conv块的某些部分乘以哈达玛积中的另一个门T。00:53:49.880 --> 00:53:53.980
所以那种感觉更强大00:53:53.980 --> 00:53:58.325
给了我更多的控制权，因为我可以取不同的部分，等等。00:53:58.325 --> 00:54:01.620
如果你再仔细想想，00:54:01.620 --> 00:54:05.380
从数学上讲，它并不比你更强大00:54:05.380 --> 00:54:09.500
可以用这个和那个表示任何你能做的事情。00:54:09.500 --> 00:54:13.530
我们可以这样想，00:54:13.530 --> 00:54:19.410
你知道，在这里你只保留了一部分身份，00:54:19.410 --> 00:54:26.840
但你能做的就是保持完整的身份，把它当成你的工作00:54:26.840 --> 00:54:30.095
减去这个没有保留的位00:54:30.095 --> 00:54:34.440
在这里的conv块中，理论上是可以做到的。00:54:34.440 --> 00:54:39.480
你可以用这个函数来计算任何东西，00:54:39.480 --> 00:54:42.830
你可以用一个ResNet块来计算。00:54:42.830 --> 00:54:47.185
在神经网络领域，00:54:47.185 --> 00:54:49.330
问题不在于，00:54:49.330 --> 00:54:53.190
某种计算的证明――可以计算，也可以不计算。00:54:53.190 --> 00:54:58.455
这可以归结为学习和规则化问题00:54:58.455 --> 00:55:01.345
不管这些中的哪一个能证明00:55:01.345 --> 00:55:05.270
更好地用于学习架构。00:55:06.430 --> 00:55:09.680
好。第二个概念。00:55:09.680 --> 00:55:11.860
嗯,批处理规范化。00:55:11.860 --> 00:55:17.405
所以当人们构建深度卷积神经网络时，00:55:17.405 --> 00:55:21.680
在2015年的加号中，00:55:21.680 --> 00:55:27.065
他们几乎总是使用批处理标准化层，因为00:55:27.065 --> 00:55:32.685
这让你的生活变得更好，如果他们不使用批处理标准化层，00:55:32.685 --> 00:55:37.070
他们通常使用人们提出的另一种不同的观点00:55:37.070 --> 00:55:42.165
比如层归一化，它的作用是一样的。00:55:42.165 --> 00:55:46.090
那么批处理标准化是做什么的呢?00:55:46.090 --> 00:55:50.650
我的意思是，我想你们很多人都已经看到了00:55:50.650 --> 00:55:56.305
否则做z变换的意思就是取数据，00:55:56.305 --> 00:55:59.100
你算出它的均值，你算出它的均值00:55:59.100 --> 00:56:03.970
然后通过减法和00:56:03.970 --> 00:56:07.710
这样你就有了一组数据00:56:07.710 --> 00:56:12.360
均值为0，标准差为1。00:56:12.360 --> 00:56:14.680
大多数人都看到了，对吧?00:56:14.680 --> 00:56:23.500
是吗?批处理规范化就是这样做的，但方式有点奇怪。00:56:23.500 --> 00:56:27.770
所以你要做的是取每一小批。00:56:27.770 --> 00:56:31.875
所以不管你把32个随机的例子放在一个小批里，00:56:31.875 --> 00:56:34.040
你让它们通过一层00:56:34.040 --> 00:56:37.355
你的神经网络就像我们之前看到的一个凸块00:56:37.355 --> 00:56:43.190
取这个小批的输出然后对它进行z变换。00:56:43.190 --> 00:56:47.295
然后进入下一个对流块，00:56:47.295 --> 00:56:49.410
下次你有不同的小批，00:56:49.410 --> 00:56:50.990
只要对它进行z变换。00:56:50.990 --> 00:56:52.290
这看起来有点奇怪。00:56:52.290 --> 00:56:56.600
你只需要对这些小批量的产品进行输出。00:56:56.600 --> 00:57:01.680
但事实证明这是非常有效的。00:57:01.680 --> 00:57:05.980
所以这就意味着结果00:57:05.980 --> 00:57:09.890
一个凸块总是有相同的尺度。00:57:09.890 --> 00:57:13.720
所以它不会有很大的波动，也不会把事情搞砸，它会00:57:13.720 --> 00:57:18.225
让模型更可靠地可训练，因为，00:57:18.225 --> 00:57:22.860
你知道，你只要对很多事情不那么挑剔就行了。00:57:22.860 --> 00:57:25.505
因为，你知道，我们谈过的很多事情，00:57:25.505 --> 00:57:28.180
关于初始化参数和00:57:28.180 --> 00:57:31.130
设定你的学习速度，00:57:31.130 --> 00:57:34.310
你必须保持事情的规模，这样他们就不会得到00:57:34.310 --> 00:57:37.810
太大或太小之类的。00:57:37.810 --> 00:57:40.280
然而，如果你在进行批处理标准化，00:57:40.280 --> 00:57:42.490
你在强迫规模，00:57:42.490 --> 00:57:45.705
每次的尺寸都一样。00:57:45.705 --> 00:57:48.370
所以s o，你不需要这么做00:57:48.370 --> 00:57:51.200
其他的东西也一样，它仍然倾向于，00:57:51.200 --> 00:57:52.710
工作得很好。00:57:52.710 --> 00:57:55.650
这是一个很好的技巧。00:57:55.690 --> 00:57:59.800
好。最后一件事。00:57:59.800 --> 00:58:02.070
有一个概念，00:58:02.070 --> 00:58:07.015
一个卷积的大小。00:58:07.015 --> 00:58:11.240
事实上，我想我真的有点，00:58:11.240 --> 00:58:14.680
重命名-我命名错了，因为我写下来了00:58:14.680 --> 00:58:18.240
一个接一个的卷积因为这是你通常看到的项。00:58:18.240 --> 00:58:22.530
但那是，嗯，你有二维卷积的视觉世界。00:58:22.530 --> 00:58:26.135
我想我应该把这个叫做卷积。00:58:26.135 --> 00:58:28.890
所以可以有卷积，00:58:28.890 --> 00:58:33.070
当你第一次看到它的时候，00:58:33.070 --> 00:58:37.840
这似乎没有任何意义，因为整个想法00:58:37.840 --> 00:58:43.305
卷积就是我取这个补丁并从中计算一些东西。00:58:43.305 --> 00:58:48.330
如果我不看别的词，00:58:48.330 --> 00:58:50.510
我当然没有算计什么。00:58:50.510 --> 00:58:54.975
但是在大小为1的卷积中，00:58:54.975 --> 00:58:59.160
如果你有很多频道00:58:59.160 --> 00:59:03.850
在上一层，如果你计算过它是什么，00:59:03.850 --> 00:59:06.605
32个频道之类的。00:59:06.605 --> 00:59:11.070
一个接一个卷积的作用是00:59:11.070 --> 00:59:16.625
一个微小的嵌入式全连接网络。00:59:16.625 --> 00:59:18.910
所以你在做00:59:18.910 --> 00:59:22.280
具体位置全连通网络，00:59:22.280 --> 00:59:26.385
对于你的每一行数据。00:59:26.385 --> 00:59:28.050
所以你可以这样做，00:59:28.050 --> 00:59:29.590
原因有很多。00:59:29.590 --> 00:59:31.920
你可以这样做，因为你想从00:59:31.920 --> 00:59:34.870
有很多频道，但频道却越来越少00:59:34.870 --> 00:59:37.460
你可以这样做，因为你想到了另一个非线性00:59:37.460 --> 00:59:40.345
这是一种非常便宜的方法。00:59:40.345 --> 00:59:44.150
因为要注意的关键是如果你排序00:59:44.150 --> 00:59:47.990
把所有东西都连接起来00:59:47.990 --> 00:59:52.930
它们包含了很多参数00:59:52.930 --> 00:59:56.650
一个卷积只包含很少的参数00:59:56.650 --> 01:00:00.310
因为你只是在一个单词的水平上做这件事。01:00:00.670 --> 01:00:03.765
嗯,好吧。01:00:03.765 --> 01:00:08.585
两个随机的东西，然后我将进入我的复杂模型。01:00:08.585 --> 01:00:10.540
这只是一种01:00:10.540 --> 01:00:13.660
几乎是一种偏见，但它只是表明01:00:13.660 --> 01:00:17.110
你可以做一些不同的，你可以玩的东西。01:00:17.110 --> 01:00:20.065
我的意思是，当我们谈到机器翻译时，01:00:20.065 --> 01:00:24.500
我们讨论了引入的SIC到SIC架构01:00:24.500 --> 01:00:29.930
2014年，在机器翻译方面非常成功。01:00:29.930 --> 01:00:32.680
但实际上，在它出版的前一年，01:00:32.680 --> 01:00:34.855
有一篇论文，01:00:34.855 --> 01:00:41.255
由英国的Nal Kalchbrenner和Phil Blunsom进行神经机器翻译。01:00:41.255 --> 01:00:44.015
这实际上是01:00:44.015 --> 01:00:48.840
近代第一篇神经机器翻译论文。01:00:48.840 --> 01:00:50.400
如果你挖得足够深，01:00:50.400 --> 01:00:52.130
实际上有几个人尝试过01:00:52.130 --> 01:00:54.135
机器翻译的神经网络01:00:54.135 --> 01:00:58.445
在80年代和90年代但是这是第一次重新开始，01:00:58.445 --> 01:01:02.205
他们实际上并没有使用SIC到SIC架构。01:01:02.205 --> 01:01:05.690
所以他们用的是编码器，01:01:05.690 --> 01:01:08.490
他们使用卷积神经网络。01:01:08.490 --> 01:01:13.430
所以他们有一堆卷积神经网络它们在不断缩小01:01:13.430 --> 01:01:18.760
向下输入，最后拉出一个句子表示，01:01:18.760 --> 01:01:22.960
然后他们使用序列模型作为解码器。01:01:22.960 --> 01:01:26.880
这是你可以做到的01:01:26.880 --> 01:01:30.520
试试其他一些应用程序，01:01:30.520 --> 01:01:33.800
使用卷积神经网络真的很简单。01:01:33.800 --> 01:01:39.175
也有使用卷积神经网络作为解码器的研究，01:01:39.175 --> 01:01:44.415
虽然让你的大脑活动起来有点困难，而且使用的也不那么多。01:01:44.415 --> 01:01:50.960
然后我想提的第二件事因为我们马上就会讲到它，01:01:50.960 --> 01:01:57.305
到目前为止，我们已经做了卷积模型01:01:57.305 --> 01:02:00.890
我们的内核正在有效地恢复01:02:00.890 --> 01:02:06.050
这些字n克单位的两个字或三个字的子序列。01:02:06.050 --> 01:02:10.190
这个想法很快就发展起来了01:02:10.190 --> 01:02:14.705
对字符使用卷积也很有用。01:02:14.705 --> 01:02:17.105
你可以运行卷积神经网络01:02:17.105 --> 01:02:19.970
对单词的字符进行尝试，01:02:19.970 --> 01:02:22.640
生成一个单词嵌入，01:02:22.640 --> 01:02:25.760
这个想法已经被探索了很多次，01:02:25.760 --> 01:02:28.505
这是你们作业的一部分01:02:28.505 --> 01:02:31.715
五是建立一个字符级对流，01:02:31.715 --> 01:02:35.180
你改进的机器翻译系统。01:02:35.180 --> 01:02:40.250
我今天不会讲太多关于这个的基础知识，01:02:40.250 --> 01:02:44.270
因为周四的课是关于子单词模型的01:02:44.270 --> 01:02:49.055
我们会详细讨论不同子单词模型的所有细节。01:02:49.055 --> 01:02:53.300
但是，我想给你们看一个复杂的骗局01:02:53.300 --> 01:02:58.010
卷积神经网络也用于文本分类。01:02:58.010 --> 01:03:01.685
本质上，这和Yoon Kim的模型是一样的01:03:01.685 --> 01:03:06.230
这个模型实际上是建立在字符上的，01:03:06.230 --> 01:03:07.700
它不是建立在文字上的。01:03:07.700 --> 01:03:10.640
所以，我们是在它的基础上，01:03:10.640 --> 01:03:13.145
有一个类似单词的模型。01:03:13.145 --> 01:03:16.775
这是2017年的一篇论文，01:03:16.775 --> 01:03:21.350
这里展示的四位作者，01:03:21.350 --> 01:03:24.170
Facebook人工智能研究人员，01:03:24.170 --> 01:03:27.635
在法国，01:03:27.635 --> 01:03:30.320
他们有一个有趣的假设01:03:30.320 --> 01:03:34.205
这篇论文本质上是说，01:03:34.205 --> 01:03:42.530
到2017年，使用深度学习来提高视力的人真的在建立，01:03:42.530 --> 01:03:47.600
真正深入的网络和fi-发现他们工作很多，01:03:47.600 --> 01:03:49.790
更适合视觉任务。01:03:49.790 --> 01:03:52.205
所以，在某种程度上，01:03:52.205 --> 01:03:58.490
突破是这些人一旦这些想法出现，01:03:58.490 --> 01:04:04.445
然后它证明了你不仅可以建造一个六层或八层，01:04:04.445 --> 01:04:07.580
卷积神经网络用于视觉任务。01:04:07.580 --> 01:04:09.200
你可以开始建造，01:04:09.200 --> 01:04:14.270
视觉任务的深度网络有几十个甚至上百个01:04:14.270 --> 01:04:21.090
这些模型在经过大量数据的训练后，效果甚至更好。01:04:21.210 --> 01:04:27.115
如果这是你脑子里的想法，然后你看，01:04:27.115 --> 01:04:33.970
看看自然语言处理的过去和现在，01:04:33.970 --> 01:04:36.410
观察结果是，01:04:36.410 --> 01:04:38.390
这些NLP的人有点可怜，01:04:38.390 --> 01:04:43.550
他们声称他们正在进行深度学习，但他们仍然在使用三层LSTMs。01:04:43.550 --> 01:04:46.475
当然，我们可以取得一些进展，01:04:46.475 --> 01:04:53.735
通过建立看起来有点像视觉网络的深层网络并使用它们，01:04:53.735 --> 01:04:57.035
自然语言处理的目标。01:04:57.035 --> 01:05:01.415
这正是他们所说的。01:05:01.415 --> 01:05:08.930
所以，他们设计并建造了一个非常深的网络，看起来有点像一个视觉堆栈，01:05:08.930 --> 01:05:14.900
它是一个建立在字符之上的卷积神经网络。01:05:14.900 --> 01:05:20.660
嗯，我在这里有它的图片，但足够深，适合它01:05:20.660 --> 01:05:23.390
这张幻灯片让人读起来(笑声)有点难01:05:23.390 --> 01:05:26.150
但我们可以试着看看这个。01:05:26.150 --> 01:05:27.260
在底部，01:05:27.260 --> 01:05:29.240
我们有文本，01:05:29.240 --> 01:05:33.965
这是一个字符序列，01:05:33.965 --> 01:05:36.980
对于文本，嗯，01:05:36.980 --> 01:05:40.640
当人们做视觉物体识别时01:05:40.640 --> 01:05:44.930
图片通常所有的图片都是同样大小的。01:05:44.930 --> 01:05:50.225
正确的。每幅画都是300×300像素的。01:05:50.225 --> 01:05:53.375
所以，它们对NLP的作用是一样的，01:05:53.375 --> 01:05:55.490
它们有大小，01:05:55.490 --> 01:05:59.690
的文件，该文件为1024个字符。01:05:59.690 --> 01:06:03.710
如果比这个长，他们就把它截短，保留第一部分。01:06:03.710 --> 01:06:06.470
如果比这个短，他们会把它垫起来直到01:06:06.470 --> 01:06:11.315
1024码，然后把它放到堆栈里。01:06:11.315 --> 01:06:15.440
第一部分是每个角色，01:06:15.440 --> 01:06:18.200
他们现在要学习嵌入字符01:06:18.200 --> 01:06:22.145
它们的字符嵌入具有维度16。01:06:22.145 --> 01:06:29.540
这段文字现在是16×1024，01:06:29.540 --> 01:06:33.770
它们会穿过卷积层01:06:33.770 --> 01:06:38.210
内核大小有3个和64个输出通道。01:06:38.210 --> 01:06:44.150
现在有64乘以1024。01:06:44.150 --> 01:06:47.900
现在把它穿过一个卷积块。01:06:47.900 --> 01:06:52.085
我将在下一张幻灯片中解释卷积块的细节，01:06:52.085 --> 01:06:56.360
你应该想想我之前展示的ResNet图片01:06:56.360 --> 01:07:01.310
可以进行一些卷积，也可以使用这个可选的快捷方式。01:07:01.310 --> 01:07:05.180
另一个ResNet，另一个残块01:07:05.180 --> 01:07:08.765
卷积是一种可选的捷径，01:07:08.765 --> 01:07:15.020
然后他们用人们通常想象的方式进行本地统筹。01:07:15.020 --> 01:07:17.990
通常人们在视觉系统中做什么01:07:17.990 --> 01:07:21.530
你在缩小图片的尺寸，01:07:21.530 --> 01:07:25.820
把每个方向的维数减半。01:07:25.820 --> 01:07:27.020
但与此同时，01:07:27.020 --> 01:07:29.014
在你的神经网络中，01:07:29.014 --> 01:07:31.715
你扩大频道的数量，01:07:31.715 --> 01:07:34.130
所以你把它做得更深01:07:34.130 --> 01:07:38.105
通道在x轴上变小的同时，01:07:38.105 --> 01:07:39.710
图像的y大小。01:07:39.710 --> 01:07:44.120
除了一维卷积，它们做的是完全一样的。01:07:44.120 --> 01:07:49.760
在1024字符中有64个通道之前，01:07:49.760 --> 01:07:54.425
嵌入，文档。01:07:54.425 --> 01:07:57.110
现在我们把它合起来，01:07:57.110 --> 01:08:03.605
我们将有512个位置，有点像成对的字符，01:08:03.605 --> 01:08:06.440
但是我们现在有128个频道01:08:06.440 --> 01:08:09.380
然后他们一遍又一遍地重复，对吧?01:08:09.380 --> 01:08:11.690
还有两个卷积块01:08:11.690 --> 01:08:14.285
解释得更多，但它们是一些剩余的块。01:08:14.285 --> 01:08:17.960
他们又把钱凑在一起，做同样的事情。01:08:17.960 --> 01:08:21.305
现在有256个，01:08:21.305 --> 01:08:26.900
这些位置就像四个字符块，它们有256个通道，01:08:26.900 --> 01:08:31.460
我指的不够高，但他们又重复了一遍，他们又一起游泳。01:08:31.460 --> 01:08:33.590
所以，现在他们有，嗯，01:08:33.590 --> 01:08:36.710
128个位置，大约8个字符01:08:36.710 --> 01:08:40.775
每个都有512个通道。01:08:40.775 --> 01:08:45.080
它们又共用了卷积块，01:08:45.080 --> 01:08:47.570
看哪，因我曾说，连01:08:47.570 --> 01:08:50.060
奇怪的想法会出现，就在那里，01:08:50.060 --> 01:08:55.325
他们在做k个最大的池，并且保留了8个最强的值，01:08:55.325 --> 01:08:57.290
每个频道都有。01:08:57.290 --> 01:08:59.300
在那个时候，01:08:59.300 --> 01:09:05.195
他们的尺寸是512×8，01:09:05.195 --> 01:09:08.510
有点像8个字符序列中的8个01:09:08.510 --> 01:09:11.705
被认为对分类很重要，它们是01:09:11.705 --> 01:09:15.455
保留，但他们排序每个通道，有512个01:09:15.455 --> 01:09:19.475
然后把它放到三个完全连接的层中。01:09:19.475 --> 01:09:22.190
通常是在顶部的视觉系统01:09:22.190 --> 01:09:25.355
在末端有几个完全连接的图层，01:09:25.355 --> 01:09:28.055
最后一个，01:09:28.055 --> 01:09:31.835
有效地进入你的Softmax。01:09:31.835 --> 01:09:36.080
它的大小是2048乘以01:09:36.080 --> 01:09:41.330
课程可能是正的，也可能是负的，不像主题课。01:09:41.330 --> 01:09:44.000
嗯，基本上是这样的01:09:44.000 --> 01:09:47.180
一个视觉堆栈，但是他们要用它来做语言。01:09:47.180 --> 01:09:48.890
嗯,好吧。01:09:48.890 --> 01:09:52.340
所以，我刚才解释的是01:09:52.340 --> 01:09:57.515
这些卷积块看起来有点像之前的图像，01:09:57.515 --> 01:09:59.975
部门稍微复杂一点。01:09:59.975 --> 01:10:02.420
所以你在做01:10:02.420 --> 01:10:05.840
大小为3的卷积块01:10:05.840 --> 01:10:10.430
根据你在序列中的位置对一些通道进行卷积。01:10:10.430 --> 01:10:13.490
然后你就像我们一样，把它变成一个批处理的标准01:10:13.490 --> 01:10:17.075
说到把它放到一个相关的非线性中，01:10:17.075 --> 01:10:21.320
把这三件事重复一遍01:10:21.320 --> 01:10:25.550
是这种跳过的连接，在这个块的外面。01:10:25.550 --> 01:10:31.190
这是一个剩余样式块，01:10:31.190 --> 01:10:34.550
这是一种复杂的结构，你可以把它们放在一起01:10:34.550 --> 01:10:38.675
如果你敢尝试PyTorch，就试试你的最终项目。01:10:38.675 --> 01:10:42.775
嗯，是的，所以，01:10:42.775 --> 01:10:46.090
对于实验来说01:10:46.090 --> 01:10:52.570
他们感兴趣并想强调的是01:10:52.570 --> 01:10:55.670
这些传统的句子和01:10:55.670 --> 01:10:58.970
文本分类数据集已在其他论文中使用01:10:58.970 --> 01:11:02.465
像Yoon Kim的论文实际上是相当小的。01:11:02.465 --> 01:11:10.550
所以，像烂番茄这样的数据集实际上只有10000个例子，5000个，01:11:10.550 --> 01:11:13.550
正5000负500001:11:13.550 --> 01:11:17.180
就像ImageNet一样01:11:17.180 --> 01:11:20.435
深度学习模式，真正展示自己的价值和愿景01:11:20.435 --> 01:11:24.155
这可能确实显示了这样一个大型模型的价值。01:11:24.155 --> 01:11:28.070
你需要有很大的数据集。01:11:28.070 --> 01:11:29.855
所以，它们变得更大，01:11:29.855 --> 01:11:32.000
文本分类数据集。01:11:32.000 --> 01:11:36.065
这是一个亚马逊评论的正负数据集，01:11:36.065 --> 01:11:39.500
他们有360万份文件，01:11:39.500 --> 01:11:43.030
Yelp评论了65万份文件。01:11:43.030 --> 01:11:45.100
这么大的数据集，01:11:45.100 --> 01:11:48.230
这是他们的实验。01:11:48.230 --> 01:11:50.930
好。上面的数字，01:11:50.930 --> 01:11:55.940
对于不同的数据集的最好的以前的结果打印在文献中，01:11:55.940 --> 01:11:58.640
如果你读到，01:11:58.640 --> 01:12:03.200
脚注，嗯，有一些事情他们想成为明星。01:12:03.200 --> 01:12:07.040
所以，那些旁边有星星的就用01:12:07.040 --> 01:12:13.225
他们不使用的外部同义词典。(噪音)01:12:13.225 --> 01:12:15.640
阳法，嗯，01:12:15.640 --> 01:12:18.610
使用一些特殊的技术，以及我切断。01:12:18.610 --> 01:12:21.580
还有一件事要提一下，01:12:21.580 --> 01:12:24.175
它们是错误率，所以低是好事。01:12:24.175 --> 01:12:26.410
所以你拿的越低越好。01:12:26.410 --> 01:12:30.940
这就是所有的结果。01:12:30.940 --> 01:12:34.770
那么你能从这些结果中得到什么呢?01:12:34.770 --> 01:12:39.545
你能注意到的第一件事基本上就是这些结果，01:12:39.545 --> 01:12:42.100
更深层次的网络运行得更好，对吧?01:12:42.100 --> 01:12:44.845
我给你们看的这个，01:12:44.845 --> 01:12:48.490
不，我想我掌握的这张图片不是全部。01:12:48.490 --> 01:12:52.720
但是他们有深度为9,17的，01:12:52.720 --> 01:12:56.680
在卷积层数方面，01:12:56.680 --> 01:13:01.150
而最深的那个总是工作得最好的那个。01:13:01.150 --> 01:13:04.255
这就是深层网络的证明。01:13:04.255 --> 01:13:07.570
那没能持续下去01:13:07.570 --> 01:13:10.690
这里有一个有趣的脚注，01:13:10.690 --> 01:13:11.935
我猜他们想，01:13:11.935 --> 01:13:13.225
哦，太酷了。01:13:13.225 --> 01:13:19.315
为什么我们不试一个更深的，有47层的，看看效果如何?01:13:19.315 --> 01:13:23.635
我的意思是，结果很有趣。01:13:23.635 --> 01:13:26.125
对于第47层，01:13:26.125 --> 01:13:28.855
它的效果比这个差一点点。01:13:28.855 --> 01:13:32.050
从某种意义上说，01:13:32.050 --> 01:13:37.900
他们展示了残余层的效果非常好。01:13:37.900 --> 01:13:40.705
所以，他们做了一个实验，让我们试着训练01:13:40.705 --> 01:13:45.325
没有使用剩余连接的47层网络。01:13:45.325 --> 01:13:47.455
而且，情况更糟。01:13:47.455 --> 01:13:49.870
这个数字下降了大约2%。01:13:49.870 --> 01:13:52.825
他们训练了一个有剩余联系的人，01:13:52.825 --> 01:13:58.870
而事实是，这些数字只差了一点点。01:13:58.870 --> 01:14:02.485
他们只差了0。1%01:14:02.485 --> 01:14:05.515
所以，你知道，他们的工作也差不多。01:14:05.515 --> 01:14:10.300
但是，无论如何，这和想象中的情况是不同的，01:14:10.300 --> 01:14:15.145
因为对于人们在视觉中使用的剩余网络，01:14:15.145 --> 01:14:19.990
这是人们使用的最小深度。01:14:19.990 --> 01:14:23.485
所以，如果你在视觉中使用剩余网络，01:14:23.485 --> 01:14:25.915
您可以使用ResNet-34。01:14:25.915 --> 01:14:29.215
如果你真的内存不足，想要一个小模型，01:14:29.215 --> 01:14:32.980
但你知道如果你使用ResNet-50，效果会更好，01:14:32.980 --> 01:14:36.730
事实上，如果你使用ResNet-101，它会再次运行得更好。01:14:36.730 --> 01:14:39.625
所以你知道，01:14:39.625 --> 01:14:41.410
是否与不同的性质有关01:14:41.410 --> 01:14:44.350
语言或者数据量，01:14:44.350 --> 01:14:47.915
你还没有达到你在视觉上能达到的深度。01:14:47.915 --> 01:14:50.620
但是其他结果，01:14:50.620 --> 01:14:54.190
这里他们比较的另一件事是01:14:54.190 --> 01:14:59.245
有三种不同的方法。01:14:59.245 --> 01:15:02.950
所以你可以用，01:15:02.950 --> 01:15:06.715
卷积中的步幅，01:15:06.715 --> 01:15:09.745
你可以使用本地MaxPooling，01:15:09.745 --> 01:15:12.805
你可以使用KMaxPooling。01:15:12.805 --> 01:15:14.350
他们都很普通，01:15:14.350 --> 01:15:16.945
正如你所看到的，它们是略有不同的数字。01:15:16.945 --> 01:15:19.990
每个人都赢了01:15:19.990 --> 01:15:23.890
至少一个或至少两个这样的数据集。01:15:23.890 --> 01:15:27.430
但MaxPooling不仅赢了四个数据集，01:15:27.430 --> 01:15:29.890
如果你看看这些数字，01:15:29.890 --> 01:15:32.245
MaxPooling总是做得很好。01:15:32.245 --> 01:15:34.495
因为MaxPooling在这里做得很好，01:15:34.495 --> 01:15:38.140
尽管卷积步幅效果很差，01:15:38.140 --> 01:15:41.605
在这里MaxPooling工作得很好，01:15:41.605 --> 01:15:45.685
KMaxPooling的效果很差。01:15:45.685 --> 01:15:50.890
所以，他们的建议是你应该经常使用，01:15:50.890 --> 01:15:53.680
只是一个简单的MaxPooling，01:15:53.680 --> 01:15:55.389
这似乎很好，01:15:55.389 --> 01:15:57.340
嗯，没有别的了。01:15:57.340 --> 01:16:01.315
嗯，这实际上是值得考虑做的麻烦。01:16:01.315 --> 01:16:10.540
好。我还有其他的结论吗?01:16:10.540 --> 01:16:13.285
好。嗯，我想这就是大部分了。01:16:13.285 --> 01:16:17.440
我想他们传达的总体信息是你可以做得非常好，01:16:17.440 --> 01:16:20.470
使用ConvNets的文本分类系统，01:16:20.470 --> 01:16:22.630
你应该把这个信息拿走。01:16:22.630 --> 01:16:26.170
好。所以，只剩几分钟了。01:16:26.170 --> 01:16:30.145
还有一件事我想提一下，01:16:30.145 --> 01:16:33.505
但是我想我会很快地提一下，01:16:33.505 --> 01:16:36.505
如果你愿意，你可以看得更详细。01:16:36.505 --> 01:16:38.785
所以，我们有这种情况01:16:38.785 --> 01:16:44.065
再循环神经网络是NLP的一个非常标准的构建模块，01:16:44.065 --> 01:16:49.405
但是它们有一个很大的问题就是它们不能很好地并行化。01:16:49.405 --> 01:16:53.800
我们得到快速计算和深度学习的方法是01:16:53.800 --> 01:16:58.180
可以很好地并行化，这样我们就可以把它们放在gpu上。01:16:58.180 --> 01:17:05.425
gpu只有在能够同时多次执行相同的计算时才会运行得快，01:17:05.425 --> 01:17:08.440
这对于卷积神经网络来说是微不足道的，01:17:08.440 --> 01:17:13.225
因为精确地说，你在做同样的计算――计算每个位置。01:17:13.225 --> 01:17:17.740
但这不是在递归神经网络中发生的，因为你必须这样做01:17:17.740 --> 01:17:19.990
求出位置1的值01:17:19.990 --> 01:17:22.930
在你开始计算位置2的值之前，01:17:22.930 --> 01:17:26.320
用来表示位置3的值。01:17:26.320 --> 01:17:28.975
这是一件作品，01:17:28.975 --> 01:17:33.030
有时由CS224N联合讲师完成01:17:33.030 --> 01:17:37.620
Richard Socher和他在Salesforce Research的一些同事01:17:37.620 --> 01:17:40.110
说到如何才能两全其美?01:17:40.110 --> 01:17:43.485
我们怎么才能得到类似于a的东西呢01:17:43.485 --> 01:17:49.650
递归神经网络，但没有不好的计算性能?01:17:49.650 --> 01:17:53.160
所以他们的想法是，01:17:53.160 --> 01:18:00.550
而不是做标准的LSTM风格的计算，01:18:00.550 --> 01:18:07.090
一个更新的候选值和你的门在前面的时间片，01:18:07.090 --> 01:18:13.510
也许我们可以做的是，我们可以坚持时间之间的关系01:18:13.510 --> 01:18:20.155
- 1和时间进入卷积神经网络的MaxPooling层。01:18:20.155 --> 01:18:26.260
我们在计算一个候选门，一个遗忘门和一个输出门。01:18:26.260 --> 01:18:30.700
但是这些，这些候选人，01:18:30.700 --> 01:18:38.500
门控值通过计算在池化层内部完成，01:18:38.500 --> 01:18:44.350
通过卷积运算。01:18:44.350 --> 01:18:46.150
所以，它有点，01:18:46.150 --> 01:18:47.650
它没有，你知道，01:18:47.650 --> 01:18:53.065
如果没有免费的午餐，你就不能得到真正的递归而不付出代价。01:18:53.065 --> 01:18:56.764
这是一个伪递归式，因为01:18:56.764 --> 01:19:02.244
在每个时间片上对相邻元素之间的关联进行建模，01:19:02.244 --> 01:19:06.310
但这只是在当地完成的，而不是继续进行，01:19:06.310 --> 01:19:08.200
嗯，在一层。01:19:08.200 --> 01:19:10.240
但是他们发现，01:19:10.240 --> 01:19:14.320
如果你用这个想法让你的网络更深入，01:19:14.320 --> 01:19:15.970
然后，你又开始，01:19:15.970 --> 01:19:18.010
扩大你的影响范围。01:19:18.010 --> 01:19:22.090
所以，你得到了一定数量的信息。01:19:22.090 --> 01:19:25.330
他们的结论是，你可以01:19:25.330 --> 01:19:28.870
建立这些模型并让它们工作，01:19:28.870 --> 01:19:32.050
在这张幻灯片上不一定更好，01:19:32.050 --> 01:19:33.625
上面说通常更好。01:19:33.625 --> 01:19:37.540
你可以让他们像LSTM一样工作，01:19:37.540 --> 01:19:41.650
但是你可以让它们更快地工作，因为你在避免01:19:41.650 --> 01:19:46.555
标准的递归运算，并保持它可以并行化，01:19:46.555 --> 01:19:49.945
在MaxPooling操作中。01:19:49.945 --> 01:19:53.035
嗯，是的，那是一种01:19:53.035 --> 01:19:57.250
这是另一种有趣的方式来获得一些好处。01:19:57.250 --> 01:20:01.825
我认为，从长远来看，这种想法不会最终胜出。01:20:01.825 --> 01:20:05.740
下周我们将讨论变压器网络，01:20:05.740 --> 01:20:09.655
这似乎是目前最流行的观点。01:20:09.655 --> 01:20:12.830
Okay. I'll stop there for today. Thanks a lot.

