WEBVTT
Kind: captions
Language: en

00:00:05.000 --> 00:00:08.985欢迎参加机器[噪音]翻译讲座，00:00:08.985 --> 00:00:11.580这有点像00:00:11.580 --> 00:00:15.870这是关于RNN和相关主题的三次讲座的顺序。00:00:15.870 --> 00:00:18.420所以，我们先来做几个通知。00:00:18.420 --> 00:00:19.590呃，首先，00:00:19.590 --> 00:00:20.940你可能在进来的时候注意到了，00:00:20.940 --> 00:00:22.770我们今天要参加。00:00:22.770 --> 00:00:27.090嗯，所以你需要和礼堂外面的助教签到。00:00:27.090 --> 00:00:28.965如果你错过了，00:00:28.965 --> 00:00:30.540别起床，没事的。00:00:30.540 --> 00:00:32.985讲座结束后有时间签到。00:00:32.985 --> 00:00:34.880嗯，然后，如果你对00:00:34.880 --> 00:00:37.130有出勤政策的特殊情况，呃，00:00:37.130 --> 00:00:41.090你应该看看我们昨晚张贴的一个广场帖子，上面有一些说明。00:00:41.090 --> 00:00:45.230[噪音]嗯，你有个提醒，作业4的内容今天就要介绍了。00:00:45.230 --> 00:00:47.690所以在今天结束的时候，你将拥有完成任务4所需的一切。00:00:47.690 --> 00:00:51.860[噪音]而且一定要早点开始，因为模型需要4个小时的训练。00:00:51.860 --> 00:00:54.585另一个公告是我们要发出噪音00:00:54.585 --> 00:00:58.580我们的季度中反馈调查可能在未来几天的某个时候，00:00:58.580 --> 00:01:00.110嗯，所以请填一下。00:01:00.110 --> 00:01:01.775你将获得0.5%的信用，00:01:01.775 --> 00:01:04.520你也将帮助我们在本季度的剩余时间里让课堂变得更好。00:01:04.520 --> 00:01:08.190[噪音]好的。00:01:08.190 --> 00:01:09.880下面是我们今天要做的事情的概述。00:01:09.880 --> 00:01:11.535[噪音]今天，首先，00:01:11.535 --> 00:01:13.820我们将在NLP中引入一个新任务，00:01:13.820 --> 00:01:16.190这是机器翻译，[噪音]然后，00:01:16.190 --> 00:01:20.200我们将介绍一种称为序列到序列的新神经结构。00:01:20.200 --> 00:01:22.590这里的联系是机器翻译00:01:22.590 --> 00:01:25.020是序列到序列的主要用例。00:01:25.020 --> 00:01:29.525[噪音]之后，我们将介绍一种新的神经技术，叫做注意力，00:01:29.525 --> 00:01:33.415这是一种可以将序列改进为序列的方法。00:01:33.415 --> 00:01:37.230可以。所以第一部分是关于，呃，00:01:37.230 --> 00:01:40.920有点机器翻译历史，预神经机器翻译。00:01:40.920 --> 00:01:45.180[噪音]所以机器翻译或机器翻译，00:01:45.180 --> 00:01:48.140呃，翻译一个句子的任务是，呃，00:01:48.140 --> 00:01:50.060我们称之为源语言，00:01:50.060 --> 00:01:52.250无论你用什么语言翻译，00:01:52.250 --> 00:01:54.560变成另一种语言的句子y，00:01:54.560 --> 00:01:56.645我们称之为目标语言。00:01:56.645 --> 00:01:59.875呃，这是一个例子。假设x是这个法语句子。00:01:59.875 --> 00:02:02.100嗯，[噪音]观众中的任何人，00:02:02.100 --> 00:02:04.300一个讲法语的人，帮我们把这个翻译成英语？[噪音][背景]是的。00:02:04.300 --> 00:02:05.430嗯，这个人生来就是自由的，而且，嗯，无论在哪里，他都是铁人。00:02:05.430 --> 00:02:12.500伟大的.00:02:12.500 --> 00:02:20.760所以这就像，00:02:20.760 --> 00:02:22.785这个人生来就是自由的，但无论在哪里，他都是铁人。00:02:22.785 --> 00:02:24.395那是个相当直译的译本。00:02:24.395 --> 00:02:27.770它通常被翻译，卢梭的这句话通常被翻译为，00:02:27.770 --> 00:02:29.880人生来就是自由的，但无论在哪里，他都被束缚着。00:02:29.880 --> 00:02:32.475但有一种模糊性：（噪音）应该是，00:02:32.475 --> 00:02:34.200嗯，真的是熨斗还是铁链？00:02:34.200 --> 00:02:35.880另外，你可以选择，呃，00:02:35.880 --> 00:02:38.670把“男人”翻译成“人类”。00:02:38.670 --> 00:02:41.570呃，这是机器翻译的一个例子，00:02:41.570 --> 00:02:42.635你知道，已经有了，00:02:42.635 --> 00:02:43.790你可以做出很多选择。00:02:43.790 --> 00:02:50.505[噪音]所以机器翻译作为人工智能任务的开始始于20世纪50年代初。00:02:50.505 --> 00:02:52.365所以，嗯，特别是，00:02:52.365 --> 00:02:55.010翻译俄语到英语有很多工作，00:02:55.010 --> 00:02:57.200因为西方国家对倾听很感兴趣00:02:57.200 --> 00:02:59.800冷战期间俄国人所说的话。00:02:59.800 --> 00:03:01.920我们有一个有趣的视频，00:03:01.920 --> 00:03:05.300[噪音]显示了1954年机器翻译的状态。00:03:05.300 --> 00:03:08.910[音乐]他们没料到00:03:08.910 --> 00:03:13.185当他们开始使用计算机翻译语言时，会产生歧义。00:03:13.185 --> 00:03:16.395一个50万美元的简单计算器，00:03:16.395 --> 00:03:18.600已知最通用的电子大脑，00:03:18.600 --> 00:03:20.850把俄语翻译成英语。00:03:20.850 --> 00:03:22.815不是数学奇才，00:03:22.815 --> 00:03:24.560俄语中的一句话将被（重叠）补上。00:03:24.560 --> 00:03:27.680计算机最早的非数值应用之一，00:03:27.680 --> 00:03:29.480[背景]它被吹捧为00:03:29.480 --> 00:03:33.380冷战时期对密切关注俄国人的所作所为的痴迷00:03:33.380 --> 00:03:37.280有人声称计算机将取代大多数的人工翻译。00:03:37.280 --> 00:03:40.380[听不见]你只是在实验阶段。00:03:40.380 --> 00:03:42.120当你开始全面生产的时候，00:03:42.120 --> 00:03:43.410容量是多少？00:03:43.410 --> 00:03:46.456我们应该能够做，00:03:46.456 --> 00:03:49.860在商用电脑的帮助下，大约一百万到两百万字，00:03:49.860 --> 00:03:53.235一个小时，这将是一个相当合适的速度来应付00:03:53.235 --> 00:03:57.735苏联的整个字母表每周只需几个小时的计算机时间。00:03:57.735 --> 00:03:59.970你什么时候才能完成这个壮举？00:03:59.970 --> 00:04:01.785如果我们的实验顺利，00:04:01.785 --> 00:04:04.810也许在五年左右。00:04:05.210 --> 00:04:08.580所以在这段视频中，我认为有很多有趣的事情。00:04:08.580 --> 00:04:11.665首先，我们可以看到一个例子，00:04:11.665 --> 00:04:13.540啊，人工智能的炒作不是什么新鲜事。00:04:13.540 --> 00:04:15.880甚至在1954年，他们还在说话00:04:15.880 --> 00:04:19.525这个机器翻译系统就像是一个电子大脑，00:04:19.525 --> 00:04:22.495我认为，呃，夸大了它的一般性。00:04:22.495 --> 00:04:24.130嗯，他们也，至少有些人，00:04:24.130 --> 00:04:26.995相当乐观的是，这种[噪音]机器翻译系统00:04:26.995 --> 00:04:30.550很快就会取代人类。00:04:30.550 --> 00:04:33.670嗯，是的，那很有趣。00:04:33.670 --> 00:04:38.125而且，嗯，[噪音]问题是这些系统实际上大部分是基于规则的，呃，00:04:38.125 --> 00:04:40.735我的意思是他们大多在使用00:04:40.735 --> 00:04:43.270一本俄语和英语双语词典，00:04:43.270 --> 00:04:46.440他们基本上只是在查俄语单词，00:04:46.440 --> 00:04:47.939查找他们的英语对应词，00:04:47.939 --> 00:04:51.540他们把这些大的双语词典放在这些大磁带上。00:04:51.540 --> 00:04:55.390嗯，当然，那是当时的一个巨大的技术成就，嗯，00:04:55.390 --> 00:04:57.110但他们，呃，有些人可能也是00:04:57.110 --> 00:05:00.065乐观的（喧闹的）它将以多快的速度取代人类。00:05:00.065 --> 00:05:03.230所以在过去的几十年里，00:05:03.230 --> 00:05:06.320嗯，现在我想告诉你关于统计机器翻译的事。00:05:06.320 --> 00:05:10.070所以统计机器翻译的核心思想是00:05:10.070 --> 00:05:14.335从数据中学习概率模型，以便进行翻译。00:05:14.335 --> 00:05:16.800作为一个例子，呃，和以前一样，00:05:16.800 --> 00:05:19.295假设我们正在从法语翻译成英语。00:05:19.295 --> 00:05:23.225你想找到最好的英语句子y，00:05:23.225 --> 00:05:24.860鉴于法语句子x，00:05:24.860 --> 00:05:26.270[噪音]而且，嗯，数学上，00:05:26.270 --> 00:05:31.640你可以把它表示为，在给定x的情况下，求这个条件概率的argmax y。00:05:31.640 --> 00:05:33.605[噪音]你学习的模型是00:05:33.605 --> 00:05:38.010这个概率分布p[噪声]我们通常做的是，00:05:38.010 --> 00:05:40.640我们把这个概率分解为，00:05:40.640 --> 00:05:43.190嗯，它的两个组成部分使用了贝叶斯规则。00:05:43.190 --> 00:05:46.995[噪音]这意味着找到y最大化，00:05:46.995 --> 00:05:48.585呃，y的概率，给定x，00:05:48.585 --> 00:05:52.340等于找到y，使x的概率最大，00:05:52.340 --> 00:05:55.255给定y，乘以y的概率。00:05:55.255 --> 00:05:57.900所以这两个部分，在左边，00:05:57.900 --> 00:05:59.475我们有一个翻译模型，00:05:59.475 --> 00:06:03.605这就是如何翻译单词和短语。00:06:03.605 --> 00:06:06.560呃，所以我的想法是它知道，呃，怎么，呃，00:06:06.560 --> 00:06:10.880法语单词和英语单词可以互相翻译，也可以翻译得很小，00:06:10.880 --> 00:06:13.610小短语和大块的单词应该翻译。00:06:13.610 --> 00:06:15.985这是从很多并行数据中得到的，00:06:15.985 --> 00:06:17.780稍后我会告诉你我们是怎么做的。00:06:17.780 --> 00:06:20.270第二组分p（y）00:06:20.270 --> 00:06:22.490[噪音]这只是一个语言模型。00:06:22.490 --> 00:06:23.990[噪音]我们上周了解到这一点。00:06:23.990 --> 00:06:26.890语言模型是一个可以预测下一个单词的系统，00:06:26.890 --> 00:06:29.360但它也可以被认为是一个系统[噪音]告诉00:06:29.360 --> 00:06:32.000你是一系列单词的概率。00:06:32.000 --> 00:06:34.595如果我们要从法语翻译成英语，00:06:34.595 --> 00:06:36.470P（Y）是一种英语模式。00:06:36.470 --> 00:06:38.560[噪音]所以想法是，00:06:38.560 --> 00:06:40.355我们想要崩溃的原因00:06:40.355 --> 00:06:44.870这个单条件-条件概率分布到，00:06:44.870 --> 00:06:49.490这是两种不同劳动分工的产物。00:06:49.490 --> 00:06:51.560我的想法是，而不是，00:06:51.560 --> 00:06:55.745需要理解如何翻译的单一条件概率分布，00:06:55.745 --> 00:06:57.529如何写出好的英文文本，00:06:57.529 --> 00:06:58.970理解句子结构，00:06:58.970 --> 00:07:02.060所有的事情都是同时发生的，想法是你把它分开，这样[噪音]00:07:02.060 --> 00:07:05.735左边蓝色的翻译模型大多只知道00:07:05.735 --> 00:07:09.170小部分单词和短语的本地翻译，00:07:09.170 --> 00:07:12.950而右边的语言模式更注重良好的英语写作，00:07:12.950 --> 00:07:15.110良好的句子结构、词序等。00:07:15.110 --> 00:07:17.465[噪音]所以你已经知道了00:07:17.465 --> 00:07:20.240如何学习一种语言模型[噪音]，因为我们上次学过。00:07:20.240 --> 00:07:21.990你只需要大量的单语数据，00:07:21.990 --> 00:07:23.240在本例中，是英文数据。00:07:23.240 --> 00:07:25.520[噪音]所以我要告诉你更多我们将如何学习00:07:25.520 --> 00:07:28.610这个翻译模型需要从并行数据中学习。00:07:28.610 --> 00:07:30.860[噪音]00:07:30.860 --> 00:07:36.565所以我们需要大量的并行数据来学习这个翻译模型。00:07:36.565 --> 00:07:39.700一个早期的平行语料库的例子，00:07:39.700 --> 00:07:41.260是罗塞塔石头。00:07:41.260 --> 00:07:46.045所以这是一块石头，有三种不同语言写的相同的文本。00:07:46.045 --> 00:07:48.985这是一件非常重要的艺术品00:07:48.985 --> 00:07:53.590对于那些试图理解古埃及人的人来说。00:07:53.590 --> 00:07:55.060所以在19世纪，00:07:55.060 --> 00:07:56.680呃，学者们发现了这块石头，00:07:56.680 --> 00:07:59.590它帮助他们找出古埃及人，因为00:07:59.590 --> 00:08:04.340这个平行的文本在他们所知道的其他语言中有相同的文本。00:08:04.340 --> 00:08:07.605这是一个非常重要的平行语料库，00:08:07.605 --> 00:08:09.210如果你在伦敦，00:08:09.210 --> 00:08:10.410你可以去大英博物馆，00:08:10.410 --> 00:08:12.320亲自去看看。00:08:12.320 --> 00:08:15.235所以这个想法是你得到平行的数据。00:08:15.235 --> 00:08:17.200显然，你需要更多的石头，00:08:17.200 --> 00:08:19.615希望它也不应该写在石头上。00:08:19.615 --> 00:08:25.105但是你可以用这个来学习你的统计机器翻译模型。00:08:25.105 --> 00:08:27.430所以你的想法是，你在努力学习00:08:27.430 --> 00:08:30.550给定y的x的条件概率分布。00:08:30.550 --> 00:08:33.520所以我们要做的是，把它进一步分解。00:08:33.520 --> 00:08:37.795我们实际上想考虑x和y的概率。00:08:37.795 --> 00:08:39.685其中a是对齐方式。00:08:39.685 --> 00:08:41.125所以，一致的想法，00:08:41.125 --> 00:08:43.300是这样的话吗00:08:43.300 --> 00:08:47.110英语句子和法语句子相互对应。00:08:47.110 --> 00:08:50.690我要举个例子来说明这一点。00:08:50.690 --> 00:08:53.190在这个例子中，00:08:53.190 --> 00:08:57.315当我们把“两次新地震震动的日本”翻译成法语时。00:08:57.315 --> 00:09:01.080然后你可以看到这里有一个非常简单的一对一排列，00:09:01.080 --> 00:09:02.595从英语单词到法语单词，00:09:02.595 --> 00:09:05.150它们也以完全相同的顺序出现。00:09:05.150 --> 00:09:10.510唯一不符合的就是法语中的“le”这个词，00:09:10.510 --> 00:09:12.490我们称之为假词，因为它不是00:09:12.490 --> 00:09:15.205在英语句子中有一个直接对应词，00:09:15.205 --> 00:09:16.630那是因为在英语中我们只是说，00:09:16.630 --> 00:09:19.670“日本”，但在法语中我们说“le japon”。00:09:19.680 --> 00:09:22.915所以对齐可能比这要复杂一点。00:09:22.915 --> 00:09:25.480例如，对齐可以是多对一。00:09:25.480 --> 00:09:28.150在这个例子中，你有，呃，00:09:28.150 --> 00:09:32.740几个法语单词有多个对应的英语单词。00:09:32.740 --> 00:09:35.050这就是我们所说的多对一排列。00:09:35.050 --> 00:09:38.455呃，它也可以往另一个方向走。00:09:38.455 --> 00:09:40.075对齐可以是一对多。00:09:40.075 --> 00:09:43.015在这里我们实现了一个英语单词，00:09:43.015 --> 00:09:44.950它有一对多的排列，因为00:09:44.950 --> 00:09:48.355与之相对应的三个字的法语短语。00:09:48.355 --> 00:09:49.900所以在左边和右边，00:09:49.900 --> 00:09:52.390我们有两种方法来描绘同一条路线。00:09:52.390 --> 00:09:57.410它要么是一种图表，要么是一个图表。00:09:58.020 --> 00:10:01.345这是另一个例子，嗯，00:10:01.345 --> 00:10:04.960一对多，好吧，对不起，对吧。00:10:04.960 --> 00:10:08.200我们称之为“实现”这个词，它是一对多的。00:10:08.200 --> 00:10:12.565我们称之为一个生词，因为这个词的意思是它有很多孩子，00:10:12.565 --> 00:10:14.095在目标句子中。00:10:14.095 --> 00:10:17.065所以事实上，有些词是非常丰富的。00:10:17.065 --> 00:10:19.690下面是一个例子，源语句，00:10:19.690 --> 00:10:22.060“il m'a entarte”是指，00:10:22.060 --> 00:10:23.455“他用馅饼打我”，00:10:23.455 --> 00:10:25.090在这里用法语，00:10:25.090 --> 00:10:28.630这个动词“entarte”的意思是，呃，用馅饼打某人，00:10:28.630 --> 00:10:29.020[笑声]00:10:29.020 --> 00:10:35.845这个词在英语中没有对应的单字。00:10:35.845 --> 00:10:38.560我们没有一个动词，意思是用馅饼打某人。00:10:38.560 --> 00:10:39.910[笑声]00:10:39.910 --> 00:10:41.695我觉得这很有趣，那个法语有一个词。00:10:41.695 --> 00:10:43.270你想知道，也许他们会这么做00:10:43.270 --> 00:10:45.100通常他们需要一个词来表达。我不知道。00:10:45.100 --> 00:10:46.750[笑声]00:10:46.750 --> 00:10:48.790这是一个很有说服力的词的例子，对吧？00:10:48.790 --> 00:10:53.360因为它需要几个对应的英语单词来翻译。00:10:54.000 --> 00:10:57.400所以我们可以一对多，多对一。00:10:57.400 --> 00:10:59.650您也可以有多对多的路线。00:10:59.650 --> 00:11:03.085你可以称之为短语级翻译，或短语对短语。00:11:03.085 --> 00:11:06.100所以这里，呃，英语句子说，00:11:06.100 --> 00:11:08.035“穷人没有钱”，00:11:08.035 --> 00:11:11.350这里没有与法语短语相对应的钱，00:11:11.350 --> 00:11:14.860这是一个多对多的排列，因为00:11:14.860 --> 00:11:18.685没有明显的方法可以将这个短语分解成00:11:18.685 --> 00:11:21.770嗯，字到字的排列。00:11:22.860 --> 00:11:25.495可以。所以这就是对齐。00:11:25.495 --> 00:11:27.880如果你还记得的话，我们在想你会怎么做00:11:27.880 --> 00:11:30.730了解这种排列的概率分布，00:11:30.730 --> 00:11:33.940嗯，为了做统计机器翻译。00:11:33.940 --> 00:11:37.180所以你要知道x和a的概率，00:11:37.180 --> 00:11:41.215把y作为多种因素或多种特征的组合。00:11:41.215 --> 00:11:43.359例如，你考虑，00:11:43.359 --> 00:11:47.140一个特定单词与另一个特定单词对齐的概率是多少？00:11:47.140 --> 00:11:48.940就像你知道的，这个英语单词和这个法语单词，00:11:48.940 --> 00:11:50.065它们多久排列一次？00:11:50.065 --> 00:11:52.090但是，这也取决于，例如，00:11:52.090 --> 00:11:53.740他们在句子中的位置是什么？00:11:53.740 --> 00:11:56.739如果它们都出现在句子的末尾，00:11:56.739 --> 00:11:58.810然后它们更有可能排列成一条直线，00:11:58.810 --> 00:12:01.855如果一个人在开始，一个人在结束，那就不太可能了。00:12:01.855 --> 00:12:04.240你也会考虑，呃，00:12:04.240 --> 00:12:08.080这个特定的法语单词具有这种特殊生育能力的概率是多少？00:12:08.080 --> 00:12:10.630比如，这个词的概率是多少？00:12:10.630 --> 00:12:13.475三个对应的英语单词等等？00:12:13.475 --> 00:12:17.070所有这些统计数据都是从你的平行数据中学习出来的，00:12:17.070 --> 00:12:20.240还有很多其他的事情你会考虑。00:12:20.240 --> 00:12:23.890所以，我们现在来看一种统计机器翻译的概述。00:12:23.890 --> 00:12:26.020你不会完全理解的，00:12:26.020 --> 00:12:28.150但我们了解它的工作原理，00:12:28.150 --> 00:12:29.725因为我们会，呃，00:12:29.725 --> 00:12:32.780将其与神经机器翻译进行比较。00:12:33.510 --> 00:12:37.660可以。所以我们在学习SMT系统，00:12:37.660 --> 00:12:40.825到目前为止，我们已经把它分解成两个主要部分。00:12:40.825 --> 00:12:42.295我们有翻译模型，00:12:42.295 --> 00:12:43.900我们有语言模型，00:12:43.900 --> 00:12:46.690我们对你如何00:12:46.690 --> 00:12:50.365通过把这个翻译模型分解成一组来学习。00:12:50.365 --> 00:12:54.040所以问题仍然存在，你如何做argmax超过y？00:12:54.040 --> 00:12:59.560你如何找到你的法语句子y，使这个概率最大化？00:12:59.560 --> 00:13:03.340所以一种蛮力的解决办法是你可以说，00:13:03.340 --> 00:13:05.830“让我们列举所有可能的y。”00:13:05.830 --> 00:13:08.305这是各种可能的法语单词序列，00:13:08.305 --> 00:13:10.270可能有点长，而且，呃，00:13:10.270 --> 00:13:12.460我们将计算所有这些的概率，00:13:12.460 --> 00:13:14.980很明显，这只是一个不可行的选择。00:13:14.980 --> 00:13:16.405太贵了，00:13:16.405 --> 00:13:19.525我们就不能用它去任何地方了。00:13:19.525 --> 00:13:22.900所以你在实践中如何做到这一点的答案是，00:13:22.900 --> 00:13:25.690你将使用某种启发式搜索算法，00:13:25.690 --> 00:13:28.150为了寻找最好的翻译，Y。00:13:28.150 --> 00:13:33.115嗯，但在这一过程中，你会放弃那些概率太低的假设。00:13:33.115 --> 00:13:34.555所以你要搜索，00:13:34.555 --> 00:13:35.785你要放弃，00:13:35.785 --> 00:13:37.990在你去的时候修剪树木以确保你不00:13:37.990 --> 00:13:41.540每一步都有太多的假设。00:13:42.090 --> 00:13:47.695所以这个寻找最佳序列的过程也叫做解码。00:13:47.695 --> 00:13:50.980下面是对SMT如何工作的概述。00:13:50.980 --> 00:13:55.750这是一个例子，你有一个德语句子，它可以翻译为，00:13:55.750 --> 00:13:57.565“他不回家”，00:13:57.565 --> 00:14:02.095你可以看到这里有一些短语对短语的排列。00:14:02.095 --> 00:14:06.550所以，呃，这个解码在SMT中是如何工作的，00:14:06.550 --> 00:14:10.270你会考虑很多不同的假设，00:14:10.270 --> 00:14:13.795关于你如何翻译这些单词，00:14:13.795 --> 00:14:18.430然后你建立它来考虑如何翻译，00:14:18.430 --> 00:14:21.235嗯，个别的短语，这些短语会变大。00:14:21.235 --> 00:14:23.740例如，你可以在右上角看到，00:14:23.740 --> 00:14:25.855如果不是太小，你可以看到，00:14:25.855 --> 00:14:27.415德语中的house，呃，00:14:27.415 --> 00:14:29.365可以翻译成英文单词，00:14:29.365 --> 00:14:32.110“房子”或“家”，或“房间”等等。00:14:32.110 --> 00:14:34.690呃，所以我们考虑所有这些不同的假设，00:14:34.690 --> 00:14:37.210看看我们如何把它们放在一起00:14:37.210 --> 00:14:40.390翻译短语，但你不能一直保留它们。00:14:40.390 --> 00:14:43.180你可以去掉那些概率太低的。00:14:43.180 --> 00:14:46.225所以这也可以被描绘成一棵树，00:14:46.225 --> 00:14:49.000你正在探索不同的选择。00:14:49.000 --> 00:14:51.265你在搜索选项空间，00:14:51.265 --> 00:14:53.125但当你去修剪树木的时候。00:14:53.125 --> 00:14:54.790所以我知道这是一个非常，00:14:54.790 --> 00:14:56.005非常高的水平，呃，00:14:56.005 --> 00:14:57.580说明解码可能如何进行。00:14:57.580 --> 00:14:59.080实际上，在这节课的后面，00:14:59.080 --> 00:15:02.150你会看到一个详细的解释00:15:02.150 --> 00:15:05.970这类解码是如何用于神经机器翻译的。00:15:07.320 --> 00:15:10.300可以。我们的，嗯，00:15:10.300 --> 00:15:12.460统计机器翻译概述，00:15:12.460 --> 00:15:14.080嗯，有效吗？00:15:14.080 --> 00:15:17.080嗯，所以SMT是一个巨大的研究领域，00:15:17.080 --> 00:15:20.425从20世纪90年代到2013年。00:15:20.425 --> 00:15:23.800在此期间，最好的系统是极其复杂的。00:15:23.800 --> 00:15:26.725它们是非常复杂和令人印象深刻的系统，00:15:26.725 --> 00:15:30.130SMT制造了世界上最好的机器翻译系统。00:15:30.130 --> 00:15:31.840但它们非常复杂。00:15:31.840 --> 00:15:32.980例如，你知道，00:15:32.980 --> 00:15:36.610这里有数百个重要的细节我们根本没有提到。00:15:36.610 --> 00:15:38.740有很多很多很多方法可以做到，呃，00:15:38.740 --> 00:15:40.435越来越复杂，00:15:40.435 --> 00:15:43.315嗯，比我今天描述的还要复杂。00:15:43.315 --> 00:15:48.565特别是，系统必须有许多单独设计的子组件。00:15:48.565 --> 00:15:50.035所以我们已经看到你，呃，00:15:50.035 --> 00:15:53.155将翻译模型分解为两个单独的部分。00:15:53.155 --> 00:15:55.645嗯，但是，你知道，比这更多的子组件，00:15:55.645 --> 00:15:57.970而且他们常常是分开学习的。00:15:57.970 --> 00:16:01.795这意味着工程师们必须做很多特性工程。00:16:01.795 --> 00:16:03.730呃，你得设计一些功能来捕捉00:16:03.730 --> 00:16:07.225你感兴趣的特殊语言现象。00:16:07.225 --> 00:16:10.330所以这意味着他们需要00:16:10.330 --> 00:16:12.865编制和维护额外资源，00:16:12.865 --> 00:16:14.455事实上，你必须，呃，00:16:14.455 --> 00:16:16.495不同语言的不同资源。00:16:16.495 --> 00:16:19.900所以这种工作方式会使你使用的语言增加很多。00:16:19.900 --> 00:16:22.120举个例子，你必须00:16:22.120 --> 00:16:23.920嗯，等价短语表。00:16:23.920 --> 00:16:27.175例如，如果你在做法语和英语翻译，那么，呃，00:16:27.175 --> 00:16:30.055他们会收集这些短语，呃，00:16:30.055 --> 00:16:32.590很抱歉，他们认为类似的短语表，00:16:32.590 --> 00:16:33.925这些都是从数据中得到的。00:16:33.925 --> 00:16:37.360但这是许多必须存储和维护的信息。00:16:37.360 --> 00:16:40.930所以总的来说，这只是人类努力维持的结果。00:16:40.930 --> 00:16:42.550嗯，还有，是的，00:16:42.550 --> 00:16:45.130如果你想的话，你必须投入更多的人力00:16:45.130 --> 00:16:48.490学习新语言对的SMT系统。00:16:48.490 --> 00:16:51.550好的，这里有关于SMT的问题吗？00:16:51.550 --> 00:16:59.290[噪音]好的。00:16:59.290 --> 00:17:01.480嗯，继续说，那是SMT。00:17:01.480 --> 00:17:02.815[噪音]现在，我们要继续，00:17:02.815 --> 00:17:04.660呃，这节课的第二部分。00:17:04.660 --> 00:17:10.030所以我想带你回到2014年，00:17:10.030 --> 00:17:12.580为了一个戏剧性的重演发生在00:17:12.580 --> 00:17:15.415机器翻译研究的世界。00:17:15.415 --> 00:17:17.980所以在2014年，发生了非常戏剧性的事情，00:17:17.980 --> 00:17:20.800所发生的事情被称为神经机器翻译，00:17:20.800 --> 00:17:22.810我觉得有点像00:17:22.810 --> 00:17:25.855如果我不太戏剧化，就有点像这样。00:17:25.855 --> 00:17:28.825那么什么是神经机器翻译呢？00:17:28.825 --> 00:17:31.450其理念是非机动车是一种方式00:17:31.450 --> 00:17:35.260机器翻译，但只使用一个单一的神经网络。00:17:35.260 --> 00:17:38.115[噪音]和他们使用的神经网络结构00:17:38.115 --> 00:17:41.220被称为序列对序列，有时也被称为seq2seq，00:17:41.220 --> 00:17:44.210嗯，涉及两个RNN。00:17:44.210 --> 00:17:46.180所以，呃，这叫做顺序到顺序，00:17:46.180 --> 00:17:48.385因为你要把一个序列映射到另一个序列。00:17:48.385 --> 00:17:52.000源句[噪音]到目标句，你需要两个RNN，00:17:52.000 --> 00:17:54.685基本上处理这两个不同的句子。00:17:54.685 --> 00:17:59.110好吧，让我们看一下图表，看看要排序的序列是什么。00:17:59.110 --> 00:18:01.690所以我们从源语句开始，00:18:01.690 --> 00:18:03.400我们将使用之前的示例00:18:03.400 --> 00:18:07.300我是说，他用馅饼打我。00:18:07.300 --> 00:18:10.810所以我们，呃，把这个输入我们的编码器RNN，00:18:10.810 --> 00:18:12.835就像你以前看到的那样，00:18:12.835 --> 00:18:15.700我画了一个单向RNN，00:18:15.700 --> 00:18:17.275但这可能是双向的。00:18:17.275 --> 00:18:19.045它也可以是多层的。00:18:19.045 --> 00:18:22.675它可以是香草，也可以是LSTM，等等。00:18:22.675 --> 00:18:25.570还有一件事要注意的是我们00:18:25.570 --> 00:18:28.240将嵌入的字传递到编码器RNN中，00:18:28.240 --> 00:18:31.000但我并没有明确描述这一步。00:18:31.000 --> 00:18:33.280[噪音]好的。00:18:33.280 --> 00:18:36.430所以编码器RNN的想法是00:18:36.430 --> 00:18:39.820生成此源语句的某种编码。00:18:39.820 --> 00:18:43.690现在，假设源语句的编码是，00:18:43.690 --> 00:18:47.305呃，编码器RNN的最终隐藏状态。00:18:47.305 --> 00:18:51.700接下来我们将传递源语句的编码。00:18:51.700 --> 00:18:54.055我们把它传给解码器RNN，00:18:54.055 --> 00:18:56.635它将被翻译成英语。00:18:56.635 --> 00:18:59.635所以解码器RNN是一种语言模型。00:18:59.635 --> 00:19:01.795特别是，它是一个条件语言模型，00:19:01.795 --> 00:19:03.340就像我们上次说的那样。00:19:03.340 --> 00:19:06.430所以它是有条件的，因为它会产生目标句子，00:19:06.430 --> 00:19:08.425但以这种编码为条件，00:19:08.425 --> 00:19:12.445编码就是围绕着橙色框的向量。00:19:12.445 --> 00:19:15.805那么这是如何工作的呢？呃，我们从喂养开始，呃，00:19:15.805 --> 00:19:20.500解码器中的起始标记，然后，呃，00:19:20.500 --> 00:19:22.765我们可以得到解码器的第一状态，00:19:22.765 --> 00:19:24.025因为我们在使用，呃，00:19:24.025 --> 00:19:28.745源语句的编码，作为解码器的初始隐藏状态。00:19:28.745 --> 00:19:31.800然后我们从解码器得到第一个输出，00:19:31.800 --> 00:19:34.725这是下一个单词的概率分布，00:19:34.725 --> 00:19:37.140假设我们把argmax放在上面，00:19:37.140 --> 00:19:39.420然后我们就知道了，呃，他。00:19:39.420 --> 00:19:41.070在这种情况下是正确的，00:19:41.070 --> 00:19:43.355因为你应该从这个词开始。00:19:43.355 --> 00:19:45.280好吧，那我们就这么说，00:19:45.280 --> 00:19:48.730然后我们在下一步把它反馈给解码器，00:19:48.730 --> 00:19:50.740然后我们又做同样的事情。00:19:50.740 --> 00:19:53.965我们拿了argmax，我们得到了一个新的词，他被击中了。00:19:53.965 --> 00:19:58.750所以这里的想法是你可以继续做这个手术，这样，00:19:58.750 --> 00:20:00.220你将产生，呃，00:20:00.220 --> 00:20:02.335你的目标句子，呃，00:20:02.335 --> 00:20:05.215就像他用馅饼打我一样，00:20:05.215 --> 00:20:09.620一旦解码器生成结束标记，就停止。00:20:09.780 --> 00:20:12.715所以这里需要注意的一点是，00:20:12.715 --> 00:20:16.510这张图片显示的是测试时发生的情况。00:20:16.510 --> 00:20:18.610这将向您展示如何生成文本。00:20:18.610 --> 00:20:20.365呃，这不是训练时发生的。00:20:20.365 --> 00:20:21.955我稍后会告诉你训练时[噪音]会发生什么。00:20:21.955 --> 00:20:23.635呃，但这件事，00:20:23.635 --> 00:20:26.005你把单词放回的粉红色虚线箭头。00:20:26.005 --> 00:20:29.275这是您在测试时生成文本的操作。00:20:29.275 --> 00:20:35.365有什么问题吗？呃，哦，00:20:35.365 --> 00:20:39.940我要注意的另一件事是你需要两套独立的单词嵌入，对吗？00:20:39.940 --> 00:20:43.360法语单词需要单词嵌入，英语单词嵌入，00:20:43.360 --> 00:20:44.680这是两个不同的组，00:20:44.680 --> 00:20:48.925两个单独的词汇表。嗯，是的。00:20:48.925 --> 00:20:51.790可以。作为旁注，呃，00:20:51.790 --> 00:20:54.595这种称为序列到序列的体系结构实际上非常通用。00:20:54.595 --> 00:20:56.800它不仅仅是一个机器翻译体系结构。00:20:56.800 --> 00:20:59.410呃，你可以，呃，呃，00:20:59.410 --> 00:21:03.535将相当多的NLP任务按顺序排列。00:21:03.535 --> 00:21:05.590例如，总结是00:21:05.590 --> 00:21:10.120排序任务的顺序，因为输入的是长文本，输出的是短文本。00:21:10.120 --> 00:21:12.640嗯，对话可能是因为00:21:12.640 --> 00:21:15.730前一句话，下一句话，呃，00:21:15.730 --> 00:21:19.345解析甚至可以看作是一个序列到序列的任务，00:21:19.345 --> 00:21:21.910因为你可以说输入文本00:21:21.910 --> 00:21:24.730输出解析将被表示为一个序列。00:21:24.730 --> 00:21:28.030这可能不是最好的解析方法，但这是一种您可以尝试的方法。00:21:28.030 --> 00:21:31.480最后，您甚至可以执行代码生成之类的操作。00:21:31.480 --> 00:21:34.150所以假设你想建立一个需要00:21:34.150 --> 00:21:35.440呃，自然语言输入，00:21:35.440 --> 00:21:39.520例如，将1-10之间的数字相加，然后输出，00:21:39.520 --> 00:21:41.665让我们说一些python代码，00:21:41.665 --> 00:21:44.920开方括号的和范围是10或类似的。00:21:44.920 --> 00:21:47.140如果你想训练，00:21:47.140 --> 00:21:48.880嗯，做这个的助手。00:21:48.880 --> 00:21:51.655从某种意义上说，作为一项翻译任务，00:21:51.655 --> 00:21:53.740从英语翻译到python。00:21:53.740 --> 00:21:55.960这是一项相当具有挑战性的翻译任务。00:21:55.960 --> 00:21:58.510这可能需要更多的逻辑，而不仅仅是，呃，你知道，00:21:58.510 --> 00:22:01.390法语对英语[噪音]但你可以尝试，人们已经尝试了。00:22:01.390 --> 00:22:05.930有研究论文中，人们使用seq2seq来完成这类任务。00:22:09.510 --> 00:22:12.370可以。所以说，呃，00:22:12.370 --> 00:22:14.950seq2seq是条件语言模型的一个例子。00:22:14.950 --> 00:22:17.695呃，这是一种语言模型，因为解码器00:22:17.695 --> 00:22:20.905是一种预测下一个目标词的语言模型。00:22:20.905 --> 00:22:24.280但它是一个条件语言模型，因为它也有条件00:22:24.280 --> 00:22:29.390源语句，由源语句的编码表示。00:22:30.150 --> 00:22:32.305所以你可以看看，00:22:32.305 --> 00:22:33.700你可以这样看。00:22:33.700 --> 00:22:36.700NMT直接计算概率00:22:36.700 --> 00:22:39.835目标句的y给出了源句x。00:22:39.835 --> 00:22:42.445所以如果你看这个，你会发现这只是，呃，00:22:42.445 --> 00:22:45.085分解序列y的概率，00:22:45.085 --> 00:22:46.450我们认为是长度，呃，00:22:46.450 --> 00:22:50.110大写T。你可以把它分解成00:22:50.110 --> 00:22:54.280y的第一个字给定x，然后y的第二个字的概率给定，00:22:54.280 --> 00:22:56.920呃，前面的单词，还有x，等等。00:22:56.920 --> 00:23:00.370所以实际上，你可以看到这个产品的每一个术语都在右边，00:23:00.370 --> 00:23:02.710这些是下一个目标词的概率00:23:02.710 --> 00:23:05.365考虑到目前为止的所有句子，以及源句，00:23:05.365 --> 00:23:09.670这就是你的语言模型产生的条件概率。00:23:09.670 --> 00:23:14.020我之所以强调这一点是因为如果你记得SMT，呃，00:23:14.020 --> 00:23:18.190我们没有直接学习y给定x的翻译模型p，00:23:18.190 --> 00:23:19.765我们把它分解成，00:23:19.765 --> 00:23:22.210呃，呃，小部件。00:23:22.210 --> 00:23:23.995而在非机动交通中，00:23:23.995 --> 00:23:26.440我们正在直接学习这个模型。00:23:26.440 --> 00:23:29.335这在某些方面是一种优势，因为这样做更简单。00:23:29.335 --> 00:23:32.545您不必学习所有这些不同的系统，并分别对它们进行优化。00:23:32.545 --> 00:23:35.720它，呃，有点，简单和简单。00:23:36.660 --> 00:23:38.710所以，呃，这是，00:23:38.710 --> 00:23:40.150这就是我们正在学习的模式。00:23:40.150 --> 00:23:43.165呃，问题是，我们如何训练这个非机动交通系统？00:23:43.165 --> 00:23:46.195希望你已经对这项工作有了很好的了解，00:23:46.195 --> 00:23:49.075鉴于我们已经看到了如何训练语言模型。00:23:49.075 --> 00:23:50.830但这里有一些细节以防万一。00:23:50.830 --> 00:23:53.080所以你得到了大量的，呃，平行语料库。00:23:53.080 --> 00:23:55.075呃，然后，呃，00:23:55.075 --> 00:23:58.930假设您的句子对来自并行语料库。00:23:58.930 --> 00:24:01.915呃，这就是训练过程中发生的事情。00:24:01.915 --> 00:24:05.005呃，你把源语句输入编码器RNN，呃，00:24:05.005 --> 00:24:09.475然后把你的目标句子输入解码器RNN，00:24:09.475 --> 00:24:10.900你会忽略的00:24:10.900 --> 00:24:14.005最终隐藏状态是解码器的初始隐藏状态。00:24:14.005 --> 00:24:18.445然后，呃，对于解码器RNN的每一步，00:24:18.445 --> 00:24:20.065你要制作，呃，00:24:20.065 --> 00:24:22.060接下来发生的事情的概率分布，00:24:22.060 --> 00:24:23.905那是，Y帽子。00:24:23.905 --> 00:24:25.375然后从这些，00:24:25.375 --> 00:24:26.875你可以计算你的损失。00:24:26.875 --> 00:24:29.410损失和我们看到的一样，00:24:29.410 --> 00:24:31.165u h，无条件语言模型。00:24:31.165 --> 00:24:33.580它是交叉熵，或者你也可以00:24:33.580 --> 00:24:37.330说出下一个单词的负对数可能性。00:24:37.330 --> 00:24:39.730例如，在那些被选中的，呃，00:24:39.730 --> 00:24:44.335损失是正确下一个单词的负对数概率。00:24:44.335 --> 00:24:47.680然后像以前一样，我们将所有00:24:47.680 --> 00:24:50.995这些损失以得到总损失为例。00:24:50.995 --> 00:24:55.440呃，你可能会注意到有人说，00:24:55.440 --> 00:24:58.770例如，研究论文就是这个短语的端到端。00:24:58.770 --> 00:25:02.415这是一个端到端学习系统的例子。00:25:02.415 --> 00:25:07.275我们的意思是，反向传播是端到端的，一端是，00:25:07.275 --> 00:25:09.000是损失，损失函数，00:25:09.000 --> 00:25:11.160另一头我想是有点像，00:25:11.160 --> 00:25:12.930编码器RNN的开始。00:25:12.930 --> 00:25:14.310重点是你，呃，00:25:14.310 --> 00:25:18.045反向传播，呃，在整个系统中流动，00:25:18.045 --> 00:25:23.280你就这个单一的，呃，损失了解整个系统。是的？00:25:23.280 --> 00:25:24.030[听不见]00:25:24.030 --> 00:25:36.280问题是，00:25:36.280 --> 00:25:41.035如果解码器RNN过早输出结束令牌，00:25:41.035 --> 00:25:43.840那么你如何衡量损失呢？00:25:43.840 --> 00:25:45.700嗯，那之后的话？00:25:45.700 --> 00:25:48.850所以这就是训练时间和测试时间的区别，00:25:48.850 --> 00:25:50.020这很让人困惑。00:25:50.020 --> 00:25:51.880所以，在训练期间，00:25:51.880 --> 00:25:55.990我们有这张照片，你把代币放回去。00:25:55.990 --> 00:25:57.160所以在这种情况下，00:25:57.160 --> 00:25:58.195一旦你产生了终结，00:25:58.195 --> 00:26:02.080然后你必须停下来，因为你不能把结尾作为下一步的开始。00:26:02.080 --> 00:26:07.060但是在训练中，你不会把你的成果输入下一步。00:26:07.060 --> 00:26:11.080在训练期间，你从语料库中输入目标句子。00:26:11.080 --> 00:26:14.470所以像黄金靶子一样把句子放进模型里。00:26:14.470 --> 00:26:16.750所以不管什么，呃，00:26:16.750 --> 00:26:19.225解码器预测一个步骤，00:26:19.225 --> 00:26:23.780你有点，除了计算损失，你什么都不用。00:26:24.150 --> 00:26:26.995还有其他问题吗？是啊。00:26:26.995 --> 00:26:31.405你为什么要，呃，端到端反向传播而不是也许00:26:31.405 --> 00:26:37.720训练像[听不见]模型的编码器然后[听不见]在一起？00:26:37.720 --> 00:26:41.530问题是，你有没有理由想在以下情况下进行端到端的培训？00:26:41.530 --> 00:26:45.205例如，您可能希望分别训练编码器和解码器？00:26:45.205 --> 00:26:48.580嗯，所以我认为，人们认为端到端的培训是有利的00:26:48.580 --> 00:26:52.000因为这个想法是你可以优化整个系统。00:26:52.000 --> 00:26:54.625你可能认为如果你单独优化零件，00:26:54.625 --> 00:26:56.065当你把它们放在一起的时候，00:26:56.065 --> 00:26:58.090它们不一定是最优的。00:26:58.090 --> 00:27:01.405如果可能的话，直接优化你所关心的事情-00:27:01.405 --> 00:27:05.110关于所有参数，更可能成功。00:27:05.110 --> 00:27:07.405但是，有一个预培训的概念。00:27:07.405 --> 00:27:09.865就像你说的，也许你想学你的，嗯，00:27:09.865 --> 00:27:13.570译码器RNN作为一种语言模型，00:27:13.570 --> 00:27:15.325一种无条件的语言模型。00:27:15.325 --> 00:27:16.825这就是人们所做的。00:27:16.825 --> 00:27:19.645你可能，呃，学习一种非常强的语言模式，00:27:19.645 --> 00:27:22.870然后用它初始化解码器RNN，00:27:22.870 --> 00:27:24.625然后根据你的任务进行微调。00:27:24.625 --> 00:27:27.430这是一件你可能会尝试做的事。00:27:27.430 --> 00:27:31.230是的。00:27:31.230 --> 00:27:37.990你总是[听不见]00:27:37.990 --> 00:27:39.760问题是，长度是00:27:39.760 --> 00:27:42.310源句和目标句的长度固定？00:27:42.310 --> 00:27:44.530例如，源句的长度是4吗？00:27:44.530 --> 00:27:45.745呃，不。00:27:45.745 --> 00:27:48.310这绝对不是真的，因为在你的平行语料库中，00:27:48.310 --> 00:27:50.125你会有各种长度的句子。00:27:50.125 --> 00:27:53.725呃，所以这更像是一个实现或实用性问题。00:27:53.725 --> 00:27:56.620呃，我的想法是这是你数学上想要的00:27:56.620 --> 00:27:59.290在每个示例的培训期间进行计算，00:27:59.290 --> 00:28:01.045你会有很多例子。00:28:01.045 --> 00:28:04.900但问题是，在实践中，你是如何实现它们的呢？00:28:04.900 --> 00:28:08.380所以你通常做什么只是因为假设你的00:28:08.380 --> 00:28:12.535批是一种均匀尺寸的张量，其中所有的长度都相同，00:28:12.535 --> 00:28:17.440您是否将任何简短的句子添加到某个预先定义的最大长度，00:28:17.440 --> 00:28:21.220或者可能是批量中最大示例的长度，呃，00:28:21.220 --> 00:28:23.755然后你要确保00:28:23.755 --> 00:28:27.420使用来自填充的任何隐藏状态。是的。00:28:27.420 --> 00:28:37.210我相信两种语言在一起[听不见]00:28:37.210 --> 00:28:38.410可能有一个系统00:28:38.410 --> 00:28:48.730[听不见]这将是一种普遍的相似语言或类似的东西？00:28:48.730 --> 00:28:51.340可以。所以我想的问题是，00:28:51.340 --> 00:28:54.820呃，有时候你好像不想把事情端到端地训练，00:28:54.820 --> 00:28:58.120在某些情况下，你可能想单独训练，00:28:58.120 --> 00:28:59.575你提到过，例如，00:28:59.575 --> 00:29:01.825把不同的语言对应起来。00:29:01.825 --> 00:29:03.355所以这是一个完全正确的观点，00:29:03.355 --> 00:29:05.170事实上，到目前为止，我们00:29:05.170 --> 00:29:09.430假设你想一对一对地学习语言A到语言B，对吗？00:29:09.430 --> 00:29:13.075这和A语言到C语言，甚至B语言到A语言是不同的。00:29:13.075 --> 00:29:17.350也就是说，你有一个n平方的，多个系统的个数，00:29:17.350 --> 00:29:18.910呃，你正在考虑的语言。00:29:18.910 --> 00:29:20.830所以，是的，这实际上是一个有效的想法，00:29:20.830 --> 00:29:22.540这是人们研究过的。00:29:22.540 --> 00:29:24.370你可能会有一种00:29:24.370 --> 00:29:26.815与编码器和解码器混合匹配。00:29:26.815 --> 00:29:29.920你可以试着，呃，训练一种通用的目的，00:29:29.920 --> 00:29:34.345让我们说英语解码器，然后将其与不同的编码器进行匹配。00:29:34.345 --> 00:29:37.315嗯，但我认为这是相当复杂的训练，00:29:37.315 --> 00:29:38.965确保他们都能一起工作。00:29:38.965 --> 00:29:41.290但是，这确实是人们所做的。00:29:41.290 --> 00:29:43.850让我查一下时间。00:29:43.890 --> 00:29:47.335可以。我们再问一个问题。是的。00:29:47.335 --> 00:29:53.300那么嵌入这个词是否也来自我们正在训练的相同语料库呢？00:29:53.340 --> 00:29:56.020问题是，嵌入这个词吗？00:29:56.020 --> 00:29:57.970也来自你正在训练的语料库？00:29:57.970 --> 00:30:02.109所以我认为有一些选择，就像我们看到的语言模型一样，你可以下载，00:30:02.109 --> 00:30:04.900嗯，像Word2vec或Glove这样的预先训练过的词向量，00:30:04.900 --> 00:30:06.265你可以用那些。00:30:06.265 --> 00:30:08.410然后你可以，或者，冻结它们，或者你可以00:30:08.410 --> 00:30:10.825作为端到端培训的一部分，对它们进行微调，00:30:10.825 --> 00:30:13.540或者你可以把你的矢量词初始化为，00:30:13.540 --> 00:30:17.140嗯，你知道，接近零随机然后从零开始学习。00:30:17.140 --> 00:30:19.585好吧。好的，继续。00:30:19.585 --> 00:30:24.670嗯，所以现在我们了解你将如何训练神经机器翻译系统。00:30:24.670 --> 00:30:26.800我们简单地谈了一下00:30:26.800 --> 00:30:28.795呃，解码或者生成。00:30:28.795 --> 00:30:32.020我之前给你看的是一个叫做贪婪解码的东西，00:30:32.020 --> 00:30:33.925这就是每一步，00:30:33.925 --> 00:30:35.110你只要选择argmax，00:30:35.110 --> 00:30:36.415最重要的一个词，00:30:36.415 --> 00:30:38.755然后在下一步把它输入。00:30:38.755 --> 00:30:42.400所以这被称为贪婪解码，因为你只是在尽最大努力，呃，00:30:42.400 --> 00:30:44.815你现在能看到的最好的选择，00:30:44.815 --> 00:30:47.200然后你真的没有办法回去了。00:30:47.200 --> 00:30:52.330那么有人能看到这个方法的问题吗？也许我已经把它给别人了，但是，嗯，是的。00:30:52.330 --> 00:30:59.665[听不见]。00:30:59.665 --> 00:31:01.525你说太贵了。00:31:01.525 --> 00:31:04.390嗯，我想我的意思是，你必须这么做，这是很昂贵的00:31:04.390 --> 00:31:07.450一个序列和这个序列通常比你可以并行做的事情更糟糕。00:31:07.450 --> 00:31:10.030但我想，嗯，也许贪婪是怎么回事？00:31:10.030 --> 00:31:11.650有人能指出贪婪是怎么回事吗？00:31:11.650 --> 00:31:19.210[听不见][噪音]那不是00:31:19.210 --> 00:31:21.820一定会给你整句话的argmax。00:31:21.820 --> 00:31:23.335没错。那是，呃，00:31:23.335 --> 00:31:26.080有点，呃，贪婪意味着什么。00:31:26.080 --> 00:31:29.035所以在实践中，这可能会给你类似的东西。00:31:29.035 --> 00:31:30.715呃，我们正在翻译00:31:30.715 --> 00:31:34.000我们的运行示例句，假设在我们说的第一步，00:31:34.000 --> 00:31:35.320“他，”然后我们说，00:31:35.320 --> 00:31:37.000“他打了，”然后我们说，00:31:37.000 --> 00:31:39.190“他打了一个，”哦，不，那不对。00:31:39.190 --> 00:31:42.880这不是最好的选择，但我们现在没有办法回去了，对吧。00:31:42.880 --> 00:31:45.430我们只需要继续努力做到最好，00:31:45.430 --> 00:31:48.760“他打了一个，”结果不太好。00:31:48.760 --> 00:31:50.920这就是贪婪解码的主要问题。00:31:50.920 --> 00:31:53.965没有办法后退，也没有办法后退。00:31:53.965 --> 00:31:56.290那么我们如何解决这个问题呢？00:31:56.290 --> 00:31:57.940这与，呃，00:31:57.940 --> 00:32:00.100我之前告诉过你的关于我们如何使用，呃，00:32:00.100 --> 00:32:04.220一种用于译码和SMT的搜索算法。00:32:04.310 --> 00:32:07.785呃，但首先，你可以，00:32:07.785 --> 00:32:09.990嗯，想彻底搜查是个好主意。00:32:09.990 --> 00:32:13.125好吧，可能不是因为这仍然是个坏主意，原因和以前一样。00:32:13.125 --> 00:32:14.955所以如果你真的想做详尽的搜索，00:32:14.955 --> 00:32:18.615搜索所有可能的法语翻译，00:32:18.615 --> 00:32:19.890然后你会再次出现，00:32:19.890 --> 00:32:23.010试着考虑Y的最大值，00:32:23.010 --> 00:32:26.885所有这些概率分布的乘积。00:32:26.885 --> 00:32:29.740所以和以前一样，如果你尝试这样做，00:32:29.740 --> 00:32:32.229呃，然后在解码器的每个步骤t上，00:32:32.229 --> 00:32:37.750你将不得不用不可能部分翻译的力量追踪v，00:32:37.750 --> 00:32:40.330呃，其中v是你的词汇量。00:32:40.330 --> 00:32:42.130所以当我说部分翻译的时候，00:32:42.130 --> 00:32:43.435我只是说，呃，有点，00:32:43.435 --> 00:32:46.945你知道，比如，到目前为止，有一半的句子，或者类似的。00:32:46.945 --> 00:32:48.850所以，当然，这个，呃，00:32:48.850 --> 00:32:52.000在V复杂度中的指数太贵了。00:32:52.000 --> 00:32:54.790所以是的，我们要用某种搜索算法，00:32:54.790 --> 00:32:57.970特别是，我们将使用波束搜索解码。00:32:57.970 --> 00:33:03.550因此，波束搜索解码的核心思想是在解码器的每一步，00:33:03.550 --> 00:33:08.515你要跟踪K最可能的部分翻译，00:33:08.515 --> 00:33:10.975我们称之为部分翻译假设，00:33:10.975 --> 00:33:14.410因为我们在追踪它们的倍数，我们不确定哪一个是最好的。00:33:14.410 --> 00:33:16.550所以我们考虑了几个。00:33:16.550 --> 00:33:21.240这里k是一个整数，我们称之为梁的尺寸，00:33:21.240 --> 00:33:25.095实际上，对于非机动车而言，这通常是5-10倍。00:33:25.095 --> 00:33:26.745所以你可以想，呃，00:33:26.745 --> 00:33:30.330你的搜索空间在任何时候都有多大。00:33:30.330 --> 00:33:32.190所以如果你增加k，那么你将00:33:32.190 --> 00:33:34.860在每个步骤中考虑更多不同的选项00:33:34.860 --> 00:33:36.810你可能希望这意味着你00:33:36.810 --> 00:33:40.930最后最好的解决方案当然会更贵。00:33:41.030 --> 00:33:44.010所以我说我们要跟踪00:33:44.010 --> 00:33:47.865K最可能的部分翻译，也就是假设。00:33:47.865 --> 00:33:50.205所以这意味着我们需要某种概念，00:33:50.205 --> 00:33:53.195这个假设的可能性有多大，或者它的分数是多少。00:33:53.195 --> 00:33:56.065所以假设的分数，呃，00:33:56.065 --> 00:33:59.110我们将其表示为y_1到y_t，00:33:59.110 --> 00:34:02.995嗯，就是它的对数概率。00:34:02.995 --> 00:34:07.180所以，呃，这个部分转换的对数概率，呃，00:34:07.180 --> 00:34:11.200根据语言模型，我们可以将其分解为00:34:11.200 --> 00:34:16.220单词的个别对数概率给出了之前的所有信息。00:34:16.920 --> 00:34:19.390所以，如果不明显，呃，00:34:19.390 --> 00:34:22.090这些分数都是负数，因为我们要记录，00:34:22.090 --> 00:34:24.100嗯，一个介于0和1之间的数字。00:34:24.100 --> 00:34:29.230嗯，分数越高越好。00:34:29.230 --> 00:34:32.830是的，因为你想要更高的概率，00:34:32.830 --> 00:34:37.465嗯，根据语言模型的假设。00:34:37.465 --> 00:34:40.660所以我们要用这个分数，呃，00:34:40.660 --> 00:34:42.220以及搜索算法00:34:42.220 --> 00:34:46.255高分假设，我们将跟踪每一步的前K。00:34:46.255 --> 00:34:49.285我马上给你举个详细的例子，00:34:49.285 --> 00:34:51.250但重要的是要知道00:34:51.250 --> 00:34:54.910这种波束搜索不一定能找到最优解。00:34:54.910 --> 00:34:57.295呃，详尽的搜索，你列举的那个，00:34:57.295 --> 00:35:00.820列举所有v到t可能的翻译，保证找到00:35:00.820 --> 00:35:04.960最佳解决方案，但它完全不可行，因为它太贵了。00:35:04.960 --> 00:35:08.080因此波束搜索不能保证找到最优解，00:35:08.080 --> 00:35:11.860当然，它比详尽的搜索更有效。00:35:11.860 --> 00:35:16.945可以。这里有一个波束搜索解码的例子。00:35:16.945 --> 00:35:19.420呃，假设光束大小等于k，呃，00:35:19.420 --> 00:35:22.795是2岁，然后作为提醒，我们，呃，00:35:22.795 --> 00:35:25.630这是你应用于部分的分数，呃，00:35:25.630 --> 00:35:28.540假设，嗯，部分翻译，00:35:28.540 --> 00:35:29.935这是一个假设。00:35:29.935 --> 00:35:32.590所以我们从我们的开始标志开始，00:35:32.590 --> 00:35:34.570我们要计算00:35:34.570 --> 00:35:37.765下一个单词的概率分布。00:35:37.765 --> 00:35:41.880所以用我们的seq2seq模型计算了概率分布，00:35:41.880 --> 00:35:45.435然后，我们只取顶部的k，这是前两个可能的选项。00:35:45.435 --> 00:35:48.695那么让我们假设前两个词是“他”和“我”。00:35:48.695 --> 00:35:52.930所以我们可以计算这两个假设的分数，00:35:52.930 --> 00:35:55.540呃，用上面的公式。00:35:55.540 --> 00:35:59.845到目前为止，这只是这个词的对数概率。00:35:59.845 --> 00:36:05.125这里，假设“他”的得分为-0.7，“我”的得分为-0.9。00:36:05.125 --> 00:36:07.750这意味着他现在是最好的。00:36:07.750 --> 00:36:09.640可以。所以我们要做的是，00:36:09.640 --> 00:36:12.655我们有两个假设，00:36:12.655 --> 00:36:14.710然后对于每一个，00:36:14.710 --> 00:36:18.160我们找到接下来可能出现的前k个单词。00:36:18.160 --> 00:36:20.005我们计算他们的分数。00:36:20.005 --> 00:36:24.055所以这意味着对于“他”和“我”，我们都能找到接下来可能出现的前两个词。00:36:24.055 --> 00:36:26.440对于这四种可能性，呃，00:36:26.440 --> 00:36:29.395假设的分数等于，00:36:29.395 --> 00:36:32.980这个新单词的对数概率加上目前的上下文00:36:32.980 --> 00:36:36.820到目前为止的分数是因为你可以积累低概率的总和。00:36:36.820 --> 00:36:39.620你不必每次都从头开始计算。00:36:40.170 --> 00:36:44.350在这里你可以看到我们有这四种可能00:36:44.350 --> 00:36:48.610前两个分数是-1.6和-1.7。00:36:48.610 --> 00:36:52.180所以这意味着命中率和是最好的两个。00:36:52.180 --> 00:36:55.570所以这个想法是，在这些k平方等于4个假设中，00:36:55.570 --> 00:36:58.795我们只需要保持k等于2个最高级的。00:36:58.795 --> 00:37:00.835然后我们继续做同样的事情。00:37:00.835 --> 00:37:03.955对于这两个，我们扩展到下两个。00:37:03.955 --> 00:37:06.295然后我们计算分数，00:37:06.295 --> 00:37:11.905然后我们保留这两个最好的，抛弃其他的，然后我们扩展。00:37:11.905 --> 00:37:13.675所以我们一次又一次地这样做，00:37:13.675 --> 00:37:18.700膨胀，然后保持顶部K，像这样膨胀，直到，00:37:18.700 --> 00:37:22.045呃，你得到了一些，呃，完成的翻译。00:37:22.045 --> 00:37:25.570我马上会告诉你更多关于停止标准是什么。00:37:25.570 --> 00:37:27.550但假设我们停在这里。00:37:27.550 --> 00:37:31.150呃，看看最右边的四个假设，00:37:31.150 --> 00:37:33.220得分最高的是，呃，00:37:33.220 --> 00:37:36.265最上面的馅饼-4.3。00:37:36.265 --> 00:37:38.170所以假设我们现在要停下来00:37:38.170 --> 00:37:39.955决定这是最高假设，00:37:39.955 --> 00:37:42.340那么我们所要做的就是回溯00:37:42.340 --> 00:37:45.070为了找到完整的译文，00:37:45.070 --> 00:37:48.790这就是“他用馅饼打我”。00:37:48.790 --> 00:37:53.185好吧。那么，嗯，让我告诉你更多的细节，我们如何确切地决定何时停止。00:37:53.185 --> 00:37:55.524所以如果你记得贪婪解码，00:37:55.524 --> 00:37:59.350通常我们只是不断地解码，直到模型产生结束标记。00:37:59.350 --> 00:38:03.460例如，这意味着你的模型实际上产生了序列，呃，00:38:03.460 --> 00:38:04.525我想它不会产生启动，00:38:04.525 --> 00:38:09.265你给它一个开始，然后它就产生了“他用馅饼打我”的序列。00:38:09.265 --> 00:38:12.310因此，波束搜索解码的问题是00:38:12.310 --> 00:38:14.920你在考虑所有这些不同的假设，00:38:14.920 --> 00:38:17.785K不同的假设同时，事情是00:38:17.785 --> 00:38:21.250这些假设可能在不同的时间产生结束标记。00:38:21.250 --> 00:38:24.100所以没有一个明显的地方可以停下来。00:38:24.100 --> 00:38:26.005所以我们在实践中所做的，00:38:26.005 --> 00:38:28.510当一个假设产生结束标记时，00:38:28.510 --> 00:38:32.935然后我们认为这个假设是完整的，我们把它放在一边。00:38:32.935 --> 00:38:35.425我们有一个完整的假设集合。00:38:35.425 --> 00:38:37.000所以我们把它从光束搜索中取出，00:38:37.000 --> 00:38:39.565我们不再继续探索它，因为它已经结束了，00:38:39.565 --> 00:38:41.800嗯，我们，是的，把它放在一边。00:38:41.800 --> 00:38:45.685你继续用光束搜索来探索其他假设。00:38:45.685 --> 00:38:49.120所以剩下的问题是你什么时候停止光束搜索？00:38:49.120 --> 00:38:51.535你什么时候停止迭代这个算法？00:38:51.535 --> 00:38:55.105所以，呃，呃，有多种可能的停止标准00:38:55.105 --> 00:38:57.910但有两个常见的例子，你可能会说，呃，00:38:57.910 --> 00:39:01.150一旦到达时间步长t，我们就停止搜索光束，00:39:01.150 --> 00:39:02.545其中t是一些，呃，00:39:02.545 --> 00:39:04.120您选择的预定义阈值。00:39:04.120 --> 00:39:05.230所以你可以说，呃，00:39:05.230 --> 00:39:08.380我们要在30步后停止光束搜索，因为我们不想00:39:08.380 --> 00:39:11.755例如，任何超过30个单词的输出句子，00:39:11.755 --> 00:39:13.120或者你可能会说，“呃，00:39:13.120 --> 00:39:17.410一旦我们收集了至少n个已完成的假设，我们将停止进行光束搜索。”00:39:17.410 --> 00:39:18.820所以你可能会说，“呃，我想要00:39:18.820 --> 00:39:23.510在我停止搜索光束之前，至少完成10个翻译。”00:39:24.510 --> 00:39:27.550可以。你最后要做的是什么？00:39:27.550 --> 00:39:29.350呃，我们完成了光束搜索，嗯，00:39:29.350 --> 00:39:32.350我们收集了完整的假设。00:39:32.350 --> 00:39:34.525嗯，我们想选最上面的那个。00:39:34.525 --> 00:39:36.835呃，我们要用的是我们的翻译。00:39:36.835 --> 00:39:41.380那么，呃，我们如何选择得分最高的人呢？00:39:41.380 --> 00:39:43.360呃，你可能认为这很简单，因为00:39:43.360 --> 00:39:45.850这些假设已经附有分数。00:39:45.850 --> 00:39:47.440但如果我们只是看看这个，呃，00:39:47.440 --> 00:39:51.745公式，呃，每个假设的得分是多少。00:39:51.745 --> 00:39:55.120呃，有人看到这个问题吗？00:39:55.120 --> 00:39:57.115如果我们有一套假设，00:39:57.115 --> 00:39:59.290然后我们选择最上面的一个00:39:59.290 --> 00:40:02.750基于得分最高的人，有人能看到问题吗？00:40:05.130 --> 00:40:09.460是啊。[噪音]所以答案是你最终会选择最短的。00:40:09.460 --> 00:40:13.630这里的问题是较长的假设在00:40:13.630 --> 00:40:18.190一般的，因为你要乘以更多的概率，所以你得到的是一个较小的，00:40:18.190 --> 00:40:20.440一个较小的整体价值，或者我猜如果我们00:40:20.440 --> 00:40:22.855低概率我们会得到更多的负值。00:40:22.855 --> 00:40:25.255所以你肯定不会选择00:40:25.255 --> 00:40:28.690最短的假设，因为如果你能00:40:28.690 --> 00:40:33.550嗯，分数较低，但肯定会倾向于缩短翻译时间，00:40:33.550 --> 00:40:35.995嗯，因为他们的分数一般都比较低。00:40:35.995 --> 00:40:38.620所以解决这个问题的方法很简单，00:40:38.620 --> 00:40:40.210你只是按长度标准化。00:40:40.210 --> 00:40:43.720所以不用我们上面的工具，你要用，呃，00:40:43.720 --> 00:40:48.490分数除以[听不见]。00:40:48.490 --> 00:40:51.010然后你用这个来选择上面的那个。00:40:51.010 --> 00:41:00.235有什么问题吗？[噪音]。00:41:00.235 --> 00:41:02.680是啊。00:41:02.680 --> 00:41:06.970我们能用结束符训练吗？这样就可以[听不见]00:41:06.970 --> 00:41:09.475我听不太清楚，你能带着结束符训练吗？00:41:09.475 --> 00:41:12.230是的，就像我们有一个结束标记。00:41:12.690 --> 00:41:16.435对.所以你用结束标记训练，如果这是你的问题。00:41:16.435 --> 00:41:19.930嗯，因为关键是你要依靠你的语言模式，00:41:19.930 --> 00:41:24.145您的解码器将生成结束令牌，以便知道何时停止。00:41:24.145 --> 00:41:27.370因此，您需要对其进行培训，通过提供以下示例来生成结束令牌：00:41:27.370 --> 00:41:33.490用结束标记训练句子。是啊。00:41:33.490 --> 00:41:37.270我们为什么不把这个分数改一下[听不见]00:41:37.270 --> 00:41:38.650好问题。问题是，00:41:38.650 --> 00:41:40.360我们为什么不用这个标准化的分数呢？00:41:40.360 --> 00:41:43.315在光束搜索过程中屏幕底部的那个？00:41:43.315 --> 00:41:45.205所以不需要这样做的原因，00:41:45.205 --> 00:41:46.825你可以，但没必要，00:41:46.825 --> 00:41:48.790因为在光束搜索过程中，00:41:48.790 --> 00:41:54.100我们只比较过相同长度的假设的分数，对吗？00:41:54.100 --> 00:41:55.990所以在每一步中，我们看到的，00:41:55.990 --> 00:42:00.040假设顶部的k是平方的，我们想选择哪个是顶部的k，00:42:00.040 --> 00:42:04.030我们正在比较四个不同假设的分数，这些假设是长度的，00:42:04.030 --> 00:42:05.755一，二，三，四，五。00:42:05.755 --> 00:42:10.135所以，嗯，这些分数确实越来越低，00:42:10.135 --> 00:42:12.760但同样的，因为它们现在都是5码长的。00:42:12.760 --> 00:42:19.095[噪音]好的。00:42:19.095 --> 00:42:23.580所以我们现在了解了你将如何训练一个非机动交通系统，以及你将如何-你00:42:23.580 --> 00:42:27.900将使用您培训过的NMT系统生成您的翻译，00:42:27.900 --> 00:42:29.585比如说，光束搜索。00:42:29.585 --> 00:42:32.290所以我们都退一步想想，00:42:32.290 --> 00:42:36.730与SMT相比，NMT的总体优势是什么？00:42:36.730 --> 00:42:41.020嗯，所以第一个优势就是性能更好。00:42:41.020 --> 00:42:45.715嗯，NMT系统往往在几个方面比SMT系统提供更好的输出。00:42:45.715 --> 00:42:48.760其一是输出往往更流畅。00:42:48.760 --> 00:42:51.325呃，这可能是因为不，呃，00:42:51.325 --> 00:42:53.425这可能是因为RNN特别擅长00:42:53.425 --> 00:42:56.005像上周学的那样学习语言模型。00:42:56.005 --> 00:42:58.510嗯，另一种更好的方法是经常使用，00:42:58.510 --> 00:43:00.760嗯，上下文更好，也就是说，00:43:00.760 --> 00:43:02.500嗯，他们更擅长调节00:43:02.500 --> 00:43:05.935源语句并使用它来更改输出。00:43:05.935 --> 00:43:09.040另一种更好的方法是他们经常，呃，00:43:09.040 --> 00:43:13.795更能概括他们所学的短语以及如何翻译。00:43:13.795 --> 00:43:17.410例如，如果它看到一个如何翻译某个00:43:17.410 --> 00:43:21.850源短语，然后它会看到源短语的稍微不同的版本，00:43:21.850 --> 00:43:24.490它更能概括00:43:24.490 --> 00:43:28.040它比SMT系统更了解第一个短语。00:43:28.950 --> 00:43:33.580与我们提到的SMT相比，NMT系统的另一大优势00:43:33.580 --> 00:43:37.505以前，它是一个可以端到端优化的单一神经网络。00:43:37.505 --> 00:43:42.490我想这里的优势主要是简单和方便。00:43:42.620 --> 00:43:47.140所以没有需要单独优化的子组件。00:43:47.250 --> 00:43:52.030另一个巨大的优势是，它需要的人力工程工作要少得多。00:43:52.030 --> 00:43:54.130当我早些时候告诉你00:43:54.130 --> 00:43:56.485人们不得不建造，呃，大，00:43:56.485 --> 00:43:58.765呃，强大的SMT系统，呃，00:43:58.765 --> 00:44:01.555NMT的工程设计工作相对较少。00:44:01.555 --> 00:44:03.010当然，非机动车交通也不容易，00:44:03.010 --> 00:44:05.485但它比SMT要简单得多。00:44:05.485 --> 00:44:08.245特别是，没有特征工程。00:44:08.245 --> 00:44:10.480你不需要定义什么特征，00:44:10.480 --> 00:44:12.520呃，你想捕捉的语言现象。00:44:12.520 --> 00:44:15.550你可以把它看作一系列的单词，尽管，00:44:15.550 --> 00:44:18.530嗯，在这方面有不同的看法。00:44:19.500 --> 00:44:22.930呃，最后一点，非机动车交通的一个好处是你可以00:44:22.930 --> 00:44:26.005对所有语言对使用几乎相同的方法。00:44:26.005 --> 00:44:27.370所以如果你，呃，你知道，00:44:27.370 --> 00:44:29.200建立你的法英翻译系统00:44:29.200 --> 00:44:31.075现在你想做一个从西班牙语到英语的版本，00:44:31.075 --> 00:44:34.375您可能可以使用基本相同的体系结构和相同的方法00:44:34.375 --> 00:44:38.720只要你能找到一个足够大的西班牙语和英语的平行语料库。00:44:38.880 --> 00:44:43.060好吧。那么NMT还有什么缺点呢？00:44:43.060 --> 00:44:44.560与SMT相比，00:44:44.560 --> 00:44:46.090有一些缺点。00:44:46.090 --> 00:44:48.970一个是非机动车的解释性较差。00:44:48.970 --> 00:44:51.670呃，我指的是你00:44:51.670 --> 00:44:54.385将源语句输入神经网络，然后再输入00:44:54.385 --> 00:44:57.310你的目标句子没有00:44:57.310 --> 00:45:00.430真的有办法弄清楚为什么会这样，对吗？00:45:00.430 --> 00:45:03.430特别是，如果目标句包含某种错误，00:45:03.430 --> 00:45:06.985嗯，你不能真正地观察神经元并理解发生了什么。00:45:06.985 --> 00:45:08.800很难将错误归为属性。00:45:08.800 --> 00:45:10.240这意味着，呃，00:45:10.240 --> 00:45:12.235NMT系统很难调试。00:45:12.235 --> 00:45:15.850因此，相比之下，SMT系统00:45:15.850 --> 00:45:17.860可解释的是你拥有00:45:17.860 --> 00:45:20.500这些不同的子组件正在做不同的工作。00:45:20.500 --> 00:45:22.600而且，呃，你能看得更清楚。00:45:22.600 --> 00:45:24.910他们不是，你知道，神经元通常是，呃，00:45:24.910 --> 00:45:27.880你知道，某些词的概率给定了其他词等等。00:45:27.880 --> 00:45:30.070而且，你知道，这绝不是一件容易的事00:45:30.070 --> 00:45:33.295但它至少比NMT更容易解释。00:45:33.295 --> 00:45:38.350嗯，另一个缺点是NMT很难控制。00:45:38.350 --> 00:45:40.210比如说，00:45:40.210 --> 00:45:42.490如果您的NMT系统是，00:45:42.490 --> 00:45:43.990呃，做了一个特别的错误，00:45:43.990 --> 00:45:46.405对你来说不太容易，呃，00:45:46.405 --> 00:45:48.490程序员指定某种00:45:48.490 --> 00:45:51.280您希望NMT系统遵循的规则或准则。00:45:51.280 --> 00:45:52.915例如，如果你想说，00:45:52.915 --> 00:45:56.380我总是想用这种方式翻译这个词。00:45:56.380 --> 00:45:59.290嗯，当-当另一件事出现时，00:45:59.290 --> 00:46:02.725好像这不太容易，呃，00:46:02.725 --> 00:46:05.635把它作为一项规则强加给非机动交通系统，00:46:05.635 --> 00:46:07.225因为你不能，呃，00:46:07.225 --> 00:46:10.540一步一步地轻松控制它在做什么。00:46:10.540 --> 00:46:12.385所以有时候你会有，呃，00:46:12.385 --> 00:46:14.320您可能会尝试执行的后处理规则，00:46:14.320 --> 00:46:16.255但总的来说你不能。00:46:16.255 --> 00:46:18.985它-它-它-它比你想象的要难，嗯，00:46:18.985 --> 00:46:22.870强加一个相当简单的形式。00:46:22.870 --> 00:46:25.360[噪音]这意味着它实际上有一些安全问题。00:46:25.360 --> 00:46:26.590因为，呃，假设，你知道，00:46:26.590 --> 00:46:29.650你不想你的NMT系统说坏话，对吧？00:46:29.650 --> 00:46:31.750实际上很难放，嗯，00:46:31.750 --> 00:46:34.180这些，呃，控制器00:46:34.180 --> 00:46:36.745阻止它说出这些你不想说的话。00:46:36.745 --> 00:46:39.640我的意思是，在某种程度上，也许从来不会说特别的坏话，00:46:39.640 --> 00:46:41.815然后确定可以从词汇表中删除它们。00:46:41.815 --> 00:46:44.005但总的来说他们很难控制，00:46:44.005 --> 00:46:47.635我们将看到一些非机动车系统的例子，00:46:47.635 --> 00:46:48.760你知道，做那些00:46:48.760 --> 00:46:51.620嗯，设计师当然不是有意的。00:46:52.350 --> 00:46:57.250可以。那么，呃，我们如何评估mt？00:46:57.250 --> 00:46:59.500呃，每个好的NLP任务都需要00:46:59.500 --> 00:47:02.395一个自动的度量，以便我们，呃，测量我们的进度。00:47:02.395 --> 00:47:05.980所以，呃，最常用的mt评估指标是00:47:05.980 --> 00:47:10.240被称为布鲁，这代表着双语评估。00:47:10.240 --> 00:47:13.090所以主要的想法是布鲁00:47:13.090 --> 00:47:17.575比较机器翻译系统生成的翻译。00:47:17.575 --> 00:47:19.150它会把它和00:47:19.150 --> 00:47:22.885同一句话的一个或几个人类书面翻译。00:47:22.885 --> 00:47:27.685然后它会根据n-gram的精度计算出一个相似性分数。00:47:27.685 --> 00:47:29.545所以当我说n克精度时，00:47:29.545 --> 00:47:31.750我的意思是你要看所有的1，2，3，00:47:31.750 --> 00:47:34.150四克出现在你的，呃，00:47:34.150 --> 00:47:37.345机器翻译和你的人工翻译。00:47:37.345 --> 00:47:39.865然后n-gram精度基本上是说，00:47:39.865 --> 00:47:42.895对于机器笔译中出现的所有n-gram，00:47:42.895 --> 00:47:44.920你知道有多少人出现在00:47:44.920 --> 00:47:48.460至少有一个人类笔译？00:47:48.460 --> 00:47:53.170另一件事，你需要添加到布鲁是一个简短的惩罚。00:47:53.170 --> 00:47:56.050呃，你是说如果00:47:56.050 --> 00:47:57.970您的系统翻译非常重要00:47:57.970 --> 00:48:00.925比所有人类书面翻译都短。00:48:00.925 --> 00:48:03.610你需要添加这个的原因是00:48:03.610 --> 00:48:07.750单靠N-gram的精确性并不能真正惩罚使用更少的词语。00:48:07.750 --> 00:48:12.640所以你可以尝试通过保守和写作来最大限度地提高N-gram的精度，00:48:12.640 --> 00:48:15.790嗯，只包含你真正确定的单词的短句，00:48:15.790 --> 00:48:17.425然后你会得到一个很好的精确分数。00:48:17.425 --> 00:48:20.260但这不是一个好的翻译，因为你可能错过了00:48:20.260 --> 00:48:23.125从源语句翻译所需的信息。00:48:23.125 --> 00:48:26.260这就是为什么你需要加上简洁，呃，惩罚。00:48:26.260 --> 00:48:31.045所以总的来说，布鲁非常有用，因为，00:48:31.045 --> 00:48:34.180为了测量进度，我们需要一个自动度量。00:48:34.180 --> 00:48:36.160你不能衡量人类评估的进展00:48:36.160 --> 00:48:38.560因为计算时间太长（噪音）。00:48:38.560 --> 00:48:41.470嗯，但当然很漂亮很完美。00:48:41.470 --> 00:48:44.350例如，你可以考虑00:48:44.350 --> 00:48:47.170翻译一个句子的方法很多。00:48:47.170 --> 00:48:48.700在这节课的开始，00:48:48.700 --> 00:48:50.995我问我们如何翻译那句话，呃，00:48:50.995 --> 00:48:54.025卢梭和至少有几个不同的选择。00:48:54.025 --> 00:48:57.744如果有很多有效的翻译方法，00:48:57.744 --> 00:48:59.575布鲁是如何认识到这一点的？00:48:59.575 --> 00:49:03.820布鲁是一个有很高N-gram重叠的[噪音]奖励句子，00:49:03.820 --> 00:49:07.315呃，一个或一些人类笔译。00:49:07.315 --> 00:49:08.815但是，如果你写了一封，00:49:08.815 --> 00:49:11.650如果你的模型写了一个有效的翻译，而人类写了00:49:11.650 --> 00:49:15.040一个不同的有效翻译，他们没有高N-gram重叠，00:49:15.040 --> 00:49:18.520然后布鲁会，呃，给你一个低分。00:49:18.520 --> 00:49:23.425所以，嗯，你将在作业4中详细了解布鲁，00:49:23.425 --> 00:49:24.850实际上，任务400:49:24.850 --> 00:49:28.225一个完整的描述-对布鲁分数的数学描述。00:49:28.225 --> 00:49:31.390所以我现在不想告诉你，嗯，是的，00:49:31.390 --> 00:49:34.270所以你要考虑布鲁和它的方式00:49:34.270 --> 00:49:38.810不完美但有用。是啊。00:49:38.940 --> 00:49:43.660那么一克，是一对一的等价物吗？00:49:43.660 --> 00:49:44.680什么？00:49:44.680 --> 00:49:47.420一克等于一对一吗？00:49:47.420 --> 00:49:51.250问题是，一个N克等于一对一吗？00:49:51.250 --> 00:49:54.355我不确定我是否理解这个问题。你问的是关于校准或者其他什么？00:49:54.355 --> 00:49:58.300呃，只是想知道他们是怎么做N-gram检查的，00:49:58.300 --> 00:50:03.520它是做所有n克排列还是像一个窗口大小？00:50:03.520 --> 00:50:05.980好吧，我想一个N克就不行了00:50:05.980 --> 00:50:07.780改变一下，因为你不能排列一克。00:50:07.780 --> 00:50:08.830可以。所以你在找例子，00:50:08.830 --> 00:50:10.990四克是不是在检查，呃，00:50:10.990 --> 00:50:13.090不管是四对的精确序列还是00:50:13.090 --> 00:50:15.280它的任何排列，它的确切序列？00:50:15.280 --> 00:50:19.660所以根据定义，n-grams是顺序重要的序列。00:50:19.660 --> 00:50:23.875可以。好吧。00:50:23.875 --> 00:50:26.395所以，呃，这就是你评估机器翻译的方法。00:50:26.395 --> 00:50:28.450现在你可以理解我们00:50:28.450 --> 00:50:30.550评估我们在机器翻译方面的进展，00:50:30.550 --> 00:50:34.030嗯，我可以给你看这个图表，你可能理解它的意思。00:50:34.030 --> 00:50:35.800所以这是，呃，00:50:35.800 --> 00:50:41.860简而言之，它显示了NMT如何改变机器翻译，00:50:41.860 --> 00:50:44.110嗯，几年后的风景。00:50:44.110 --> 00:50:48.070所以在这个图中，我们得到了Bleu的分数是y轴。00:50:48.070 --> 00:50:50.860嗯，你有两种不同类型的SMT00:50:50.860 --> 00:50:53.560这是红色和深蓝色，呃，条形图。00:50:53.560 --> 00:50:55.165现在发生的是，00:50:55.165 --> 00:50:56.725呃，2015年，呃，00:50:56.725 --> 00:51:01.630神经网络机器头一次进入现场，它没有做得像SMT那样好，00:51:01.630 --> 00:51:04.735接下来的一年，它的表现突然超过了SMT。00:51:04.735 --> 00:51:08.380这里是一些特定的固定数据集的bleu分数，比如，00:51:08.380 --> 00:51:10.450嗯，很多人都有一个共同的任务，00:51:10.450 --> 00:51:12.100嗯，提交系统。00:51:12.100 --> 00:51:15.130[噪音]所以这里要注意的是00:51:15.130 --> 00:51:18.550SMT系统所取得的进展是，00:51:18.550 --> 00:51:21.490你知道，布鲁每年都有相当温和的增长。00:51:21.490 --> 00:51:23.305再过一年，00:51:23.305 --> 00:51:25.435非机动车辆到达后突然开始行动，00:51:25.435 --> 00:51:27.040呃，进展得更快。00:51:27.040 --> 00:51:31.790所以我认为这证明了为什么流星的照片可能不太像侏罗纪。00:51:33.000 --> 00:51:38.545所以你可以把NMT称为NLP在深度学习中最大的成功案例。00:51:38.545 --> 00:51:40.930呃，因为如果你想想这件事的历史，00:51:40.930 --> 00:51:45.850非机动化技术从2014年的边缘研究活动发展到实际应用。00:51:45.850 --> 00:51:51.2502016年世界领先的机器翻译标准方法。00:51:51.250 --> 00:51:53.109尤其是2014年，00:51:53.109 --> 00:51:54.880发表了第一篇SEQ2SEQ论文。00:51:54.880 --> 00:51:59.1852016年，谷歌将开关从SMT转换为NMT。00:51:59.185 --> 00:52:03.175这是一个相当显著的转变，只有两年时间。00:52:03.175 --> 00:52:05.245所以这太神奇了，00:52:05.245 --> 00:52:07.060不仅仅因为这是一个快速的转变，00:52:07.060 --> 00:52:10.180但如果你考虑到人类的努力程度。00:52:10.180 --> 00:52:12.310例如，这些SMT系统00:52:12.310 --> 00:52:15.010谷歌翻译SMT系统是由00:52:15.010 --> 00:52:17.965毫无疑问，多年来有数百名工程师。00:52:17.965 --> 00:52:23.875这个，呃，这个SMT系统的性能比一个经过培训的NMT系统好，00:52:23.875 --> 00:52:27.265嗯，你知道，相对来说，几个月内只有少数几个工程师。00:52:27.265 --> 00:52:29.680所以我不是-我不是在减少困难，00:52:29.680 --> 00:52:31.165嗯，建立NMT系统，00:52:31.165 --> 00:52:33.670当然，我相信谷歌的NMT系统00:52:33.670 --> 00:52:36.745今天是由几个工程师在几个月内建成的。00:52:36.745 --> 00:52:38.650我相信这是一个非常大的手术。00:52:38.650 --> 00:52:40.509呃，但是如果不知道，00:52:40.509 --> 00:52:42.355嗯，开始超越SMT，00:52:42.355 --> 00:52:45.220这是非常了不起的，它是如何做到的，00:52:45.220 --> 00:52:48.550呃，根据所付出的努力。是啊。00:52:48.550 --> 00:52:55.785考虑到NMT的（听不见的）缺点，有没有研究将两者结合起来，如果有，这是什么样子的？00:52:55.785 --> 00:53:00.010是的，很好。问题是我们知道00:53:00.010 --> 00:53:03.865即使与SMT相比，NMT的一些缺点，00:53:03.865 --> 00:53:05.770把这两者结合起来有什么工作吗？00:53:05.770 --> 00:53:07.300所以，是的。我想是的。00:53:07.300 --> 00:53:10.390嗯，有很多非机动车研究正在进行，特别是，00:53:10.390 --> 00:53:13.195人们有时会关注这些特定的缺点。00:53:13.195 --> 00:53:16.660而且，呃，在技术方面有很多工作00:53:16.660 --> 00:53:20.200以及数十年来SMT研究的理念和智慧，00:53:20.200 --> 00:53:23.005然后将它们集成到新的NMT范式中。00:53:23.005 --> 00:53:30.590所以是的。[噪音]。可以。00:53:31.050 --> 00:53:35.290那么机器翻译解决了吗？00:53:35.290 --> 00:53:38.470我们都能回家吗？我认为答案显然是否定的。00:53:38.470 --> 00:53:42.085嗯，NMT绝对不是完美的机器翻译。00:53:42.085 --> 00:53:46.480所以，嗯，只是想强调一下NMT仍然存在的一些困难。00:53:46.480 --> 00:53:48.610呃，一个是词汇表上的单词。00:53:48.610 --> 00:53:52.090嗯，这是一个基本问题，但很棘手。00:53:52.090 --> 00:53:54.445你知道，如果你想翻译怎么办？00:53:54.445 --> 00:53:57.610包含一个不在源词汇表中的单词的句子，00:53:57.610 --> 00:54:00.925或者，如果你试图产生一个不在你的目标词汇表中的单词怎么办？00:54:00.925 --> 00:54:03.625嗯，做这个肯定有很多工作，00:54:03.625 --> 00:54:05.920稍后你会在课堂上听到00:54:05.920 --> 00:54:08.305可能会尝试用例如，00:54:08.305 --> 00:54:10.435嗯，子词建模可以更容易。00:54:10.435 --> 00:54:13.435呃，但这是个大问题。00:54:13.435 --> 00:54:15.775另一个是域不匹配。00:54:15.775 --> 00:54:20.170因此，假设您对机器翻译系统进行了一系列公平的培训，00:54:20.170 --> 00:54:21.760呃，正式文本，比如说，00:54:21.760 --> 00:54:23.785嗯，维基百科之类的。00:54:23.785 --> 00:54:26.560呃，但是你要把它部署到翻译中去00:54:26.560 --> 00:54:29.590非正式的文本，比如人们在Twitter上聊天之类的。00:54:29.590 --> 00:54:32.080通常情况下，你会发现它的性能不太好00:54:32.080 --> 00:54:34.480在这个不同的域上，因为域不匹配。00:54:34.480 --> 00:54:37.060呃，那是个大问题。00:54:37.060 --> 00:54:40.465另一个方法是在较长的文本上维护上下文。00:54:40.465 --> 00:54:43.120所以到目前为止我们所说的一切都假设你00:54:43.120 --> 00:54:45.805把一个句子翻译成一个句子，00:54:45.805 --> 00:54:48.655没有其他更广泛的背景。00:54:48.655 --> 00:54:50.560呃，但是，你知道如果你想用00:54:50.560 --> 00:54:54.640一个机器翻译系统，可以翻译整个新闻文章甚至一本书，00:54:54.640 --> 00:54:58.270那么你很可能会想用进来的上下文00:54:58.270 --> 00:55:03.475前面的句子是为了正确地翻译当前句子中的内容。00:55:03.475 --> 00:55:06.070所以，呃，这是一个活跃的研究领域，00:55:06.070 --> 00:55:08.410如何使非机动车系统适应00:55:08.410 --> 00:55:12.620更大的上下文片段而不会变得太昂贵等等？00:55:13.200 --> 00:55:16.600另一个困难是低资源语言对。00:55:16.600 --> 00:55:19.690嗯，到目前为止我们所说的一切都假设你00:55:19.690 --> 00:55:22.915访问一个非常大的并行语料库，但是如果不访问呢？00:55:22.915 --> 00:55:25.180如果你要翻译的语言是00:55:25.180 --> 00:55:27.430文本相对较少，00:55:27.430 --> 00:55:29.380嗯，比如说在线？00:55:29.380 --> 00:55:31.630所以这很困难。00:55:31.630 --> 00:55:35.170这里有几个机器翻译搞砸的例子，00:55:35.170 --> 00:55:37.525呃，有具体的错误。00:55:37.525 --> 00:55:43.195所以，这里有一个例子，说明对于非机动交通系统来说常识是多么的困难。00:55:43.195 --> 00:55:45.640左边是英语短语卡纸，00:55:45.640 --> 00:55:47.380也就是说当你的打印机00:55:47.380 --> 00:55:50.830被纸塞住了，里面全是，呃，乱七八糟的。00:55:50.830 --> 00:55:52.255然后在右边，00:55:52.255 --> 00:55:54.940我们有一个非常直译的西班牙语版本，00:55:54.940 --> 00:55:56.395实际上是说果酱，00:55:56.395 --> 00:55:58.420纸做的可食用果酱，00:55:58.420 --> 00:56:01.240这显然不是正确的解释。00:56:01.240 --> 00:56:04.060在这里，我们有一个NMT系统00:56:04.060 --> 00:56:07.120非常直译，显然没有任何常识。00:56:07.120 --> 00:56:11.365你不能用纸做果酱。嗯，这是另一个例子。00:56:11.365 --> 00:56:14.350NMT可以在训练数据中发现偏差。00:56:14.350 --> 00:56:16.585我们已经在00:56:16.585 --> 00:56:18.310嗯，嵌入级别，00:56:18.310 --> 00:56:19.885词语的表现。00:56:19.885 --> 00:56:22.255呃，但这也可能是你知道的一个问题，00:56:22.255 --> 00:56:24.175翻译东西时的句子级别。00:56:24.175 --> 00:56:26.170在这个例子中，00:56:26.170 --> 00:56:27.340呃，在左边，00:56:27.340 --> 00:56:30.460我们在马来语中有两个句子，大致意思是，00:56:30.460 --> 00:56:34.090嗯，他们是护士，他们是程序员。00:56:34.090 --> 00:56:35.350点在左边，00:56:35.350 --> 00:56:38.260代词中没有关于性别的信息。00:56:38.260 --> 00:56:40.645但当它被翻译成英语时，00:56:40.645 --> 00:56:43.150然后我们突然发现性别不知从何而来，00:56:43.150 --> 00:56:46.105她是护士，他是程序员。00:56:46.105 --> 00:56:48.430这很可能是因为在我们的培训数据中，00:56:48.430 --> 00:56:52.090我们有更多的女性护士和男性程序员的例子。00:56:52.090 --> 00:56:55.600所以你可以从机器学习中理解为什么，00:56:55.600 --> 00:56:57.880最大化目标观点，00:56:57.880 --> 00:56:59.995嗯，英语模式已经学会了。00:56:59.995 --> 00:57:02.530但问题是机器翻译不好。00:57:02.530 --> 00:57:08.590在这里，系统正在编造源语句中没有的信息。00:57:08.590 --> 00:57:10.720所以这当然是一个错误00:57:10.720 --> 00:57:13.780机器翻译不应该这样做，因为它只是不准确。00:57:13.780 --> 00:57:17.575更糟糕的是，它正在传播，呃，性别角色。00:57:17.575 --> 00:57:20.860这是另一个非常奇怪的例子。00:57:20.860 --> 00:57:29.680[笑声]这里发生了什么？00:57:29.680 --> 00:57:30.880呃，在左边，00:57:30.880 --> 00:57:33.100我们有一个无意义的句子，00:57:33.100 --> 00:57:35.970这只是一种重复的音节。00:57:35.970 --> 00:57:38.610据说我们是从索马里翻译过来的。00:57:38.610 --> 00:57:40.590嗯，然后我们要把这个翻译成00:57:40.590 --> 00:57:43.350英语，然后我们什么也得不到。00:57:43.350 --> 00:57:46.665因为耶和华的名是用希伯来语写的，00:57:46.665 --> 00:57:48.675它是用希伯来民族的语言写的，00:57:48.675 --> 00:57:51.720你可能会想，“那到底是从哪里来的？”00:57:51.720 --> 00:57:54.675事实上，正如你所知道的，媒体报道了这件事，00:57:54.675 --> 00:57:57.390谷歌翻译想把你变成它的宗教或其他什么。00:57:57.390 --> 00:58:00.960[笑声]嗯，当然，00:58:00.960 --> 00:58:02.175这是非常令人吃惊的。00:58:02.175 --> 00:58:05.860但事实上有一个相当合理的解释。00:58:05.860 --> 00:58:08.830所以这里发生的是，00:58:08.830 --> 00:58:12.175嗯，通常对于低资源语言，00:58:12.175 --> 00:58:14.095比如索马里，嗯，00:58:14.095 --> 00:58:19.420平行文本最好的资源之一是圣经。00:58:19.420 --> 00:58:24.040所以你训练索马里语到英语，用圣经作为训练文本，00:58:24.040 --> 00:58:25.615也许在其他文本中。00:58:25.615 --> 00:58:27.520好吧，这是第一个拼图。00:58:27.520 --> 00:58:30.460但另一个难题是无意义的输入。00:58:30.460 --> 00:58:35.020所以，当输入内容不是索马里语或任何类型的文本时，对吗？00:58:35.020 --> 00:58:36.895它只是一个音节，一遍又一遍。00:58:36.895 --> 00:58:40.615然后，NMT系统没有任何合理的条件。00:58:40.615 --> 00:58:42.595基本上是胡说八道，只是噪音。00:58:42.595 --> 00:58:44.545那么NMT系统做什么呢？00:58:44.545 --> 00:58:48.100正确的？它不能真正使用，也不能真正限定源语句。00:58:48.100 --> 00:58:51.475那么它的作用是，它只是使用英语模式，对吗？00:58:51.475 --> 00:58:54.460你可以把它看作是解码器RNN的英语模型。00:58:54.460 --> 00:58:57.835就像自动驾驶仪开始生成随机文本，00:58:57.835 --> 00:59:00.760就像我们上周看到的，呃，00:59:00.760 --> 00:59:02.380在奥巴马演讲中训练的语言模式，或00:59:02.380 --> 00:59:04.720哈利波特只会以这种方式生成文本。00:59:04.720 --> 00:59:06.760这就是圣经里发生的事情，00:59:06.760 --> 00:59:08.740因为我们没有任何有用的信息，00:59:08.740 --> 00:59:11.320嗯，从左边的句子。00:59:11.320 --> 00:59:15.700嗯，这就是为什么，呃，00:59:15.700 --> 00:59:18.730特别是神经机器翻译会造成这样的错误，00:59:18.730 --> 00:59:20.890因为这个系统不可理解。00:59:20.890 --> 00:59:23.665所以你不知道这会发生直到它发生，00:59:23.665 --> 00:59:25.450也许谷歌不知道00:59:25.450 --> 00:59:27.640直到它发生并被报告。00:59:27.640 --> 00:59:32.410嗯，这是不可理解性的一个缺点是，真正奇怪的效果00:59:32.410 --> 00:59:34.690一旦发生，你就看不到他们的到来00:59:34.690 --> 00:59:37.210总是很容易解释他们为什么会发生。是啊？00:59:37.210 --> 00:59:38.830[听不见]。00:59:38.830 --> 00:59:47.530啊，问题是如果你真的从爱尔兰语翻译过来会发生什么？00:59:47.530 --> 00:59:49.810我想这就是谷歌试图自动检测的地方00:59:49.810 --> 00:59:53.050语言，也许它认为ag ag ag更像爱尔兰语而不是索马里语，00:59:53.050 --> 00:59:56.530[笑声]我想如果你把爱尔兰语译成英语，00:59:56.530 --> 00:59:58.615可能还有更多，呃，00:59:58.615 --> 01:00:00.130爱尔兰英语培训数据。01:00:00.130 --> 01:00:02.725所以也许它不会如此专注于圣经。01:00:02.725 --> 01:00:05.680嗯，是的，网上有很多这样的例子01:00:05.680 --> 01:00:09.260你用不同的语言做不同种类的无意义音节。01:00:09.870 --> 01:00:14.035所以NMT还有很多挑战。01:00:14.035 --> 01:00:16.360研究还在继续。01:00:16.360 --> 01:00:21.370所以，我认为，NMT仍然是NLP深入学习的旗舰任务之一。01:00:21.370 --> 01:00:24.400事实上，非机动车交通研究开创了许多01:00:24.400 --> 01:00:27.205NLP深度学习的成功创新。01:00:27.205 --> 01:00:30.400呃，所以今天在2019年，01:00:30.400 --> 01:00:32.710NMT研究继续蓬勃发展，还有很多，01:00:32.710 --> 01:00:35.530很多论文，呃，一直都是在非机动交通上发表的。01:00:35.530 --> 01:00:38.260事实上，研究人员发现01:00:38.260 --> 01:00:41.770我今天向您展示的相当普通的seq2seq模型的改进。01:00:41.770 --> 01:00:43.285呃，但事实上，01:00:43.285 --> 01:00:45.849有一个改进是如此完整01:00:45.849 --> 01:00:48.655你可以把它当作新的香草。01:00:48.655 --> 01:00:50.785这就是我们今天要学习的进步，01:00:50.785 --> 01:00:53.050这叫做注意力。01:00:53.050 --> 01:00:58.225可以。所以第三部分是要注意的。什么是注意力？01:00:58.225 --> 01:01:01.855首先，我要激发我们为什么需要这种叫做注意力的东西。01:01:01.855 --> 01:01:05.440让我们看一下我们之前看到的这个图。01:01:05.440 --> 01:01:07.705记住，当我们假设，01:01:07.705 --> 01:01:09.370呃，源语句的编码，01:01:09.370 --> 01:01:13.315橙色方框中的，代表整个句子。01:01:13.315 --> 01:01:18.320呃，有人能自愿提出一个你能看到的关于这个建筑的问题吗？01:01:19.230 --> 01:01:23.530特别是，这个观点的一个问题，即单向量01:01:23.530 --> 01:01:27.100源语句的编码。是啊？01:01:27.100 --> 01:01:39.385[听不见]01:01:39.385 --> 01:01:41.650好吧，所以答案是，嗯，你只是在看01:01:41.650 --> 01:01:44.095一个词，你的意思是像源句中的最后一个词？01:01:44.095 --> 01:01:45.955你看不到更多的信息。01:01:45.955 --> 01:01:47.710是的，有些-是的，有点像那样。01:01:47.710 --> 01:01:49.045还有其他想法吗？是的。01:01:49.045 --> 01:01:50.905我们可能在01:01:50.905 --> 01:01:53.575当你到达句尾时，句子的开头。01:01:53.575 --> 01:01:56.530是啊。你可能在句子开头的[噪音]中丢失了信息，01:01:56.530 --> 01:01:59.740到了结尾的时候，尤其是超过四个字的时候。01:01:59.740 --> 01:02:01.561正确的。我认为这是不同的表达方式01:02:01.561 --> 01:02:06.730类似的想法[噪音]，即我们有一种信息瓶颈。01:02:06.730 --> 01:02:10.600呃，我们强迫所有关于源语句的信息都被捕获01:02:10.600 --> 01:02:14.675在这个单一的向量中，因为这是唯一一个被赋予解码器的东西。01:02:14.675 --> 01:02:17.265如果源语句的某些信息不在向量中，01:02:17.265 --> 01:02:20.610那么解码器就无法正确翻译了。01:02:20.610 --> 01:02:21.930所以这是，是的，01:02:21.930 --> 01:02:23.310这是一个信息瓶颈。01:02:23.310 --> 01:02:25.650[噪音]压力太大了01:02:25.650 --> 01:02:29.180这个单一的向量可以很好地表示编码器的[噪声]。01:02:29.180 --> 01:02:32.290所以这就是注意力的动机。01:02:32.290 --> 01:02:36.445注意力是一种神经技术，它可以解决瓶颈问题。01:02:36.445 --> 01:02:39.520核心思想是在解码器的每一步[噪声]上，01:02:39.520 --> 01:02:42.459你要直接连接编码器01:02:42.459 --> 01:02:46.520集中于源序列的特定部分。01:02:47.100 --> 01:02:50.560所以首先我要告诉你什么是注意力01:02:50.560 --> 01:02:53.140通过图表，这是一种直观的解释。01:02:53.140 --> 01:02:55.405然后我再给你们看方程。01:02:55.405 --> 01:02:59.455下面是seq-sequence到sequence的工作原理。01:02:59.455 --> 01:03:01.960所以在解码器的第一步，01:03:01.960 --> 01:03:05.095呃，我们有第一个解码器隐藏状态。01:03:05.095 --> 01:03:08.320所以我们要做的是，把点积01:03:08.320 --> 01:03:11.590那个解码器的隐藏状态和第一个[噪声]编码器的隐藏状态。01:03:11.590 --> 01:03:13.090然后我们接到一个电话01:03:13.090 --> 01:03:16.570我用圆点表示的注意力得分。这是一个标量。01:03:16.570 --> 01:03:18.070[噪音]事实上，01:03:18.070 --> 01:03:20.305我们取解码器隐藏状态之间的点积01:03:20.305 --> 01:03:23.050以及所有编码器隐藏状态。01:03:23.050 --> 01:03:27.550这意味着我们得到一个注意力分数或者一个标量，01:03:27.550 --> 01:03:29.980嗯，有效的来源词。01:03:29.980 --> 01:03:35.980接下来我们要做的是，取这四个数字的分数，然后应用SoftMax，01:03:35.980 --> 01:03:38.020分布函数SoftMax函数01:03:38.020 --> 01:03:41.155然后我们得到一个概率分布。01:03:41.155 --> 01:03:45.685在这里，我将把概率分布表示为一个条形图。01:03:45.685 --> 01:03:50.200嗯，我们称之为注意力分布，这个总结为1。01:03:50.200 --> 01:03:54.805在这里，你可以看到大部分概率质量都在第一个词上。01:03:54.805 --> 01:03:58.300这有点道理，因为我们的第一个词基本上就是“他”，而且，01:03:58.300 --> 01:04:02.375呃，我们的目标句中会先出现“他”这个词。01:04:02.375 --> 01:04:04.965一旦我们得到了注意力的分配，01:04:04.965 --> 01:04:11.295呃，我们要用它来产生一种叫做注意力输出的东西。01:04:11.295 --> 01:04:15.400所以我们的想法是注意力输出是01:04:15.400 --> 01:04:20.515编码器的隐藏状态和权重是注意力的分布。01:04:20.515 --> 01:04:22.450所以我有这些虚线箭头01:04:22.450 --> 01:04:24.400注意力分布到注意力输出，01:04:24.400 --> 01:04:26.380可能也应该有点箭头01:04:26.380 --> 01:04:28.270编码器RNN，但很难描述。01:04:28.270 --> 01:04:32.215[噪音]但你的想法是总结这些编码器RNN，呃，01:04:32.215 --> 01:04:34.480隐藏状态，[噪音]但你要给每个01:04:34.480 --> 01:04:37.555一个是根据他们的注意力分布。01:04:37.555 --> 01:04:41.620这意味着你的注意力输出是一个单一的向量01:04:41.620 --> 01:04:45.370主要包含来自高度关注的隐藏状态的信息。01:04:45.370 --> 01:04:49.730在这种情况下，主要是来自第一个隐藏状态的信息。01:04:52.230 --> 01:04:54.370所以在你这么做之后，01:04:54.370 --> 01:04:58.390你将使用注意力输出来影响你对下一个单词的预测。01:04:58.390 --> 01:05:00.520所以你通常做的是连接01:05:00.520 --> 01:05:03.610注意输出与你的解码器隐藏状态，然后，01:05:03.610 --> 01:05:06.070嗯，用这种连接对的方式01:05:06.070 --> 01:05:08.905你以前会单独使用解码器的隐藏状态。01:05:08.905 --> 01:05:11.590这样你就可以得到你的概率分布，01:05:11.590 --> 01:05:14.215嗯，你说接下来要发生的事。01:05:14.215 --> 01:05:17.170所以和以前一样，我们可以用它来给你的下一个词取样。01:05:17.170 --> 01:05:19.360[噪音]下一步，01:05:19.360 --> 01:05:20.740你只要再做同样的事情。01:05:20.740 --> 01:05:22.690你有第二个解码器隐藏状态。01:05:22.690 --> 01:05:25.480同样，您采用具有所有编码器隐藏状态的点积。01:05:25.480 --> 01:05:28.225你把SoftMax放在上面得到注意力分布。01:05:28.225 --> 01:05:30.685在这里，你可以看到注意力的分布是不同的。01:05:30.685 --> 01:05:33.595我们更关注的是，01:05:33.595 --> 01:05:36.520单词entart_是因为我们将要生成单词hit。01:05:36.520 --> 01:05:38.590嗯，但我们也参加了一点01:05:38.590 --> 01:05:42.295第二个单词a，因为它告诉我们hit是过去时。01:05:42.295 --> 01:05:46.495所以这里发生的一件很酷的事情是我们得到了一个（噪音）软对齐。01:05:46.495 --> 01:05:49.555如果你还记得我们在研究SMT系统中的对齐方式时，01:05:49.555 --> 01:05:50.665主要是这个，呃，01:05:50.665 --> 01:05:54.595开或关的硬二进制事物，这些词要么对齐，要么不对齐。01:05:54.595 --> 01:05:59.140在这里，你有一个更加灵活的路线软概念，01:05:59.140 --> 01:06:01.300呃，每个词都有一个分布01:06:01.300 --> 01:06:04.135源句中对应的词。01:06:04.135 --> 01:06:06.805另一个需要注意的东西是一个旁注，01:06:06.805 --> 01:06:08.440是不是有时候，呃，01:06:08.440 --> 01:06:11.920我们注意到前一个隐藏状态的输出，呃，01:06:11.920 --> 01:06:16.495我们把它和通常的单词一起再次输入解码器。01:06:16.495 --> 01:06:19.510所以这就意味着你从第一步开始注意输出01:06:19.510 --> 01:06:23.080将它连接到单词vector for he，然后在解码器中使用它。01:06:23.080 --> 01:06:26.515嗯，这是因为有时候有这个很有用，01:06:26.515 --> 01:06:30.715信息来自，注意上一步的下一步。01:06:30.715 --> 01:06:33.475所以我告诉你这是因为这是我们在作业4中所做的01:06:33.475 --> 01:06:37.130这是一种相当常见的技术，但有时人们不这么做。01:06:37.170 --> 01:06:40.390可以。所以，嗯，理论是，01:06:40.390 --> 01:06:42.010你只需要注意一下，01:06:42.010 --> 01:06:44.245嗯，每一步的计算。01:06:44.245 --> 01:06:46.690在每一步中，你都要注意不同的事情。01:06:46.690 --> 01:06:48.850在第三步的例子中，01:06:48.850 --> 01:06:51.730我们看着M，这意味着当我们01:06:51.730 --> 01:06:53.350把我带到最后01:06:53.350 --> 01:06:55.105三[噪音]我们可能只是想看看这个，01:06:55.105 --> 01:06:58.600嗯，有种的单词entart来生产一个馅饼打我。01:06:58.600 --> 01:07:02.005[噪音]我要继续走，因为我们时间不多。01:07:02.005 --> 01:07:04.690呃，这是描述注意力的方程式。01:07:04.690 --> 01:07:06.520呃，我想在里面看可能更容易些01:07:06.520 --> 01:07:09.025你自己的时间迟了，而不是现在在课堂上看。01:07:09.025 --> 01:07:10.870但这些方程本质上01:07:10.870 --> 01:07:13.330和图中所说的一样。01:07:13.330 --> 01:07:17.110所以你有你的编码器隐藏状态h_1到h_n。01:07:17.110 --> 01:07:19.795然后在解码器的时间步T上，01:07:19.795 --> 01:07:22.855我们还有一个解码器隐藏状态。01:07:22.855 --> 01:07:27.100所以我们会得到注意力得分，我们会用01:07:27.100 --> 01:07:30.730您的解码器的点积隐藏状态与每个编码器的隐藏状态。01:07:30.730 --> 01:07:32.425[噪音]这给了你，呃，01:07:32.425 --> 01:07:34.900一个和，呃，01:07:34.900 --> 01:07:39.355编码器[噪音]句子，因为每个源词都有一个分数。01:07:39.355 --> 01:07:42.580下一步，你把SoftMax超过这些分数01:07:42.580 --> 01:07:45.280为了得到1的注意力分布，01:07:45.280 --> 01:07:47.635我们称之为阿尔法。01:07:47.635 --> 01:07:50.860然后用alpha取01:07:50.860 --> 01:07:54.535编码器隐藏的状态，这给你的注意力输出。01:07:54.535 --> 01:07:57.490所以我们称之为A的注意力输出是一个向量01:07:57.490 --> 01:08:01.645与编码器隐藏状态的大小相同。01:08:01.645 --> 01:08:06.610最后，你注意输出A，然后你，[噪音]呃，01:08:06.610 --> 01:08:09.640将其与解码器隐藏状态连接起来，然后01:08:09.640 --> 01:08:13.940按照之前在“不注意”模式中所教的那样继续。01:08:14.520 --> 01:08:17.770所以注意，如果不清楚的话，这很酷。01:08:17.770 --> 01:08:19.510它有很多优点。01:08:19.510 --> 01:08:23.905因此，一个优势是注意力只是显著提高了NMT的性能。01:08:23.905 --> 01:08:25.675它改进的主要原因是，01:08:25.675 --> 01:08:28.690是因为事实证明允许解码器[噪声]使用01:08:28.690 --> 01:08:32.215翻译时，要注意源句的某些部分。01:08:32.215 --> 01:08:33.940你能明白为什么这是有道理的，对吧？01:08:33.940 --> 01:08:35.740因为有一个非常自然的一致性概念，01:08:35.740 --> 01:08:38.619如果你能专注于你要翻译的一个或多个词，01:08:38.619 --> 01:08:40.585你可能会做得更好。01:08:40.585 --> 01:08:44.410注意力很酷的另一个原因是它解决了瓶颈问题。01:08:44.410 --> 01:08:46.540呃，我们注意到01:08:46.540 --> 01:08:49.870一个向量，它必须表示整个源语句[噪声]，这是01:08:49.870 --> 01:08:52.780信息从编码器传递到解码器的唯一途径01:08:52.780 --> 01:08:56.590意味着如果编码不是很好，那么，呃，你不会做得很好。01:08:56.590 --> 01:08:59.140相比之下，呃，注意，01:08:59.140 --> 01:09:01.690解码器可以直接查看编码器和01:09:01.690 --> 01:09:04.240源句和翻译没有瓶颈。01:09:04.240 --> 01:09:07.060[噪音]01:09:07.060 --> 01:09:10.915注意力的另一个好处是它有助于消除梯度问题，01:09:10.915 --> 01:09:13.240尤其是当你的句子很长的时候。01:09:13.240 --> 01:09:16.060嗯，注意力有帮助的原因是01:09:16.060 --> 01:09:19.390解码器和编码器之间的直接连接，01:09:19.390 --> 01:09:21.145有点像很多时间步骤。01:09:21.145 --> 01:09:22.825所以这就像一个快捷连接。01:09:22.825 --> 01:09:24.730正如我们上次了解到的，01:09:24.730 --> 01:09:28.255跳过连接对于减少消失梯度非常有用。01:09:28.255 --> 01:09:29.590这里的概念是一样的。01:09:29.590 --> 01:09:31.270我们有这些，嗯，长途电话01:09:31.270 --> 01:09:34.580[噪音]有助于梯度流更好的直接连接。01:09:34.620 --> 01:09:38.230注意力的另一个好处是它提供了一些可解释性。01:09:38.230 --> 01:09:41.155如果你看一下注意力的分布，01:09:41.155 --> 01:09:42.880你经常翻译出来。01:09:42.880 --> 01:09:46.360呃，你可以看到解码器在每一步都关注什么。01:09:46.360 --> 01:09:48.820例如，如果我们运行系统并翻译，01:09:48.820 --> 01:09:50.125我们的跑步范例，01:09:50.125 --> 01:09:51.670然后我们可以制作一个情节，01:09:51.670 --> 01:09:54.490像这样显示注意力分布。01:09:54.490 --> 01:09:58.255所以在这里，黑暗意味着高关注度，而白色意味着低关注度。01:09:58.255 --> 01:10:00.295所以你可能会看到这样的东西，01:10:00.295 --> 01:10:03.550嗯，是的，它专注于不同的词语和不同的步骤。01:10:03.550 --> 01:10:06.160这基本上是同一种01:10:06.160 --> 01:10:08.710我们之前用一个很难实现一致性的概念绘制的图，01:10:08.710 --> 01:10:11.245呃，除了我们，呃，01:10:11.245 --> 01:10:14.380我们有更大的灵活性来实现更软的对齐方式01:10:14.380 --> 01:10:17.995例如，当我们生成英语单词hit时，01:10:17.995 --> 01:10:22.400也许我们主要在看恩塔特，但我们也在看一点。01:10:22.880 --> 01:10:25.980所以这，呃，意味着我们得到，01:10:25.980 --> 01:10:27.315呃，免费校准。01:10:27.315 --> 01:10:31.424我说免费的原因是当你记得SNT系统的时候，01:10:31.424 --> 01:10:33.210关键是你必须学会01:10:33.210 --> 01:10:36.580一个故意的和单独的校准系统。01:10:36.580 --> 01:10:38.410你必须定义对齐的概念，01:10:38.410 --> 01:10:40.060你必须定义计算模型，01:10:40.060 --> 01:10:42.865不同路线的概率是多少并训练它。01:10:42.865 --> 01:10:47.035然而在这里，我们从来没有告诉过NMT系统关于校准的事情。01:10:47.035 --> 01:10:49.105我们从未明确训练过校准系统。01:10:49.105 --> 01:10:52.225我们从来没有一个损失函数告诉你你的排列有多好。01:10:52.225 --> 01:10:55.600我们刚把仪器给了非机动车系统01:10:55.600 --> 01:10:59.245做一些像对齐的事情，告诉它最大化，01:10:59.245 --> 01:11:02.380嗯，做机器翻译的交叉熵损失。01:11:02.380 --> 01:11:05.560然后网络就自己学会了对齐。01:11:05.560 --> 01:11:07.900我觉得这是最酷的注意力，01:11:07.900 --> 01:11:12.410它是以某种不受监督的方式学习某种结构。01:11:13.290 --> 01:11:15.520好吧，在最后几分钟，01:11:15.520 --> 01:11:18.340我要概括一下注意力的概念。01:11:18.340 --> 01:11:21.280因为事实证明注意力是非常普遍的，01:11:21.280 --> 01:11:25.315你可以在很多不同的环境中应用的深度学习技巧。01:11:25.315 --> 01:11:27.610所以你已经看到注意力是一个很好的提高方法01:11:27.610 --> 01:11:29.875mt的序列到序列模型，01:11:29.875 --> 01:11:32.710但实际上，您可以将注意力用于其他没有01:11:32.710 --> 01:11:36.220seq2seq和非mt的任务。01:11:36.220 --> 01:11:37.975为了理解这一点，01:11:37.975 --> 01:11:42.085我要重新定义一个更一般的定义。01:11:42.085 --> 01:11:44.185这是我们更一般的定义。01:11:44.185 --> 01:11:46.885假设您有一组值，01:11:46.885 --> 01:11:48.190每个都是向量，01:11:48.190 --> 01:11:51.370您还拥有一个调用查询的向量。01:11:51.370 --> 01:11:53.650那么注意力就是一种方式，呃，01:11:53.650 --> 01:11:56.350计算值的加权和。01:11:56.350 --> 01:11:59.380但是您对它进行加权的方式取决于查询。01:11:59.380 --> 01:12:03.925[噪音]所以我们经常这样说，01:12:03.925 --> 01:12:06.655呃，比如说，查询关注的是值。01:12:06.655 --> 01:12:09.520我们的想法是，你拥有价值观中的所有信息，01:12:09.520 --> 01:12:13.810查询以某种方式决定了它将如何关注这些值。01:12:13.810 --> 01:12:16.345例如在Seq2seq中，01:12:16.345 --> 01:12:19.435解码器隐藏状态是查询。01:12:19.435 --> 01:12:22.450呃，特定时间步的解码器隐藏状态是查询01:12:22.450 --> 01:12:27.050并注意所有编码器的隐藏状态，即值。01:12:27.090 --> 01:12:29.515好吧，这又是我们的定义。01:12:29.515 --> 01:12:33.445这里有一种直观的理解方法，两种可选的方法。01:12:33.445 --> 01:12:35.530一种是这样想。01:12:35.530 --> 01:12:38.290你可以把它看成是加权和01:12:38.290 --> 01:12:42.235值中信息的选择性摘要。01:12:42.235 --> 01:12:45.760我说选择性是因为你选择了多少01:12:45.760 --> 01:12:49.345从每个值中提取取决于注意力的分布。01:12:49.345 --> 01:12:53.095呃，所以分布，呃，取决于查询。01:12:53.095 --> 01:12:57.595所以这个查询决定了你要从不同的，呃，值中选择多少。01:12:57.595 --> 01:13:01.555这类似于本周早些时候学到的LSTM。01:13:01.555 --> 01:13:04.300LSTMS规则基于一个门的概念，呃，01:13:04.300 --> 01:13:07.150[噪音]定义了需要多少信息-01:13:07.150 --> 01:13:08.845[噪音]应该来自不同的元素。01:13:08.845 --> 01:13:11.395门取决于上下文。01:13:11.395 --> 01:13:14.710因此，LSTM的优势来自这样一种理念，即基于上下文，01:13:14.710 --> 01:13:17.185你决定从哪里获取信息。01:13:17.185 --> 01:13:19.435这是同样的想法。01:13:19.435 --> 01:13:24.070第二种思考注意力的方法是你可以说它是一种获得01:13:24.070 --> 01:13:28.420一组任意表示形式中的固定大小表示。01:13:28.420 --> 01:13:29.965所以当我说任意集时，01:13:29.965 --> 01:13:32.875我是说我们有一组向量，叫做值，对吗？01:13:32.875 --> 01:13:34.090你可以有10个值。01:13:34.090 --> 01:13:35.380你可以有100个值。01:13:35.380 --> 01:13:36.400你可以，呃，01:13:36.400 --> 01:13:38.305这些向量的任意数量的[噪声]。01:13:38.305 --> 01:13:42.460但是注意力给了你一个方法来得到一个向量，01:13:42.460 --> 01:13:48.350嗯，总结一下，这是注意力输出，呃，使用您的查询。01:13:48.420 --> 01:13:51.055好吧，呃，最后一件事，呃，01:13:51.055 --> 01:13:53.710实际上有几种不同的01:13:53.710 --> 01:13:57.655注意，这是我们在作业4中将要看到的一些东西。01:13:57.655 --> 01:13:59.920所以在我们更一般的环境中，01:13:59.920 --> 01:14:02.290我们已经看到查询中有一些值。01:14:02.290 --> 01:14:06.265注意总是需要计算注意[噪音]分数，01:14:06.265 --> 01:14:09.880然后应用SoftMax获得注意力分布。01:14:09.880 --> 01:14:13.870然后你用注意力的分布，得到一个加权和。01:14:13.870 --> 01:14:17.240这就是，呃，注意力是如何工作的。01:14:17.240 --> 01:14:19.920可能不同的部分是，呃，第一。01:14:19.920 --> 01:14:23.520有多种方法可以计算分数。01:14:23.520 --> 01:14:27.135所以，呃，最后一张幻灯片，01:14:27.135 --> 01:14:30.035这里有所有不同的方法，你可以重复得分。01:14:30.035 --> 01:14:34.615所以你今天已经看到的第一个是基本的点产品关注。01:14:34.615 --> 01:14:38.575这里的想法是[噪音]一个特定值的分数，01:14:38.575 --> 01:14:42.850嗨，只是查询和那个特定值的点积。01:14:42.850 --> 01:14:46.720[噪音]特别是这个假设01:14:46.720 --> 01:14:48.190您的查询向量和01:14:48.190 --> 01:14:50.740你的值向量必须是相同的，因为你要取点积。01:14:50.740 --> 01:14:54.865[噪音]另一个版本，01:14:54.865 --> 01:14:57.685嗯，注意力叫做乘法注意力。01:14:57.685 --> 01:15:00.385这里的想法是你的分数，呃，01:15:00.385 --> 01:15:03.220Value嗨，就是这个，呃，01:15:03.220 --> 01:15:06.625查询和该值的双线性函数。01:15:06.625 --> 01:15:08.500特别是，我们把这个重量矩阵01:15:08.500 --> 01:15:10.690中间那是一个可学习的参数。01:15:10.690 --> 01:15:14.650你在学习矩阵-马-权矩阵的最佳方法来获得分数，01:15:14.650 --> 01:15:16.885有用的注意力得分。01:15:16.885 --> 01:15:19.855最后一个被称为附加注意。01:15:19.855 --> 01:15:26.215所以这里发生的是，hi值的分数是，呃，01:15:26.215 --> 01:15:28.435你通过申请得到它01:15:28.435 --> 01:15:33.400对值和查询进行线性转换，然后将它们添加到一起。01:15:33.400 --> 01:15:35.875然后你把它们通过一个非线性，像tanh。01:15:35.875 --> 01:15:37.390最后，呃，01:15:37.390 --> 01:15:40.270你用这个向量，然后用点积01:15:40.270 --> 01:15:43.720一个权重向量，给你一个单一的分数。01:15:43.720 --> 01:15:46.450[噪音]所以这里，你有01:15:46.450 --> 01:15:47.980两个不同的权重矩阵和[噪声]01:15:47.980 --> 01:15:51.190也是一个权重向量，它是可学习的参数。01:15:51.190 --> 01:15:55.765这里有一点不同，那就是有一种额外的超参数，01:15:55.765 --> 01:15:58.000这就是注意力的维度。01:15:58.000 --> 01:16:00.295所以[噪音]就是这样的，呃，01:16:00.295 --> 01:16:04.810我想这是w1和w2的高度，这是v的长度，对吗？01:16:04.810 --> 01:16:07.420您可以选择尺寸。01:16:07.420 --> 01:16:10.225它有点像计算中的隐藏层。01:16:10.225 --> 01:16:14.995所以，嗯，你可以决定你想要中间表示有多大。01:16:14.995 --> 01:16:17.470好吧，我不会再告诉你了，因为01:16:17.470 --> 01:16:19.510实际上是作业中的一个问题，呃，01:16:19.510 --> 01:16:20.860作业4是思考01:16:20.860 --> 01:16:24.010这些模型的相对优缺点。01:16:24.010 --> 01:16:25.645[噪音]好的。01:16:25.645 --> 01:16:26.890下面是今天的总结。01:16:26.890 --> 01:16:28.150[噪音]这确实是最后一张幻灯片，01:16:28.150 --> 01:16:29.800[背景]倒数第二次，最后一次，01:16:29.800 --> 01:16:30.820但这是最后一张幻灯片。01:16:30.820 --> 01:16:33.010[背景]所以我们了解了山的历史。01:16:33.010 --> 01:16:37.960【噪音】我们了解到2014年【噪音】神经机器翻译是如何革新机器翻译的。01:16:37.960 --> 01:16:41.005[噪音]我们学习了如何排序01:16:41.005 --> 01:16:44.395是NMT的正确架构，它使用两个RNN。01:16:44.395 --> 01:16:47.080最后，我们了解了注意力（噪音）是如何集中注意力的。01:16:47.080 --> 01:16:50.660
on particular parts of the input. All right, thanks.

