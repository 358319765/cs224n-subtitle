WEBVTT
Kind: captions
Language: en

00:00:04.190 --> 00:00:08.790
今天，我们很高兴请到，00:00:08.790 --> 00:00:10.980
特邀演讲者Richard Socher，00:00:10.980 --> 00:00:14.085
他是Salesforce的首席科学家。00:00:14.085 --> 00:00:18.040
理查德和这门课也有很多联系，00:00:18.040 --> 00:00:21.840
因为，几年来，00:00:21.840 --> 00:00:24.970
理查德作为讲师参与其中00:00:24.970 --> 00:00:29.110
在斯坦福大学，00:00:29.110 --> 00:00:32.605
所以他对这门课很熟悉。00:00:32.605 --> 00:00:34.030
所以今天，00:00:34.030 --> 00:00:38.830
他将谈论一些挑战和最近的工作00:00:38.830 --> 00:00:43.690
在自然语言处理的多任务学习中。欢迎,Richard。00:00:43.690 --> 00:00:46.595
谢谢你！你好,每个人。我很高兴来到这里。00:00:46.595 --> 00:00:49.380
是的，我今天想和你们谈谈我们，00:00:49.380 --> 00:00:51.280
简而言之，叫做decaNLP。00:00:51.280 --> 00:00:54.635
首先，我想大声喊出布莱恩・麦肯。00:00:54.635 --> 00:00:56.900
他是这篇论文的第一作者00:00:56.900 --> 00:01:00.200
最后我把这个想法告诉了很多人，00:01:00.200 --> 00:01:01.280
三到四年，00:01:01.280 --> 00:01:02.405
大多数人会说，00:01:02.405 --> 00:01:04.730
“这是太多的预处理，因为你试图这么做00:01:04.730 --> 00:01:07.290
在一个模型中完成10个不同的任务。”00:01:07.290 --> 00:01:09.510
这就是十项全能，00:01:09.510 --> 00:01:11.805
措辞已经出来了，但是他00:01:11.805 --> 00:01:13.305
他真的很坚持00:01:13.305 --> 00:01:16.730
做了所有的预处理和所有你现在知道的东西，比如记号化，00:01:16.730 --> 00:01:18.500
结果是很多不同的数据集，00:01:18.500 --> 00:01:20.270
对单词有不同的理解。00:01:20.270 --> 00:01:21.710
这不是两个字，00:01:21.710 --> 00:01:23.480
或者一个词，00:01:23.480 --> 00:01:25.355
诸如此类的事情，会改变你的方式00:01:25.355 --> 00:01:27.470
编写所有的评估脚本。00:01:27.470 --> 00:01:29.165
布莱恩00:01:29.165 --> 00:01:30.770
是一位杰出的研究者，00:01:30.770 --> 00:01:31.985
和我们在一起00:01:31.985 --> 00:01:35.340
Nitish在优化方面帮了我们很多，00:01:35.340 --> 00:01:36.480
然后熊才明，00:01:36.480 --> 00:01:38.415
研究主任做了很多00:01:38.415 --> 00:01:41.735
非常出色的工作，对我们所有的项目都很有帮助。00:01:41.735 --> 00:01:44.830
所以我要告诉你们一些不同的，00:01:44.830 --> 00:01:48.560
我们得出的结论是，00:01:48.560 --> 00:01:50.525
多任务学习的概念。00:01:50.525 --> 00:01:54.170
第一个是试着退一步看看这个领域，00:01:54.170 --> 00:01:58.955
我注意到这门课不太像历史课但基本上是在2010年之前，00:01:58.955 --> 00:02:04.340
大多数自然语言处理都有一些非常手工设计的功能，00:02:04.340 --> 00:02:05.660
我们基本上，00:02:05.660 --> 00:02:08.710
机器学习是一种学习重量的方法，00:02:08.710 --> 00:02:12.680
在这些人为设计的功能的优化过程中。00:02:12.680 --> 00:02:20.780
所以在2010年，Chris和我以及其他人开始为特征学习做深度学习。00:02:20.780 --> 00:02:22.150
所有东西都是一个矢量，00:02:22.150 --> 00:02:25.910
我们可以将它们反向传播，并学习这些表示。00:02:25.910 --> 00:02:27.410
我认为目前，00:02:27.410 --> 00:02:28.880
我们在做很多事情00:02:28.880 --> 00:02:31.940
针对特定任务的深度架构工程，00:02:31.940 --> 00:02:33.110
你们已经看过了。00:02:33.110 --> 00:02:34.700
你有一个NER模型，00:02:34.700 --> 00:02:36.350
你有一个问答模式，00:02:36.350 --> 00:02:37.745
你有一个翻译模型，00:02:37.745 --> 00:02:39.110
现在，00:02:39.110 --> 00:02:41.990
每个社区至少有，00:02:41.990 --> 00:02:44.660
汇聚的可能是某种神经网络，00:02:44.660 --> 00:02:47.570
但是仍然有很多不同的结构00:02:47.570 --> 00:02:51.045
这些神经网络是你为每个不同的任务而工作的。00:02:51.045 --> 00:02:52.590
问题是，00:02:52.590 --> 00:02:54.125
我们可能会这样做00:02:54.125 --> 00:02:57.170
又过了几年，因为我们取得了很大的进步，00:02:57.170 --> 00:02:58.550
但是接下来，00:02:58.550 --> 00:02:59.990
在研究方面?00:02:59.990 --> 00:03:02.480
我非常喜欢这门课00:03:02.480 --> 00:03:05.000
你可能不太了解NLP00:03:05.000 --> 00:03:07.715
一切到你基本上就能明白了00:03:07.715 --> 00:03:10.880
最先进的研究论文，00:03:10.880 --> 00:03:12.950
这个，这个就是其中之一。00:03:12.950 --> 00:03:15.480
所以[噪音]为什么，00:03:15.480 --> 00:03:17.840
为什么不继续在这个多任务系统中工作呢?00:03:17.840 --> 00:03:19.280
在某种程度上，我觉得00:03:19.280 --> 00:03:20.960
社区有点，呃，00:03:20.960 --> 00:03:22.700
就像这只可爱的狗，00:03:22.700 --> 00:03:25.960
每次项目结束后，都要随机重启。00:03:25.960 --> 00:03:29.840
我很清楚，如果你有很多训练数据，00:03:29.840 --> 00:03:34.920
你在数据集上定义一个特定的数据集和任务，00:03:34.920 --> 00:03:39.080
你开始在你的模型中做架构工程师，在一个特定的度量上爬山，00:03:39.080 --> 00:03:41.420
或者排行榜，或者出版物，00:03:41.420 --> 00:03:43.655
或者产品什么的，呃，00:03:43.655 --> 00:03:45.710
只要你的数据集有00:03:45.710 --> 00:03:48.090
大致是一个很好的代表性集合00:03:48.090 --> 00:03:50.875
1000乘以输出类的数量，00:03:50.875 --> 00:03:56.210
你可能会得到一个regi-制度，在那里你在百分之八十到九十的准确性，00:03:56.210 --> 00:03:59.360
或者如果有，你基本上做得很好。00:03:59.360 --> 00:04:02.300
当然，现在当你在ImageNet上看到趋势时，00:04:02.300 --> 00:04:05.000
你有1000个不同的计算机视觉课程，00:04:05.000 --> 00:04:08.640
1000个不同的类，每个类有1000个图像。00:04:08.640 --> 00:04:11.460
如果你有大约一百万张图片，你做得很好。00:04:11.460 --> 00:04:13.740
在机器翻译中，理想情况下，00:04:13.740 --> 00:04:16.250
你知道，我有很多，我有成千上万的话，00:04:16.250 --> 00:04:21.725
所以你需要数以百万计的例子，00:04:21.725 --> 00:04:23.090
呃，单词的语境。00:04:23.090 --> 00:04:24.830
当然，需要注意的是00:04:24.830 --> 00:04:27.620
机器翻译无法达到人类的水平，00:04:27.620 --> 00:04:30.110
但至少在产品中使用它效果很好，00:04:30.110 --> 00:04:34.750
即使是最优秀的人类译者也会将其作为预译，00:04:34.750 --> 00:04:37.030
呃，有点，清理一下。00:04:37.030 --> 00:04:39.985
所以我也很清楚，在这种体制下，00:04:39.985 --> 00:04:41.480
如果我们想，00:04:41.480 --> 00:04:43.550
更一般的人工智能功能，00:04:43.550 --> 00:04:47.360
我们需要对单个模型进行某种更持续的学习。00:04:47.360 --> 00:04:49.840
因为如果我们在每个项目上都重新开始，00:04:49.840 --> 00:04:51.830
我们永远不会得到一个单一的模型，00:04:51.830 --> 00:04:55.715
包含了越来越多的自然语言的复杂性。00:04:55.715 --> 00:04:59.115
当我说我们从随机开始时，00:04:59.115 --> 00:05:01.295
你当然知道这并不完全正确00:05:01.295 --> 00:05:04.190
因为我们确实有一些预先训练过的东西，00:05:04.190 --> 00:05:06.290
即单词向量，在计算机视觉中，00:05:06.290 --> 00:05:07.520
我们有更多的东西。00:05:07.520 --> 00:05:09.020
所以在某种程度上，00:05:09.020 --> 00:05:11.745
一个有抱负的理想为NLP，00:05:11.745 --> 00:05:13.860
因为在计算机视觉中，00:05:13.860 --> 00:05:15.590
不使用一些是很疯狂的00:05:15.590 --> 00:05:19.610
卷积神经网络已经预先训练过了00:05:19.610 --> 00:05:22.520
像ImageNet这样的任务，当你开始你的项目时00:05:22.520 --> 00:05:25.985
试着对对象进行分类或者做对象检测以及其他很多事情。00:05:25.985 --> 00:05:29.750
在某种程度上，整个社区可以很快地支持它，00:05:29.750 --> 00:05:32.460
因为我是说，一旦成功了，00:05:32.460 --> 00:05:34.125
很好，因为有一种，00:05:34.125 --> 00:05:35.990
计算机视觉中的单块任务。00:05:35.990 --> 00:05:38.610
如果你甚至分不清猫狗和房子，00:05:38.610 --> 00:05:42.420
想一个更大的，呃，愿景项目是没有意义的。00:05:42.420 --> 00:05:45.210
在NLP中，我们已经在单词向量方面取得了很多成功，00:05:45.210 --> 00:05:46.650
你现在知道很多了，00:05:46.650 --> 00:05:48.750
开始的时候只是一点点00:05:48.750 --> 00:05:51.780
基于窗口的方法或Word2Vec和GloVe，00:05:51.780 --> 00:05:55.020
然后我们有经过训练的上下文向量，00:05:55.020 --> 00:05:57.295
关于机器翻译，但基本上，00:05:57.295 --> 00:06:00.050
不是只有一组单词，00:06:00.050 --> 00:06:04.455
我们实际上预先训练了一些NLSTMs它们位于这些词向量之上，00:06:04.455 --> 00:06:06.925
我们训练它的方式00:06:06.925 --> 00:06:09.050
布莱恩・麦肯的论文00:06:09.050 --> 00:06:12.530
使用机器翻译的上下文向量，然后是ELMo，00:06:12.530 --> 00:06:16.260
用语言建模代替机器翻译00:06:16.260 --> 00:06:18.570
这当然更好因为有更多的训练数据，00:06:18.570 --> 00:06:20.340
它仍然告诉你很多，00:06:20.340 --> 00:06:23.210
在某种程度上抓住了一个更复杂的版本00:06:23.210 --> 00:06:26.900
在简单的词向量中，00:06:26.900 --> 00:06:29.640
伯特并不是一个语言模型，00:06:29.640 --> 00:06:31.610
试着在语境中预测单词，00:06:31.610 --> 00:06:34.400
但在培训之前，我们要有更多的层次和更深入的网络。00:06:34.400 --> 00:06:39.700
所以我们看到了训练前一定重量的成功。00:06:39.700 --> 00:06:41.265
所以问题是，00:06:41.265 --> 00:06:44.310
为什么不试着对整个模型进行预培训呢?00:06:44.310 --> 00:06:46.650
包括你的输出，00:06:46.650 --> 00:06:50.145
你的softmax，你的指针机制等等，00:06:50.145 --> 00:06:54.240
然后用一个完全预先训练好的模型试着做点什么，00:06:54.240 --> 00:06:56.895
这就是我们的目标。00:06:56.895 --> 00:06:58.890
所以，呃，我们，有点，00:06:58.890 --> 00:07:00.525
问问我们自己为什么没有发生这种事?00:07:00.525 --> 00:07:01.740
为什么我们，00:07:01.740 --> 00:07:03.430
第一个想到的是，00:07:03.430 --> 00:07:05.810
试着对整个模型进行预培训，00:07:05.810 --> 00:07:07.370
编码器和解码器，00:07:07.370 --> 00:07:08.420
输出，等等。00:07:08.420 --> 00:07:12.740
我认为部分原因是NLP需要很多不同的推理。00:07:12.740 --> 00:07:14.420
你们已经见过很多了。00:07:14.420 --> 00:07:18.290
你有一些逻辑推理，比如在座的550人，00:07:18.290 --> 00:07:20.300
离开，房间里还有人吗，00:07:20.300 --> 00:07:22.790
你可以从逻辑上回答这个问题，00:07:22.790 --> 00:07:25.930
你有很多不同种类的语言和情感推理，00:07:25.930 --> 00:07:27.470
情感分析，00:07:27.470 --> 00:07:30.140
这是典型的尼古拉斯凯奇电影，然后你需要知道这是00:07:30.140 --> 00:07:33.590
可能是负面评论，除非你喜欢尼古拉斯凯奇的电影。00:07:33.590 --> 00:07:36.470
嗯,没有判断。哦,00:07:36.470 --> 00:07:38.180
你知道，视觉类型的推理等等。00:07:38.180 --> 00:07:41.450
所以我认为部分是因为一开始的复杂性，00:07:41.450 --> 00:07:46.580
并没有取得很大的进步，只是偶尔把它们分开。00:07:46.580 --> 00:07:50.675
我认为在某些情况下，被人为地划分为所有这些独立的任务，00:07:50.675 --> 00:07:52.340
比如你有命名实体识别，00:07:52.340 --> 00:07:55.795
词性标注，语义角色标注等等。00:07:55.795 --> 00:07:58.560
在某种程度上，这听起来有点刻薄，00:07:58.560 --> 00:07:59.990
这在当时很有意义，00:07:59.990 --> 00:08:02.540
它让我们在社区里取得了很大的进步，00:08:02.540 --> 00:08:04.850
但基本上我们开始追求这些基准，00:08:04.850 --> 00:08:06.290
所有这些不同的社区，00:08:06.290 --> 00:08:08.610
开始以他们自己的方式离开00:08:08.610 --> 00:08:10.320
我们甚至有一些团体说，00:08:10.320 --> 00:08:11.950
“我们做一般的问题回答，00:08:11.950 --> 00:08:14.990
有很多关于一般问题回答的研讨会，当我问，00:08:14.990 --> 00:08:18.350
呃，组织者，“我能问一下你的模特对这条推特的看法吗?”00:08:18.350 --> 00:08:21.240
他们会说，“不，那是情绪分析。去那个不同的工作室。00:08:21.240 --> 00:08:22.515
就在楼下，走廊的那头。”00:08:22.515 --> 00:08:24.270
但我说，“这是个问题。00:08:24.270 --> 00:08:27.335
你为什么不能在问答课上回答呢?”00:08:27.335 --> 00:08:29.940
所以很多人会说，00:08:29.940 --> 00:08:31.540
“嗯，如果你想研究更一般的东西，00:08:31.540 --> 00:08:33.860
它必须是一种无监督的，00:08:33.860 --> 00:08:36.695
任务和功能将不会被监督。”00:08:36.695 --> 00:08:40.490
我不认为NLP会完全没有监督，00:08:40.490 --> 00:08:42.830
我们不会完全在无人监督的情况下解出它，00:08:42.830 --> 00:08:45.410
因为说到底，语言对人们有很大的监督作用，00:08:45.410 --> 00:08:49.020
我想，对于系统也是如此。00:08:49.020 --> 00:08:52.620
你不会，你知道的00:08:52.620 --> 00:08:54.600
如果你有一个孩子，他在丛林里，00:08:54.600 --> 00:08:57.290
它可能会自己形成一个很好的视觉皮层，00:08:57.290 --> 00:08:59.365
但它不会自己发展语言。00:08:59.365 --> 00:09:01.230
然后，然后，就像，00:09:01.230 --> 00:09:03.720
我想如果你能让人工智能互相交流，00:09:03.720 --> 00:09:06.200
对于他们来说，想要找出as是没有任何意义的00:09:06.200 --> 00:09:09.140
和人类一样，通信协议的效率很低，00:09:09.140 --> 00:09:13.970
语言的顺序处理因为算法和计算机可以，00:09:13.970 --> 00:09:16.070
如果没有对人类语言的监督，00:09:16.070 --> 00:09:19.455
他们可以用更有效的方式交流。00:09:19.455 --> 00:09:21.055
所以我认为很明显，00:09:21.055 --> 00:09:24.490
我们在NLP里需要很多的监督。00:09:24.490 --> 00:09:27.840
所以基本上，所有这些都让我们，00:09:27.840 --> 00:09:34.340
试图为许多不同的NLP任务考虑一个统一的多任务模型。00:09:34.340 --> 00:09:36.515
顺便说一下，如果你有任何问题，请举手。00:09:36.515 --> 00:09:39.110
好吧，让我们让这个互动起来。00:09:39.110 --> 00:09:42.555
基本上，我们想要这个统一的模型，00:09:42.555 --> 00:09:45.570
决定如何传递知识，00:09:45.570 --> 00:09:47.885
而不是手工分配。00:09:47.885 --> 00:09:49.280
大多数情况下，00:09:49.280 --> 00:09:50.870
当你分配你的项目时，你会说，00:09:50.870 --> 00:09:55.030
我知道命名实体识别是语音标记的一部分，它可以互相帮助。00:09:55.030 --> 00:09:56.870
因为一旦你知道某物是名词，00:09:56.870 --> 00:10:00.730
那么它更有可能也是一个命名实体。”00:10:00.730 --> 00:10:05.090
在这种情况下，我们想要允许单一的统一模型00:10:05.090 --> 00:10:09.890
了解自己如何进行领域适应以及如何分享权重，00:10:09.890 --> 00:10:12.650
希望这能带来很多，00:10:12.650 --> 00:10:15.935
转移学习和零射击学习能力。00:10:15.935 --> 00:10:19.100
我还认为，如果我们讲到这里，00:10:19.100 --> 00:10:23.265
有一个单一fa-单一统一的多任务模型的硬目标，00:10:23.265 --> 00:10:27.140
然后我们就能更容易地适应它00:10:27.140 --> 00:10:31.085
新任务，我们也能更快地将其部署到生产中。00:10:31.085 --> 00:10:32.405
如果现在你想建造00:10:32.405 --> 00:10:35.570
一个小松鼠探测器，连接到你的洒水系统，00:10:35.570 --> 00:10:37.895
你可以下载一些现成的软件，00:10:37.895 --> 00:10:40.200
这基本上是可行的。00:10:40.200 --> 00:10:42.170
如果你试图这样做，情况就不是这样了00:10:42.170 --> 00:10:44.390
一个非常复杂的语言项目00:10:44.390 --> 00:10:46.955
想要翻译成一种全新的语言，00:10:46.955 --> 00:10:50.240
你知道，分析一些网站，然后做一些其他的事情。00:10:50.240 --> 00:10:51.890
所以你也是00:10:51.890 --> 00:10:56.370
当你真正尝试部署和使用这些工具和公司时，00:10:56.370 --> 00:10:59.075
你会发现有很多不同种类的群体。00:10:59.075 --> 00:11:00.200
这是搜索组，00:11:00.200 --> 00:11:01.310
聊天机器人团队，00:11:01.310 --> 00:11:02.540
翻译团队，00:11:02.540 --> 00:11:05.930
还有社会情绪分析小组，00:11:05.930 --> 00:11:07.100
他们都使用不同的模型，00:11:07.100 --> 00:11:08.390
他们都部署了不同的模型，00:11:08.390 --> 00:11:10.850
而且他们都需要投入大量的开销00:11:10.850 --> 00:11:15.150
人工智能模型的核心，或者说围绕这个核心。00:11:15.150 --> 00:11:18.240
最后，00:11:18.240 --> 00:11:20.435
就像我们和这只狗一样。00:11:20.435 --> 00:11:22.170
我认为一旦我们有了这个统一的模型，00:11:22.170 --> 00:11:24.380
这也是能够做到这一点的第一步00:11:24.380 --> 00:11:26.870
然后不断学习这个，只有一个模型00:11:26.870 --> 00:11:28.880
随着时间的推移变得越来越好，并开始00:11:28.880 --> 00:11:32.030
捕捉越来越复杂的语言。00:11:32.030 --> 00:11:33.980
好的，有问题吗，00:11:33.980 --> 00:11:37.560
动机高水平?00:11:41.700 --> 00:11:44.860
好吧。那么,呃,00:11:44.860 --> 00:11:48.370
这是一个问题，我们如何实现它?00:11:48.370 --> 00:11:52.135
然后我们，我先坐下来，看着，00:11:52.135 --> 00:11:56.560
你可能经历过的所有任务的一般格式00:11:56.560 --> 00:11:58.510
这个类和那个NLP有一个字段00:11:58.510 --> 00:12:01.000
我认为它们可以大致分类，00:12:01.000 --> 00:12:03.100
可以分为这三个不同的类别。00:12:03.100 --> 00:12:04.900
序列标记，你们已经知道了。00:12:04.900 --> 00:12:07.840
比如NER或特定方面的情感或in00:12:07.840 --> 00:12:12.250
一个特定的上下文，我们想分类一个词是积极的还是消极的。00:12:12.250 --> 00:12:14.380
然后是文本分类，00:12:14.380 --> 00:12:17.290
整个文本只有一个标签00:12:17.290 --> 00:12:20.335
然后对很多不同的序列进行排序，00:12:20.335 --> 00:12:23.575
我个人很喜欢，00:12:23.575 --> 00:12:27.490
这三个特别的任务:机器翻译，总结，问题回答。00:12:27.490 --> 00:12:31.450
因为它们很有用，你不需要向别人解释，00:12:31.450 --> 00:12:34.195
“哦，但是为什么需要语义角色标签器或解析器呢?””00:12:34.195 --> 00:12:36.490
如果你是个门外汉00:12:36.490 --> 00:12:38.620
在互联网上，你马上就会明白为什么会这样00:12:38.620 --> 00:12:41.140
对做总结，回答问题，00:12:41.140 --> 00:12:43.240
或者翻译和改进00:12:43.240 --> 00:12:46.840
这些任务可以立即转化为更好的产品，00:12:46.840 --> 00:12:51.430
人们能够更好更有效地用语言交流。00:12:51.430 --> 00:12:57.400
所以，这种分析让我们想到，00:12:57.400 --> 00:13:01.030
关于我所说的NLP的三个等价超任务。00:13:01.030 --> 00:13:03.910
基本上是这样的00:13:03.910 --> 00:13:07.780
语言建模，问题回答现在-问题回答和对话系统。00:13:07.780 --> 00:13:11.410
语言建模，基本上是预测下一个单词，00:13:11.410 --> 00:13:12.430
你已经做过了。00:13:12.430 --> 00:13:18.775
嗯，通常这些天它只被用来重新调整或基本用于预培训。00:13:18.775 --> 00:13:22.645
但是如果你问我一个问题然后你试着预测接下来的几个单词，00:13:22.645 --> 00:13:25.435
这也是语言建模00:13:25.435 --> 00:13:28.810
如果你能预测出问题后的几个单词，比如，00:13:28.810 --> 00:13:32.350
句子中的命名实体是什么然后你就可以生成，00:13:32.350 --> 00:13:34.120
德累斯顿是一个地方，00:13:34.120 --> 00:13:36.430
理查德是个人什么的。00:13:36.430 --> 00:13:41.140
然后你就可以把所有这些任务转换成语言模型。00:13:41.140 --> 00:13:42.580
类似的问题回答，00:13:42.580 --> 00:13:44.080
你可以问任何问题，00:13:44.080 --> 00:13:45.430
翻译是什么，00:13:45.430 --> 00:13:48.115
总结是什么，等等，00:13:48.115 --> 00:13:50.770
现在的对话有点棘手，因为有00:13:50.770 --> 00:13:55.930
没有真正好的对话数据集很多时候你想要一些互动，00:13:55.930 --> 00:14:00.010
您必须运行用户研究，而大多数现有的NLP任务将运行用户研究00:14:00.010 --> 00:14:04.360
基本上是非常短的一步对话，比如什么是命名实体标签，00:14:04.360 --> 00:14:05.560
你给他们，就这样。00:14:05.560 --> 00:14:09.850
这有点过分了，正因为如此，我们基本上收敛了，00:14:09.850 --> 00:14:13.525
回答问题是我们的主要形式主义。00:14:13.525 --> 00:14:18.355
下面是我们所做的10项不同任务的概述，00:14:18.355 --> 00:14:21.610
我们把它们都选为问答题。00:14:21.610 --> 00:14:25.120
这些就是字面上的tr-训练，00:14:25.120 --> 00:14:27.700
培训数据集的格式00:14:27.700 --> 00:14:30.880
最后也是我们表述的方式00:14:30.880 --> 00:14:35.530
测试集，你会看到基本上每个任务，00:14:35.530 --> 00:14:38.605
你有一个上下文作为某种文档。00:14:38.605 --> 00:14:39.700
可以是维基百科上的一篇文章，00:14:39.700 --> 00:14:41.500
可以是一条推特，也可以是一份更长的文件，00:14:41.500 --> 00:14:45.550
无论如何，你问一个关于它的问题，你想要得到一个答案。00:14:45.550 --> 00:14:49.090
我很好奇你们能不能想出NLP中的任何任务00:14:49.090 --> 00:14:52.795
它不能用这种结构表示。00:14:52.795 --> 00:14:55.720
我们来复习一下这些。00:14:55.720 --> 00:14:57.865
第一个是标准，00:14:57.865 --> 00:15:00.145
你们都熟悉的任务。00:15:00.145 --> 00:15:02.440
团队，斯坦福问答数据集。00:15:02.440 --> 00:15:06.880
呃，答案基本上是语境中的一个短语。00:15:06.880 --> 00:15:12.265
但是，第二个是你在大多数情况下都不会看到的，00:15:12.265 --> 00:15:16.900
概括的问题回答研讨会，也就是，00:15:16.900 --> 00:15:20.560
有一个句子的上下文，询问翻译来自哪里00:15:20.560 --> 00:15:25.090
将英语转换成德语，输出同样是一系列单词，但在本例中，00:15:25.090 --> 00:15:26.500
这里我们用不同的颜色。00:15:26.500 --> 00:15:31.870
呃，这是蓝色的，因为所有这些词基本上都不在上下文中00:15:31.870 --> 00:15:35.110
这个问题，我们会生成它们00:15:35.110 --> 00:15:39.280
用一个标准的softmax来回答这个问题。00:15:39.280 --> 00:15:43.390
我们也可以问总结是什么，你们可以看到00:15:43.390 --> 00:15:47.290
在某种程度上，把两个问题变成自然语言问题是人为的。00:15:47.290 --> 00:15:51.250
你可以说翻译或者总结，就像这样00:15:51.250 --> 00:15:56.140
网络中的一种任务令牌，但实际上是这些任务的一半。00:15:56.140 --> 00:16:02.305
这是有意义的，因为问题ac-对于每个例子都是不同的。00:16:02.305 --> 00:16:06.040
这是自然语言推理，NLI，00:16:06.040 --> 00:16:10.920
她还谈到了我们想问的两个句子是否相互包含，00:16:10.920 --> 00:16:14.810
相互矛盾或者它们之间有某种中立的关系。00:16:14.810 --> 00:16:16.900
你已经看到了很多情绪。00:16:16.900 --> 00:16:18.580
这一点很重要。00:16:18.580 --> 00:16:22.600
我们实际上问的是，这句话是积极的还是消极的，而情感是什么00:16:22.600 --> 00:16:27.745
这很重要的原因是你在这里看到的绿色，00:16:27.745 --> 00:16:30.760
这个答案实际上来自00:16:30.760 --> 00:16:34.380
一个有疑问的词，如果我们这样表述，00:16:34.380 --> 00:16:39.330
我们最终可以做零杆学习，我们可以问一个新的问题00:16:39.330 --> 00:16:44.155
从未要求过一套新的标签，在某些情况下，00:16:44.155 --> 00:16:46.180
它仍然有效，你知道，00:16:46.180 --> 00:16:50.500
问que――我们可以问这样的问题:这个故事是快乐的还是悲伤的00:16:50.500 --> 00:16:52.120
给我们一个答案，即使我们从来没有给过00:16:52.120 --> 00:16:55.195
它是一个训练有素的数据集，包含了一堆快乐和悲伤的故事。00:16:55.195 --> 00:16:59.740
这是一种零概率分类00:16:59.740 --> 00:17:02.230
有些情况下，如果你用一种方式表达你的问题00:17:02.230 --> 00:17:05.270
答案是问题的一部分。00:17:05.270 --> 00:17:08.340
然后我们有语义角色标签。00:17:08.340 --> 00:17:15.540
那么经历了什么，一个随机的奇怪的问题。00:17:15.540 --> 00:17:18.450
然后我们有一个零概率关系提取谁是谁00:17:18.450 --> 00:17:22.255
狼人循环的插画师00:17:22.255 --> 00:17:24.580
我们也有一些对话状态跟踪。00:17:24.580 --> 00:17:28.615
对话中的当前状态是什么00:17:28.615 --> 00:17:33.985
随着对话的发展，我们还有SQL，00:17:33.985 --> 00:17:37.690
Wiki SQL翻译任务，但没有翻译成00:17:37.690 --> 00:17:42.025
另一种自然语言转换成SQL数据库查询。00:17:42.025 --> 00:17:43.720
这实际上是一个非常有用的任务。00:17:43.720 --> 00:17:47.830
你知道，有很多数据存储在数据库中。00:17:47.830 --> 00:17:50.440
如果你不需要询问就能访问它00:17:50.440 --> 00:17:53.380
知道如何编写SQL的人00:17:53.380 --> 00:17:56.200
这些数据可供更多的人使用00:17:56.200 --> 00:17:59.260
他们可以分析it，像商业分析等等。00:17:59.260 --> 00:18:02.740
这里是Winograd模式和回指解析。00:18:02.740 --> 00:18:06.100
有些人称之为常识推理，00:18:06.100 --> 00:18:10.225
你知道，在这种情况下，大多数只是照应决议。00:18:10.225 --> 00:18:12.385
呃，谁，你知道，00:18:12.385 --> 00:18:15.550
比如谁提供了帮助00:18:15.550 --> 00:18:19.030
是Susan还是Joanne，然后基于这个背景，00:18:19.030 --> 00:18:22.900
你们应该能算出来，00:18:22.900 --> 00:18:26.860
每个例子的问题都不一样。好吧,是吗?00:18:26.860 --> 00:18:29.890
当你测试的时候，比如你问，00:18:29.890 --> 00:18:31.795
这个句子是肯定的还是否定的，00:18:31.795 --> 00:18:35.290
有时会(听不清)吗?00:18:35.290 --> 00:18:37.765
好问题。问题是，当我问，00:18:37.765 --> 00:18:40.510
这句话是肯定的还是否定的00:18:40.510 --> 00:18:43.915
不小心转到别的任务上去了00:18:43.915 --> 00:18:47.110
我们有一张幻灯片，答案是它非常擅长00:18:47.110 --> 00:18:52.780
知道如何去做任务，从哪里得到答案，从哪里得到答案。00:18:52.780 --> 00:18:56.860
嗯，是的，当我们看完模型后，它们会在几张幻灯片中更有意义。00:18:56.860 --> 00:18:58.555
还有其他问题吗，00:18:58.555 --> 00:19:00.820
回答形式主义的问题?00:19:00.820 --> 00:19:04.930
你能以问答的形式来组织文本生成吗?00:19:04.930 --> 00:19:06.685
比如，给我讲个故事。00:19:06.685 --> 00:19:10.195
好问题。我们能不能做文本生成，00:19:10.195 --> 00:19:11.800
给我讲个故事00:19:11.800 --> 00:19:14.590
从一种随机的形式主义。00:19:14.590 --> 00:19:19.450
我们没有把它作为一项任务，因为在很大程度上它很难评估。00:19:19.450 --> 00:19:22.120
它会告诉你一些随机的事情然后这是不是一个好故事，00:19:22.120 --> 00:19:24.325
它是否符合语法，你必须想出很多，00:19:24.325 --> 00:19:25.750
呃，有点，呃，00:19:25.750 --> 00:19:28.420
我们正在做的评估指标00:19:28.420 --> 00:19:31.330
一些对话系统，在对话的情况下，00:19:31.330 --> 00:19:33.280
为什么它们是等价的00:19:33.280 --> 00:19:36.160
环境可以不断发展，每次，00:19:36.160 --> 00:19:38.395
用户说了些什么00:19:38.395 --> 00:19:43.525
你基本上是在预测对话中的下一个答案。00:19:43.525 --> 00:19:48.700
所以我认为你可以很容易地用它来产生文本。00:19:48.700 --> 00:19:51.220
你只要问就行了，就像你知道的，00:19:51.220 --> 00:19:54.490
这个故事的结局很好，你可以用like开头00:19:54.490 --> 00:19:58.420
然后你让模型生成越来越多的单词，00:19:58.420 --> 00:20:01.975
这个网络的形式我待会再讲。是吗?00:20:01.975 --> 00:20:04.720
我在想你训练的时候00:20:04.720 --> 00:20:07.795
你要像研究一项新任务一样去研究它。00:20:07.795 --> 00:20:11.470
是用更少的数据学习吗?00:20:11.470 --> 00:20:14.320
这是一个令人惊讶的深思熟虑的问题00:20:14.320 --> 00:20:16.930
这很重要，我们会放很多幻灯片。00:20:16.930 --> 00:20:20.980
也许我们会，我们会继续，我们会回答那个问题，00:20:20.980 --> 00:20:25.075
非常详细，因为这就是为什么我们要这么做的原因，简单的回答是肯定的。00:20:25.075 --> 00:20:27.855
但我们会讲到更多细节。好吧。00:20:27.855 --> 00:20:30.210
这就是10个任务。00:20:30.210 --> 00:20:33.970
这是它的实际形式。00:20:33.970 --> 00:20:35.890
所以如果你有问题，00:20:35.890 --> 00:20:37.810
你可以把它转换成这种格式，00:20:37.810 --> 00:20:40.630
你只需要，呃，打开源代码，然后运行它，00:20:40.630 --> 00:20:42.025
呃，会有用的。00:20:42.025 --> 00:20:45.010
所以当你分析和思考我们在这里做了什么。00:20:45.010 --> 00:20:47.680
在某种程度上，我们承担了这些任务00:20:47.680 --> 00:20:50.950
通常是在你的脑海里，但不是给模型的。00:20:50.950 --> 00:20:54.730
这个模型只给出了输入x和输出y00:20:54.730 --> 00:21:00.760
我们实际上是把任务包括在输入中，00:21:00.760 --> 00:21:05.950
在模型的输入集合中。所以你可以称之为元监督学习。00:21:05.950 --> 00:21:08.260
所以问题是，00:21:08.260 --> 00:21:11.140
是我们对每个不同任务的任务定义。00:21:11.140 --> 00:21:13.570
模型必须自己找出什么时候该问这个问题00:21:13.570 --> 00:21:16.180
这样，它也可以自己找出什么时候00:21:16.180 --> 00:21:21.565
从这些任务中转移知识，y就是答案。00:21:21.565 --> 00:21:25.330
所以，在某种程度上，这是元监督学习，我很兴奋00:21:25.330 --> 00:21:29.560
因为一旦你允许把任务作为输入给模型，00:21:29.560 --> 00:21:32.170
它可以自己决定怎么做00:21:32.170 --> 00:21:35.020
现在你可以学习，00:21:35.020 --> 00:21:36.835
更强大的模型。00:21:36.835 --> 00:21:39.310
一旦我们有了数据集，00:21:39.310 --> 00:21:42.265
我们想"好吧，我们现在怎么解决这个问题?"00:21:42.265 --> 00:21:43.960
最简单的方法就是你可以说，00:21:43.960 --> 00:21:45.010
我有一个很大的if语句，00:21:45.010 --> 00:21:47.260
一开始我有一个分类器，然后我分类。00:21:47.260 --> 00:21:49.225
如果这是一个机器翻译任务，00:21:49.225 --> 00:21:51.015
然后运行我的机器翻译模型。”00:21:51.015 --> 00:21:54.300
一般来说，在Python中，它还是像一条大Python，00:21:54.300 --> 00:21:56.430
模型中有很多if语句，对吧?00:21:56.430 --> 00:21:58.770
这不是我们的目标，因为那样的话我们就不会达到任何目标00:21:58.770 --> 00:22:02.195
我们所希望的转移学习和零目标能力。00:22:02.195 --> 00:22:07.630
所以[噪音]我们想要这个模型00:22:07.630 --> 00:22:10.105
有内部调整的能力00:22:10.105 --> 00:22:13.880
去完成这些不同的任务，然后自己做决定。00:22:15.360 --> 00:22:18.490
基本上，所有这些考虑00:22:18.490 --> 00:22:20.620
这些想法让我们想到了这个模型。00:22:20.620 --> 00:22:22.120
在我走之前，00:22:22.120 --> 00:22:23.455
再详细一点。00:22:23.455 --> 00:22:25.825
我会给你们一个高层次的概述。00:22:25.825 --> 00:22:27.925
同样，您从上下文开始。00:22:27.925 --> 00:22:30.715
你开始问一个关于00:22:30.715 --> 00:22:33.700
这个context文档，然后我们会生成，00:22:33.700 --> 00:22:38.560
每次回答一个单词，要么指向上下文，00:22:38.560 --> 00:22:40.045
你们已经学过指针了，对吧?00:22:40.045 --> 00:22:44.035
指针网络?太好了。指向一个疑问词，00:22:44.035 --> 00:22:48.190
或者使用标准softmax分类器从外部词汇表中选择一个单词。00:22:48.190 --> 00:22:52.630
我们会有一个指针开关机制00:22:52.630 --> 00:22:57.415
选择多少权重[噪声]这三个产生机制的每一个。00:22:57.415 --> 00:23:00.760
让我们深入研究一下这个模型。00:23:00.760 --> 00:23:04.600
幸运的是，在某种程度上这是最好的，00:23:04.600 --> 00:23:09.160
在某种程度上，00:23:09.160 --> 00:23:11.560
呃，这个概括得够好了。00:23:11.560 --> 00:23:14.140
你可以看看decanlp.com上的所有代码00:23:14.140 --> 00:23:16.870
它有成千上万的，00:23:16.870 --> 00:23:20.400
星星和叉子之类的东西加在一起00:23:20.400 --> 00:23:21.795
你可以，你知道，00:23:21.795 --> 00:23:24.180
基本上是运行一切00:23:24.180 --> 00:23:29.755
在这个实验中，只有一个命令。00:23:29.755 --> 00:23:33.610
它会加倍，你得到所有的数据集和所有的东西，然后运行所有的东西，00:23:33.610 --> 00:23:36.340
你可以探索它的样子，但是让我们00:23:36.340 --> 00:23:39.370
深入研究一下这个模型告诉我们的细节。00:23:39.370 --> 00:23:41.065
在某种程度上，它只是00:23:41.065 --> 00:23:43.870
所有来自深度学习[噪音]NLP的最好的成分，00:23:43.870 --> 00:23:48.490
其中大部分你已经学过了，并以一种合理的方式把它们组合在一起。00:23:48.490 --> 00:23:50.470
所以我们从固定的手套嵌入开始。00:23:50.470 --> 00:23:52.630
最终我们会更新00:23:52.630 --> 00:23:54.730
嵌入到海湾嵌入，呃，00:23:54.730 --> 00:23:57.715
如果将它们更新为BERT embeddings，可能会更好。00:23:57.715 --> 00:24:00.820
但在某种程度上，我们不得不继续做其他的事情。00:24:00.820 --> 00:24:03.460
但基本上，你有一组固定的词向量，00:24:03.460 --> 00:24:05.860
这很重要，因为在其中的一些，00:24:05.860 --> 00:24:08.545
数据集，它们比其他的要小得多。00:24:08.545 --> 00:24:10.360
你从《小队》里知道的00:24:10.360 --> 00:24:12.580
如果你把它反向传播到向量这个词上，00:24:12.580 --> 00:24:14.680
你只是在训练好的数据集上做得非常非常好，00:24:14.680 --> 00:24:18.310
但是你不会泛化，因为大部分的[噪音]文本，00:24:18.310 --> 00:24:21.430
测试文档会包含你从未见过的单词。00:24:21.430 --> 00:24:24.640
所以如果你在训练过程中改变了所有的向量，00:24:24.640 --> 00:24:28.300
它不会――它不会在测试时很好地工作，也不会概括那些看不见的单词。00:24:28.300 --> 00:24:30.355
固定手套嵌入，00:24:30.355 --> 00:24:31.990
如果没有词向量，00:24:31.990 --> 00:24:35.140
对于不可见的单词，我们也有字符n克嵌入。00:24:35.140 --> 00:24:37.870
然后我们把它们通过一个简单的线性层，00:24:37.870 --> 00:24:39.250
然后我们分享了一个00:24:39.250 --> 00:24:42.535
具有跳过连接的双向LSTM。00:24:42.535 --> 00:24:46.255
所以，嗯，这是一个很深很深的层次，你可以跳到更高的层次，00:24:46.255 --> 00:24:49.090
它由上下文和问题共享。00:24:49.090 --> 00:24:51.850
所以它们的权重基本上是相同的。00:24:51.850 --> 00:24:56.440
然后，我们有一个共同注意层。00:24:56.440 --> 00:24:58.840
我们基本上只有外部产物，00:24:58.840 --> 00:25:03.400
在这两个序列的所有隐藏状态之间，00:25:03.400 --> 00:25:06.070
还有，跳过连接，00:25:06.070 --> 00:25:08.050
也为了避开那些00:25:08.050 --> 00:25:11.200
所以现在你有了与上下文或问题相关的东西，00:25:11.200 --> 00:25:15.460
上下文表示[噪音]或-或该上下文的表示。00:25:15.460 --> 00:25:18.970
[噪音]然后我们把这些输入变压器层，00:25:18.970 --> 00:25:23.575
事实上我们试着用变形金刚来做所有的事情，00:25:23.575 --> 00:25:25.765
没有LSTMs之类的。00:25:25.765 --> 00:25:28.735
不幸的是，变压器层00:25:28.735 --> 00:25:32.590
非常，呃，非常挑剔，很难优化，00:25:32.590 --> 00:25:35.020
有很多关于学习速度的诡计，00:25:35.020 --> 00:25:38.515
我们不能让他们表现得很好，00:25:38.515 --> 00:25:41.755
这10个不同的任务。00:25:41.755 --> 00:25:45.760
[噪音]有时你有一个变压器层，一个变压器网络，00:25:45.760 --> 00:25:46.930
在一项任务中表现很好，00:25:46.930 --> 00:25:49.330
但只有其他变压器网络运行良好00:25:49.330 --> 00:25:51.895
在第二个任务中有一半的图层。00:25:51.895 --> 00:25:55.150
一旦你想要一个有相同层数的网络，00:25:55.150 --> 00:25:57.715
它对这两个任务都不起作用了。00:25:57.715 --> 00:26:00.640
不幸的是，他们都很好00:26:00.640 --> 00:26:03.579
因为它们在gpu中很容易瘫痪，00:26:03.579 --> 00:26:05.110
他们还不够强壮00:26:05.110 --> 00:26:06.820
用来做这个的。00:26:06.820 --> 00:26:09.280
[噪音]所以我们必须有这些LSTMs，00:26:09.280 --> 00:26:11.200
在变压器层之前和之后。00:26:11.200 --> 00:26:15.295
[噪音]然后我们基本上就有了一种标准的自回归，00:26:15.295 --> 00:26:17.770
解码器，给定最后一个状态，00:26:17.770 --> 00:26:19.720
我们生成下一个单词。00:26:19.720 --> 00:26:22.090
然后我们有这三个指针机制。00:26:22.090 --> 00:26:24.460
嗯，它们非常类似指针ne-机制，你已经知道。00:26:24.460 --> 00:26:28.405
但是现在在这些非常语境化的表现之上，00:26:28.405 --> 00:26:30.580
在这个编码器的最后，00:26:30.580 --> 00:26:33.640
它基本上学会了用任何一点来质疑单词，00:26:33.640 --> 00:26:35.770
基于隐含状态的上下文词，00:26:35.770 --> 00:26:38.125
或者有一个标准的softmax，00:26:38.125 --> 00:26:41.395
然后我们有一个加权和，00:26:41.395 --> 00:26:45.490
凸和，这三个不同分布的输出字。00:26:45.490 --> 00:26:48.115
(噪音)。00:26:48.115 --> 00:26:52.690
我想这些都是你们已经见过的标准组件，00:26:52.690 --> 00:26:54.610
呃，给你的――已经看过他们所有的细节了。00:26:54.610 --> 00:26:55.944
但是如果你有任何问题，00:26:55.944 --> 00:26:58.690
嗯，关于我们如何把它组合在一起?是吗?00:26:58.690 --> 00:27:02.920
[噪音]所以输出-输出必须是一个单词。00:27:02.920 --> 00:27:06.610
这是正确的。输出必须是一个单词，它总是来自上下文的单词，00:27:06.610 --> 00:27:08.470
问题中的一个单词或softmax中的一个单词。00:27:08.470 --> 00:27:11.050
(噪音)00:27:11.050 --> 00:27:15.610
我想每个任务的数据预处理是不同的。00:27:15.610 --> 00:27:18.220
所以每个任务的数据预处理是不同的，00:27:18.220 --> 00:27:20.950
但我们必须把所有的东西都标准化00:27:20.950 --> 00:27:23.710
同样的符号化，等等。(噪音)00:27:23.710 --> 00:27:29.770
编码中的双箭头表示双向吗?00:27:29.770 --> 00:27:30.125
是的。00:27:30.125 --> 00:27:30.775
好吧。00:27:30.775 --> 00:27:32.395
是的。但是双箭头，00:27:32.395 --> 00:27:34.000
这里是双向的。00:27:34.000 --> 00:27:38.080
所以对于LSTMs，从左到右，从右到左。好吧。00:27:38.080 --> 00:27:41.050
我们用的是什么数据集?00:27:41.050 --> 00:27:44.125
我说过一开始我很头疼。00:27:44.125 --> 00:27:46.540
我们当然想要包括很多的序列00:27:46.540 --> 00:27:49.720
我们觉得顺序任务非常，00:27:49.720 --> 00:27:54.055
有点高水平，我马上就有用了，00:27:54.055 --> 00:27:57.955
在某种程度上，这也说明了00:27:57.955 --> 00:28:03.310
现在你不需要在一些中间表示上花太多功夫，00:28:03.310 --> 00:28:05.275
恩，不再是NLP了。00:28:05.275 --> 00:28:09.490
你可以直接去找那些真正用户可能关心的最终任务，00:28:09.490 --> 00:28:12.340
然后有这些端到端的可训练系统，00:28:12.340 --> 00:28:14.695
嗯，做得很好。00:28:14.695 --> 00:28:17.290
我自己也做了很多分析工作。00:28:17.290 --> 00:28:18.415
所以我不想，00:28:18.415 --> 00:28:19.540
说我们不需要它。00:28:19.540 --> 00:28:21.580
当然，你仍然需要它来完成一些任务，00:28:21.580 --> 00:28:26.095
但令人惊讶的是，你可以直接去翻译或总结00:28:26.095 --> 00:28:28.870
没有中间表示00:28:28.870 --> 00:28:32.035
都是特别手工设计的。00:28:32.035 --> 00:28:36.310
所以我们做了那三个非常有趣，又艰巨的任务。00:28:36.310 --> 00:28:38.380
回答问题，机器翻译，总结。00:28:38.380 --> 00:28:41.260
他们实际上也有三个最大的数据集，00:28:41.260 --> 00:28:42.820
呃，所有这些。00:28:42.820 --> 00:28:46.960
然后我们有了NLI，基本上，00:28:46.960 --> 00:28:52.195
所有这些，呃，10个数据集[噪音]都是，呃，00:28:52.195 --> 00:28:56.875
在一些情况下，特别是对于翻译，00:28:56.875 --> 00:29:01.030
你可以找到更大的，呃，翻译数据集，00:29:01.030 --> 00:29:03.790
但我们也想留着它00:29:03.790 --> 00:29:08.530
到一个普通人不会在大公司工作的规模00:29:08.530 --> 00:29:13.540
GPU基础设施仍然可以运行实验，[噪音]呃，它们自己。00:29:13.540 --> 00:29:16.630
所以大学和人们，呃，仍然可以运行它。00:29:16.630 --> 00:29:18.985
基本上，如果你只有一个GPU，00:29:18.985 --> 00:29:21.385
大概需要一周左右的时间00:29:21.385 --> 00:29:23.680
做一个实验。00:29:23.680 --> 00:29:26.635
如果在一台大型AWS机器上有多个gpu，00:29:26.635 --> 00:29:29.560
你可以在一两天内做一个实验。00:29:29.560 --> 00:29:31.750
尤其是对于翻译，00:29:31.750 --> 00:29:35.605
你可以得到比IWSLT多得多的数据。00:29:35.605 --> 00:29:38.470
每一个，呃，00:29:38.470 --> 00:29:42.100
社区、数据集和任务都有自己的度量标准。00:29:42.100 --> 00:29:44.050
一开始我们确实尝试过，00:29:44.050 --> 00:29:46.330
我们有很多关于我们应该如何做的讨论00:29:46.330 --> 00:29:49.870
定义此项目的成功度量。00:29:49.870 --> 00:29:51.565
这说不通啊00:29:51.565 --> 00:29:55.300
要对所有不同的任务都有一个标准化的F1分数，00:29:55.300 --> 00:29:57.310
但后来我们基本上意识到了这一点00:29:57.310 --> 00:30:00.250
这些不同的社区有不同的度量标准是有原因的。00:30:00.250 --> 00:30:05.005
不幸的是，至少所有这些指标理论上都是0-100。00:30:05.005 --> 00:30:07.405
当然，实际上，你很少看到，00:30:07.405 --> 00:30:10.270
翻译系统是100，00:30:10.270 --> 00:30:12.280
甚至是蓝线的90分，00:30:12.280 --> 00:30:14.935
或者这些非常非常高的胭脂分数。00:30:14.935 --> 00:30:18.550
但是，你知道，理论上它们从0到100，00:30:18.550 --> 00:30:24.039
我们基本上完整地保留了每个社区的不同评估指标，00:30:24.039 --> 00:30:26.440
我们刚刚说过要把它们加起来。00:30:26.440 --> 00:30:29.380
当我们第一次谈论这个的时候，00:30:29.380 --> 00:30:31.150
我们进行了很多讨论，00:30:31.150 --> 00:30:32.890
和其他人也一样，00:30:32.890 --> 00:30:35.530
但是翻译更重要，因为它很重要00:30:35.530 --> 00:30:38.245
更大，而且比你更有用，00:30:38.245 --> 00:30:40.630
就像Winograd模式一样00:30:40.630 --> 00:30:43.150
只有几百个训练样本。00:30:43.150 --> 00:30:45.730
所以你应该更多地加权平移00:30:45.730 --> 00:30:48.310
五个问题之后有人会说，00:30:48.310 --> 00:30:50.140
“你为什么不更重视代词的分辨率呢?”00:30:50.140 --> 00:30:54.370
这是一项非常困难的任务，它抓住了一些常识推理，00:30:54.370 --> 00:30:56.590
语言和语义的复杂性，00:30:56.590 --> 00:31:00.340
与所有这些不同的是，就像你在翻译中做的统计模式匹配(噪音)。”00:31:00.340 --> 00:31:03.190
我说，我曾经和那个家伙(笑声)说，00:31:03.190 --> 00:31:04.510
希望最后，00:31:04.510 --> 00:31:08.050
我们都同意把它们加起来是合理的00:31:08.050 --> 00:31:13.645
当然，当你做实验的时候，你也要解决这个问题。00:31:13.645 --> 00:31:17.845
机器学习的复杂性，00:31:17.845 --> 00:31:21.625
很少有人会讨论偏态分布。00:31:21.625 --> 00:31:24.610
所以你有翻译，呃，00:31:24.610 --> 00:31:26.620
成千上万的例子，00:31:26.620 --> 00:31:27.730
你有Winograd模式，00:31:27.730 --> 00:31:29.920
只有几百个。00:31:29.920 --> 00:31:34.750
如何训练它，使它不完全忽略较小的数据集。00:31:34.750 --> 00:31:38.350
我们会讲到一些优化技巧，00:31:38.350 --> 00:31:42.010
那个尼蒂什花了几个月的时间00:31:42.010 --> 00:31:45.310
但我首先想给你们看第一组实验。00:31:45.310 --> 00:31:46.960
所以你可以从所有的数字中看到，00:31:46.960 --> 00:31:48.565
有很多实验，00:31:48.565 --> 00:31:50.695
我们跑到这里00:31:50.695 --> 00:31:52.960
所以我们会很仔细地讲一遍。00:31:52.960 --> 00:31:56.110
我希望你们能得到一些关于烧蚀的想法，00:31:56.110 --> 00:31:59.800
或者你想做的实验00:31:59.800 --> 00:32:01.210
在你们的实验中，00:32:01.210 --> 00:32:03.670
问题-最后-最后的项目。00:32:03.670 --> 00:32:05.290
那么我们在看什么呢?00:32:05.290 --> 00:32:07.405
所以基本上，在左边，00:32:07.405 --> 00:32:08.770
我们只有一个任务性能。00:32:08.770 --> 00:32:13.475
这里，每一个数字都来自于它经过训练的不同模型，00:32:13.475 --> 00:32:16.330
单独完成一项任务。00:32:16.330 --> 00:32:22.540
这里的每一行每一列都是相同的结构，00:32:22.540 --> 00:32:23.935
在右边，00:32:23.935 --> 00:32:25.435
我们基本上有，00:32:25.435 --> 00:32:31.165
对于每个列，基本上都是相同的体系结构和相同的模型。00:32:31.165 --> 00:32:34.675
这里，我们有四个不同的模型，00:32:34.675 --> 00:32:37.165
我们有40种不同的型号，00:32:37.165 --> 00:32:40.105
每一列都是相同的结构。00:32:40.105 --> 00:32:41.725
最简单的，00:32:41.725 --> 00:32:44.620
这里的第一列只是一个标准的序列对序列00:32:44.620 --> 00:32:48.280
模型只有很少的铃铛和哨子和一些指针，00:32:48.280 --> 00:32:49.960
但没什么大不了的。00:32:49.960 --> 00:32:51.270
非常深，你知道，00:32:51.270 --> 00:32:53.550
堆栈双向LSTM跳过连接，00:32:53.550 --> 00:32:57.780
对于序列到序列模型，所有标准的、良好调优的东西。00:32:57.780 --> 00:33:00.945
然后我们增加了自我关注。00:33:00.945 --> 00:33:03.405
呃，这个，这个，呃，00:33:03.405 --> 00:33:06.310
基本上，呃，变压器层。00:33:06.310 --> 00:33:08.110
然后我们有这个共同注意层00:33:08.110 --> 00:33:10.225
我们一开始提到的外积，00:33:10.225 --> 00:33:12.715
然后我们还添加了问题指针。00:33:12.715 --> 00:33:17.060
所以有能力在一个问题中指出一个词。00:33:18.330 --> 00:33:21.670
好吧。关于这张桌子有什么问题吗?00:33:21.670 --> 00:33:23.320
我们将深入研究一些细节。00:33:23.320 --> 00:33:25.090
呃,好吧。好吧，我们开始吧00:33:25.090 --> 00:33:27.760
首先是细节，然后你们可以思考一些问题。00:33:27.760 --> 00:33:29.830
我们来分析一下，00:33:29.830 --> 00:33:32.740
这张表里发生了什么因为有很多数字，00:33:32.740 --> 00:33:36.510
你需要仔细分析和区分。00:33:36.510 --> 00:33:37.890
我想我的第一个00:33:37.890 --> 00:33:40.590
观察发现，哇，我们可以有一个单一的建筑。00:33:40.590 --> 00:33:43.170
即使这不是我们想要的，对吧?00:33:43.170 --> 00:33:44.535
我们想要一个单一的模型。00:33:44.535 --> 00:33:46.140
但即使是这样，00:33:46.140 --> 00:33:51.429
你可以有一个单独的架构，它确实做得很好，而且有点随机，00:33:51.429 --> 00:33:53.920
在某些情况下，它实际上得到了最先进的结果。00:33:53.920 --> 00:33:56.020
比如Wiki SQL，00:33:56.020 --> 00:33:59.200
这个架构有最好的模型00:33:59.200 --> 00:34:02.245
要将自然语言的英语问题翻译成SQL查询，00:34:02.245 --> 00:34:05.530
这对我们来说是一个惊喜，因为这是第九个数据集。00:34:05.530 --> 00:34:08.950
在我们设计的时候，这并不是我们的首要任务00:34:08.950 --> 00:34:12.970
该模型考虑了如何生成单词和指针等机制。00:34:12.970 --> 00:34:16.390
我们有SQL单词的标准上下文00:34:16.390 --> 00:34:19.990
我们问的问题是SQL的翻译是什么，然后，00:34:19.990 --> 00:34:24.790
让我们有点惊讶的是这个特殊的建筑有最先进的，00:34:24.790 --> 00:34:27.820
关于SQL生成和社区中的一些人00:34:27.820 --> 00:34:30.865
因为它有最先进的技术00:34:30.865 --> 00:34:32.590
不幸的是，00:34:32.590 --> 00:34:34.915
它没有那么多先进的数字，00:34:34.915 --> 00:34:36.400
这就是为什么更难的原因00:34:36.400 --> 00:34:37.750
这实际上是一项更加艰巨的任务。00:34:37.750 --> 00:34:40.195
你还观察到，00:34:40.195 --> 00:34:42.325
在一些案例中，00:34:42.325 --> 00:34:44.080
使用多任务模型，00:34:44.080 --> 00:34:46.645
所以对于所有的10个任务都有一个单一的模型，00:34:46.645 --> 00:34:48.880
实际上一开始会影响性能。00:34:48.880 --> 00:34:52.120
这也是你很少在报纸上读到的东西，因为报纸00:34:52.120 --> 00:34:55.210
有强烈的选择偏见，只发表积极的结果。00:34:55.210 --> 00:35:00.310
当你看大多数迁移学习和多任务学习的论文时，00:35:00.310 --> 00:35:04.660
它们在实际模型考虑之外，00:35:04.660 --> 00:35:09.100
好吧，让我们只把我们知道彼此能很好合作的任务结合起来。00:35:09.100 --> 00:35:11.050
如果它们不起作用，影响绩效，00:35:11.050 --> 00:35:13.285
然后我们把它们排除在实验之外。00:35:13.285 --> 00:35:16.615
所以你不会看到很多负面的任务结果，00:35:16.615 --> 00:35:20.215
在文献中有一些论文，00:35:20.215 --> 00:35:24.910
学习基本上是转移学习的对立面，00:35:24.910 --> 00:35:28.315
灾难性的干扰和灾难性的遗忘。00:35:28.315 --> 00:35:32.110
干扰是当你在同一个模型中训练两个不同的任务时，00:35:32.110 --> 00:35:35.155
然后互相干扰，就会损害对方的表现。00:35:35.155 --> 00:35:37.960
灾难性的遗忘是如果你不断地训练00:35:37.960 --> 00:35:41.305
你先是在一个任务上训练，然后在另一个任务上训练，00:35:41.305 --> 00:35:42.895
人们曾经认为，00:35:42.895 --> 00:35:44.080
“哦，你知道，00:35:44.080 --> 00:35:45.790
基本上第一个任务就是完成00:35:45.790 --> 00:35:48.970
然后你就能很好地完成第二个任务。00:35:48.970 --> 00:35:52.750
如果你按照顺序训练神经网络一个任务然后00:35:52.750 --> 00:35:56.850
另一个有点令人惊讶的是，00:35:56.850 --> 00:35:59.160
我们发现事实并非如此00:35:59.160 --> 00:36:01.935
灾难性地被遗忘在这些模型中，00:36:01.935 --> 00:36:04.410
如果你按顺序训练它们00:36:04.410 --> 00:36:07.065
你在第一个任务中加入一点原始的，00:36:07.065 --> 00:36:08.760
它回来得非常非常快。00:36:08.760 --> 00:36:10.655
所以虽然表现很差，00:36:10.655 --> 00:36:12.910
你可以得到非常好的表现，00:36:12.910 --> 00:36:14.470
非常快在非常少的迭代。00:36:14.470 --> 00:36:18.115
但这是我们发现的许多有趣的趣闻之一，00:36:18.115 --> 00:36:20.905
在这个过程中，我们甚至还没有发表。好吧。00:36:20.905 --> 00:36:24.055
所以，呃，专注于，呃，00:36:24.055 --> 00:36:26.560
这里的变压器层基本上就是变压器00:36:26.560 --> 00:36:29.275
对原始序列进行序列建模有很大的帮助。00:36:29.275 --> 00:36:33.415
所以如果你仔细调整它们并把它们和，00:36:33.415 --> 00:36:36.235
一些双向的LSTMs等等，00:36:36.235 --> 00:36:38.410
他们很有帮助，而且进步很大，00:36:38.410 --> 00:36:41.800
通过一系列不同的数据集，在某些情况下非常重要。00:36:41.800 --> 00:36:46.390
另一个观察是问答和语义角色标记，00:36:46.390 --> 00:36:49.660
实际上可以很好地预测对方的表现。00:36:49.660 --> 00:36:51.670
如果一个很好，另一个也很好，00:36:51.670 --> 00:36:53.140
呃，反之亦然。00:36:53.140 --> 00:36:54.400
如果效果不好，00:36:54.400 --> 00:36:56.590
两种方法都不太好。00:36:56.590 --> 00:37:00.849
有趣的是这两项任务都有不同的问题，00:37:00.849 --> 00:37:04.075
每个训练的例子。00:37:04.075 --> 00:37:07.780
指向。所以问题是，00:37:07.780 --> 00:37:09.520
是非常重要的。00:37:09.520 --> 00:37:11.695
实际上在某些情况下，00:37:11.695 --> 00:37:13.915
即使是两倍的表现，00:37:13.915 --> 00:37:15.565
这让我们有点惊讶，00:37:15.565 --> 00:37:18.700
一个简单的分类任务，您可以只有一个标准的Softmax。00:37:18.700 --> 00:37:22.645
但不是说你有一个隐含的，矛盾的软最大值，等等，00:37:22.645 --> 00:37:25.015
你基本上，呃，00:37:25.015 --> 00:37:28.015
指出问题中包含的单词。00:37:28.015 --> 00:37:32.050
Winograd模式也是如此，它也从中受益良多，00:37:32.050 --> 00:37:34.000
从这个指针机制。00:37:34.000 --> 00:37:36.190
(噪音)00:37:36.190 --> 00:37:36.880
你能解释一下吗?00:37:36.880 --> 00:37:39.490
确定。我们能解释一下吗?为什么- - - - - -00:37:39.490 --> 00:37:41.470
(听不清)00:37:41.470 --> 00:37:42.760
为什么这么有用?00:37:42.760 --> 00:37:44.980
在某些方面，00:37:44.980 --> 00:37:47.860
我认为部分原因是整个建筑00:37:47.860 --> 00:37:51.160
变得越来越擅长指路了。00:37:51.160 --> 00:37:53.320
我们这么做的部分原因是，00:37:53.320 --> 00:37:54.730
翻译很差，00:37:54.730 --> 00:37:59.020
在我们的第一个实验中，这是唯一让我们受伤的，00:37:59.020 --> 00:38:02.500
在多任务设置中，这是现在唯一需要生成的任务，00:38:02.500 --> 00:38:05.440
结果是完全独立的Softmax软件00:38:05.440 --> 00:38:07.660
而建筑的其他部分，00:38:07.660 --> 00:38:12.535
非常擅长指着东西回答问题，任何问题。00:38:12.535 --> 00:38:15.550
但在某些方面，00:38:15.550 --> 00:38:17.560
我认为这是一种解释，00:38:17.560 --> 00:38:19.720
但是我，我不认为这就是全部。00:38:19.720 --> 00:38:29.005
我认为我们还需要找出更多的原因。好吧。00:38:29.005 --> 00:38:32.200
现在，多任务学习是最多的00:38:32.200 --> 00:38:35.470
当谈到零点的时候，我真的很兴奋。00:38:35.470 --> 00:38:39.835
所以这是一个零概率关系提取你有不同种类的，00:38:39.835 --> 00:38:42.430
关系，你可能想提取，你可能从来没有00:38:42.430 --> 00:38:45.550
就像你正在尝试的师生关系00:38:45.550 --> 00:38:47.860
在特定的上下文中识别00:38:47.860 --> 00:38:51.745
产品公司关系之类的。00:38:51.745 --> 00:38:55.480
所以，呃，那个实际上，呃，00:38:55.480 --> 00:38:58.180
获益匪浅，几乎赚了两倍00:38:58.180 --> 00:39:00.280
就精确度而言，00:39:00.280 --> 00:39:02.380
当你和其他东西一起学习的时候。00:39:02.380 --> 00:39:04.360
这些都是以前从未见过的问题，00:39:04.360 --> 00:39:06.265
前所未有的关系，00:39:06.265 --> 00:39:08.725
结果好了一倍00:39:08.725 --> 00:39:13.210
尤其是从其他类型的问题中受益。00:39:13.210 --> 00:39:16.870
在某种程度上，我们也要为球队增光添彩，00:39:16.870 --> 00:39:18.895
因为小队作为数据集00:39:18.895 --> 00:39:24.760
某种程度上促使人们把指针看作是一种产生答案的机制。00:39:24.760 --> 00:39:28.750
指针，我们把它们看作是给定的它们没有得到那么多学分，00:39:28.750 --> 00:39:33.535
但它们能让你预测出你在训练时从未见过的答案。00:39:33.535 --> 00:39:36.040
为了产生你从未见过的词汇，00:39:36.040 --> 00:39:39.850
这真的非常非常令人惊讶。好吧。00:39:39.850 --> 00:39:43.090
现在，主要的观察00:39:43.090 --> 00:39:46.810
如果你有一个神谕会告诉你00:39:46.810 --> 00:39:50.275
确切地说，您当前正在执行的是哪个任务00:39:50.275 --> 00:39:54.685
你可以把它们完美地分成10种不同的模型，00:39:54.685 --> 00:39:58.945
也许它们都是相同的结构但是仍然有10种不同的模型，00:39:58.945 --> 00:40:02.410
你还是会做得更好，00:40:02.410 --> 00:40:06.535
比起第一个版本的多任务学习模型。00:40:06.535 --> 00:40:09.070
这主要是因为我们00:40:09.070 --> 00:40:12.430
选择包含一堆无关紧要的不同任务00:40:12.430 --> 00:40:15.130
我们希望社区能够建立起来00:40:15.130 --> 00:40:18.310
考虑处理灾难性的干扰，对吧?00:40:18.310 --> 00:40:21.685
如果你学习一门新语言，00:40:21.685 --> 00:40:24.670
你学习如何理解Twitter上的社交媒体，00:40:24.670 --> 00:40:26.860
你不能取代你所有的语言，00:40:26.860 --> 00:40:28.825
你知道，在你的大脑里。00:40:28.825 --> 00:40:30.820
你只有一个大脑，它会变得越来越聪明，00:40:30.820 --> 00:40:32.065
你不断学习新技能，00:40:32.065 --> 00:40:35.140
即使你的新技能非常，00:40:35.140 --> 00:40:36.520
和以前的技能大不相同。00:40:36.520 --> 00:40:40.420
所以在某种程度上，我们可能使我们的生活过于艰难，00:40:40.420 --> 00:40:41.770
现在我们实际上在想，00:40:41.770 --> 00:40:44.620
如果你想发表一篇更好的关于多任务学习的论文，00:40:44.620 --> 00:40:46.810
我们来看看所有互相帮助的任务，00:40:46.810 --> 00:40:48.880
然后我们会有一组任务，00:40:48.880 --> 00:40:51.445
然后我可以很快地发表，00:40:51.445 --> 00:40:54.010
一些，一些很先进的论文。00:40:54.010 --> 00:40:57.370
但基本上在这里我们还是00:40:57.370 --> 00:41:03.910
在十核模型和一个模型之间有很大的差别。00:41:03.910 --> 00:41:06.280
现在，这当然有点像甲骨文的分数，00:41:06.280 --> 00:41:09.805
这就是为什么我们把它放在括号里因为你实际上没有这个oracle。00:41:09.805 --> 00:41:11.260
在某些情况下，00:41:11.260 --> 00:41:13.780
构建一个几乎完美的分类器非常容易。00:41:13.780 --> 00:41:16.615
所以，你知道，分开总结是什么00:41:16.615 --> 00:41:19.810
基于这个问题以及从英语到德语的翻译，00:41:19.810 --> 00:41:21.610
你可以做到几乎100%的准确性。00:41:21.610 --> 00:41:25.090
但是，小队，回答问题，00:41:25.090 --> 00:41:26.665
和零炮点关系提取，00:41:26.665 --> 00:41:29.575
作为语义角色标签的问答，00:41:29.575 --> 00:41:33.220
这些实际上很容易混淆00:41:33.220 --> 00:41:37.330
产生答案，你可能不太清楚，00:41:37.330 --> 00:41:40.870
呃，到哪一个模型去，呃，这个。00:41:40.870 --> 00:41:44.935
所以在某种意义上，这是一种理论。好吧。00:41:44.935 --> 00:41:47.710
现在，我提到了这个问题00:41:47.710 --> 00:41:51.730
优化策略中的复杂性这是众多复杂性之一，00:41:51.730 --> 00:41:55.795
这些问题没有得到足够的报道。00:41:55.795 --> 00:41:57.535
但是当你有一个，00:41:57.535 --> 00:41:59.785
不平衡或倾斜的数据集，00:41:59.785 --> 00:42:05.005
很容易丢失跟踪，并基本上压倒较小的数据集任务。00:42:05.005 --> 00:42:07.510
所以，第一个，00:42:07.510 --> 00:42:10.780
最简单的训练――我们实际上尝试了很多不同的训练策略，00:42:10.780 --> 00:42:13.600
但最后，这个完全连接的很好。00:42:13.600 --> 00:42:18.160
但实际上我答应过你去等问题，呃，在这张桌子上。00:42:18.160 --> 00:42:20.680
到目前为止，对这些结果有什么问题吗?是吗?00:42:20.680 --> 00:42:24.550
既然你说过如果你有的话00:42:24.550 --> 00:42:26.740
一个神谕会告诉你这是哪个任务00:42:26.740 --> 00:42:29.215
有两种更好的方法有10种不同的。00:42:29.215 --> 00:42:32.440
所以试着训练一个模型00:42:32.440 --> 00:42:35.710
就像数据意味着什么任务对这个特定的版本感兴趣?00:42:35.710 --> 00:42:38.310
我们所做的。所以它很困惑，00:42:38.310 --> 00:42:42.240
小队，还有那些任务，另一个，基本上是另一个，00:42:42.240 --> 00:42:47.265
还有两种类型的问题，问问题回答。00:42:47.265 --> 00:42:49.350
所以它混淆了这些。00:42:49.350 --> 00:42:53.490
但是其他的很多，它都能很完美地做到。00:42:53.490 --> 00:42:56.190
但是基本上，一旦你，00:42:56.190 --> 00:43:01.105
然后我们试着建立一个完整的模型然后得到一个decaScore，00:43:01.105 --> 00:43:05.395
如果你的分类器有90%的准确率，00:43:05.395 --> 00:43:08.530
把这个乘以0。900:43:08.530 --> 00:43:11.680
你被弄得很惨，以至于它不再有竞争力了。00:43:11.680 --> 00:43:14.350
所以如果你想要建造它实际上是很困难的00:43:14.350 --> 00:43:17.080
整个系统，不断添加if-then else语句，00:43:17.080 --> 00:43:18.880
为了让这个00:43:18.880 --> 00:43:20.885
变成一个单一的系统。是吗?00:43:20.885 --> 00:43:24.090
你有没有试过告诉模型它在做什么任务，00:43:24.090 --> 00:43:27.330
只是快速给出任务的指示?00:43:27.330 --> 00:43:29.010
我的意思是，在某种程度上，00:43:29.010 --> 00:43:30.120
在这个例子中，00:43:30.120 --> 00:43:33.360
因为我们只对每个模型进行单独的训练。00:43:33.360 --> 00:43:34.280
(听不清)00:43:34.280 --> 00:43:36.905
只有通过这个问题。00:43:36.905 --> 00:43:39.185
是的。因为我在想00:43:39.185 --> 00:43:42.760
嗯，也许模型算出我们想要的并不重要00:43:42.760 --> 00:43:44.965
在实际应用[噪音]00:43:44.965 --> 00:43:47.560
如果我们能告诉它我们现在想做什么?00:43:47.560 --> 00:43:49.420
在某些情况下，你能看出来。00:43:49.420 --> 00:43:51.430
所以问题是，00:43:51.430 --> 00:43:53.260
即使在多任务的情况下，00:43:53.260 --> 00:43:56.095
你可以用一个额外的符号来表示，00:43:56.095 --> 00:43:58.150
现在，你在做总结。00:43:58.150 --> 00:43:59.950
这是另一个输入。”00:43:59.950 --> 00:44:01.255
在某些方面，00:44:01.255 --> 00:44:03.610
无论您是否有摘要令牌，00:44:03.610 --> 00:44:05.650
或者你问总结是什么?00:44:05.650 --> 00:44:08.125
实际上，我不认为这有什么区别。00:44:08.125 --> 00:44:11.190
现在您可以查询这个模型00:44:11.190 --> 00:44:13.140
非常自然的语言，而不是必须知道00:44:13.140 --> 00:44:15.600
这是一个特殊的令牌，用来查询模型。00:44:15.600 --> 00:44:19.710
我们会在几张幻灯片中看到这个模型没有混淆，00:44:19.710 --> 00:44:22.860
当涉及到如何产生答案的时候。00:44:22.860 --> 00:44:24.710
所以，对于每一个任务，00:44:24.710 --> 00:44:28.660
它非常清楚如何生成正确的单词，00:44:28.660 --> 00:44:30.700
得到一个合理准确的答案。00:44:30.700 --> 00:44:36.520
[噪音]嗯，在-[听不清]做模型00:44:36.520 --> 00:44:42.580
看看所有的数据然后[听不清]那个类或者它只包括一个[听不清]?00:44:42.580 --> 00:44:45.400
哦,很好的问题。那么，我们如何训练单一任务模型呢?00:44:45.400 --> 00:44:47.980
他们只在那个数据集上受过训练。00:44:47.980 --> 00:44:51.700
所以，这里的队号只是一个只看过球队训练的模型。00:44:51.700 --> 00:44:57.250
[噪音]你的观点是，00:44:57.250 --> 00:44:59.050
指针异常，00:44:59.050 --> 00:45:02.310
[听不清]通常比[听不清]更有用?00:45:02.310 --> 00:45:04.830
令人惊讶的是，00:45:04.830 --> 00:45:06.315
在这个案子里，00:45:06.315 --> 00:45:09.065
这是多项式，00:45:09.065 --> 00:45:10.690
这个特殊的模型，00:45:10.690 --> 00:45:12.550
如果你有一个标准的序列对序列，00:45:12.550 --> 00:45:14.035
它只是生成，00:45:14.035 --> 00:45:16.660
还有一个softmax，呃，那个标签。00:45:16.660 --> 00:45:18.640
所以在这个意义上，这是非常相似的。00:45:18.640 --> 00:45:23.650
但事实上，它能更好地指向，这实际上让我们，00:45:23.650 --> 00:45:27.730
有一段时间，我在想也许我们应该有一个项目，我们只说指向00:45:27.730 --> 00:45:32.125
所有的事情，只是摆脱软最大分类器永远。00:45:32.125 --> 00:45:35.890
问题是当你尝试翻译的时候，00:45:35.890 --> 00:45:37.210
就像好哇，00:45:37.210 --> 00:45:38.395
你指的是什么?00:45:38.395 --> 00:45:40.420
然后你对它进行预培训00:45:40.420 --> 00:45:43.750
一些对齐，它会变得非常大，你会指向很多不同的地方，00:45:43.750 --> 00:45:46.360
你可能有成千上万的潜在候选人。00:45:46.360 --> 00:45:49.540
所以我们把它当作一个统一的模型，00:45:49.540 --> 00:45:51.895
但是你可以指出很多不同的地方，00:45:51.895 --> 00:45:52.990
就像这些任务一样，00:45:52.990 --> 00:45:54.280
你可以指向00:45:54.280 --> 00:45:59.030
我认为这是另一个有趣的衍生项目，是的。00:46:01.440 --> 00:46:03.745
只是一个关于如何，00:46:03.745 --> 00:46:06.910
多么敏感[听不清]多么敏感，嗯，00:46:06.910 --> 00:46:09.850
个体的组成部分(听不清)是当你00:46:09.850 --> 00:46:13.240
轻微扰动它们在损失函数中的相对权重?00:46:13.240 --> 00:46:16.855
所以，我们的问题是，00:46:16.855 --> 00:46:19.795
这些任务很敏感，00:46:19.795 --> 00:46:22.825
给不同的任务增加权重?00:46:22.825 --> 00:46:27.490
我们(噪音)在优化过程中耍了很多花招00:46:27.490 --> 00:46:32.080
如何训练它，但我们从来没有说过这个任务只与0。5有关。00:46:32.080 --> 00:46:34.930
所以，我们没有做那个分析。是吗?00:46:34.930 --> 00:46:37.990
共同关注似乎有点负担。00:46:37.990 --> 00:46:39.070
在某些情况下，是的。00:46:39.070 --> 00:46:44.425
是[听不清]共同注意和秩序但没有共同注意还是，00:46:44.425 --> 00:46:47.320
“哦，你已经看到了测试数据，所以，你不能使用这些。”00:46:47.320 --> 00:46:49.045
这些都是dep集。00:46:49.045 --> 00:46:53.560
但是，你绝对可以做更多的建筑工程。00:46:53.560 --> 00:46:55.900
事实上，这整个领域我都不认为00:46:55.900 --> 00:46:58.690
你得到了，对吧，神经结构搜索?00:46:58.690 --> 00:47:02.515
是的。你可以把强化学习结合起来，00:47:02.515 --> 00:47:05.695
你说的是强化学习代理的作用空间00:47:05.695 --> 00:47:07.360
想要几个吗00:47:07.360 --> 00:47:09.580
不同的神经网络模块就像你想要的那样00:47:09.580 --> 00:47:11.185
比如CNN层，然后是00:47:11.185 --> 00:47:14.320
一个内存层，然后是LSTM层，可能是双向的00:47:14.320 --> 00:47:19.465
基本上让强化学习代理来做所有这些决定。00:47:19.465 --> 00:47:22.855
所以我觉得申请这个职位是很了不起的00:47:22.855 --> 00:47:25.210
神经结构搜索不是为了什么00:47:25.210 --> 00:47:27.790
通常我们已经知道如何进行图像分类，00:47:27.790 --> 00:47:30.715
我们会用NAS，神经结构搜索做得更好。00:47:30.715 --> 00:47:31.930
但实际上我们试图找到00:47:31.930 --> 00:47:34.810
一个我们不知道的用于多任务学习的单一架构。00:47:34.810 --> 00:47:38.620
当然，问题是已经达到了这些。00:47:38.620 --> 00:47:41.470
所有这些数字都花费了大量的计算时间00:47:41.470 --> 00:47:44.875
摆弄东西，我可以，00:47:44.875 --> 00:47:48.985
我只能给你们一个大概的概念就像我们经常说，00:47:48.985 --> 00:47:50.890
“哦，天哪，我们得到了这个惊人的结果00:47:50.890 --> 00:47:53.110
但它需要这样的学习速度。”00:47:53.110 --> 00:47:55.000
结果是相同的模型，00:47:55.000 --> 00:47:57.100
同样的超参数集，00:47:57.100 --> 00:48:01.555
但要想取得好的成绩，另一项任务则需要更高的学习速度。00:48:01.555 --> 00:48:05.650
现在，你试着把这两个任务结合起来，00:48:05.650 --> 00:48:07.345
“好的，你现在如何选择你的学习速度?”00:48:07.345 --> 00:48:09.070
你可以选择，00:48:09.070 --> 00:48:11.650
如果你选择了这个任务，从这个任务中学习的速度，00:48:11.650 --> 00:48:13.780
比小任务更大的任务是行不通的00:48:13.780 --> 00:48:15.970
因为它需要更高的学习速度。00:48:15.970 --> 00:48:19.405
如果你使用更小的任务和更小的数据集，00:48:19.405 --> 00:48:23.995
嗯，那件大的太合身了，也不太合适。00:48:23.995 --> 00:48:25.960
如果你想求平均值，这两个都不行。00:48:25.960 --> 00:48:29.560
多任务学习非常复杂。00:48:29.560 --> 00:48:33.860
这就是为什么，我认为这是一个很有趣的研究挑战。00:48:35.100 --> 00:48:38.415
关于第一个结果还有什么问题吗?00:48:38.415 --> 00:48:39.780
他们得到了，他们会变得更好。00:48:39.780 --> 00:48:42.270
我们已经有了一些想法，00:48:42.270 --> 00:48:45.370
呃，关于，关于如何改进它们。00:48:47.250 --> 00:48:49.780
好吧。所以,呃,00:48:49.780 --> 00:48:51.775
我们是如何训练这一切的呢?00:48:51.775 --> 00:48:54.895
我们尝试了很多不同的方法，但最后，00:48:54.895 --> 00:48:58.990
这种非常简单的完全联合训练策略实际上效果最好。00:48:58.990 --> 00:49:02.800
基本上就是从每一种中取一小批00:49:02.800 --> 00:49:07.540
不同的任务，你只需要训练那个小批任务。00:49:07.540 --> 00:49:11.470
基本上就是完成所有的10个任务然后循环，00:49:11.470 --> 00:49:13.690
仔细检查一下。00:49:13.690 --> 00:49:16.825
现在发现，00:49:16.825 --> 00:49:19.090
那是行不通的，00:49:19.090 --> 00:49:21.460
还好，00:49:21.460 --> 00:49:26.050
作为另一种培训策略，如果你研究最优化，00:49:26.050 --> 00:49:27.685
神经网络的策略00:49:27.685 --> 00:49:29.170
实际上有几篇论文00:49:29.170 --> 00:49:31.720
所谓的课程学习，00:49:31.720 --> 00:49:36.430
您首先使用简单的问题实例来训练您的模型。00:49:36.430 --> 00:49:38.830
比如，在翻译方面，你开始接受培训00:49:38.830 --> 00:49:41.995
非常短的句子，然后你会看到越来越大的句子，00:49:41.995 --> 00:49:44.560
呃，句子，呃，或者越来越长的句子。00:49:44.560 --> 00:49:47.545
现在多任务学习，00:49:47.545 --> 00:49:49.285
实际上你想做的是相反的。00:49:49.285 --> 00:49:52.045
你想做反课程学习。00:49:52.045 --> 00:49:55.330
你从最难的任务开始，然后不断重复00:49:55.330 --> 00:49:58.930
那些先做一段时间，然后你再添加简单的任务。00:49:58.930 --> 00:50:02.050
在某种程度上，我认为这很直观，因为什么时候00:50:02.050 --> 00:50:07.780
你训练这个巨大而强大的模型，00:50:07.780 --> 00:50:11.020
做一个很简单的任务00:50:11.020 --> 00:50:14.515
你只需要把所有事情分成积极的和消极的。00:50:14.515 --> 00:50:18.220
你训练了所有这些重量，00:50:18.220 --> 00:50:20.710
局部最优是非常深入的00:50:20.710 --> 00:50:24.370
专门用来产生这两个词如果你想摆脱它，00:50:24.370 --> 00:50:27.430
从这个局部最优值出发来完成这个简单的任务00:50:27.430 --> 00:50:30.655
然后试着生成所有这些其他类型的单词并指向不同的，00:50:30.655 --> 00:50:33.925
你知道，以前从来没见过这样的词，00:50:33.925 --> 00:50:36.940
很难从局部最优解中出来。00:50:36.940 --> 00:50:40.975
这就是我的直觉，为什么说，00:50:40.975 --> 00:50:44.935
“让我们从小队和机器翻译开始，还有一些更艰巨的任务。00:50:44.935 --> 00:50:47.020
我们将使模型非常通用。00:50:47.020 --> 00:50:48.910
它必须产生很多不同的东西，00:50:48.910 --> 00:50:52.240
创建一个softmax，德语单词，00:50:52.240 --> 00:50:54.460
它必须指向各种各样的00:50:54.460 --> 00:50:57.895
不同的单词，能够分析各种不同的维基百科段落。”00:50:57.895 --> 00:51:01.315
你这样做了几次，一旦你完成了，00:51:01.315 --> 00:51:03.190
这种预培训00:51:03.190 --> 00:51:09.220
舞台或反课程，然后你继续添加一些简单的小任务。00:51:09.220 --> 00:51:11.590
所以[噪音]00:51:11.590 --> 00:51:15.085
相对简单的改变，00:51:15.085 --> 00:51:17.455
有很多不同的实验要做。00:51:17.455 --> 00:51:20.200
我们实际上00:51:20.200 --> 00:51:22.045
关闭或者，呃，呃，00:51:22.045 --> 00:51:25.570
现在，嗯，00:51:25.570 --> 00:51:30.330
我们只差14岁00:51:30.330 --> 00:51:32.780
是的，14岁左右。00:51:32.780 --> 00:51:35.180
但还是有00:51:35.180 --> 00:51:37.700
差距很大，最大的，00:51:37.700 --> 00:51:40.880
我们遇到的麻烦和问题是翻译。00:51:40.880 --> 00:51:42.845
基本上，如果你看看所有这些，00:51:42.845 --> 00:51:44.914
大多数事情都差不多，00:51:44.914 --> 00:51:49.160
稍微好一点，嗯，有点像抛硬币，但是然后，00:51:49.160 --> 00:51:52.130
大致相似，但翻译很差。00:51:52.130 --> 00:51:53.450
几乎只有一半00:51:53.450 --> 00:51:56.420
在多任务学习设置中的性能，00:51:56.420 --> 00:52:00.110
部分原因是翻译是唯一的任务00:52:00.110 --> 00:52:05.960
一个非常大的Softmax词汇表，没有其他任务。00:52:05.960 --> 00:52:08.075
大多数其他的任务，00:52:08.075 --> 00:52:10.430
实际上我们在指向方面做得很好。00:52:10.430 --> 00:52:14.570
所以，我对这个的解释是中间层，00:52:14.570 --> 00:52:16.550
我们学过的所有这些表示00:52:16.550 --> 00:52:19.520
双向LSTMs和变压器，他们真的，00:52:19.520 --> 00:52:21.875
很擅长被指出来，00:52:21.875 --> 00:52:27.560
比如创建答案模块可以非常精确地指向的隐藏表示。00:52:27.560 --> 00:52:29.465
然后你有一个任务，00:52:29.465 --> 00:52:31.085
我没有指出任何东西，00:52:31.085 --> 00:52:34.235
我基本上只是生成其他单词，然后生成不同的词汇。00:52:34.235 --> 00:52:37.610
所以那些隐藏的表示对这个任务就没那么有用了。00:52:37.610 --> 00:52:41.360
所以，这就是其中一个深刻的见解00:52:41.360 --> 00:52:45.020
这是一种改进的方法。00:52:45.020 --> 00:52:47.615
我们遇到的一个有趣的问题是，00:52:47.615 --> 00:52:49.040
当我们改进模型时，00:52:49.040 --> 00:52:51.500
所有10个任务的多单模型，00:52:51.500 --> 00:52:53.090
很多时候我们说，00:52:53.090 --> 00:52:55.280
但现在我们还得回去跑00:52:55.280 --> 00:52:59.060
10个以上的实验对所有的单一任务进行适当的比较，对吗?00:52:59.060 --> 00:53:01.280
因为如果你调整你所关心的东西，00:53:01.280 --> 00:53:04.790
你停止调整你想要展示你可以做得更好的东西，00:53:04.790 --> 00:53:06.275
那就不公平了。00:53:06.275 --> 00:53:09.470
所以你总是想付出那么多00:53:09.470 --> 00:53:13.655
薄层色谱法，聚焦和实验时间到你的基线。00:53:13.655 --> 00:53:17.789
所以，在某些情况下，00:53:18.670 --> 00:53:22.415
改进了一些，改进了一些。00:53:22.415 --> 00:53:26.495
然后，我们改进了10个单独的模型和我们的模型，00:53:26.495 --> 00:53:29.090
有些情况下，比如10个单独的模型得到了改善，甚至更多。00:53:29.090 --> 00:53:30.485
差距就更大了。00:53:30.485 --> 00:53:32.720
这和我们想要展示的相反，但总的来说，00:53:32.720 --> 00:53:34.220
两个测试都比较好，00:53:34.220 --> 00:53:36.530
呃，总体架构。00:53:36.530 --> 00:53:37.970
所以基本上，我们开始，00:53:37.970 --> 00:53:40.220
通过这种全面的联合训练，我们已经00:53:40.220 --> 00:53:42.515
这是一组单一的模型，00:53:42.515 --> 00:53:44.150
从理论上讲，00:53:44.150 --> 00:53:45.335
总结一下，00:53:45.335 --> 00:53:47.015
在他们的分数上，得到一个十分。00:53:47.015 --> 00:53:49.115
差距从23开始。00:53:49.115 --> 00:53:53.030
然后，我们进行了反课程培训，00:53:53.030 --> 00:53:55.790
这就把差距缩小到了15。00:53:55.790 --> 00:53:57.380
所以我们有点兴奋，00:53:57.380 --> 00:53:58.760
进展顺利。00:53:58.760 --> 00:53:59.930
然后我们换了，00:53:59.930 --> 00:54:01.880
手套和使用湾。00:54:01.880 --> 00:54:04.055
所以上下文向量，00:54:04.055 --> 00:54:06.320
这实际上又一次拉大了差距。00:54:06.320 --> 00:54:09.325
所以一切都变好了，但是10个不同的模型变好了00:54:09.325 --> 00:54:13.000
甚至比完成10项任务的单个模型还要好。00:54:13.000 --> 00:54:14.650
差距越来越大，00:54:14.650 --> 00:54:17.140
但每个人的表现都有所提高。00:54:17.140 --> 00:54:19.510
所以总的来说还是一件好事。00:54:19.510 --> 00:54:22.780
然后我们就想00:54:22.780 --> 00:54:24.610
尤其是机器翻译的问题，00:54:24.610 --> 00:54:26.470
我们不应该只在球队训练前，00:54:26.470 --> 00:54:30.100
但我们也应该包括机器翻译00:54:30.100 --> 00:54:34.845
这是一开始的预训练，所以模型并不只是开始学习指向。00:54:34.845 --> 00:54:37.625
这对我们很有帮助00:54:37.625 --> 00:54:40.160
为了缩小10个独立模型之间的差距，00:54:40.160 --> 00:54:43.085
而甲骨文的单机到五点左右。00:54:43.085 --> 00:54:44.690
然后，我们基本上说，00:54:44.690 --> 00:54:46.640
好吧，翻译还是不够好。00:54:46.640 --> 00:54:47.780
我们只是一直采样过多。00:54:47.780 --> 00:54:52.760
所以，每次我们处理这些小型轮询批处理集时，00:54:52.760 --> 00:54:54.740
我们总是包括机器翻译。00:54:54.740 --> 00:54:59.270
这基本上让我们缩小了差距，00:54:59.270 --> 00:55:01.025
就一个点。00:55:01.025 --> 00:55:03.590
所以现在我们开始00:55:03.590 --> 00:55:06.650
几个月前，在586。00:55:06.650 --> 00:55:08.960
现在是单身，00:55:08.960 --> 00:55:11.330
oracle有10种不同的模型，00:55:11.330 --> 00:55:12.560
如果你要把它们加起来，00:55:12.560 --> 00:55:16.100
得到618，然后，你知道，00:55:16.100 --> 00:55:19.985
更好的上下文向量，调整和增加更多的翻译，00:55:19.985 --> 00:55:23.210
翻译仍然没有我们希望的那么好，00:55:23.210 --> 00:55:26.525
但现在，其他一些任务也从中受益。00:55:26.525 --> 00:55:30.140
现在我们离十倍核只有一步之遥了00:55:30.140 --> 00:55:33.740
有一个单一的模型，可以做10种不同的。00:55:33.740 --> 00:55:36.395
基本上，00:55:36.395 --> 00:55:38.525
你可以做更多的实验，00:55:38.525 --> 00:55:41.930
在某种程度上，你可以在AWS上花费数百万美元，00:55:41.930 --> 00:55:47.180
因为大多数时候我们保持这些不同模型的超参数不变。00:55:47.180 --> 00:55:49.385
就像这些，你也可以说，00:55:49.385 --> 00:55:52.010
也许这个多任务模型需要50多个图层，00:55:52.010 --> 00:55:53.720
或者是19层，00:55:53.720 --> 00:55:56.225
或者是5层或者是1000层，00:55:56.225 --> 00:55:57.860
你知道，在他们隐藏的维度上更宽。00:55:57.860 --> 00:56:01.310
你可以做更多的实验。00:56:01.310 --> 00:56:03.830
也许希望最终社区能联合起来，00:56:03.830 --> 00:56:06.170
然后我们可以向那个方向移动。00:56:06.170 --> 00:56:08.480
但是我们想，好吧，我们已经很接近了，00:56:08.480 --> 00:56:13.850
所以我们继续讲一些其他的东西，也许明年我会告诉你们。00:56:13.850 --> 00:56:16.715
[笑声]但基本上，嗯，00:56:16.715 --> 00:56:18.980
让我们分析一下这个项目中发生了什么。00:56:18.980 --> 00:56:22.235
我想我也会鼓励大家去做这件事。00:56:22.235 --> 00:56:25.460
像你一样，你可以追上一段时间的数字，00:56:25.460 --> 00:56:28.385
你应该对自己的评估持怀疑态度。00:56:28.385 --> 00:56:29.780
在某些情况下，00:56:29.780 --> 00:56:33.230
你已经看到-我们已经看到在NLP社区的人00:56:33.230 --> 00:56:36.935
基本上就是优化BLEU的翻译分数很多年。00:56:36.935 --> 00:56:38.690
然后有人拿着一张纸出来说，00:56:38.690 --> 00:56:44.510
结果是BLEU指标和人类对翻译质量的评估，00:56:44.510 --> 00:56:46.175
实际上并没有那么相关。00:56:46.175 --> 00:56:48.320
你会说，啊，那太糟了，00:56:48.320 --> 00:56:53.000
我们花了数年的时间来调整这个标准并发表了一些论文。00:56:53.000 --> 00:56:57.290
所以在某种程度上，所有这些指标都有缺陷，00:56:57.290 --> 00:57:00.140
根分数的总结是超，00:57:00.140 --> 00:57:03.380
主观的任务。00:57:03.380 --> 00:57:05.465
举个例子，00:57:05.465 --> 00:57:07.730
当你分析错误时，00:57:07.730 --> 00:57:10.595
您经常会发现字向量也有问题。00:57:10.595 --> 00:57:12.920
举个例子，杰森，约翰的矢量，00:57:12.920 --> 00:57:15.290
杰里米都差不多，对吧?00:57:15.290 --> 00:57:16.940
它们都有相似之处00:57:16.940 --> 00:57:20.045
分布、类似上下文、窗口等等。00:57:20.045 --> 00:57:22.610
所以名字的单词向量非常相似。00:57:22.610 --> 00:57:25.835
在总结错误时，你会发现，00:57:25.835 --> 00:57:29.300
你知道，这篇新闻报道说杰里米被绑架了。00:57:29.300 --> 00:57:31.160
但是总结说杰森被绑架了。00:57:31.160 --> 00:57:33.650
你喜欢，嗯，你知道，在评估度规中00:57:33.650 --> 00:57:36.320
只有一个词是错的，其他的都是正确的，00:57:36.320 --> 00:57:38.000
但这是一个非常重要的词。00:57:38.000 --> 00:57:40.970
所以，单词向量有一些问题00:57:40.970 --> 00:57:44.075
总结起来很基本，我不认为，00:57:44.075 --> 00:57:46.835
呃，现在谁的抢断都很好。00:57:46.835 --> 00:57:48.875
所有这些指标都有问题。00:57:48.875 --> 00:57:51.620
我认为把10结合起来00:57:51.620 --> 00:57:54.440
让问题更少，更有意义，00:57:54.440 --> 00:57:56.630
而不是分开看。00:57:56.630 --> 00:58:00.725
因为现在你不能用00:58:00.725 --> 00:58:04.970
一个特殊的评估指标，让你的分数更高一点。00:58:04.970 --> 00:58:09.740
因为，如果你只是在脑海中调整那个特定的东西，00:58:09.740 --> 00:58:13.370
它会伤害到其他的一些任务，00:58:13.370 --> 00:58:15.950
NLP模型就简单多了。00:58:15.950 --> 00:58:18.605
好吧。现在，让我们做一些分析，00:58:18.605 --> 00:58:20.645
这个模型的，呃，00:58:20.645 --> 00:58:24.140
看，这就是其中一个问题的答案。00:58:24.140 --> 00:58:28.295
嗯，这个模型能够为正确的任务生成正确的单词吗?00:58:28.295 --> 00:58:31.775
在这里，我们主要研究了频率的分布，00:58:31.775 --> 00:58:37.100
这个模型产生了这些不同的单词――有了这三种不同的机制，00:58:37.100 --> 00:58:40.370
Softmax词汇表、上下文指针或问题指针。00:58:40.370 --> 00:58:42.515
正如你所看到的00:58:42.515 --> 00:58:45.500
在大多数情况下，它确切地知道如何生成。00:58:45.500 --> 00:58:47.915
所以，呃，为了，呃，00:58:47.915 --> 00:58:51.110
问题、回答和语义角色标记，00:58:51.110 --> 00:58:55.355
还有SQuAD和Wiki SQL，00:58:55.355 --> 00:58:59.150
总结，它基本上使用上下文指针。00:58:59.150 --> 00:59:01.565
它指向context文档。00:59:01.565 --> 00:59:02.795
我们知道对于小队来说，00:59:02.795 --> 00:59:05.990
这基本上就是数据集如何生成的[噪声]。00:59:05.990 --> 00:59:08.600
这是唯一有意义的。00:59:08.600 --> 00:59:11.930
在某些情况下，像总结，00:59:11.930 --> 00:59:14.240
它有时会创造新词，00:59:14.240 --> 00:59:17.330
这不在指向的上下文文档中。00:59:17.330 --> 00:59:19.910
对于零距离关系提取，00:59:19.910 --> 00:59:21.455
有时也用，呃，00:59:21.455 --> 00:59:24.050
这个外部词汇表，在某些情况下还有上下文指针。00:59:24.050 --> 00:59:26.210
所以在很大程度上，00:59:26.210 --> 00:59:31.970
这个模型不会混淆如何执行给定的任务，00:59:31.970 --> 00:59:35.180
这个问题是形式主义而不是，00:59:35.180 --> 00:59:37.370
这是一项任务，00:59:37.370 --> 00:59:39.840
做这个特殊的测试。00:59:41.200 --> 00:59:44.030
你可能会说，00:59:44.030 --> 00:59:45.830
好吧，我不是很在意，00:59:45.830 --> 00:59:48.500
一个模型与另一个模型的性能略有相同00:59:48.500 --> 00:59:51.590
10个独立的模型，即使你想要正确地部署它，00:59:51.590 --> 00:59:53.255
比如，使用更少的内存，00:59:53.255 --> 00:59:54.965
假设它们大小相同，00:59:54.965 --> 00:59:57.080
而你知道，只有十分之一的大小。00:59:57.080 --> 01:00:00.710
但更让我兴奋的是接下来的几个结果。01:00:00.710 --> 01:00:02.750
也就是说，这种转移学习，01:00:02.750 --> 01:00:04.550
域自适应，零距离，01:00:04.550 --> 01:00:06.020
这些能力。01:00:06.020 --> 01:00:11.630
这里，我们选择了两个数据集，这两个数据集不在最初的10个数据集中。01:00:11.630 --> 01:00:17.795
我们基本上训练了一个预先训练的模型和一个随机模型。01:00:17.795 --> 01:00:20.510
在这里，01:00:20.510 --> 01:00:21.859
它们是相同的结构，01:00:21.859 --> 01:00:25.295
预训练是指整个模型都经过了预训练。01:00:25.295 --> 01:00:26.945
所有的，你知道，01:00:26.945 --> 01:00:31.325
编码器包括Softmax中的解码器等等，01:00:31.325 --> 01:00:36.140
另外两个任务是另一个IWSLT语言对，01:00:36.140 --> 01:00:37.685
从英语翻译成捷克语01:00:37.685 --> 01:00:40.880
以及大家都很熟悉的命名实体识别任务。01:00:40.880 --> 01:00:43.460
所以我们发现，01:00:43.460 --> 01:00:45.935
它收敛得更快，01:00:45.935 --> 01:00:47.810
一开始，然后01:00:47.810 --> 01:00:51.200
虽然差距很大，但并不是很大。01:00:51.200 --> 01:00:55.595
所以对这些完全独立的任务的预培训是有帮助的。01:00:55.595 --> 01:00:58.745
我想那是01:00:58.745 --> 01:01:00.365
这很令人兴奋，01:01:00.365 --> 01:01:02.420
特别是更快的收敛，比如，01:01:02.420 --> 01:01:04.165
学得更快，01:01:04.165 --> 01:01:06.310
不管你有什么新任务，01:01:06.310 --> 01:01:09.010
这也意味着在某些情况下你可以逃脱惩罚01:01:09.010 --> 01:01:11.950
减少关于这些新任务的培训数据。01:01:11.950 --> 01:01:15.970
领域适应是转移学习的一种更简单的形式，01:01:15.970 --> 01:01:19.280
你基本上有一个不同的，01:01:19.280 --> 01:01:21.410
呃，有点，呃，01:01:21.410 --> 01:01:23.060
你知道，你的话语的传播。01:01:23.060 --> 01:01:26.750
我们说过我们有斯坦福情感树银行做情感分析。01:01:26.750 --> 01:01:29.780
然后我们用不同的方法来分析，01:01:29.780 --> 01:01:31.610
情感数据集01:01:31.610 --> 01:01:34.505
即亚马逊产品评论和Yelp餐厅评论，01:01:34.505 --> 01:01:36.605
没有经过任何训练，01:01:36.605 --> 01:01:39.965
该模型对这两个数据集的准确率都达到了80%。01:01:39.965 --> 01:01:42.319
我认为对于从业者来说，01:01:42.319 --> 01:01:45.140
这非常令人兴奋，因为你基本上不需要训练任何东西，01:01:45.140 --> 01:01:46.610
这是一种开箱即用的方法，01:01:46.610 --> 01:01:48.830
从GitHub下载并运行它。01:01:48.830 --> 01:01:51.620
SNLI，有点不同。01:01:51.620 --> 01:01:53.330
但效果不太好。01:01:53.330 --> 01:01:55.280
这是另一个自然语言推理数据集，01:01:55.280 --> 01:01:59.135
但是有非常不同的分布，非常不同的，01:01:59.135 --> 01:02:01.040
一些域名，呃，那个，01:02:01.040 --> 01:02:03.290
这些隐含的问题会被问到。01:02:03.290 --> 01:02:06.980
在这里，开箱即用达到了62。01:02:06.980 --> 01:02:10.200
但是，一旦你把它调好了01:02:10.200 --> 01:02:14.230
与这些实验相似的是，我们继续对这些数据集进行训练，01:02:14.230 --> 01:02:17.680
很快收敛到8701:02:17.680 --> 01:02:21.625
与随机或初始化的McCann模型相比，仍然有2%的增益。是的。01:02:21.625 --> 01:02:29.075
在那个实验中，你有没有评估过你可以少拿多少数据?01:02:29.075 --> 01:02:32.900
我们是否评估过我们可以少拿多少数据?我们没有。01:02:32.900 --> 01:02:35.510
在某种程度上，无论你什么时候做这个实验，01:02:35.510 --> 01:02:38.000
你基本上会说，你还是做得不够好。01:02:38.000 --> 01:02:41.555
就像，所有的这些模型都能更好地处理更多的训练数据。01:02:41.555 --> 01:02:43.640
所以你可能会说，01:02:43.640 --> 01:02:46.220
就像，剪切模糊的结果，对吧?01:02:46.220 --> 01:02:48.140
你可能会说，用1 / 1001:02:48.140 --> 01:02:50.885
到50，而另一个模型可能只到40，01:02:50.885 --> 01:02:52.160
做类似的事情。01:02:52.160 --> 01:02:54.830
我们没有，我没有这些数据。01:02:54.830 --> 01:02:57.380
这实际上也是一种整洁，整洁，呃，01:02:57.380 --> 01:02:59.750
分析。是的。01:02:59.750 --> 01:03:06.835
所以如果你想训练一个新的任务(听不清)01:03:06.835 --> 01:03:07.935
是的。01:03:07.935 --> 01:03:10.165
(听不清)。01:03:10.165 --> 01:03:13.105
那么，我们有训练新任务的代码吗?是的,我们做的。01:03:13.105 --> 01:03:14.695
你可以编辑一下01:03:14.695 --> 01:03:16.795
使用上下文将其转换为这种格式。01:03:16.795 --> 01:03:19.465
这里有个问题，很简单，比如CSV格式，01:03:19.465 --> 01:03:24.160
然后你把它加进去，你们都可以自己训练训练前的模型。01:03:24.160 --> 01:03:28.690
您可以下载一个预先训练好的模型并添加它。我查一下。01:03:28.690 --> 01:03:34.795
您知道这与使用其他类型的预训练表示(比如BERT)相比如何吗?01:03:34.795 --> 01:03:37.330
这是个很好的问题。01:03:37.330 --> 01:03:40.120
那么这和其他训练前的表现形式，比如伯特，有什么不同呢?01:03:40.120 --> 01:03:41.935
所以，在某种程度上，01:03:41.935 --> 01:03:44.200
人们说伯特是那种无所不能的模型，01:03:44.200 --> 01:03:46.690
但是当你真正读到这篇文章的时候，你会意识到，01:03:46.690 --> 01:03:49.930
它是这些不同任务的独立模型，对吧?01:03:49.930 --> 01:03:52.375
如果你想要一个分类任务，01:03:52.375 --> 01:03:54.070
一开始有个小标记，01:03:54.070 --> 01:03:55.330
你有一个不同的顶层。01:03:55.330 --> 01:03:57.400
如果你想做一个序列标记任务，01:03:57.400 --> 01:03:58.450
你有一个不同的顶层。01:03:58.450 --> 01:04:00.400
如果你想做一个序列提取任务，01:04:00.400 --> 01:04:01.765
你有一个不同的顶层。01:04:01.765 --> 01:04:06.220
因此，伯特并不是所有这些不同任务的单一模型。01:04:06.220 --> 01:04:08.410
啊，然后，对所有的结果，01:04:08.410 --> 01:04:11.800
每个数据集都有很多额外的调优，01:04:11.800 --> 01:04:13.765
还有任务，呃，你知道的，01:04:13.765 --> 01:04:16.030
不同的学习速度，01:04:16.030 --> 01:04:19.120
不同的大小，不同的伯特，等等。01:04:19.120 --> 01:04:21.670
所以，我们也超级兴奋，我们想，也许这就是它，01:04:21.670 --> 01:04:23.590
我们把所有的东西都放在伯特身上01:04:23.590 --> 01:04:25.180
然后我们研究了所有的细节，01:04:25.180 --> 01:04:26.920
开始的时候有很多令人兴奋的事情。01:04:26.920 --> 01:04:29.020
我们对细节挖掘得越多，01:04:29.020 --> 01:04:31.795
我们变得越不兴奋，因为这就像是答案，01:04:31.795 --> 01:04:33.580
因为它不是一个单一的模型。01:04:33.580 --> 01:04:36.880
从某种意义上说，在训练前最好。01:04:36.880 --> 01:04:38.290
所以不是CoVe，01:04:38.290 --> 01:04:41.140
你可以在一开始就有伯特，01:04:41.140 --> 01:04:43.450
我的预感是一切都会好一点，01:04:43.450 --> 01:04:46.075
但你还是需要01:04:46.075 --> 01:04:52.120
还有很多其他类型的建模架构。01:04:52.120 --> 01:04:56.050
然后可悲的是得到了最先进的结果01:04:56.050 --> 01:05:00.355
最后的顶层有很多特定于特定任务的优化。01:05:00.355 --> 01:05:04.525
所以，如果你试图统一特定于任务的调优，01:05:04.525 --> 01:05:06.700
你失去了伯特的很多好表现。01:05:06.700 --> 01:05:10.495
不幸的是，01:05:10.495 --> 01:05:12.175
“哦，就用伯特吧，01:05:12.175 --> 01:05:15.220
你会得到最先进的数字和所有的东西。”01:05:15.220 --> 01:05:18.565
我可以多谈谈，但是01:05:18.565 --> 01:05:21.295
我认为这样想还是有意义的，01:05:21.295 --> 01:05:23.065
伯特的一些观点，01:05:23.065 --> 01:05:26.355
基本上，add是语言建模的任务之一。01:05:26.355 --> 01:05:30.990
这很可能是对其他任务帮助最大的任务，01:05:30.990 --> 01:05:33.480
我们应该包括这个01:05:33.480 --> 01:05:37.535
如果现在有一个更快的型号就好了。01:05:37.535 --> 01:05:40.270
很难做语言建模非常非常大，01:05:40.270 --> 01:05:41.740
它从，01:05:41.740 --> 01:05:43.840
你知道，数以亿计的单词。01:05:43.840 --> 01:05:45.670
很难训练麦肯模型，01:05:45.670 --> 01:05:48.940
本文提出了当前问题答疑模式中问题的共同注意机制01:05:48.940 --> 01:05:52.029
有着越来越大的背景。01:05:52.029 --> 01:05:54.970
所以你也要像伯特一样把它分开，01:05:54.970 --> 01:05:59.020
我想最多也就500字左右，01:05:59.020 --> 01:06:02.050
如果你想要做总结，你基本上就得剪切01:06:02.050 --> 01:06:06.490
原来的文件只有500字，然后试着总结一下。01:06:06.490 --> 01:06:09.820
所以，有很多像魔鬼一样的细节他们不需要弄清楚，01:06:09.820 --> 01:06:12.520
因为他们说，“我们就像单词向量一样，01:06:12.520 --> 01:06:16.420
我们可以接受他们，然后我们做很多其他的事情，这是特定于任务的，01:06:16.420 --> 01:06:18.775
用这些向量，01:06:18.775 --> 01:06:20.350
或者伯特建筑。”01:06:20.350 --> 01:06:22.720
我仍然――我不想――这个伯特显然很神奇，01:06:22.720 --> 01:06:25.120
我们正在研究如何利用其中的想法。01:06:25.120 --> 01:06:27.400
但不幸的是，这并不是一种灵丹妙药01:06:27.400 --> 01:06:33.355
解决多任务学习。嗯哼?01:06:33.355 --> 01:06:35.515
培训前的流程需要考虑01:06:35.515 --> 01:06:40.990
优先抽样的基础上有多少更少的组，有多少损失?01:06:40.990 --> 01:06:42.670
对不起，我们再说一遍了吗?01:06:42.670 --> 01:06:46.390
你会考虑优先采样(听不清)吗?01:06:46.390 --> 01:06:48.370
那么，我们是否考虑过优先抽样呢?01:06:48.370 --> 01:06:51.760
所以在某种程度上，通过这种预先训练过的策略，01:06:51.760 --> 01:06:56.500
这就是我们通过专注于这些非常困难的任务所做的。01:06:56.500 --> 01:07:02.140
很多时候，就像最后的差距是通过真正的等待而改善的，01:07:02.140 --> 01:07:04.555
比如最后的四个任务，01:07:04.555 --> 01:07:05.995
呃，bef- unti-你知道，呃，01:07:05.995 --> 01:07:08.560
直到你经历过01:07:08.560 --> 01:07:10.750
对所有这些进行过采样，01:07:10.750 --> 01:07:11.800
呃，非常难的任务。01:07:11.800 --> 01:07:16.375
在最后10分钟，基本上，01:07:16.375 --> 01:07:18.400
最令人兴奋的是01:07:18.400 --> 01:07:22.540
最后，我认为你们可以在这个方向上做更多的工作。01:07:22.540 --> 01:07:24.460
我提到了唯一的问题指针01:07:24.460 --> 01:07:26.380
开始的时候没有什么短时间的学习，01:07:26.380 --> 01:07:29.965
我们基本上只是试着玩一下，01:07:29.965 --> 01:07:32.185
发现在某些情况下，01:07:32.185 --> 01:07:35.080
它实际上神奇地起作用了。01:07:35.080 --> 01:07:37.060
在这里，我们试着，01:07:37.060 --> 01:07:38.725
约翰举办了一个聚会，01:07:38.725 --> 01:07:40.855
但是没有人来，只有他一个人。01:07:40.855 --> 01:07:43.960
然后我们问，“这个故事是悲伤的，还是快乐的?”01:07:43.960 --> 01:07:46.120
虽然模型可以，01:07:46.120 --> 01:07:47.920
随机生成一些德语单词，01:07:47.920 --> 01:07:49.570
或者一些随机的SQL单词，01:07:49.570 --> 01:07:51.235
或者只是随便说说，01:07:51.235 --> 01:07:54.490
它实际上指向，在所有的单词中，01:07:54.490 --> 01:07:56.440
你可以在上下文或者问题中指出01:07:56.440 --> 01:07:58.825
指向“Sad”，这很酷。01:07:58.825 --> 01:08:01.750
这只是一个小样本，01:08:01.750 --> 01:08:03.580
你知道，你可以做得更多，01:08:03.580 --> 01:08:08.905
你可以尝试提出一个很大的零概率分类数据集，01:08:08.905 --> 01:08:10.300
这也有点难。01:08:10.300 --> 01:08:12.550
你必须非常有创意，而不是说，“哦，01:08:12.550 --> 01:08:13.750
它只需要所有这些评论，01:08:13.750 --> 01:08:15.700
把它们标记为，正负号。01:08:15.700 --> 01:08:19.810
但是，我认为我们需要在这方面做更多的工作。01:08:19.810 --> 01:08:23.230
希望有人能创建一个零概率的任务数据集，01:08:23.230 --> 01:08:25.570
这不仅仅是零概率，01:08:25.570 --> 01:08:29.050
这是一种新的分布或者完全不同的输出。01:08:29.050 --> 01:08:31.810
但我们试了几次01:08:31.810 --> 01:08:32.950
它并不总是有效的。01:08:32.950 --> 01:08:34.510
你可以反对它，01:08:34.510 --> 01:08:38.470
你可以让它看起来最像，01:08:38.470 --> 01:08:40.510
这种情绪是积极的还是消极的?01:08:40.510 --> 01:08:42.805
这个森，这个句子是肯定的还是否定的?01:08:42.805 --> 01:08:45.955
这就是我们情感分析的形式主义。01:08:45.955 --> 01:08:47.665
所以你可以，01:08:47.665 --> 01:08:50.380
如果你让问题变得越来越不同，01:08:50.380 --> 01:08:52.000
最终，它会被绊倒。01:08:52.000 --> 01:08:55.015
很明显这对我们有好处01:08:55.015 --> 01:08:57.010
从向量这个词，01:08:57.010 --> 01:08:59.020
悲伤接近消极，01:08:59.020 --> 01:09:01.360
然后通过这些来理解，01:09:01.360 --> 01:09:03.715
相关性，还有，还有，01:09:03.715 --> 01:09:08.920
在这种背景下还有其他一些悲伤的词语，01:09:08.920 --> 01:09:10.120
或者――或者别的什么。01:09:10.120 --> 01:09:12.370
所以它可以指向这个。01:09:12.370 --> 01:09:14.740
但你也可以是对抗性的，这并不总是奏效。01:09:14.740 --> 01:09:16.780
但即使是这样01:09:16.780 --> 01:09:20.335
这是一种基于单词向量的零概率分类，01:09:20.335 --> 01:09:22.150
对于新的问题，01:09:22.150 --> 01:09:24.070
就我个人而言，这让我很兴奋。01:09:24.070 --> 01:09:26.170
我们还尝试了其他一些方法，01:09:26.170 --> 01:09:28.615
布莱恩做了个演讲，没人鼓掌。01:09:28.615 --> 01:09:29.650
布莱恩是高兴还是悲伤?01:09:29.650 --> 01:09:30.670
它也做对了。01:09:30.670 --> 01:09:33.295
有一些，01:09:33.295 --> 01:09:36.190
这些例子，至少是快乐或悲伤的事情。01:09:36.190 --> 01:09:39.295
然后，还有一些其他的形容词问题，01:09:39.295 --> 01:09:40.780
我们试过了，但是01:09:40.780 --> 01:09:43.690
我――我最兴奋的是最终01:09:43.690 --> 01:09:47.755
试图有一个零概率的分类任务，01:09:47.755 --> 01:09:49.675
这也结合了不同的任务。01:09:49.675 --> 01:09:52.540
不幸的是，没有相关的数据集，01:09:52.540 --> 01:09:54.460
所以我们没有训练它，所以它不会发生在模型上。01:09:54.460 --> 01:09:57.729
但是理论上，如果你问总和是多少，你可以总结一下，01:09:57.729 --> 01:09:59.995
你可以把英语翻译成德语，01:09:59.995 --> 01:10:02.515
为什么你不能要求模型提供一个德语摘要呢?01:10:02.515 --> 01:10:04.240
如果这最终奏效，01:10:04.240 --> 01:10:05.650
那就更神奇了，01:10:05.650 --> 01:10:07.390
但它现在不管用了01:10:07.390 --> 01:10:09.190
因为我们从来没有要求过01:10:09.190 --> 01:10:12.310
组合任务――这些组合任务问题。01:10:12.310 --> 01:10:15.490
但我认为这是另一个有趣的研究方向。01:10:15.490 --> 01:10:16.675
呃,好吧。01:10:16.675 --> 01:10:19.150
所以，我希望我能向你们展示这种01:10:19.150 --> 01:10:24.130
decaNLP框架是通用NLP的一个有趣的新基准。01:10:24.130 --> 01:10:27.160
我认为这是一个很好的框架01:10:27.160 --> 01:10:30.310
感谢他在这个领域里解决了一堆非常难的问题。01:10:30.310 --> 01:10:32.259
更一般的语言理解，01:10:32.259 --> 01:10:33.550
当然还有回答问题，01:10:33.550 --> 01:10:37.180
多任务学习，领域适应，01:10:37.180 --> 01:10:39.790
我们用情感分析了一下，01:10:39.790 --> 01:10:41.815
SNLI和multinli，01:10:41.815 --> 01:10:44.710
转移学习，然后分享体重。01:10:44.710 --> 01:10:46.780
很明显，每个人都喜欢分担体重，01:10:46.780 --> 01:10:48.850
你想要分享尽可能多的重量。01:10:48.850 --> 01:10:52.375
word vector从ELMo开始01:10:52.375 --> 01:10:55.300
CoVe和BERT分享的越来越多，01:10:55.300 --> 01:10:56.545
越来越深的层次。01:10:56.545 --> 01:10:59.560
如果我们能把最后一点也统一起来就太好了01:10:59.560 --> 01:11:02.575
然后分享整个网络，01:11:02.575 --> 01:11:05.200
最终有希望达到零学习。01:11:05.200 --> 01:11:07.330
现在，有一堆相关的工作。01:11:07.330 --> 01:11:09.220
最初的论文有100多篇，01:11:09.220 --> 01:11:11.725
里面的引文，01:11:11.725 --> 01:11:13.525
你知道，给别人的文件，01:11:13.525 --> 01:11:16.405
其他的，呃，工作线。01:11:16.405 --> 01:11:18.490
但是，呃，这个实际上是零――至少有一部分是零01:11:18.490 --> 01:11:21.670
对我们影响最大的模型和论文，01:11:21.670 --> 01:11:23.920
呃，在我们的思维和造型中。01:11:23.920 --> 01:11:25.465
其中一个来自，01:11:25.465 --> 01:11:27.550
班上的两位老师。01:11:27.550 --> 01:11:31.165
所以希望我们能01:11:31.165 --> 01:11:35.050
你知道，想想在所有这些建筑工程之后接下来会发生什么。01:11:35.050 --> 01:11:38.125
我想一个可能的答案是，01:11:38.125 --> 01:11:42.400
是针对更广义的NLP模型的单任务多任务学习。01:11:42.400 --> 01:11:53.620
[NOISE] All right. Thank you. [APPLAUSE]

