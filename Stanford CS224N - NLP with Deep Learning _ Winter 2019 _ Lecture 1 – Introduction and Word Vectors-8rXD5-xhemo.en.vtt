WEBVTT
Kind: captions
Language: en

00:00:04.430 --> 00:00:07.410可以。大家好。00:00:07.410 --> 00:00:11.265[笑声]好吧，我们该开始了。00:00:11.265 --> 00:00:14.640嗯，实际上还剩下不少座位。00:00:14.640 --> 00:00:15.960如果你真的想大胆，00:00:15.960 --> 00:00:18.525在我前面前排有几个座位。00:00:18.525 --> 00:00:20.445如果你再大胆一点的话。00:00:20.445 --> 00:00:23.940嗯，但他们也在一些排的中间位置。00:00:23.940 --> 00:00:28.080因此，如果人们想真正具有公民意识，有些人可以00:00:28.080 --> 00:00:32.280向边缘挤压，使其更容易接近，嗯，00:00:32.280 --> 00:00:35.685教室里还有一些座位。00:00:35.685 --> 00:00:39.435可以。嗯，那么，嗯，00:00:39.435 --> 00:00:42.890在这里见到这么多人真是太令人兴奋了。00:00:42.890 --> 00:00:47.390所以我非常欢迎来到CS224N，偶尔也会00:00:47.390 --> 00:00:52.625被称为凌284，这是自然语言处理与深入学习。00:00:52.625 --> 00:00:55.420嗯，作为一种个人轶事，00:00:55.420 --> 00:00:59.720这几天有那么多人来上这门课，我还是有点心烦意乱。00:00:59.720 --> 00:01:03.980所以，在我教NLP的头十年里，00:01:03.980 --> 00:01:08.180你知道我每年约有45人。00:01:08.180 --> 00:01:11.240所以它的数量级比00:01:11.240 --> 00:01:14.360现在是了，但我想这说明了很多00:01:14.360 --> 00:01:17.450关于革命的影响00:01:17.450 --> 00:01:20.870一般的人工智能和机器学习，00:01:20.870 --> 00:01:25.600深入学习，NLP开始在现代社会。00:01:25.600 --> 00:01:28.860可以。所以这是我们今天的计划。00:01:28.860 --> 00:01:32.750所以，嗯，嗯，我们今天真的要直接谈正事了。00:01:32.750 --> 00:01:37.970因此，他们将简要介绍一些物流课程，00:01:37.970 --> 00:01:42.140非常简短的讨论和谈论人类语言和00:01:42.140 --> 00:01:46.370字面意思，然后我们想直接谈论，嗯，00:01:46.370 --> 00:01:50.540我们要做的第一件事就是找出单词向量并寻找00:01:50.540 --> 00:01:55.010在word2vec算法中，这将填充类的其余部分。00:01:55.010 --> 00:01:56.840还有两个座位在里面00:01:56.840 --> 00:01:59.480前排是想坐在我前面的人，00:01:59.480 --> 00:02:02.760只是让你知道[笑声]。00:02:02.760 --> 00:02:06.365可以。可以。下面是课程物流的简要介绍。00:02:06.365 --> 00:02:08.345我叫克里斯托弗·曼宁，00:02:08.345 --> 00:02:15.290勇敢成为头目的人是阿比盖尔，看到就在那里。00:02:15.290 --> 00:02:18.920然后我们有很多很棒的助教。00:02:18.920 --> 00:02:22.700对那些很好的助教来说，只是站了一会儿。00:02:22.700 --> 00:02:26.810所以，嗯，[笑声]我们对很棒的助教有些感觉。00:02:26.810 --> 00:02:28.900[笑声]很好。00:02:28.900 --> 00:02:31.320嗯，好吧。00:02:31.320 --> 00:02:33.260所以你知道什么时候演讲是因为你成功了00:02:33.260 --> 00:02:37.100这里也欢迎SCPD的人。00:02:37.100 --> 00:02:41.300这也是一个SCPD类，您可以在视频中观看它。00:02:41.300 --> 00:02:44.300但我们喜欢斯坦福的学生00:02:44.300 --> 00:02:47.300在教室里展示他们美丽的面孔。00:02:47.300 --> 00:02:52.810可以。所以，嗯，网页上有所有关于教学大纲和其他课程的信息。00:02:52.810 --> 00:02:56.175可以。所以这节课我们希望教什么？00:02:56.175 --> 00:02:59.240所以，我们想教的一件事是，呃，你知道，00:02:59.240 --> 00:03:02.495对有效的现代深度学习方法的理解。00:03:02.495 --> 00:03:05.090从回顾一些基本知识开始，然后00:03:05.090 --> 00:03:08.780特别是说各种技术，包括嗯，00:03:08.780 --> 00:03:11.450经常性的网络和广泛的关注00:03:11.450 --> 00:03:14.570用于自然语言处理模型。00:03:14.570 --> 00:03:18.770我们要教的第二件事是了解00:03:18.770 --> 00:03:23.075人类语言和理解和产生它们的一些困难。00:03:23.075 --> 00:03:25.490当然，如果你想了解很多人类语言，00:03:25.490 --> 00:03:29.060有一个完整的语言学系，你可以做很多课程。00:03:29.060 --> 00:03:33.590嗯，但是我想至少给你点欣赏，这样你就知道什么是00:03:33.590 --> 00:03:38.235人类语言的挑战、困难和多样性。00:03:38.235 --> 00:03:41.315这也是一门实践课。00:03:41.315 --> 00:03:44.960就像我们真的想教你怎么做00:03:44.960 --> 00:03:49.670为NLP的一些主要部分构建实用的系统。00:03:49.670 --> 00:03:53.750所以如果你去一家科技公司找工作，他们会说“嘿，00:03:53.750 --> 00:03:55.789你能给我们建立一个命名的实体识别器吗？”00:03:55.789 --> 00:03:58.130你可以说“当然，我能做到。”00:03:58.130 --> 00:04:00.530所以对于一些问题，00:04:00.530 --> 00:04:02.090显然我们不能做任何事，00:04:02.090 --> 00:04:03.230我们要做单词的意思，00:04:03.230 --> 00:04:07.579依赖性分析、机器翻译，您可以选择回答问题，00:04:07.579 --> 00:04:10.340实际上，我正在为它们构建系统。00:04:10.340 --> 00:04:15.080如果你过去几年一直在和上过课的朋友聊天，00:04:15.080 --> 00:04:18.860嗯，这是今年的区别，只是为了把事情弄清楚。00:04:18.860 --> 00:04:21.830嗯，我们已经更新了课程的一些内容。00:04:21.830 --> 00:04:26.285所以，呃，在我和客座演讲之间有新的内容。00:04:26.285 --> 00:04:28.470好吧，那看起来很糟糕。00:04:29.030 --> 00:04:32.505不知道这会不会继续发生，我们会知道的。00:04:32.505 --> 00:04:38.165有新的内容和各种各样的主题，这些都是发展中的领域。00:04:38.165 --> 00:04:41.300这门课程的一个问题是在00:04:41.300 --> 00:04:44.755现在还只是发展得很快。00:04:44.755 --> 00:04:47.480所以，似乎一年前的内容已经00:04:47.480 --> 00:04:51.290我们正在尝试更新数据。00:04:51.290 --> 00:04:54.140我们今年要做的一个重大改变是00:04:54.140 --> 00:04:56.930有五个一周的任务而不是00:04:56.930 --> 00:04:59.450开始时三个两周的作业00:04:59.450 --> 00:05:02.795课程，我会在一分钟内多说一点。00:05:02.795 --> 00:05:06.215今年我们要用Pythorch而不是TensorFlow，00:05:06.215 --> 00:05:08.860我们稍后也会讨论这个问题。00:05:08.860 --> 00:05:13.880嗯，星期二或星期四的作业要在课前交。00:05:13.880 --> 00:05:16.585所以你不会分心，可以来上课。00:05:16.585 --> 00:05:20.355所以开始吧，嗯，是的。00:05:20.355 --> 00:05:22.680所以我们想让00:05:22.680 --> 00:05:26.510但另一方面，快速上升。00:05:26.510 --> 00:05:29.555所以我们有了第一个任务，有点简单，呃，00:05:29.555 --> 00:05:34.040但现在有空，下周二就到。00:05:34.040 --> 00:05:37.460最后一件事是我们今年没有期中考试。00:05:37.460 --> 00:05:39.395嗯，好吧。00:05:39.395 --> 00:05:40.790所以这就是我们要做的。00:05:40.790 --> 00:05:44.300所以我刚才提到的任务有五个。00:05:44.300 --> 00:05:46.340嗯，前一个是百分之六，00:05:46.340 --> 00:05:49.090其他的各占12%，00:05:49.090 --> 00:05:52.185嗯，我已经说过了。00:05:52.185 --> 00:05:54.230我们要用评分器评分。00:05:54.230 --> 00:05:56.780如果你能用的话，这对助教会有很大帮助00:05:56.780 --> 00:06:01.010您的Sunet ID作为您的GradeScope帐户ID。00:06:01.010 --> 00:06:04.205嗯，那么在课程的第二部分，00:06:04.205 --> 00:06:08.795人们做一个最终的项目，最终的项目有两种选择。00:06:08.795 --> 00:06:12.080你要么做我们默认的最终项目，00:06:12.080 --> 00:06:14.030对很多人来说这是一个很好的选择，00:06:14.030 --> 00:06:15.890或者你可以做一个定制的最终项目，我会00:06:15.890 --> 00:06:19.010在开始的时候多谈这个。00:06:19.010 --> 00:06:21.200这行不通。00:06:21.200 --> 00:06:25.130嗯，最后我们有了00:06:25.130 --> 00:06:30.425最后一次海报展示会，预计您会出席，00:06:30.425 --> 00:06:34.580我们将在晚上的那个星期三举行。00:06:34.580 --> 00:06:37.460可能还不到五个小时，但会在那个窗口里，00:06:37.460 --> 00:06:39.485我们会尽快解决细节问题。00:06:39.485 --> 00:06:41.5103%的参与率，00:06:41.510 --> 00:06:43.390有关详细信息，请访问网站。00:06:43.390 --> 00:06:45.885晚了六天，嗯，00:06:45.885 --> 00:06:50.330协作，就像在计算机科学课上一样，00:06:50.330 --> 00:06:55.340我们希望你自己做你自己的工作，不要从别人的GitHubs和00:06:55.340 --> 00:06:57.650所以我们确实强调你应该00:06:57.650 --> 00:07:01.150阅读并注意协作策略。00:07:01.150 --> 00:07:04.700可以。下面是问题集的高级计划。00:07:04.700 --> 00:07:07.790现在有家庭作业，00:07:07.790 --> 00:07:10.130是个很容易上坡的地方。00:07:10.130 --> 00:07:11.720在ipython笔记本上，00:07:11.720 --> 00:07:13.565帮助大家跟上进度。00:07:13.565 --> 00:07:17.750家庭作业二是纯Python加上numpy但那00:07:17.750 --> 00:07:22.190会开始教你更多关于潜在的东西，00:07:22.190 --> 00:07:24.260我们如何进行深度学习？00:07:24.260 --> 00:07:29.430如果你不是很好，有点生锈，或者从没见过，嗯，00:07:29.430 --> 00:07:31.155巨蟒还是麻木，嗯，00:07:31.155 --> 00:07:34.730星期五我们会有一个额外的部分。00:07:34.730 --> 00:07:38.210所以星期五从1:30到2:50嗯，00:07:38.210 --> 00:07:42.710在Skilling礼堂，我们将有一个关于python的评论部分。00:07:42.710 --> 00:07:44.610那是我们目前唯一的计划部门，00:07:44.610 --> 00:07:46.595我们不会有一个正常的部门。00:07:46.595 --> 00:07:49.550嗯，所以鼓励你去那，那也将是00:07:49.550 --> 00:07:53.515为SCPD录制，也可用于视频。00:07:53.515 --> 00:07:56.790嗯，然后是家庭作业三嗯，00:07:56.790 --> 00:08:00.850会让我们开始使用pytorch。00:08:00.850 --> 00:08:04.760然后我们将要使用的家庭作业4和500:08:04.760 --> 00:08:08.720PY-PyTorch在GPU上，我们实际上将使用00:08:08.720 --> 00:08:13.520Microsoft Azure，非常感谢您对我们的支持00:08:13.520 --> 00:08:19.165赞助了我们过去三年的GPU计算。00:08:19.165 --> 00:08:24.995嗯，是的。所以基本上我的意思是所有的现代深度学习都已经开始使用了00:08:24.995 --> 00:08:30.589像Pythort TensorFlow这样的大型深度学习图书馆，00:08:30.589 --> 00:08:32.210链测器或MXnet Um，00:08:32.210 --> 00:08:36.440然后在GPU上进行计算。00:08:36.440 --> 00:08:38.600当然，既然我们在一栋楼里，00:08:38.600 --> 00:08:40.450我们当然应该用，嗯，00:08:40.450 --> 00:08:42.620GPU[笑声]但我的意思是总的来说00:08:42.620 --> 00:08:48.830GPU的可扩展性是现代深度学习的主要动力。00:08:48.830 --> 00:08:50.720可以。最后一个项目。00:08:50.720 --> 00:08:55.460所以对于最终的项目，有两件事你可以做。00:08:55.460 --> 00:09:00.665所以我们有一个默认的最终项目，它本质上是我们的最终项目。00:09:00.665 --> 00:09:06.215所以这是建立一个问答系统，我们通过班数据集来实现。00:09:06.215 --> 00:09:11.450所以，你建立什么以及如何提高你的表现完全取决于你自己。00:09:11.450 --> 00:09:14.480它是开放式的，但它有一个更容易的开始，00:09:14.480 --> 00:09:16.910一个明确的目标，我们可以00:09:16.910 --> 00:09:19.775有一个关于事情进展的排行榜。00:09:19.775 --> 00:09:24.680如果你没有明确的研究目标，那对你来说是个不错的选择00:09:24.680 --> 00:09:29.600或者你可以提出定制的最终项目，假设它是合理的，00:09:29.600 --> 00:09:32.539我们将批准您的自定义最终项目，00:09:32.539 --> 00:09:34.190我们会给你反馈，嗯，00:09:34.190 --> 00:09:36.755形成一个导师，嗯，00:09:36.755 --> 00:09:42.410无论哪种方式，对于最终的项目，我们都允许一个、两个或三个团队。00:09:42.410 --> 00:09:45.200因为家庭作业应该是你自己做的。00:09:45.200 --> 00:09:50.020当然，你可以用一种通用的方式和人们谈论这些问题。00:09:50.020 --> 00:09:53.010可以。所以这就是课程。00:09:53.010 --> 00:09:55.700一切都很好，甚至还没有落后计划。00:09:55.700 --> 00:10:01.730可以。所以下一部分是人类语言和词义。00:10:01.730 --> 00:10:04.745你知道，如果我是嗯，00:10:04.745 --> 00:10:10.265真的会告诉你很多关于人类语言的事情，这需要很多时间，嗯，00:10:10.265 --> 00:10:12.110我真的没有。00:10:12.110 --> 00:10:14.015所以我只想告诉你，00:10:14.015 --> 00:10:16.655关于人类语言的两件轶事。00:10:16.655 --> 00:10:19.970第一个是XKCD卡通。00:10:19.970 --> 00:10:22.520嗯，我的意思是这不是，00:10:22.520 --> 00:10:25.110我不知道为什么会这样。00:10:26.050 --> 00:10:28.250我不知道该怎么办。00:10:28.250 --> 00:10:34.070嗯，我真的很喜欢这个XKCD卡通。00:10:34.070 --> 00:10:37.310这不是你经常在这里看到的经典之作，00:10:37.310 --> 00:10:42.140但我认为它对语言有很大的影响，值得思考。00:10:42.140 --> 00:10:45.650就像我想很多时候都是为了那些来的人00:10:45.650 --> 00:10:49.384对于这个阶级来说，他们主要是像CS人一样的人，00:10:49.384 --> 00:10:51.950以及EE人员和随机其他人。00:10:51.950 --> 00:10:55.250我认识其他一些人，因为他们是语言学家等等。00:10:55.250 --> 00:10:57.050但对很多人来说，00:10:57.050 --> 00:11:01.610你已经花了一辈子的时间研究正式的语言和印象00:11:01.610 --> 00:11:06.185人类的语言是不是有点破坏形式语言？00:11:06.185 --> 00:11:08.570但其实还有很多事情要做，对吧？00:11:08.570 --> 00:11:11.165那语言真是太棒了，嗯，00:11:11.165 --> 00:11:15.110用于00:11:15.110 --> 00:11:19.520各种目的，并能适应各种目的。00:11:19.520 --> 00:11:23.750所以你可以做任何事情，从描述数学和人类语言00:11:23.750 --> 00:11:28.520嗯，跟你最好的朋友搭讪，让他们更好地了解你。00:11:28.520 --> 00:11:31.910所以人类语言确实有一个惊人的东西。不管怎样，我会读的。00:11:31.910 --> 00:11:34.655嗯，所以这是第一个人，00:11:34.655 --> 00:11:36.185黑发人说，00:11:36.185 --> 00:11:38.105“不管怎样，我不在乎。”00:11:38.105 --> 00:11:40.010她的朋友说，00:11:40.010 --> 00:11:42.440“我想你是说你不在乎。”00:11:42.440 --> 00:11:46.490说你可以少关心意味着你至少关心一些。00:11:46.490 --> 00:11:49.775黑发人说，“我不知道，00:11:49.775 --> 00:11:54.590我们是这些令人难以置信的复杂的大脑，在空虚中挣扎00:11:54.590 --> 00:11:59.630盲目地把话扔到黑暗中，彼此联系是徒劳的。”00:11:59.630 --> 00:12:02.720各式措辞、拼写和语气，00:12:02.720 --> 00:12:07.775时间携带着无数的信号、上下文和子文本等等。00:12:07.775 --> 00:12:11.435每个听众都用自己的方式解释这些信号。00:12:11.435 --> 00:12:13.565语言不是一个正式的系统，00:12:13.565 --> 00:12:16.235语言是光荣的混乱。00:12:16.235 --> 00:12:20.750你永远不知道任何一句话对任何人意味着什么。00:12:20.750 --> 00:12:26.150你所能做的就是更好地猜测你的话是如何影响人们的，所以00:12:26.150 --> 00:12:28.790你可以有机会找到那些00:12:28.790 --> 00:12:31.790他们的感觉就像你想让他们感受到的一样。00:12:31.790 --> 00:12:34.235其他一切都是毫无意义的。00:12:34.235 --> 00:12:37.390我想你是在给我提示你如何解释00:12:37.390 --> 00:12:41.065因为你想让我不那么孤单。00:12:41.065 --> 00:12:43.510如果是，谢谢。00:12:43.510 --> 00:12:45.585这意味着很多。00:12:45.585 --> 00:12:48.440但如果你只是把我的话说出来00:12:48.440 --> 00:12:51.785一些精神检查表，这样你就可以展示你对它的了解，00:12:51.785 --> 00:12:53.180那我就不在乎了。00:12:53.180 --> 00:13:02.825[噪音]嗯，我想嗯，00:13:02.825 --> 00:13:07.790我认为事实上，这有一些很好的信息，关于语言是如何不确定的00:13:07.790 --> 00:13:13.340发展了沟通系统，但不知何故，我们有足够的共识，你知道，00:13:13.340 --> 00:13:15.500我们可以进行很多沟通。00:13:15.500 --> 00:13:16.865但我们正在做一些你知道的事00:13:16.865 --> 00:13:20.540猜测人的意思的概率推理00:13:20.540 --> 00:13:22.070使用语言不仅仅是为了00:13:22.070 --> 00:13:26.195信息功能，但不包括社会功能等。00:13:26.195 --> 00:13:32.310可以。这是我的另一个想法，我复习过语言。00:13:33.490 --> 00:13:40.565所以，从本质上说，如果我们想拥有智能的人工智能，00:13:40.565 --> 00:13:43.940我们需要什么才能达到拥有的目的00:13:43.940 --> 00:13:48.560计算机-有人类知识的计算机，对吗？00:13:48.560 --> 00:13:52.430因为人类的知识给予他们智慧。00:13:52.430 --> 00:13:55.460如果你想想我们00:13:55.460 --> 00:13:59.270在我们人类世界的各个地方传播知识，00:13:59.270 --> 00:14:04.025我们主要是通过人类语言来完成的。00:14:04.025 --> 00:14:06.410你知道，你可以从某种程度上00:14:06.410 --> 00:14:09.260做正确的体育锻炼，00:14:09.260 --> 00:14:11.900我可以拿着这个放下它，我学到了一些东西。00:14:11.900 --> 00:14:13.760所以我必须在那里学到一些知识。00:14:13.760 --> 00:14:17.180但你头脑中的大部分知识以及你为什么坐在那里00:14:17.180 --> 00:14:21.980这间教室来自用人类语言与你交流的人们。00:14:21.980 --> 00:14:24.260嗯，一个著名的，00:14:24.260 --> 00:14:26.990最著名的陡坡学子延乐村，00:14:26.990 --> 00:14:29.165他喜欢说这句话，00:14:29.165 --> 00:14:33.380哦，你知道吗？我想你知道没什么区别00:14:33.380 --> 00:14:37.965在人类的智慧和猩猩之间。00:14:37.965 --> 00:14:40.510我真的认为他在这方面是错的。00:14:40.510 --> 00:14:42.790就像他说的那样，00:14:42.790 --> 00:14:45.835猩猩有一个非常好的视觉系统。00:14:45.835 --> 00:14:48.610猩猩有很好的控制力00:14:48.610 --> 00:14:52.060他们的手臂就像人类一样拿起东西。00:14:52.060 --> 00:14:58.970猩猩可以使用工具，所以猩猩可以制定计划00:14:58.970 --> 00:15:02.270如果你把食物放在他们必须移动的地方00:15:02.270 --> 00:15:05.960带着食物去岛上的木板，他们可以做这样的计划。00:15:05.960 --> 00:15:09.890所以是的，从某种意义上说，他们很聪明，但是你知道，00:15:09.890 --> 00:15:13.385有点猩猩不像人类。00:15:13.385 --> 00:15:16.100为什么他们不喜欢人类？00:15:16.100 --> 00:15:21.605我想向你们建议，这就是人类所取得的成就的原因，00:15:21.605 --> 00:15:25.070我们不是只有一台电脑00:15:25.070 --> 00:15:29.825A你知道在你妈妈的车库里有一台尘土飞扬的旧IBM电脑。00:15:29.825 --> 00:15:33.740我们拥有的是一个人类计算机网络。00:15:33.740 --> 00:15:37.520我们实现人类计算机网络的方法是，00:15:37.520 --> 00:15:41.285我们使用人类语言作为我们的网络语言。00:15:41.285 --> 00:15:44.690嗯，所以，当你想起来的时候，00:15:44.690 --> 00:15:51.815所以在任何一种进化尺度上，语言都是超超超最近的，对吧？00:15:51.815 --> 00:15:57.470嗯，生物对不太了解的人有远见，但你知道，00:15:57.470 --> 00:16:00.980也许是7500万年或者更长，对吧？00:16:00.980 --> 00:16:03.845一段很长的时间。00:16:03.845 --> 00:16:07.295人类有语言多久了？00:16:07.295 --> 00:16:09.860你知道人们也不知道，因为事实证明你知道，00:16:09.860 --> 00:16:11.015当你有化石的时候，00:16:11.015 --> 00:16:13.490你不能一边敲脑袋一边说，00:16:13.490 --> 00:16:15.050你没有语言吗？00:16:15.050 --> 00:16:19.100嗯，但是你知道，大多数人估计那种语言是00:16:19.100 --> 00:16:25.985一项最新的发明，在现在的人类离开非洲之前。00:16:25.985 --> 00:16:28.550所以很多人认为我们只有语言00:16:28.550 --> 00:16:31.460比如说10万年或者类似的。00:16:31.460 --> 00:16:35.450所以这就是你所知道的进化时间尺度上的转瞬即逝。00:16:35.450 --> 00:16:39.740但你知道，这是语言的发展[听不见]00:16:39.740 --> 00:16:43.970那种让人看不见的东西——无敌的噪音，对吧？00:16:43.970 --> 00:16:46.475不是这样，人类，嗯，00:16:46.475 --> 00:16:51.410毒牙发达或奔跑能力发达00:16:51.410 --> 00:16:53.660比任何其他生物都快，或者00:16:53.660 --> 00:16:56.210在他们的头上放一个大喇叭或类似的东西，对吗？00:16:56.210 --> 00:16:59.060你知道，人类基本上很弱小，嗯，00:16:59.060 --> 00:17:01.190但他们有这个嗯，00:17:01.190 --> 00:17:04.310他们能与之沟通的无与伦比的优势00:17:04.310 --> 00:17:07.880彼此之间，因此在团队中更有效地工作。00:17:07.880 --> 00:17:11.495这基本上使人类无敌。00:17:11.495 --> 00:17:15.575但你知道，即使在那时人类也是有限的，对吧？00:17:15.575 --> 00:17:18.140这让你了解石器时代的情况，对吧？00:17:18.140 --> 00:17:20.390在那里你可以敲击你的石头00:17:20.390 --> 00:17:23.240合适的石头可以做成锋利的东西。00:17:23.240 --> 00:17:25.685嗯，是什么让人类超越了这一点，00:17:25.685 --> 00:17:28.100是他们发明了文字。00:17:28.100 --> 00:17:32.915所以写作是一种你可以接受知识的能力00:17:32.915 --> 00:17:37.730不仅和你看到的人进行了口耳相传。00:17:37.730 --> 00:17:41.660你可以把它放在你的纸莎草纸上，这样你的泥板或者其他什么东西就可以了。00:17:41.660 --> 00:17:45.620这是一开始，然后知识可以被发送到地方。00:17:45.620 --> 00:17:50.270它可以被空间发送到世界各地，然后它可以00:17:50.270 --> 00:17:55.430在时间上是暂时的。00:17:55.430 --> 00:17:57.290好吧，写作年龄有多大？00:17:57.290 --> 00:18:00.890我的意思是，我们基本上知道写作有多古老，对吧？00:18:00.890 --> 00:18:04.115那篇文章大约有5000年的历史。00:18:04.115 --> 00:18:09.740这在进化史上是难以置信的最近，但是你知道，00:18:09.740 --> 00:18:16.730从本质上说，写作是一种拥有知识的方式，在那5000人中00:18:16.730 --> 00:18:24.035让人类从石器时代的锋利碎片或燧石到你知道的年代，00:18:24.035 --> 00:18:26.240有了iPhone和所有这些东西，00:18:26.240 --> 00:18:28.790所有这些难以置信的复杂设备。00:18:28.790 --> 00:18:32.960所以，语言是非常特别的东西，我想建议。00:18:32.960 --> 00:18:37.910嗯，但是你知道，如果我回到我的类比中，那就允许人类00:18:37.910 --> 00:18:43.280构建一个网络化的计算机，其功能远比um强大，00:18:43.280 --> 00:18:47.600就像猩猩一样聪明的个体生物。00:18:47.600 --> 00:18:50.525嗯，你把它和我们的电脑网络比较一下，00:18:50.525 --> 00:18:53.045这是一种非常有趣的网络，对吧？00:18:53.045 --> 00:18:55.745你知道这些天，嗯，00:18:55.745 --> 00:19:01.805我们的网络运行在我们有大网络带宽的地方，对吗？00:19:01.805 --> 00:19:03.770你知道，我们有时可能会因为00:19:03.770 --> 00:19:06.530我们的Netflix下载，但大体上你知道，00:19:06.530 --> 00:19:09.755我们可以轻松快速地下载数百兆字节。00:19:09.755 --> 00:19:11.570我们认为速度不够快，00:19:11.570 --> 00:19:13.670所以我们将推出5G网络。00:19:13.670 --> 00:19:16.400所以它又快了一个数量级。00:19:16.400 --> 00:19:18.800我是说，相比之下，我是说，00:19:18.800 --> 00:19:23.540人类语言是一个慢得可怜的网络，对吧？00:19:23.540 --> 00:19:29.465你能用人类语言传达的信息量是非常缓慢的。00:19:29.465 --> 00:19:33.950我是说，不管是什么，我一秒钟说15个字，对吧，00:19:33.950 --> 00:19:35.420你可以开始做，嗯，00:19:35.420 --> 00:19:37.550你的信息理论，如果你知道一些正确的？00:19:37.550 --> 00:19:41.060但是，嗯，实际上你没有太多的带宽。00:19:41.060 --> 00:19:44.405然后你就可以想到，00:19:44.405 --> 00:19:45.980那它是怎么工作的？00:19:45.980 --> 00:19:47.570所以，人类想出了00:19:47.570 --> 00:19:53.390这个令人印象深刻的系统，本质上是压缩的形式。00:19:53.390 --> 00:19:56.120一种非常适应的压缩形式，00:19:56.120 --> 00:19:58.070所以当我们与人交谈时，00:19:58.070 --> 00:20:02.870我们假设他们头脑中有大量的知识，00:20:02.870 --> 00:20:07.640我和你说话的时候是不是和我的不一样，但大体上和我的相似？00:20:07.640 --> 00:20:10.565你知道英语单词的意思，00:20:10.565 --> 00:20:13.850你对世界是如何运作的很了解。00:20:13.850 --> 00:20:17.149因此，我可以说一个简短的信息和沟通00:20:17.149 --> 00:20:22.820只有一个相对较短的位字符串，您实际上可以理解很多。好吧？00:20:22.820 --> 00:20:26.030所以，我可以随便说，00:20:26.030 --> 00:20:28.850想象一下一个繁忙的购物中心00:20:28.850 --> 00:20:31.630有两个人站在化妆台前，00:20:31.630 --> 00:20:36.290你知道，我只说了大概200比特的00:20:36.290 --> 00:20:38.960但这使你能够00:20:38.960 --> 00:20:42.340一个完整的视觉场景，我们把兆字节带到嗯，00:20:42.340 --> 00:20:44.385表示为图像。00:20:44.385 --> 00:20:46.625所以，这就是为什么语言是好的。00:20:46.625 --> 00:20:49.100嗯，从更权威的层面来说，00:20:49.100 --> 00:20:51.425我现在要回到具体的东西上。00:20:51.425 --> 00:20:55.925我们在这节课上要做的不是解决整个语言，00:20:55.925 --> 00:20:57.950但我们要代表，嗯，00:20:57.950 --> 00:21:00.380单词的意思，对吗？00:21:00.380 --> 00:21:03.230所以，很多语言都是用词和它们的含义来联系的00:21:03.230 --> 00:21:06.200单词可以有很丰富的含义，对吗？00:21:06.200 --> 00:21:07.970只要你说一句话，老师，00:21:07.970 --> 00:21:12.530这是相当丰富的意义，或者你可以有丰富的意义的行动。00:21:12.530 --> 00:21:17.225所以，如果我说预言之类的话，00:21:17.225 --> 00:21:19.070嗯，总而言之，或者你知道的，00:21:19.070 --> 00:21:22.385这些词有着丰富的含义和许多细微的差别。00:21:22.385 --> 00:21:24.395所以我们要代表意义。00:21:24.395 --> 00:21:26.510所以，问题是什么意思？00:21:26.510 --> 00:21:29.360所以，你当然可以——字典是用来告诉你意义的。00:21:29.360 --> 00:21:31.490所以，你可以查字典，嗯，00:21:31.490 --> 00:21:35.720韦伯斯特说，有点试图把意义和想法联系起来。00:21:35.720 --> 00:21:39.515一个词或短语所代表的思想。00:21:39.515 --> 00:21:44.240一个人想用文字符号等来表达的想法。00:21:44.240 --> 00:21:46.190我是说，你知道，00:21:46.190 --> 00:21:49.730你可以认为这些定义是一种逃避，因为看起来00:21:49.730 --> 00:21:53.015就像他们在用“想法”这个词改写意思，00:21:53.015 --> 00:21:55.040这真的让你得到了什么。00:21:55.040 --> 00:21:58.370嗯，语言学家是如何看待意义的？00:21:58.370 --> 00:22:03.110我是说，语言学家最常想到的方法00:22:03.110 --> 00:22:05.660意思是一个被称为外延的概念00:22:05.660 --> 00:22:08.420在编程语言中也使用的语义。00:22:08.420 --> 00:22:14.810所以，我们认为意义就是事物所代表的。00:22:14.810 --> 00:22:16.955所以，如果我说“椅子”这个词，00:22:16.955 --> 00:22:21.140“椅子”一词的外延包括这里的这个和那个，00:22:21.140 --> 00:22:22.325那个，那个，那个。00:22:22.325 --> 00:22:24.919所以“椅子”这个词有点代表00:22:24.919 --> 00:22:28.580所有的东西都是椅子，你可以，嗯，00:22:28.580 --> 00:22:33.410然后你也可以考虑跑步之类的事情，因为你知道有一套00:22:33.410 --> 00:22:37.985人们可以参与的行动——这就是他们的象征。00:22:37.985 --> 00:22:42.200这就是你在哲学或语言学中最常看到的外延。00:22:42.200 --> 00:22:47.135这是一件很难掌握的事情，嗯，从计算上来说。00:22:47.135 --> 00:22:50.480那么，嗯，什么类型的人最常见00:22:50.480 --> 00:22:54.020做或使用最常用的是我想我现在应该说00:22:54.020 --> 00:22:57.530在计算机上计算单词的意思00:22:57.530 --> 00:23:01.115通常情况下，这会变成有点像字典的东西。00:23:01.115 --> 00:23:06.200特别喜欢的在线词汇是这个叫做wordnet的在线词典。00:23:06.200 --> 00:23:11.510有点告诉你单词的意思和单词意思之间的关系。00:23:11.510 --> 00:23:16.445嗯，这只是给你一种切碎的感觉，00:23:16.445 --> 00:23:19.820嗯，在WordNet里。00:23:19.820 --> 00:23:24.485嗯，这是上面的一段真正的python代码，00:23:24.485 --> 00:23:28.370嗯，在你的电脑里输入，然后自己运行并做这个。00:23:28.370 --> 00:23:31.040嗯，所以这使用了一个叫做NLTK的东西。00:23:31.040 --> 00:23:33.725嗯，所以NLTK有点像00:23:33.725 --> 00:23:39.364“瑞士军刀NLP”意味着它对任何东西都没有太大的好处，00:23:39.364 --> 00:23:41.570但是它有很多基本的工具。00:23:41.570 --> 00:23:46.460所以，如果你想做一些事情，比如从WordNet中取出一些东西并展示出来，00:23:46.460 --> 00:23:49.625这是最好的选择。嗯，好吧。00:23:49.625 --> 00:23:54.830所以，嗯，从NLTK我要导入WordNet，然后我可以说，00:23:54.830 --> 00:24:01.355“好吧，嗯，对于good这个词，告诉我good参与的同义词集。”00:24:01.355 --> 00:24:03.440有一个名词叫善。00:24:03.440 --> 00:24:04.760有一个形容词good。00:24:04.760 --> 00:24:08.330有一个值得尊敬的好的，可敬的，可敬的。00:24:08.330 --> 00:24:11.150嗯，这看起来很复杂，很难理解。00:24:11.150 --> 00:24:13.700但是单词网的概念使00:24:13.700 --> 00:24:18.080一个词的感觉之间的细微差别。00:24:18.080 --> 00:24:20.675那么，什么样的说法是好的，嗯，00:24:20.675 --> 00:24:23.570有些传感器是名词，对吧？00:24:23.570 --> 00:24:24.755你就是这样的，00:24:24.755 --> 00:24:27.200我买了一些旅行用品，对吧？00:24:27.200 --> 00:24:28.880所以，有点，嗯，00:24:28.880 --> 00:24:32.780我想是这类名词传感器中的一个。00:24:32.780 --> 00:24:35.480嗯，还有形容词传感器，它试图00:24:35.480 --> 00:24:38.840区分-有一个基本的形容词，好的感觉是好的。00:24:38.840 --> 00:24:41.270然后在某些情况下，嗯，传感器，00:24:41.270 --> 00:24:44.750在不同的方向上有这些扩展的良好传感器。00:24:44.750 --> 00:24:48.515所以，我想这是有益的，嗯，00:24:48.515 --> 00:24:52.925这是一种值得尊敬的人。00:24:52.925 --> 00:24:55.580他是个好人，或者类似的人，对吧？00:24:55.580 --> 00:24:56.855所以，嗯，但是你知道，00:24:56.855 --> 00:24:59.660是什么让我们00:24:59.660 --> 00:25:02.630思考很有问题，练习使用它是否试图使00:25:02.630 --> 00:25:06.850所有这些人类传感器之间的细微差别00:25:06.850 --> 00:25:11.410几乎不了解他们之间的区别，嗯，和。00:25:11.410 --> 00:25:13.690嗯，这样你就可以用WordNet做其他事情了。00:25:13.690 --> 00:25:18.460所以，这段代码你可以很好地向上走，它是一种层次结构。00:25:18.460 --> 00:25:21.635所以，它有点像传统的，嗯，数据库。00:25:21.635 --> 00:25:29.030所以，如果我从熊猫开始，然后说-[噪音]如果我从熊猫开始。00:25:29.030 --> 00:25:32.180嗯，走上去，嗯，00:25:32.180 --> 00:25:35.330熊猫[听不见]。00:25:35.330 --> 00:25:37.640也许你们会把食肉动物比作生物，00:25:37.640 --> 00:25:39.545胎盘，哺乳动物，等等。00:25:39.545 --> 00:25:44.135好吧，那么，嗯，这就是你能从WordNet中得到的东西。00:25:44.135 --> 00:25:47.105嗯，你知道，实际上，WordNet已经。00:25:47.105 --> 00:25:49.580每个人都习惯用它，因为它00:25:49.580 --> 00:25:51.995你对这个词的意思有某种理解。00:25:51.995 --> 00:25:54.125但你也知道这是众所周知的。00:25:54.125 --> 00:25:56.540它从来没有那么好地工作过。00:25:56.540 --> 00:26:02.720嗯，所以你知道这类同义词集遗漏了很多细微差别。00:26:02.720 --> 00:26:05.270所以，你知道一个好的同义词集有00:26:05.270 --> 00:26:08.240精通IT，有点像精通00:26:08.240 --> 00:26:11.495但“精通”并没有更多的内涵和细微差别吗？00:26:11.495 --> 00:26:13.250我想是的。00:26:13.250 --> 00:26:18.080嗯，WordNet和大多数手工构建的资源一样，有点不完整。00:26:18.080 --> 00:26:21.290所以，一旦你开始了解单词的新含义，00:26:21.290 --> 00:26:23.705或是新词和俚语，00:26:23.705 --> 00:26:25.310好吧，那就什么也不给你了。00:26:25.310 --> 00:26:28.985嗯，它是用人工建造的，00:26:28.985 --> 00:26:35.030嗯，以你知道的方式，很难去创造和适应。00:26:35.030 --> 00:26:37.670特别是，我们要关注的是，00:26:37.670 --> 00:26:41.870似乎是你想用词做的一件基本的事情，事实上至少00:26:41.870 --> 00:26:45.920理解单词含义之间的相似性和关系。00:26:45.920 --> 00:26:49.520事实证明，你知道WordNet做得并不好00:26:49.520 --> 00:26:53.600因为它只有这些固定的离散同义词集。00:26:53.600 --> 00:26:56.090所以，如果你在同义词中有一个词说00:26:56.090 --> 00:26:59.075有点像同义词，可能含义不完全相同，00:26:59.075 --> 00:27:00.800它们不是同一组同义词，00:27:00.800 --> 00:27:04.580你不能真正地测量部分相似性作为他们的意义。00:27:04.580 --> 00:27:08.435所以，如果好的和神奇的东西不是在同一个同义词集中，00:27:08.435 --> 00:27:11.960但他们有一些共同点，你想代表。00:27:11.960 --> 00:27:16.880可以。所以，嗯，这有点让人觉得00:27:16.880 --> 00:27:21.935我们想做一些不同的更好的词义。00:27:21.935 --> 00:27:25.730而且，嗯，在到那里之前，我只是想再整理一下00:27:25.730 --> 00:27:29.495从传统的NLP构建一点。00:27:29.495 --> 00:27:33.275所以，在这门课的背景下，传统的NLP方法00:27:33.275 --> 00:27:39.275直到2012年左右的自然语言处理。00:27:39.275 --> 00:27:43.640有一些早期的先例，但基本上，嗯，00:27:43.640 --> 00:27:47.6002013年，情况真的开始改变00:27:47.600 --> 00:27:53.060人们开始使用神经网络风格的表达来处理自然语言。00:27:53.060 --> 00:27:55.430所以，到2012年，00:27:55.430 --> 00:27:58.055嗯，标准来说，你知道我们有话要说。00:27:58.055 --> 00:28:02.210它们只是文字而已。所以，我们开了酒店会议旅馆。00:28:02.210 --> 00:28:06.650它们是单词，我们会让你知道词汇，并把单词放到我们的模型中。00:28:06.650 --> 00:28:12.290在神经网络领域，这被称为局域表示。00:28:12.290 --> 00:28:14.960下次我再回到这些条款上来。00:28:14.960 --> 00:28:20.015但这就意味着，对于任何概念，都有一种特殊的，00:28:20.015 --> 00:28:24.080嗯，旅馆这个词还是汽车旅馆这个词。00:28:24.080 --> 00:28:26.465一种思考的方式是思考00:28:26.465 --> 00:28:29.615关于构建机器学习模型时会发生什么。00:28:29.615 --> 00:28:34.759所以，如果你有一个分类变量，就像你有选择单词的单词一样00:28:34.759 --> 00:28:40.130你想把它放到机器学习模型中的分类器里，00:28:40.130 --> 00:28:42.905不知何故，你必须对分类变量进行编码，00:28:42.905 --> 00:28:46.550标准的方法是通过00:28:46.550 --> 00:28:51.275变量的不同级别，这意味着你有一个向量，00:28:51.275 --> 00:28:53.840你知道，这就是“房子”这个词。00:28:53.840 --> 00:28:55.670这是“猫”这个词。这是“狗”这个词。00:28:55.670 --> 00:28:57.020这是“一些椅子”这个词。00:28:57.020 --> 00:28:58.190这是一个令人愉快的词。00:28:58.190 --> 00:28:59.465这是另外一个词。00:28:59.465 --> 00:29:01.415就是这个词，嗯，00:29:01.415 --> 00:29:05.750旅馆，嗯，这是另一个不同的词，对吧？00:29:05.750 --> 00:29:08.075所以你把一个放在这个位置上00:29:08.075 --> 00:29:11.120神经网络陆地，我们称之为热向量，00:29:11.120 --> 00:29:12.470所以这些可能是，啊，00:29:12.470 --> 00:29:16.250酒店和汽车旅馆的热门话题。00:29:16.250 --> 00:29:19.040所以，这里有一些不好的地方。00:29:19.040 --> 00:29:21.005嗯，就是那种，啊，00:29:21.005 --> 00:29:27.140实际的麻烦是你知道语言有很多单词。00:29:27.140 --> 00:29:30.590啊，所以，这是你可能还记得的那种字典00:29:30.590 --> 00:29:35.450学校里大概有25万个单词。00:29:35.450 --> 00:29:37.400但是你知道，如果你开始00:29:37.400 --> 00:29:41.855更多的技术和科学英语很容易达到一百万个单词。00:29:41.855 --> 00:29:45.690我的意思是，实际上你在一种语言中有多少个单词，嗯，00:29:45.690 --> 00:29:48.620就像英语实际上是无限的，因为我们00:29:48.620 --> 00:29:52.220这些过程被称为衍生形态学，00:29:52.220 --> 00:29:56.930嗯，在这里你可以通过在现有单词上添加词尾来生成更多的单词。00:29:56.930 --> 00:29:59.660所以，你知道你可以从家长主义开始，00:29:59.660 --> 00:30:03.470父爱，然后你可以从母亲那里说，00:30:03.470 --> 00:30:06.275你可以说家长主义，或者家长主义，00:30:06.275 --> 00:30:10.070家长主义和爸爸-我是家长主义的。00:30:10.070 --> 00:30:14.255正确的？现在，所有这些方法，你可以通过添加更多的东西来烘烤更大的单词。00:30:14.255 --> 00:30:18.905嗯，所以你最终会得到一个无限的单词空间。00:30:18.905 --> 00:30:22.880嗯，是的。这是个小问题，对吧？00:30:22.880 --> 00:30:28.275如果我们想表示一个合理大小的词汇表，我们有很大的向量。00:30:28.275 --> 00:30:31.990嗯，但还有一个比这更大的问题，那就是，00:30:31.990 --> 00:30:35.200我们一直想做的就是，00:30:35.200 --> 00:30:38.590有点，理解关系和单词的含义。00:30:38.590 --> 00:30:42.380所以，你知道，一个明显的例子就是网络搜索。00:30:42.380 --> 00:30:45.350所以，如果我搜索西雅图汽车旅馆，00:30:45.350 --> 00:30:48.710如果它也显示了00:30:48.710 --> 00:30:52.655西雅图酒店在页面上，反之亦然，因为，00:30:52.655 --> 00:30:55.415你知道，旅馆和汽车旅馆几乎是一样的。00:30:55.415 --> 00:30:59.900但是，你知道，如果我们有一个像以前一样的热向量00:30:59.900 --> 00:31:04.250他们之间没有S-相似关系，对吗？00:31:04.250 --> 00:31:05.675所以，用数学术语来说，00:31:05.675 --> 00:31:07.775这两个向量是正交的。00:31:07.775 --> 00:31:10.865他们之间没有相似关系。00:31:10.865 --> 00:31:12.650嗯，所以你，00:31:12.650 --> 00:31:14.705有点，什么也得不到。00:31:14.705 --> 00:31:16.880现在，你知道，有些事情你可以做，00:31:16.880 --> 00:31:18.710我-我刚给你看了WordNet的。00:31:18.710 --> 00:31:20.840WordNet为您显示了一些同义词和内容。00:31:20.840 --> 00:31:22.610所以这可能会有点帮助。00:31:22.610 --> 00:31:24.035还有其他事情你可以做。00:31:24.035 --> 00:31:25.415你可以说，等等，00:31:25.415 --> 00:31:29.645为什么不建一个大桌子，我们有一个大桌子，00:31:29.645 --> 00:31:32.675嗯，词的相似性，我们可以用它。00:31:32.675 --> 00:31:34.910而且，你知道，人们过去常常尝试这样做，对吗？00:31:34.910 --> 00:31:39.770你知道，这就是谷歌2005年的做法。00:31:39.770 --> 00:31:42.080你知道，它有单词相似度表。00:31:42.080 --> 00:31:44.510这样做的问题是，00:31:44.510 --> 00:31:48.290我们在谈论我们想要50万个词。00:31:48.290 --> 00:31:52.040如果你想建立一个单词相似度表00:31:52.040 --> 00:31:56.060从一个热门的表达中我们的成对单词，00:31:56.060 --> 00:31:58.640嗯，你-这意味着那张桌子的大小，00:31:58.640 --> 00:32:00.380因为我的数学很差，00:32:00.380 --> 00:32:02.315是2.5万亿吗？00:32:02.315 --> 00:32:07.130在你的相似度矩阵中有很多细胞。00:32:07.130 --> 00:32:09.230所以这几乎是不可能的。00:32:09.230 --> 00:32:13.715所以，我们要做的是探索一种方法，00:32:13.715 --> 00:32:16.670我们将把单词表示为向量，00:32:16.670 --> 00:32:18.140在某种程度上，我会告诉你，嗯，00:32:18.140 --> 00:32:21.770以这样的方式表示的一分钟00:32:21.770 --> 00:32:26.480一言以蔽之，相似点就没有了。00:32:26.480 --> 00:32:30.635可以。所以这会引出这些不同的想法。00:32:30.635 --> 00:32:34.175所以，我之前提到了表示语义。00:32:34.175 --> 00:32:39.115这里还有一个表达单词意思的方法，00:32:39.115 --> 00:32:41.980这就是所谓的分布语义。00:32:41.980 --> 00:32:45.140所以分布语义的概念是，00:32:45.140 --> 00:32:50.900我们要如何通过观察上下文来表示一个词的意思，00:32:50.900 --> 00:32:52.925嗯，在里面。00:32:52.925 --> 00:32:56.510这是一张英国语言学家小弗斯的照片。00:32:56.510 --> 00:32:58.400嗯，他以这句话出名，00:32:58.400 --> 00:33:01.535“你应该知道它所保存的公司的一个词。”00:33:01.535 --> 00:33:06.950嗯，但另一个以发展这种意义概念而出名的人是，嗯，00:33:06.950 --> 00:33:10.670哲学家路德维希-路德维希·维特根斯坦在他后来的著作中，00:33:10.670 --> 00:33:13.445他称之为“会议意义的使用理论”。00:33:13.445 --> 00:33:16.070嗯，事实上，他用了一个我不知道的德语大字，00:33:16.070 --> 00:33:18.530但是，我们称之为意义使用理论。00:33:18.530 --> 00:33:22.535你知道，关键是，嗯，你知道，00:33:22.535 --> 00:33:26.780如果你能解释的话00:33:26.780 --> 00:33:31.160解释使用某个词的正确语境，00:33:31.160 --> 00:33:34.595在什么语境下使用错误的词，00:33:34.595 --> 00:33:38.135这可能会让你回忆起高中时的英语，00:33:38.135 --> 00:33:40.490当人们说，“啊，这是个错误的词”时，00:33:40.490 --> 00:33:43.205嗯，那你就明白这个词的意思了，对吧？00:33:43.205 --> 00:33:47.045嗯，这就是分布语义学的概念。00:33:47.045 --> 00:33:49.790它一直是-所以最成功的想法之一00:33:49.790 --> 00:33:54.005现代统计学的NLP，因为它给你一个很好的方法来学习单词的意思。00:33:54.005 --> 00:33:56.615所以我们要做的是说，00:33:56.615 --> 00:33:58.925哈哈，我想知道银行这个词是什么意思。00:33:58.925 --> 00:34:01.730所以，我会抓取很多短信，00:34:01.730 --> 00:34:04.520当我们拥有万维网时，这很容易做到，00:34:04.520 --> 00:34:07.955我会找到很多使用“银行”这个词的句子，00:34:07.955 --> 00:34:12.7702009年发生的政府债务问题演变成银行危机。00:34:12.770 --> 00:34:15.845这两个-我只想说00:34:15.845 --> 00:34:19.115这是“银行”这个词的意思。00:34:19.115 --> 00:34:23.750嗯，这些就是使用“银行”这个词的上下文。00:34:23.750 --> 00:34:29.495这看起来很简单，甚至可能不是很正确的想法，00:34:29.495 --> 00:34:34.880但事实证明，这是一个非常有用的想法，在捕捉意义方面做得很好。00:34:34.880 --> 00:34:38.300所以我们要做的是说而不是说00:34:38.300 --> 00:34:42.950我们以前的地方代表我们现在要00:34:42.950 --> 00:34:48.215用我们称之为分布式表示的方式表示单词。00:34:48.215 --> 00:34:51.830因此，对于分布式表示，我们仍将继续00:34:51.830 --> 00:34:55.655to[噪声]表示一个词作为数字向量的意义。00:34:55.655 --> 00:34:59.480但现在我们要说的是每个词的意思是，00:34:59.480 --> 00:35:01.520小矢量，嗯，00:35:01.520 --> 00:35:07.760但它将是一个稠密的向量，所有的数字都是非零的。00:35:07.760 --> 00:35:10.010所以银行业的意义将是00:35:10.010 --> 00:35:13.340分布在这个向量的维上。00:35:13.340 --> 00:35:19.190现在，这里的向量是第九维度的，因为我想保持滑动，嗯，很好。00:35:19.190 --> 00:35:23.195嗯，生活在实践中不是很好。00:35:23.195 --> 00:35:25.970当我们这样做的时候，我们使用一个更大的维度，00:35:25.970 --> 00:35:29.075有点，固体，人们使用的最低限度是50。00:35:29.075 --> 00:35:32.330嗯，你在笔记本电脑上使用的一个典型数字是00:35:32.330 --> 00:35:35.945300如果你真的想最大限度地发挥性能，00:35:35.945 --> 00:35:38.885嗯，可能是1000，2000，4000。00:35:38.885 --> 00:35:42.020但是，你知道，尽管如此，（噪音）数量级是00:35:42.020 --> 00:35:46.320比长度为500000的向量小。00:35:46.810 --> 00:35:51.890可以。所以我们有了带有向量表示的单词。00:35:51.890 --> 00:35:55.790因为每个词都有一个向量，嗯，00:35:55.790 --> 00:36:01.160然后我们就有了一个向量空间，在这个空间中我们可以放置所有的单词。00:36:01.160 --> 00:36:03.980嗯，那是完全不可读的，嗯，00:36:03.980 --> 00:36:08.135但是如果你放大到向量空间，它仍然是完全不可读的。00:36:08.135 --> 00:36:10.115但是如果你再放大一点，00:36:10.115 --> 00:36:13.100嗯，你可以找到这个空间的不同部分。00:36:13.100 --> 00:36:16.820这就是各国参与的部分，00:36:16.820 --> 00:36:18.950嗯，有日语，德语，00:36:18.950 --> 00:36:21.950法语、俄语、英属澳大利亚裔美国人，00:36:21.950 --> 00:36:25.130嗯，法国，英国，德国等等。00:36:25.130 --> 00:36:27.770你可以转移到空间的另一部分。00:36:27.770 --> 00:36:31.040这是空间的一部分，这里有各种动词，00:36:31.040 --> 00:36:33.485过去，过去，过去，现在也是。00:36:33.485 --> 00:36:40.880哎呀。嗯，嗯，[听不见]总是在那里。00:36:40.880 --> 00:36:43.970你甚至可以看到一些形态在一起，00:36:43.970 --> 00:36:46.100像是说，00:36:46.100 --> 00:36:48.770想一想，期待那些能接受的东西，有点，恭维。00:36:48.770 --> 00:36:50.795他说或想了些什么。00:36:50.795 --> 00:36:52.415嗯，他们在一起。00:36:52.415 --> 00:36:55.010现在，我到底在给你看什么？00:36:55.010 --> 00:36:57.755嗯，你知道，这真的是从00:36:57.755 --> 00:37:00.575啊，100维字向量。00:37:00.575 --> 00:37:05.630而且这个问题很难想象100维的词向量。00:37:05.630 --> 00:37:09.860所以，这里实际发生的是这些，嗯，00:37:09.860 --> 00:37:15.110100维字向量正向下投射到两维中，00:37:15.110 --> 00:37:17.990你看到的是二维视图，00:37:17.990 --> 00:37:19.790我稍后再谈。00:37:19.790 --> 00:37:22.400嗯，一方面，嗯，00:37:22.400 --> 00:37:24.410每当你看到这些照片时，你应该紧紧抓住00:37:24.410 --> 00:37:26.840你的钱包因为里面有很多00:37:26.840 --> 00:37:31.535关于原始向量空间的细节被完全杀死然后消失了，嗯，00:37:31.535 --> 00:37:32.839在二维投影中，00:37:32.839 --> 00:37:37.070事实上，有些东西在二维空间中推动了事物的发展，00:37:37.070 --> 00:37:39.875嗯，投影可能真的，真的，00:37:39.875 --> 00:37:42.590真的歪曲了原始空间中的内容。00:37:42.590 --> 00:37:45.740嗯，但即使看这些二维表示，00:37:45.740 --> 00:37:46.850总体感觉是，00:37:46.850 --> 00:37:48.920天哪，这真是一种工作，不是吗？00:37:48.920 --> 00:37:54.365嗯，我们可以看出单词之间的相似之处。00:37:54.365 --> 00:38:02.375可以。所以，嗯，哈-那就是我们想做什么的想法。00:38:02.375 --> 00:38:04.310下一部分，嗯，00:38:04.310 --> 00:38:07.940那我们该怎么做呢？00:38:07.940 --> 00:38:10.445我会停下来呼吸半分钟。00:38:10.445 --> 00:38:12.710有人想问一个问题吗？00:38:12.710 --> 00:38:20.300[噪音]是的。00:38:20.300 --> 00:38:26.720向量在哪里，嗯，00:38:26.720 --> 00:38:28.460每个联系人的顺序不同，00:38:28.460 --> 00:38:30.530比如说，第一个十进制向量，00:38:30.530 --> 00:38:32.840第二个十进制向量，是标准向量吗？00:38:32.840 --> 00:38:35.475在所有的理论中，还是人们自己选择它们？00:38:35.475 --> 00:38:42.340嗯，它们不是整个NLP的标准，而且根本没有被选中。00:38:42.340 --> 00:38:45.055所以我们要展示的是一个学习算法。00:38:45.055 --> 00:38:48.430所以我们只是在大量文本中乱序排列00:38:48.430 --> 00:38:51.970奇迹般地，这些文字载体出现了。00:38:51.970 --> 00:38:57.760因此L-学习算法本身决定了维度。00:38:57.760 --> 00:39:03.085但这实际上让我想起了我想说的话，是的，00:39:03.085 --> 00:39:05.425因为这是一个向量空间，00:39:05.425 --> 00:39:09.580在某种意义上，任意权利的维度，00:39:09.580 --> 00:39:12.570因为你可以知道你的基向量00:39:12.570 --> 00:39:15.945任何不同的方向，你都可以重新表现，00:39:15.945 --> 00:39:19.715嗯，向量空间中的单词有不同的基础，00:39:19.715 --> 00:39:22.930基向量和它是完全相同的向量空间00:39:22.930 --> 00:39:26.380只是旋转到新的，向量。00:39:26.380 --> 00:39:30.580所以，你知道，你不应该对这些元素读太多。00:39:30.580 --> 00:39:32.860事实证明，这是因为00:39:32.860 --> 00:39:36.070深入学习UM操作工作，00:39:36.070 --> 00:39:38.170他们做的一些事情，都是因地制宜的。00:39:38.170 --> 00:39:42.775因此，维度确实趋向于对它们有某种意义，事实证明。00:39:42.775 --> 00:39:46.900不过，虽然我想说的是，00:39:46.900 --> 00:39:52.240你知道我们能想到的一件事就是事情有多接近00:39:52.240 --> 00:39:54.250在向量空间中，这是00:39:54.250 --> 00:39:57.805我们将要利用的意义相似的概念。00:39:57.805 --> 00:40:00.640但你可能希望你得到更多，00:40:00.640 --> 00:40:03.010你可能真的认为00:40:03.010 --> 00:40:06.925矢量空间中的不同维度和方向。00:40:06.925 --> 00:40:11.335答案是肯定的，我稍后再谈。00:40:11.335 --> 00:40:17.770可以。嗯，从某种意义上说这件事00:40:17.770 --> 00:40:22.240最大的影响，嗯，在某种程度上改变了世界00:40:22.240 --> 00:40:27.625神经网络方向的NLP就是这张图。00:40:27.625 --> 00:40:32.260嗯，这个算法是不是00:40:32.260 --> 00:40:37.345Thomas Mikolov于2013年提出了一种称为Word2vec算法的算法。00:40:37.345 --> 00:40:43.210所以这不是第一部作品，也不是分布式的文字表达。00:40:43.210 --> 00:40:45.730所以Yoshua Bengio的老作品00:40:45.730 --> 00:40:48.370回到千年之交，00:40:48.370 --> 00:40:52.780不知怎的，它不是真的击中了他们的头上的世界00:40:52.780 --> 00:40:57.730一个巨大的影响和托马斯·米科洛夫的表现非常简单，00:40:57.730 --> 00:41:00.070非常可扩展的学习方式00:41:00.070 --> 00:41:05.005用矢量表示的单词和这类单词真的打开了泄洪闸。00:41:05.005 --> 00:41:08.650这就是我现在要展示的算法。00:41:08.650 --> 00:41:15.775可以。所以这个算法的思想是从一大堆文本开始。00:41:15.775 --> 00:41:20.650嗯，所以无论你在什么地方发现报纸上的文章或什么东西，00:41:20.650 --> 00:41:22.480很多连续的文本，对吗？00:41:22.480 --> 00:41:26.350实际的句子，因为我们想学习两个词的意思上下文。00:41:26.350 --> 00:41:32.470嗯，NLP把一大堆文本称为语料库。00:41:32.470 --> 00:41:35.890我的意思是这只是身体的拉丁词，对吧？00:41:35.890 --> 00:41:37.915这是一个正文。00:41:37.915 --> 00:41:43.224重要的是，如果你想看起来受过真正的教育，就要注意拉丁语，00:41:43.224 --> 00:41:46.690这是第四个下倾名词。00:41:46.690 --> 00:41:49.900所以语料库的复数形式是语料库。00:41:49.900 --> 00:41:51.190如果你说00:41:51.190 --> 00:41:55.390核心派每个人都会知道你高中时没有学过拉丁语。00:41:55.390 --> 00:42:00.490[笑声]嗯，好的。00:42:00.490 --> 00:42:06.460嗯，对了-所以我们想说每一个词00:42:06.460 --> 00:42:08.890用固定的词汇00:42:08.890 --> 00:42:12.445语料库的词汇由矢量表示。00:42:12.445 --> 00:42:16.615我们把这些向量作为随机向量。00:42:16.615 --> 00:42:18.340所以我们要做的就是00:42:18.340 --> 00:42:22.585这是一个大的迭代算法，我们将遍历文本中的每个位置。00:42:22.585 --> 00:42:24.715我们说，课文里有一个词。00:42:24.715 --> 00:42:30.520让我们看看周围的文字，我们要做的就是说好，00:42:30.520 --> 00:42:32.890一个词的意思就是它的使用环境。00:42:32.890 --> 00:42:35.290所以我们想要这个词的表现形式00:42:35.290 --> 00:42:37.870在中间能够预测00:42:37.870 --> 00:42:43.720围绕着它，我们将通过移动矢量这个词的位置来达到这个目的。00:42:43.720 --> 00:42:47.500我们只是重复了十亿次00:42:47.500 --> 00:42:51.190不知怎的，奇迹发生了，结果是我们00:42:51.190 --> 00:42:54.790一个字向量空间，看起来像我展示的图片00:42:54.790 --> 00:42:59.530好的词义符合好的词义表达。00:42:59.530 --> 00:43:03.090所以稍微多一点，嗯，00:43:03.090 --> 00:43:07.240嗯，从图形上看，稍微有点。00:43:07.240 --> 00:43:08.440情况就是这样。00:43:08.440 --> 00:43:12.835所以我们的部分语料库问题变成了银行危机，00:43:12.835 --> 00:43:14.290所以我们想说的是，00:43:14.290 --> 00:43:17.725我们想知道单词into的意思，所以我们希望00:43:17.725 --> 00:43:21.400它的表示可以用一种00:43:21.400 --> 00:43:24.820精确预测单词的出现00:43:24.820 --> 00:43:28.600into的上下文，因为这就是into的含义。00:43:28.600 --> 00:43:31.525所以我们要尝试做出这些预测，00:43:31.525 --> 00:43:34.855看看我们能预测多少然后改变00:43:34.855 --> 00:43:39.475用一种我们可以做得更好的预测的方式来表示词的矢量。00:43:39.475 --> 00:43:41.320一旦我们处理好，00:43:41.320 --> 00:43:43.765接下来我们说，00:43:43.765 --> 00:43:46.060好吧，让我们以银行业为例。00:43:46.060 --> 00:43:49.795银行业的意义在于预测银行业发生的环境。00:43:49.795 --> 00:43:51.265这里有一个背景。00:43:51.265 --> 00:43:54.550让我们试着预测一下银行业和00:43:54.550 --> 00:43:58.735看看我们是怎么做的，然后我们会从那里继续前进。00:43:58.735 --> 00:44:02.470可以。嗯，听起来很简单。00:44:02.470 --> 00:44:06.100嗯，[噪音]现在我们继续做更多的事情。00:44:06.100 --> 00:44:12.460可以。所以总的来说，我们有一个很大的大写T字语料库。00:44:12.460 --> 00:44:17.125所以如果我们有很多文档，我们只是将它们连接在一起，然后说，00:44:17.125 --> 00:44:19.014好吧，这是10亿字，00:44:19.014 --> 00:44:21.745一大堆单词。00:44:21.745 --> 00:44:23.305所以我们要做的是，00:44:23.305 --> 00:44:26.875是我们要做的第一个嗯产品00:44:26.875 --> 00:44:30.954通读所有单词，然后再看第二个产品，00:44:30.954 --> 00:44:34.630我们会说-我们会选择固定大小的窗户，你知道，00:44:34.630 --> 00:44:37.990可能每边都有五个字，或者什么的，我们要试着00:44:37.990 --> 00:44:42.010预测围绕中心词的10个词。00:44:42.010 --> 00:44:44.200我们要预测的是00:44:44.200 --> 00:44:46.780根据中心词预测那个词。00:44:46.780 --> 00:44:48.460这就是我们的概率模型。00:44:48.460 --> 00:44:51.175所以如果我们把所有的东西都乘起来，00:44:51.175 --> 00:44:54.610这就是我们的模式，很可能是一份好工作。00:44:54.610 --> 00:44:58.375预测每一个词周围的词。00:44:58.375 --> 00:45:01.600这个模型的可能性将取决于00:45:01.600 --> 00:45:05.185关于我们模型的参数，我们写为θ。00:45:05.185 --> 00:45:07.855在这个特殊的模型中，00:45:07.855 --> 00:45:10.690其中唯一的参数实际上是00:45:10.690 --> 00:45:13.810我们给出的是向量表示。00:45:13.810 --> 00:45:16.945这个模型绝对没有其他参数。00:45:16.945 --> 00:45:20.050所以，我们只是说我们代表00:45:20.050 --> 00:45:23.695在向量空间中有向量的单词，以及00:45:23.695 --> 00:45:27.880它的代表性就是它的意义，然后我们将能够00:45:27.880 --> 00:45:32.335用它来预测我将要向你展示的其他单词的发生方式。00:45:32.335 --> 00:45:37.240可以。所以，嗯，这是我们的可能性，所以我们在所有00:45:37.240 --> 00:45:42.280这些模型是我们定义一个目标函数，然后我们将00:45:42.280 --> 00:45:45.880我想提出在00:45:45.880 --> 00:45:50.740使我们的目标函数最小化的方法。00:45:50.740 --> 00:45:56.380所以目标函数基本上和幻灯片上半部分的一样，00:45:56.380 --> 00:45:58.045但我们改变了一些事情。00:45:58.045 --> 00:46:03.040我们在它前面加一个负号，这样我们可以做最小化，而不是最大化。00:46:03.040 --> 00:46:05.515完全武断是没有区别的。00:46:05.515 --> 00:46:08.125嗯，我们把一个T放在它前面，00:46:08.125 --> 00:46:11.800所以我们计算出平均值00:46:11.800 --> 00:46:16.150对每个中心词的选择都有预测的好处。00:46:16.150 --> 00:46:19.360再说一次，这没什么区别，但它有点像00:46:19.360 --> 00:46:23.095事情啊，不取决于语料库的大小。00:46:23.095 --> 00:46:27.235嗯，最重要的是我们在00:46:27.235 --> 00:46:31.690上面的函数，因为一切都会变得很好。00:46:31.690 --> 00:46:33.805所以当你贴上原木找到产品的时候00:46:33.805 --> 00:46:36.370当你做优化之类的事情时。00:46:36.370 --> 00:46:38.860所以，当我们这样做的时候，我们得到了00:46:38.860 --> 00:46:42.430所有这些产品都能让我们改变你所知道的，00:46:42.430 --> 00:46:46.300这个概率的对数之和00:46:46.300 --> 00:46:50.755我们会在一分钟内再做一遍。00:46:50.755 --> 00:46:55.420可以。如果我们能改变的话00:46:55.420 --> 00:47:00.865我们用向量表示这些词，以使θ的j最小化，00:47:00.865 --> 00:47:06.110这意味着我们将擅长在另一个词的上下文中预测单词。00:47:06.540 --> 00:47:10.450那么，这一切听起来都不错，但都是00:47:10.450 --> 00:47:13.960依赖于你想要的概率函数00:47:13.960 --> 00:47:17.020预测一个词在00:47:17.020 --> 00:47:20.635中心词的上下文问题是，00:47:20.635 --> 00:47:23.620你怎么可能做到？00:47:23.620 --> 00:47:28.390嗯，嗯，记住我说的是我们的模特00:47:28.390 --> 00:47:33.655有单词的向量表示，这是模型的唯一参数。00:47:33.655 --> 00:47:35.650现在，这几乎是真的。00:47:35.650 --> 00:47:37.105这不完全正确。00:47:37.105 --> 00:47:39.220嗯，实际上我们有点作弊。00:47:39.220 --> 00:47:42.400因为我们实际上提出了00:47:42.400 --> 00:47:46.600每一个词，这使得做这件事更简单。00:47:46.600 --> 00:47:48.070嗯，你不能这样做，00:47:48.070 --> 00:47:50.620有很多方法可以绕过它，但这是最简单的方法。00:47:50.620 --> 00:47:54.610所以我们有一个向量，当它是预测的中心词时00:47:54.610 --> 00:47:59.500换言之，但当每个词是上下文词时，我们有第二个向量，00:47:59.500 --> 00:48:01.225所以这是上下文中的一个词。00:48:01.225 --> 00:48:02.680所以对于每种单词类型，00:48:02.680 --> 00:48:06.850我们把这两个向量作为中心词，作为上下文词。00:48:06.850 --> 00:48:12.700嗯，那么我们来计算一个词在上下文中的概率，00:48:12.700 --> 00:48:14.574考虑到中心词，00:48:14.574 --> 00:48:22.105纯粹根据这些向量，我们这样做，就是用这个方程，00:48:22.105 --> 00:48:25.165我一会儿再解释。00:48:25.165 --> 00:48:29.650所以我们仍然处于完全相同的情况，对吗？00:48:29.650 --> 00:48:32.050我们想算出00:48:32.050 --> 00:48:35.665出现在中心词上下文中的词。00:48:35.665 --> 00:48:38.785所以中心词是c，上下文词用00:48:38.785 --> 00:48:42.370这些[听不见]的幻灯片符号，但有点，00:48:42.370 --> 00:48:44.890我们基本上是说有一种00:48:44.890 --> 00:48:47.590中心词的矢量是另一种矢量00:48:47.590 --> 00:48:53.665对于上下文词，我们要计算出概率预测，嗯，00:48:53.665 --> 00:48:56.470就这些词向量而言。00:48:56.470 --> 00:48:59.260可以。那我们该怎么做呢？00:48:59.260 --> 00:49:02.950嗯，我们这样做是为了这个嗯，00:49:02.950 --> 00:49:07.870这里的公式是你一次又一次看到的形状，嗯，00:49:07.870 --> 00:49:10.300在深入学习与分类工作人员。00:49:10.300 --> 00:49:12.670所以对于它的中心部分，00:49:12.670 --> 00:49:17.815橙色的比特更多的是相同的事情发生在，嗯，分母。00:49:17.815 --> 00:49:21.130我们要做的是计算一个点积。00:49:21.130 --> 00:49:24.460所以，我们要通过向量的分量，我们要00:49:24.460 --> 00:49:28.750把它们相乘，这意味着如果嗯，00:49:28.750 --> 00:49:32.800不同的词有相同符号的B成分，00:49:32.800 --> 00:49:35.620加或减，在相同的位置，00:49:35.620 --> 00:49:38.920DOT产品会很大，如果00:49:38.920 --> 00:49:42.460他们有不同的标志，或者一个大一个小，00:49:42.460 --> 00:49:44.410DOT产品会小很多。00:49:44.410 --> 00:49:48.100所以橙色部分直接计算，00:49:48.100 --> 00:49:51.670有点像单词之间的相似之处00:49:51.670 --> 00:49:55.345相似性是指向量看起来是一样的，对吧？00:49:55.345 --> 00:49:57.610嗯，这就是它的核心，对吧？00:49:57.610 --> 00:50:00.130所以我们会有相似向量的单词，00:50:00.130 --> 00:50:04.240在向量空间中紧密相连具有相似的意义。00:50:04.240 --> 00:50:06.580嗯，剩下的部分-嗯，00:50:06.580 --> 00:50:10.330接下来我们要做的就是把这个数字加上一个x。00:50:10.330 --> 00:50:12.100所以，嗯，指数00:50:12.100 --> 00:50:15.295这是一个很好的属性，不管你坚持什么数字，00:50:15.295 --> 00:50:17.845因为点积可能是正的或负的，00:50:17.845 --> 00:50:20.890它会变成一个正数，如果00:50:20.890 --> 00:50:24.160我们最终想要得到一个概率，嗯，那真的很好。00:50:24.160 --> 00:50:28.450如果我们有正数而不是负数，那就好了。00:50:28.450 --> 00:50:33.370嗯，第三部分是蓝色的出价是我们想要的00:50:33.370 --> 00:50:36.070概率和概率的总和00:50:36.070 --> 00:50:39.970首先，我们用最标准、最愚蠢的方式来做。00:50:39.970 --> 00:50:42.205我们把这个数量加起来，00:50:42.205 --> 00:50:47.080我们词汇表中的每一个不同的词00:50:47.080 --> 00:50:52.315它使事物正常化，并把它们变成概率分布。00:50:52.315 --> 00:50:54.685是的，所以在实践中，00:50:54.685 --> 00:50:55.990有两部分。00:50:55.990 --> 00:50:59.110有橙色部分，这就是使用00:50:59.110 --> 00:51:03.580点积和向量空间作为词与词之间的相似度度量00:51:03.580 --> 00:51:07.480然后第二部分是我们喂养它的所有其他部分。00:51:07.480 --> 00:51:11.665通过我们一直将新闻称为SoftMax发行版。00:51:11.665 --> 00:51:17.530因此，expen规范化的两部分为您提供了一个softmax分布。00:51:17.530 --> 00:51:22.120嗯，SoftMax函数将把任何数字映射到00:51:22.120 --> 00:51:26.950概率分布总是因为我给出的两个原因，所以，00:51:26.950 --> 00:51:30.000它被称为SoftMax Um，00:51:30.000 --> 00:51:33.525因为它的工作原理就像是一个SoftMax，对吧？00:51:33.525 --> 00:51:35.040所以如果你有数字，00:51:35.040 --> 00:51:39.735你可以说这些数字的最大值是多少，嗯，00:51:39.735 --> 00:51:46.810你知道这有点热-如果你把你的原始数字映射到，00:51:46.810 --> 00:51:49.390如果它是最大值，其他都是零，00:51:49.390 --> 00:51:51.160这有点难。00:51:51.160 --> 00:51:56.935嗯，软-这是一个软最大值，因为指数i-你知道，00:51:56.935 --> 00:52:00.310如果你想象一下，但是-如果我们忽略了这个问题00:52:00.310 --> 00:52:04.315有一段时间是负数，你就不用计算经验了，嗯，00:52:04.315 --> 00:52:06.220那你就有点出来00:52:06.220 --> 00:52:09.640概率分布，但大体上是公平的。00:52:09.640 --> 00:52:12.070平的，不会特别挑出00:52:12.070 --> 00:52:15.310不同的XI数，而当你用幂指数表示它们时，00:52:15.310 --> 00:52:18.670这使得大数字变得更大，所以，00:52:18.670 --> 00:52:25.990这种软最大值主要把质量放在最大值或最大值对的地方。00:52:25.990 --> 00:52:29.920嗯，这是最大部分，而软部分则不是00:52:29.920 --> 00:52:34.900一个艰难的决定仍然会在其他地方传播一点概率质量。00:52:34.900 --> 00:52:40.540好了，现在我们有了，失去功能。00:52:40.540 --> 00:52:45.160我们有一个损失函数，里面有一个概率模型，我们可以00:52:45.160 --> 00:52:50.230所以我们想做的就是，00:52:50.230 --> 00:52:55.690移动单词的矢量表示00:52:55.690 --> 00:53:01.075这样他们就能够很好地预测在其他单词的上下文中会发生什么单词。00:53:01.075 --> 00:53:06.400嗯，现在我们要做的就是优化。00:53:06.400 --> 00:53:10.465所以，我们有不同单词的向量分量。00:53:10.465 --> 00:53:13.180我们又有了一个很高的空间，但是这里，00:53:13.180 --> 00:53:16.270我只有两张照片，我们要00:53:16.270 --> 00:53:19.510说怎么做-我们怎样才能最小化这个函数，我们要00:53:19.510 --> 00:53:23.920希望在中晃动单词表示中使用的数字00:53:23.920 --> 00:53:28.990我们正沿着这个空间的斜坡走下去。00:53:28.990 --> 00:53:32.095我沿着斜坡走，嗯，00:53:32.095 --> 00:53:37.330然后我们要最小化我们找到的单词的良好表示的函数。00:53:37.330 --> 00:53:39.775为了这个案子这么做，00:53:39.775 --> 00:53:42.070我们想在00:53:42.070 --> 00:53:45.400所有参数的高维向量空间00:53:45.400 --> 00:53:48.730我们的模型和这个模型唯一的参数00:53:48.730 --> 00:53:53.095has是单词的向量空间表示。00:53:53.095 --> 00:53:56.170所以如果有100维的单词表示，00:53:56.170 --> 00:53:59.320它们是aardvark和context的100个参数，00:53:59.320 --> 00:54:03.400100个参数用于单词a-in-context和cetera，00:54:03.400 --> 00:54:08.020100个参数，用于将单词aardvark[噪音]作为中心单词et cetera，00:54:08.020 --> 00:54:12.520等等，通过这给了我们一个很大的参数向量00:54:12.520 --> 00:54:18.265优化，我们将运行这个优化，然后，嗯，把它们下移。00:54:18.265 --> 00:54:23.740嗯，[噪音]是的，所以这基本上就是你要做的。00:54:23.740 --> 00:54:26.365嗯，我有点想通过嗯，00:54:26.365 --> 00:54:28.990这个的细节，嗯，00:54:28.990 --> 00:54:32.440只是为了让我们具体地经历一些事情00:54:32.440 --> 00:54:36.070确保所有人都在同一页。00:54:36.070 --> 00:54:39.475嗯，所以我怀疑，你知道，00:54:39.475 --> 00:54:43.510如果我尝试具体地做这件事，00:54:43.510 --> 00:54:45.865嗯，有很多人嗯，00:54:45.865 --> 00:54:50.830这会让人厌烦，有些人会非常讨厌，00:54:50.830 --> 00:54:54.415嗯，所以我为你道歉，00:54:54.415 --> 00:54:55.810但是你知道，00:54:55.810 --> 00:54:59.140我希望并认为00:54:59.140 --> 00:55:02.650有些人最近没做那么多的事情00:55:02.650 --> 00:55:05.740实际上，具体地做这件事可能是件好事00:55:05.740 --> 00:55:09.760让每个人在开始的时候都加快速度。是啊？00:55:09.760 --> 00:55:14.680[听不见]我们如何具体计算[听不见]？00:55:14.680 --> 00:55:20.275好吧，那么，我们-那么我们计算的方式，00:55:20.275 --> 00:55:26.050u和v向量是我们从一个随机向量开始00:55:26.050 --> 00:55:33.010每一个单词，然后我们在学习时迭代地改变这些向量。00:55:33.010 --> 00:55:37.135我们要解决如何改变他们的方法是说，00:55:37.135 --> 00:55:42.400“我想做优化，”这将被实现为可以的。00:55:42.400 --> 00:55:44.830我们有每个词的当前向量。00:55:44.830 --> 00:55:51.550让我做一些微积分来计算我如何改变向量这个词的意思，00:55:51.550 --> 00:55:55.780矢量这个词可以计算出00:55:55.780 --> 00:56:00.160在这个中心词的上下文中实际出现的词。00:56:00.160 --> 00:56:01.855我们会这样做的，00:56:01.855 --> 00:56:03.925我们会一次又一次地做，00:56:03.925 --> 00:56:06.760最终会得到好的词向量。00:56:06.760 --> 00:56:08.260谢谢你的提问，00:56:08.260 --> 00:56:10.780因为这是一个你应该理解的概念。00:56:10.780 --> 00:56:13.330是这样吗？也许我没有00:56:13.330 --> 00:56:16.645把那个高级菜谱解释清楚，是的。00:56:16.645 --> 00:56:20.410好吧，那么好吧，让我们来看看。我们已经看到了，对吧？00:56:20.410 --> 00:56:24.070所以，我们有一个我们想要最大化的公式，00:56:24.070 --> 00:56:32.410我们原来的函数，是t的乘积，等于t，00:56:32.410 --> 00:56:35.995然后是单词的产物，呃，00:56:35.995 --> 00:56:40.720位置减去m小于或等于j，00:56:40.720 --> 00:56:42.460小于或等于m，00:56:42.460 --> 00:56:46.000j不等于零，嗯，00:56:46.000 --> 00:56:51.640素数为t时w的概率00:56:51.640 --> 00:56:57.700加上j，根据我们模型的参数给出wt。00:56:57.700 --> 00:57:01.330好吧，然后我们已经看到我们要把它转换成00:57:01.330 --> 00:57:05.515我们要用到的函数中，θ的j，00:57:05.515 --> 00:57:15.490其中t和的负1等于m和的1到t，00:57:15.490 --> 00:57:17.770小于或等于j小于或等于m，00:57:17.770 --> 00:57:27.400j不等于w乘以t的概率对数的零，加上j，w，00:57:27.400 --> 00:57:31.840T.好吧，我们有了，然后我们有了00:57:31.840 --> 00:57:36.490这个公式，外边词的概率给定00:57:36.490 --> 00:57:46.360上下文词就是我们刚刚通过的这个公式00:57:46.360 --> 00:57:56.770w的和等于xu wt vc的词汇大小。00:57:56.770 --> 00:57:59.530好吧，这就是我们的模型。00:57:59.530 --> 00:58:03.835我们想把这个最小化。00:58:03.835 --> 00:58:11.230所以，我们想最小化这个，我们想通过改变这些参数来最小化这个。00:58:11.230 --> 00:58:15.415这些参数就是这些向量的内容。00:58:15.415 --> 00:58:17.635所以，我们现在想做的，00:58:17.635 --> 00:58:23.560是做微积分，我们想说，让我们来计算这些参数，它们是，00:58:23.560 --> 00:58:25.960u和v向量，嗯，00:58:25.960 --> 00:58:30.115对于我们随机初始化的参数的当前值。00:58:30.115 --> 00:58:32.050空间的坡度是多少？00:58:32.050 --> 00:58:33.490下坡在哪里？00:58:33.490 --> 00:58:35.770因为如果我们能走下坡路，00:58:35.770 --> 00:58:39.115我们只要走下坡路，我们的模型就会好起来。00:58:39.115 --> 00:58:42.010所以，我们要用衍生工具计算出00:58:42.010 --> 00:58:45.610下坡的方向是，然后我们想走那条路，是的。00:58:45.610 --> 00:58:50.230那么，为什么我们要最大化可能的优势，就像，00:58:50.230 --> 00:58:51.805就像通读每一个字，00:58:51.805 --> 00:58:57.640就像[听不见]给[听不见]00:58:57.640 --> 00:58:59.665所以，好吧，那么，那么，00:58:59.665 --> 00:59:02.905我想达到这个目的，嗯，00:59:02.905 --> 00:59:08.395对于我的意义分配概念，我想实现的是，00:59:08.395 --> 00:59:11.500我有一个有意义的词，一个向量。00:59:11.500 --> 00:59:16.810这个向量知道在00:59:16.810 --> 00:59:19.525嗯，一句话。00:59:19.525 --> 00:59:23.020并且知道在它的上下文中出现的单词意味着什么，00:59:23.020 --> 00:59:24.790它能准确地给出00:59:24.790 --> 00:59:28.945对上下文中出现的单词的高概率估计，00:59:28.945 --> 00:59:32.319它会给出低概率的估计00:59:32.319 --> 00:59:35.050对于上下文中通常不会出现的单词。00:59:35.050 --> 00:59:37.240所以，你知道，如果这个词是bank，00:59:37.240 --> 00:59:39.549我希望像树枝这样的词，00:59:39.549 --> 00:59:41.575开放和退出，00:59:41.575 --> 00:59:43.360很有可能，00:59:43.360 --> 00:59:45.445因为它们往往出现在单词bank中。00:59:45.445 --> 00:59:49.950我希望还有别的话，嗯，00:59:49.950 --> 00:59:52.740像神经网络之类的00:59:52.740 --> 00:59:57.970概率较低，因为它们不容易出现在单词库中。00:59:58.290 --> 01:00:01.525好吧，嗯，这有道理吗？01:00:01.525 --> 01:00:01.780是啊。01:00:01.780 --> 01:00:03.730是啊。另一件事是，01:00:03.730 --> 01:00:06.865我忘了要说的是，你知道，很明显，01:00:06.865 --> 01:00:10.480我们不能做得太好，否则就是做不好，01:00:10.480 --> 01:00:13.180我们可以说上下文中的所有单词01:00:13.180 --> 01:00:15.880这个词的概率是0.97，对吗？01:00:15.880 --> 01:00:19.750因为我们用的是一个简单的概率分布01:00:19.750 --> 01:00:23.230预测我们上下文中的所有单词。01:00:23.230 --> 01:00:27.880所以，特别是，我们用它来预测10个不同的词，对吗？01:00:27.880 --> 01:00:32.425所以，充其量，我们可以给其中一个百分之五的机会，对吧？01:00:32.425 --> 01:00:33.820我们不可能，01:00:33.820 --> 01:00:35.950所以每次都猜对了。01:00:35.950 --> 01:00:37.390嗯，嗯，你知道，01:00:37.390 --> 01:00:40.255它们将是不同的上下文，包含不同的单词。01:00:40.255 --> 01:00:44.610所以，你知道，这将是一个非常宽松的模型，01:00:44.610 --> 01:00:48.660但不管怎样，我们要抓住这个事实，你知道，01:00:48.660 --> 01:00:51.330退出的可能性更大，嗯，01:00:51.330 --> 01:00:57.580发生在世界银行附近，而不是足球。01:00:57.580 --> 01:01:01.030那就是，你知道，基本上我们的目标是什么。01:01:01.030 --> 01:01:07.360好的，嗯，是的，所以我们想最大化这个，01:01:07.360 --> 01:01:12.610通过最小化，这意味着我们要做一些微积分来解决这个问题。01:01:12.610 --> 01:01:14.740所以，我们接下来要做的是，01:01:14.740 --> 01:01:16.720我们要说的是，01:01:16.720 --> 01:01:19.495这些参数是我们的词向量01:01:19.495 --> 01:01:22.630我们要移动这些词向量，01:01:22.630 --> 01:01:28.180嗯，到，嗯，想办法，嗯，走下坡路。01:01:28.180 --> 01:01:32.440所以，我现在要做的是研究01:01:32.440 --> 01:01:38.275这个中心词vc，并计算出如何做与之相关的事情。01:01:38.275 --> 01:01:40.750嗯，现在，这不是你唯一想做的事，01:01:40.750 --> 01:01:44.905你还需要计算出相对于uo向量的斜率。01:01:44.905 --> 01:01:47.965嗯，但我不会那样做，因为上课时间快用完了。01:01:47.965 --> 01:01:49.750所以，如果你在01:01:49.750 --> 01:01:51.715回家后你会觉得更有能力。01:01:51.715 --> 01:01:57.130好吧，那么，嗯，我想让你做的是求偏导数01:01:57.130 --> 01:02:03.205关于这个量的vc向量表示，01:02:03.205 --> 01:02:04.810我们只是在看。01:02:04.810 --> 01:02:08.290也就是这里的数量，01:02:08.290 --> 01:02:11.980嗯，我们把那个数量的对数记在哪里。01:02:11.980 --> 01:02:17.560对，x的对数，01:02:17.560 --> 01:02:20.140O，T，V，C，01:02:20.140 --> 01:02:26.830在w的和上等于u的x的1到v，01:02:26.830 --> 01:02:30.220O，T，V，C。好的，01:02:30.220 --> 01:02:33.219所以这个，嗯，现在我们有了一个部门的记录，01:02:33.219 --> 01:02:35.695很容易重写，嗯，01:02:35.695 --> 01:02:39.595我们得到了对数的偏导数01:02:39.595 --> 01:02:47.560分子减号和01:02:47.560 --> 01:02:49.690我可以分布偏导数。01:02:49.690 --> 01:02:53.395所以，我可以得到负的偏导数，01:02:53.395 --> 01:02:56.680嗯，分母的，01:02:56.680 --> 01:02:59.710嗯，这是这件事的记录。01:02:59.710 --> 01:03:08.865[噪音]01:03:08.865 --> 01:03:18.190可以。嗯，这是分子，这是分母。01:03:19.190 --> 01:03:27.060可以。所以，嗯，分子的部分很容易。01:03:27.060 --> 01:03:29.130事实上，也许我可以把它放在这里。01:03:29.130 --> 01:03:33.450嗯，所以登录exp只是彼此的反比，01:03:33.450 --> 01:03:34.800所以他们取消了。01:03:34.800 --> 01:03:43.650所以，我们得到了u，t，v，c的偏导数。01:03:43.650 --> 01:03:47.460好吧，这一点我应该，嗯，只是，嗯，01:03:47.460 --> 01:03:51.630提醒人们，这里的向量是-um，01:03:51.630 --> 01:03:56.130它仍然是一个向量，因为我们有一个单词的100维表示。01:03:56.130 --> 01:04:00.330嗯，这就是多元微积分。01:04:00.330 --> 01:04:02.790嗯，所以你知道，如果你是，01:04:02.790 --> 01:04:04.530如果你能，嗯，01:04:04.530 --> 01:04:06.105记住这些东西，01:04:06.105 --> 01:04:08.175你可以说，“哈，这是微不足道的”。01:04:08.175 --> 01:04:12.390答案是你完成了，嗯，那太好了。01:04:12.390 --> 01:04:14.955但你知道，如果你，嗯，感觉，嗯，01:04:14.955 --> 01:04:17.550这些东西都不怎么好，嗯，01:04:17.550 --> 01:04:19.125你想，嗯，01:04:19.125 --> 01:04:22.440有点不小心，试着弄清楚它是什么，01:04:22.440 --> 01:04:24.180嗯，你可以说，01:04:24.180 --> 01:04:25.980“好吧，让我呃，01:04:25.980 --> 01:04:28.380求出偏导数，01:04:28.380 --> 01:04:34.200嗯，关于这个向量的一个元素，就像这个向量的第一个元素。01:04:34.200 --> 01:04:42.869好吧，我在这里得到的这个点积是，我有一个乘以一个V。01:04:42.869 --> 01:04:49.560加上u-o两次v-c二次加上点，点，01:04:49.560 --> 01:04:56.910圆点加上U_o 100乘以V_c 100，对吧，01:04:56.910 --> 01:05:02.535我找到了这个关于v_c one的偏导数，01:05:02.535 --> 01:05:05.490希望还记得高中的微积分01:05:05.490 --> 01:05:09.135这些术语中没有一个涉及到vu-c-one。01:05:09.135 --> 01:05:12.660唯一剩下的就是这个U-O，01:05:12.660 --> 01:05:15.960这就是我对这个维度的理解。01:05:15.960 --> 01:05:17.850所以，这个特殊的参数。01:05:17.850 --> 01:05:23.265但我不仅想做V_c向量的第一个分量，01:05:23.265 --> 01:05:26.745我还想做V_c矢量等的第二个分量，01:05:26.745 --> 01:05:30.630这意味着我最终会和他们所有人在一起01:05:30.630 --> 01:05:35.685出现在这些事情中的一个。01:05:35.685 --> 01:05:41.190最终的结果是我得到了向量u_。01:05:41.190 --> 01:05:43.620可以。但是你知道，01:05:43.620 --> 01:05:47.220如果你有点困惑，你的大脑也在崩溃，01:05:47.220 --> 01:05:52.050我认为把事情再简化到某种程度上是有用的，01:05:52.050 --> 01:05:58.275单维微积分，实际上有点玩弄实际发生的事情。01:05:58.275 --> 01:06:00.840嗯，不管怎样，这部分很简单。01:06:00.840 --> 01:06:03.540分子，我们得到了。01:06:03.540 --> 01:06:08.085嗯，所以当我们做分母的时候事情就不太好了。01:06:08.085 --> 01:06:11.640所以我们现在想要这个，嗯，B&D，01:06:11.640 --> 01:06:17.010w和的对数的v_c等于01:06:17.010 --> 01:06:22.845一个到U-O-T-V-C的P-X。01:06:22.845 --> 01:06:25.800可以。所以，现在在这一点上，01:06:25.800 --> 01:06:27.450我不太漂亮。01:06:27.450 --> 01:06:31.035我们有这个对数和x的组合，你会看到很多，01:06:31.035 --> 01:06:35.640所以在这一点上，你必须记住，有e，链式法则。01:06:35.640 --> 01:06:38.520可以。所以，我们可以说，这是你知道的，01:06:38.520 --> 01:06:42.540我们的函数f，这里是函数体，01:06:42.540 --> 01:06:46.245所以我们想做的是，01:06:46.245 --> 01:06:48.630分两个阶段进行。01:06:48.630 --> 01:06:51.570嗯，所以在一天结束的时候，01:06:51.570 --> 01:06:53.430我们最后有了这个V&U C。01:06:53.430 --> 01:06:57.105所以，我们这里有一些函数。01:06:57.105 --> 01:06:59.910最终有一个v_c的函数，01:06:59.910 --> 01:07:02.220所以我们要用链式法则。01:07:02.220 --> 01:07:05.040我们要说的是，连锁规则是我们首先01:07:05.040 --> 01:07:09.135这个外物的导数，放入这个身体，01:07:09.135 --> 01:07:13.680然后我们记得对数的导数是x上的。01:07:13.680 --> 01:07:22.920所以，我们有一个除以w的和等于u-o-t-v-c的exp的1-v01:07:22.920 --> 01:07:26.640然后我们需要把它乘以01:07:26.640 --> 01:07:32.610内部部分的导数，是。01:07:32.610 --> 01:07:34.750我们这里有什么。01:07:40.490 --> 01:07:44.850可以。乘以内部部分的导数01:07:44.850 --> 01:07:48.600重要的提醒是你需要改变变量，01:07:48.600 --> 01:07:53.460对于内部部分，使用您要求和的另一个变量。01:07:53.460 --> 01:08:00.810可以。所以，现在我们要求x和的导数。01:08:00.810 --> 01:08:05.055我们能做的第一件事是V-非常简单。01:08:05.055 --> 01:08:08.865我们可以把导数移到和里面。01:08:08.865 --> 01:08:14.430所以，我们可以重写它，得到x的和的第一等于01:08:14.430 --> 01:08:20.430关于[听不见]的偏导数的v。01:08:20.430 --> 01:08:22.575嗯，这有点进步。01:08:22.575 --> 01:08:26.730嗯，这一点，我们必须再做一次链式法则，对吧。01:08:26.730 --> 01:08:33.210所以，这是我们的函数，这是它里面的东西，它是v_c的函数。01:08:33.210 --> 01:08:37.605所以，我们再一次想，嗯，链式法则。01:08:37.605 --> 01:08:41.340所以我们就有了好的，01:08:41.340 --> 01:08:45.720x-um的导数是exp。01:08:45.720 --> 01:08:54.630那么，x的和等于u，x，v，c的exp的和，01:08:54.630 --> 01:09:00.150然后用偏导数乘以01:09:00.150 --> 01:09:05.700考虑到内部u x t v_c的t v_c。01:09:05.700 --> 01:09:08.160嗯，我们以前看到过，所以，01:09:08.160 --> 01:09:13.200它的导数是u-嗯，01:09:13.200 --> 01:09:16.320是的，因为我们是通过一个不同的X来做的，对吧。01:09:16.320 --> 01:09:18.780这就变成了u_x，01:09:18.780 --> 01:09:23.850所以x的和等于01:09:23.850 --> 01:09:30.030这个表达式的v x t b c乘以x的u。01:09:30.030 --> 01:09:34.995可以。所以，通过做两次链式法则，我们就得到了。01:09:34.995 --> 01:09:38.190所以，现在如果我们把它放在一起，你知道，01:09:38.190 --> 01:09:43.050V_c关于整体的导数，01:09:43.050 --> 01:09:46.500给定c的概率的对数，对吧。01:09:46.500 --> 01:09:51.210对于分子来说，它只是一个u o，01:09:51.210 --> 01:09:54.030然后减法，01:09:54.030 --> 01:09:57.645我们这里有这个词，嗯，01:09:57.645 --> 01:09:59.730这是一种分母，01:09:59.730 --> 01:10:03.870我们这里有一个术语，分子。01:10:03.870 --> 01:10:07.725所以，我们要减去分子，01:10:07.725 --> 01:10:12.270我们得到x的和等于01:10:12.270 --> 01:10:18.765u x t v_c的exp乘以u x，01:10:18.765 --> 01:10:25.395然后在分母中，我们有，嗯，01:10:25.395 --> 01:10:35.920w的和等于u-w-t-v-c的exp的1到v。01:10:36.350 --> 01:10:40.035嗯，好吧，我们有点明白了。01:10:40.035 --> 01:10:44.025哦，等等。是啊。是的，我明白了。01:10:44.025 --> 01:10:45.900是的，没错。嗯，好吧。01:10:45.900 --> 01:10:52.170我们得到了，然后我们可以重新安排一下。01:10:52.170 --> 01:10:56.475所以，我们可以直接得到这个总数，01:10:56.475 --> 01:11:03.284我们可以说这是一个很大的和，x等于1:v，01:11:03.284 --> 01:11:09.870我们可以把你的话拿出来说，好吧。01:11:09.870 --> 01:11:13.155让我们把它叫做“U”，01:11:13.155 --> 01:11:15.090如果我们这样做，01:11:15.090 --> 01:11:20.295发生了一件有趣的事情，因为你看这里，01:11:20.295 --> 01:11:25.830我们重新发现了完全相同的形式01:11:25.830 --> 01:11:31.425我们用它作为概率分布来预测单词的概率。01:11:31.425 --> 01:11:37.860所以，根据我们的模型，这是x给定c的概率。01:11:37.860 --> 01:11:46.155嗯，我们可以重写这个，然后说我们得到的是u减去01:11:46.155 --> 01:11:54.795x等于给定c乘以u x的x的概率的1到v。01:11:54.795 --> 01:11:58.755如果你仔细想想，这有一种有趣的意义。01:11:58.755 --> 01:12:01.365所以，这实际上是给我们，你知道，01:12:01.365 --> 01:12:04.200我们在这个多维空间的斜坡01:12:04.200 --> 01:12:07.215我们是如何得到这个斜坡的01:12:07.215 --> 01:12:11.280观察到的01:12:11.280 --> 01:12:18.450上下文词，我们从中减去我们的模型的想法，嗯，01:12:18.450 --> 01:12:20.955上下文应该是这样的。01:12:20.955 --> 01:12:24.465模型认为上下文应该是什么样的？01:12:24.465 --> 01:12:27.330这里的这一部分在预期中是正式的。01:12:27.330 --> 01:12:31.395所以，你要做的是找到加权平均数01:12:31.395 --> 01:12:36.375每一个词的表达模式，01:12:36.375 --> 01:12:39.990乘以当前模型中的概率。01:12:39.990 --> 01:12:45.315所以，根据我们当前的模型，这是一种预期的上下文词，01:12:45.315 --> 01:12:47.460所以我们要考虑01:12:47.460 --> 01:12:52.170出现的预期上下文词和实际上下文词，01:12:52.170 --> 01:12:55.560然后这一区别就证明了01:12:55.560 --> 01:12:58.890我们应该朝哪个方向倾斜01:12:58.890 --> 01:13:01.050走着换字01:13:01.050 --> 01:13:06.715为了提高模型的预测能力。01:13:06.715 --> 01:13:11.565可以。嗯，我们会的，01:13:11.565 --> 01:13:14.100嗯，任务二，嗯，是的。01:13:14.100 --> 01:13:18.060所以，嗯，这对你们来说是个很好的锻炼，01:13:18.060 --> 01:13:20.115嗯，到，嗯，01:13:20.115 --> 01:13:22.830要想为中央情报局做这件事，等等，01:13:22.830 --> 01:13:26.640嗯，我做的中心词也试图寻找上下文词01:13:26.640 --> 01:13:31.125向你展示你可以做同样的数学，然后让它出来。01:13:31.125 --> 01:13:35.655嗯，如果我最后还有几分钟的话。01:13:35.655 --> 01:13:43.320嗯，我只是想告诉你，如果我能把所有这些都做好的话。01:13:43.320 --> 01:13:49.950嗯，我们走这边[听不见]。01:13:49.950 --> 01:13:53.590好的，找到我的。01:13:54.200 --> 01:14:00.075可以。嗯，我只是想给你举个简单的例子。01:14:00.075 --> 01:14:01.935所以，对于第一个任务，01:14:01.935 --> 01:14:04.170嗯，又是一本ipython笔记本。01:14:04.170 --> 01:14:09.015所以，如果你都设置好了，你可以做一个Jupyter笔记本。01:14:09.015 --> 01:14:12.945嗯，你还有一些笔记本。01:14:12.945 --> 01:14:16.630嗯，这是我的小笔记本，我要给你看，01:14:17.180 --> 01:14:23.620嗯，诀窍是让这个足够大，人们可以看到它。01:14:30.940 --> 01:14:35.530可读吗？[笑声]好吧，嗯，01:14:35.530 --> 01:14:39.210所以对了，所以麻木是一种，01:14:39.210 --> 01:14:41.930嗯，用python做数学包。01:14:41.930 --> 01:14:43.120你会想知道的。01:14:43.120 --> 01:14:44.445如果你不知道的话。01:14:44.445 --> 01:14:46.440嗯，Matplotlib有点，01:14:46.440 --> 01:14:49.040最基本的绘图包之一01:14:49.040 --> 01:14:51.755如果你不知道，你会想知道的。01:14:51.755 --> 01:14:55.900这有点像伊普西顿或朱彼特的特色01:14:55.900 --> 01:14:59.755让你有一个互动的Matplotlib um，里面。01:14:59.755 --> 01:15:03.675如果你想得到幻想，你可以玩它-玩你的图形风格。01:15:03.675 --> 01:15:06.615嗯，就是这样。01:15:06.615 --> 01:15:10.465Scikit学习是一种通用的机器学习包。01:15:10.465 --> 01:15:13.350嗯，根西姆不是一个深入的学习包。01:15:13.350 --> 01:15:17.590gensim是一种单词相似度包，它从um开始，01:15:17.590 --> 01:15:20.760用隐式狄氏分析等方法。01:15:20.760 --> 01:15:22.530如果你从模特的话知道的话01:15:22.530 --> 01:15:25.940类似的是成长为一个好的包裹嗯，01:15:25.940 --> 01:15:28.570做，嗯，单词向量也是。01:15:28.570 --> 01:15:31.650所以，它经常用于单词向量和01:15:31.650 --> 01:15:36.105相似性这个词在大规模的工作中是有效的。01:15:36.105 --> 01:15:37.720嗯，是的。01:15:37.720 --> 01:15:41.360所以，我还没告诉你下次威尔的事01:15:41.360 --> 01:15:46.400我们自己的词汇载体形式，即手套式词汇载体。01:15:46.400 --> 01:15:51.270我使用它们不是因为它对我展示的东西很重要，但是你知道，01:15:51.270 --> 01:15:55.745这些向量很小。01:15:55.745 --> 01:16:00.470事实证明，Facebook和谷歌01:16:00.470 --> 01:16:05.940分布是非常大的词汇和非常高的维度。01:16:05.940 --> 01:16:08.940所以花我太长时间装进去01:16:08.940 --> 01:16:12.860在这节课的最后五分钟01:16:12.860 --> 01:16:16.865在我们的斯坦福向量中，我们有100个维向量，嗯，01:16:16.865 --> 01:16:19.160以及50个维度向量01:16:19.160 --> 01:16:21.755坦率地说，在笔记本电脑上做一些小事情很好。01:16:21.755 --> 01:16:27.330嗯，所以，我在这里做的是Gensim不支持本地人01:16:27.330 --> 01:16:30.210手套向量，但它们实际上提供了一个实用程序01:16:30.210 --> 01:16:33.390将手套文件格式转换为Word2vec文件格式。01:16:33.390 --> 01:16:40.285所以我做到了。然后我加载了一个预先训练过的单词向量模型。01:16:40.285 --> 01:16:44.430这就是他们所说的键控向量。01:16:44.430 --> 01:16:46.890所以，键控向量并不是什么稀奇的东西。01:16:46.890 --> 01:16:51.660只是你有土豆之类的词，每个词都有一个向量。01:16:51.660 --> 01:16:55.435所以它实际上就是一本大字典，每件事都有一个向量。01:16:55.435 --> 01:16:58.690但是，这个模型是一个经过训练的模型01:16:58.690 --> 01:17:02.230我们只是使用我们研究的算法，01:17:02.230 --> 01:17:06.730你知道，经过数十亿次的训练来摆弄我们的词汇载体。01:17:06.730 --> 01:17:11.255嗯，一旦我们有了，我们就可以，嗯，01:17:11.255 --> 01:17:14.265问一些问题，比如，我们可以说，01:17:14.265 --> 01:17:17.110与其他词最相似的词是什么？01:17:17.110 --> 01:17:19.650所以我们可以拿一些，嗯，01:17:19.650 --> 01:17:23.175我们来说说，与奥巴马最相似的词是什么？01:17:23.175 --> 01:17:25.770我们回到营房，布什，克林顿，01:17:25.770 --> 01:17:29.040麦凯恩、戈尔、希拉里·多尔、马丁、亨利。01:17:29.040 --> 01:17:31.425这似乎有点有趣。01:17:31.425 --> 01:17:34.050这些因素来自几年前。01:17:34.050 --> 01:17:37.150所以我们没有后奥巴马的工作人员。01:17:37.150 --> 01:17:40.750我的意思是如果你换个词，嗯，你知道，01:17:40.750 --> 01:17:44.100我们可以放进香蕉之类的东西，然后我们得到椰子，01:17:44.100 --> 01:17:46.600芒果，香蕉，土豆，菠萝。01:17:46.600 --> 01:17:49.430我们有热带食物。01:17:49.430 --> 01:17:54.070所以，你可以-你可以问呃，01:17:54.070 --> 01:17:56.995因为与言语不同。01:17:56.995 --> 01:17:59.700相异者本身并不是很有用。01:17:59.700 --> 01:18:04.550所以如果我问最相似的问题，我会说，01:18:04.550 --> 01:18:09.290负等于，嗯，香蕉，01:18:09.290 --> 01:18:14.720嗯，我不知道你对什么最不一样的概念，01:18:14.720 --> 01:18:16.620嗯，香蕉是，但你知道，01:18:16.620 --> 01:18:22.655事实上，你自己并没有从中得到任何有用的东西，嗯，01:18:22.655 --> 01:18:28.000因为，嗯，你只是得到这些奇怪的非常罕见的词嗯，01:18:28.000 --> 01:18:31.440当然，嗯，[笑声]不是那些想的人。01:18:31.440 --> 01:18:37.570嗯，但事实证明你可以用这个消极的想法做一些真正有用的事情01:18:37.570 --> 01:18:39.000它是01:18:39.000 --> 01:18:44.175词载体在其最初出现时的高度著名的结果。01:18:44.175 --> 01:18:50.205这就是这个概念，在这个空间里有意义的维度。01:18:50.205 --> 01:18:54.820所以这是最著名的例子，嗯，就是，01:18:54.820 --> 01:18:59.985我们能做的是从单词King开始，然后减去01:18:59.985 --> 01:19:05.355从中可以看出男人的意思，然后我们可以把女人的意思加上去。01:19:05.355 --> 01:19:09.110然后我们可以说，在我们的向量空间中，哪个词是01:19:09.110 --> 01:19:13.060意思和那个词最相似。01:19:13.060 --> 01:19:15.810这将是一种做类比的方式。01:19:15.810 --> 01:19:18.640可以做，嗯，类比，01:19:18.640 --> 01:19:22.050男人是国王，女人是对什么？01:19:22.050 --> 01:19:26.500所以，我们要做的就是说我们想和国王一样01:19:26.500 --> 01:19:31.220还有女人，因为她们都是积极的，离男人很远。01:19:31.220 --> 01:19:35.185所以，我们可以手动操作，01:19:35.185 --> 01:19:36.950这是手工说的，01:19:36.950 --> 01:19:41.050最相似的正面女性国王，负面男性。01:19:41.050 --> 01:19:45.410我们可以运行它，看到它产生皇后。01:19:45.410 --> 01:19:48.570为了让这更容易一点，我定义了这个类比，01:19:48.570 --> 01:19:53.485嗯，类比谓词，这样我可以运行其他谓词。01:19:53.485 --> 01:19:59.095所以我可以用另一个类似日本日语的例子，01:19:59.095 --> 01:20:01.160奥地利是奥地利人。01:20:01.160 --> 01:20:03.125嗯，你知道，01:20:03.125 --> 01:20:07.150我认为当人们首先01:20:07.150 --> 01:20:10.950看到你可以有一个简单的数学并运行它，01:20:10.950 --> 01:20:12.950学习单词的含义。01:20:12.950 --> 01:20:18.465我的意思是，这实际上有点让人们觉得这是多么有效。01:20:18.465 --> 01:20:22.030你知道的。就像那里-这里没有镜子和绳子，对吧？01:20:22.030 --> 01:20:24.225你知道不是我有一个单独的-01:20:24.225 --> 01:20:28.330在我的python中有一个特殊的列表，我在其中查找困难，01:20:28.330 --> 01:20:30.240呃，奥地利人，01:20:30.240 --> 01:20:31.910嗯，还有类似的事情。01:20:31.910 --> 01:20:35.310但是这些矢量表示01:20:35.310 --> 01:20:38.760这样，它实际上是对这些语义关系进行编码，01:20:38.760 --> 01:20:40.920你知道，所以你可以尝试不同的，01:20:40.920 --> 01:20:43.360你知道，不是只有这一个有效。01:20:43.360 --> 01:20:46.185我可以把法语说成法语。01:20:46.185 --> 01:20:49.780我可以放在德国，上面写着德语，01:20:49.780 --> 01:20:54.590我可以把它放在澳大利亚而不是奥地利，上面写着澳大利亚，01:20:54.590 --> 01:20:59.480你知道，如果你想用向量表示01:20:59.480 --> 01:21:04.815像理解单词之间的关系这样的想法，01:21:04.815 --> 01:21:10.595你只是在100维的数字上做向量空间操作，01:21:10.595 --> 01:21:15.830它实际上知道它们。这不仅是词义的相似性，而且01:21:15.830 --> 01:21:18.255实际上不同的语义关系01:21:18.255 --> 01:21:21.635在诸如国名和他们的民族等词之间。01:21:21.635 --> 01:21:23.850是的，这真是太神奇了。01:21:23.850 --> 01:21:31.340真的，你知道，在嗯上运行这样一个愚蠢的算法有点奇怪，01:21:31.340 --> 01:21:35.100数字矢量可以很好地捕捉单词的含义。01:21:35.100 --> 01:21:38.160这就成了很多种类的基础01:21:38.160 --> 01:21:41.355现代分布式语言的神经表示。01:21:41.355 --> 01:21:42.630好的，我会停在那儿。01:21:42.630 --> 01:21:46.700
Thanks a lot guys and see you on Thursday. [NOISE]

