WEBVTT
Kind: captions
Language: en

00:00:05.210 --> 00:00:07.950大家好。我是艾比，00:00:07.950 --> 00:00:09.544我是这个班的助教组长00:00:09.544 --> 00:00:12.510我也是斯坦福大学NLP小组的博士生。00:00:12.510 --> 00:00:14.670今天我要告诉你00:00:14.670 --> 00:00:17.040语言模型和循环神经网络。00:00:17.040 --> 00:00:19.980下面是我们今天要做的事情的概述。00:00:19.980 --> 00:00:24.885今天，首先，我们将介绍一个新的NLP任务，即语言建模，00:00:24.885 --> 00:00:29.504这将激励我们学习新的神经网络家族，00:00:29.504 --> 00:00:32.730这就是反复出现的神经网络或RNN。00:00:32.730 --> 00:00:34.440所以，我想说这是00:00:34.440 --> 00:00:37.415剩下的课程你将学习的最重要的想法。00:00:37.415 --> 00:00:41.070所以，我们今天要介绍一些相当酷的材料。00:00:41.070 --> 00:00:44.305那么，让我们从语言建模开始。00:00:44.305 --> 00:00:48.775语言建模是预测下一个单词的任务。00:00:48.775 --> 00:00:52.230所以，根据这段课文，学生们打开了空白处，00:00:52.230 --> 00:00:56.200有人能喊出一个你认为下一个可能会出现的词吗？00:00:58.370 --> 00:00:59.550目的。[噪音]。00:00:59.550 --> 00:01:03.840[重叠]注意，还有什么？我听不太清楚，00:01:03.840 --> 00:01:06.720但是，呃，是的，这些都是可能的，对吧？00:01:06.720 --> 00:01:08.150所以，这些是我想的，00:01:08.150 --> 00:01:09.490学生们可能会开学，呃，00:01:09.490 --> 00:01:11.200学生们打开书本，似乎很有可能。00:01:11.200 --> 00:01:13.280学生们打开笔记本电脑，00:01:13.280 --> 00:01:15.040学生们开始考试，00:01:15.040 --> 00:01:16.580学生们敞开心扉，难以置信，00:01:16.580 --> 00:01:18.700有人想出了一个，刚才那个，00:01:18.700 --> 00:01:20.330嗯，这是开场白的隐喻意义。00:01:20.330 --> 00:01:23.675所以，你们现在都在执行语言建模。00:01:23.675 --> 00:01:25.475想想下一个词是什么，00:01:25.475 --> 00:01:27.250你是一个语言模型。00:01:27.250 --> 00:01:31.385这里有一个更正式的语言模型定义。00:01:31.385 --> 00:01:34.430给定一个从x1到xt的单词序列，00:01:34.430 --> 00:01:37.340语言模型是一种计算00:01:37.340 --> 00:01:41.200下一个词的概率分布，xt加1。00:01:41.200 --> 00:01:44.194所以，语言模型给出了概率分布，00:01:44.194 --> 00:01:49.067条件概率，即x t加1得到的单词。00:01:49.067 --> 00:01:50.810这里我们假设，xt加100:01:50.810 --> 00:01:53.960可以是固定词汇V中的任何单词w。00:01:53.960 --> 00:01:55.520所以我们假设00:01:55.520 --> 00:01:58.205我们正在考虑的预先定义的单词列表。00:01:58.205 --> 00:02:00.140通过这种方式，您可以查看语言建模00:02:00.140 --> 00:02:01.850作为一种分类任务，00:02:01.850 --> 00:02:04.580因为有很多种可能。00:02:04.580 --> 00:02:09.360我们称这个系统为语言模型。00:02:09.850 --> 00:02:12.050有另一种思维方式00:02:12.050 --> 00:02:13.715关于语言模型。00:02:13.715 --> 00:02:15.200你可以想到一个语言模型00:02:15.200 --> 00:02:19.060把概率分配给一段文字的系统。00:02:19.060 --> 00:02:21.470例如，如果我们有一段文字，00:02:21.470 --> 00:02:23.180x到x大写t，00:02:23.180 --> 00:02:25.040那么，这篇文章的概率00:02:25.040 --> 00:02:27.830根据语言模型可以分解。00:02:27.830 --> 00:02:29.250所以，根据定义，00:02:29.250 --> 00:02:31.250你可以说概率等于，00:02:31.250 --> 00:02:34.530所有这些条件概率的乘积。00:02:34.530 --> 00:02:37.375还有，呃，里面的表格，00:02:37.375 --> 00:02:40.480产品正是语言模型所提供的。00:02:40.480 --> 00:02:42.465所以，你可以认为这些东西是等价的。00:02:42.465 --> 00:02:44.720预测下一个单词，给你一个系统，00:02:44.720 --> 00:02:49.110这可以给出给定文本的概率。00:02:49.270 --> 00:02:52.745所以，事实上，你每天都使用语言模型。00:02:52.745 --> 00:02:56.120例如，当你在手机上发短信，并且在写信息时，00:02:56.120 --> 00:02:57.610如果你有智能手机，00:02:57.610 --> 00:03:00.080它将预测你将要说什么词。00:03:00.080 --> 00:03:01.790所以，如果你说，嗯，我会在-00:03:01.790 --> 00:03:04.250你的电话可能暗示你是指机场或咖啡馆，00:03:04.250 --> 00:03:06.025例如，办公室。00:03:06.025 --> 00:03:08.905每天使用语言模型的另一种情况00:03:08.905 --> 00:03:11.495就是当你在互联网上搜索一些东西时，比如谷歌，00:03:11.495 --> 00:03:12.830然后你开始输入你的查询，00:03:12.830 --> 00:03:15.952然后谷歌会尝试为你完成查询，这就是语言建模。00:03:15.952 --> 00:03:19.020它在预测下一个词或词会是什么。00:03:20.480 --> 00:03:23.715所以，这就是语言模型，00:03:23.715 --> 00:03:26.675问题是，你将如何学习语言模型？00:03:26.675 --> 00:03:29.917所以，如果我在学前时代问这个问题，00:03:29.917 --> 00:03:31.730那是几年前的事了，00:03:31.730 --> 00:03:35.005答案是，你将学习一个n-gram语言模型。00:03:35.005 --> 00:03:38.570所以，今天我们首先要学习n-gram语言模型。00:03:38.570 --> 00:03:41.330所以，在我告诉你N-gram语言模型是什么之前，00:03:41.330 --> 00:03:43.160你需要知道N-克是什么。00:03:43.160 --> 00:03:47.905因此，根据定义，n-gram是n个连续单词的一块。00:03:47.905 --> 00:03:50.400例如，一克或一克，00:03:50.400 --> 00:03:52.050只是所有的个别词00:03:52.050 --> 00:03:55.020按照“学生打开-00:03:55.020 --> 00:03:58.810一个两克或两克的单词都是连续的成对单词块，00:03:58.810 --> 00:04:00.980“学生”，“学生打开”，“打开”00:04:00.980 --> 00:04:04.560对于三角函数和四克函数等等。00:04:05.050 --> 00:04:08.575因此，n-gram语言模型的核心思想00:04:08.575 --> 00:04:11.035是为了预测下一个词是什么，00:04:11.035 --> 00:04:12.815你要收集一些统计数据，00:04:12.815 --> 00:04:14.930关于不同n-gram的频率，00:04:14.930 --> 00:04:16.490从某种训练数据中，00:04:16.490 --> 00:04:18.110然后你可以用这些统计数据00:04:18.110 --> 00:04:21.120预测下一个词可能是什么。00:04:21.830 --> 00:04:23.640这里有更多的细节。00:04:23.640 --> 00:04:26.325所以，要建立一个n-gram语言模型，00:04:26.325 --> 00:04:28.490首先你需要做一个简单的假设，00:04:28.490 --> 00:04:30.305这是你的假设。00:04:30.305 --> 00:04:33.350你说下一个词xt加100:04:33.350 --> 00:04:37.535只取决于前面的n-1个词。00:04:37.535 --> 00:04:39.900所以，我们假设，00:04:39.900 --> 00:04:41.650是概率分布，00:04:41.650 --> 00:04:45.020xt加1的条件概率给出了它们后面的所有单词，00:04:45.020 --> 00:04:46.160我们只是简化一下，00:04:46.160 --> 00:04:50.485说它只取决于最后的n-1个词，这就是我们的假设。00:04:50.485 --> 00:04:53.950所以，根据条件概率的定义，00:04:53.950 --> 00:04:55.600我们可以说这个概率，00:04:55.600 --> 00:04:58.385只是两个不同概率的比值。00:04:58.385 --> 00:05:01.180所以，在顶部，你有可能00:05:01.180 --> 00:05:03.220一个特定的N克，在底部我们有00:05:03.220 --> 00:05:06.192得到一个n-1克的概率00:05:06.192 --> 00:05:08.020这有点难读，因为所有的上标00:05:08.020 --> 00:05:11.015但我要举一个例子，在下一张幻灯片上写下单词。00:05:11.015 --> 00:05:15.055可以。这就是下一个词概率的定义，00:05:15.055 --> 00:05:17.140但问题仍然存在，我们如何获得00:05:17.140 --> 00:05:19.980这些n-gram和n-1 gram的概率？00:05:19.980 --> 00:05:22.300所以，答案是，我们要让他们通过00:05:22.300 --> 00:05:25.050在一些大的文本文集中计算它们。00:05:25.050 --> 00:05:26.510所以，我们要估计一下，00:05:26.510 --> 00:05:29.560这些概率只是根据00:05:29.560 --> 00:05:34.190这些特定的n-grams和n-1 grams出现在我们的训练语料库中。00:05:34.410 --> 00:05:37.370可以。下面是一个有一些单词的例子。00:05:37.370 --> 00:05:40.565假设我们试图学习一个4-gram语言模型，00:05:40.565 --> 00:05:42.830假设我们有一段文字，上面写着，00:05:42.830 --> 00:05:44.540“当监察员开始计时时，00:05:44.540 --> 00:05:46.100学生们打开了空白处“，00:05:46.100 --> 00:05:48.895我们试图预测下一个词是什么。00:05:48.895 --> 00:05:51.740所以，因为我们正在学习一个4克的语言模型，00:05:51.740 --> 00:05:55.910一个简单的假设是，下一个词只取决于最后三个词，00:05:55.910 --> 00:05:57.605最后n-1个单词。00:05:57.605 --> 00:06:01.520所以，除了最后几句话，我们将抛弃所有的上下文，00:06:01.520 --> 00:06:03.780也就是说，“学生们打开了他们的。”00:06:03.800 --> 00:06:07.620因此，作为提醒，n-gram语言模型说，00:06:07.620 --> 00:06:08.940下一个词的概率是，00:06:08.940 --> 00:06:13.230词汇表中的某个特定单词w等于我们看到的次数00:06:13.230 --> 00:06:15.510学生打开W除以00:06:15.510 --> 00:06:18.655有时我们看到学生们在训练语料库中打开了他们的。00:06:18.655 --> 00:06:21.440那么，假设在我们的训练语料库中，00:06:21.440 --> 00:06:24.215我们看到“学生打开”这个短语1000次。00:06:24.215 --> 00:06:28.340假设这一点，我们看到“学生打开书本”400次。00:06:28.340 --> 00:06:32.220这意味着下一个单词成为书的概率是0.4。00:06:32.220 --> 00:06:36.810同样的，假设我们看到学生100次开学，00:06:36.810 --> 00:06:39.260这意味着给学生的考试概率00:06:39.260 --> 00:06:41.930打开它们的是0.1。有问题吗？00:06:41.930 --> 00:06:44.900[听不见]。00:06:44.900 --> 00:06:47.010问题是，单词的顺序有关系吗？00:06:47.010 --> 00:06:50.340答案是肯定的，学生开放的顺序很重要。00:06:50.340 --> 00:06:53.190这和“学生开放”不同。00:06:53.190 --> 00:06:56.985所以，我现在想提出的问题是，00:06:56.985 --> 00:07:00.805我们放弃普罗克特的背景是个好主意吗？00:07:00.805 --> 00:07:03.115如果你看看我们的实际例子，00:07:03.115 --> 00:07:06.070例如，当程序启动时钟时，00:07:06.070 --> 00:07:07.850学生们打开了空白处。00:07:07.850 --> 00:07:12.360那么，我们是否认为，考虑到实际情况，书籍或考试更有可能，00:07:12.360 --> 00:07:14.550完整的背景？是的。00:07:14.550 --> 00:07:15.450考试。00:07:15.450 --> 00:07:17.795正确的。考试更有可能是因为学监和00:07:17.795 --> 00:07:20.260时钟意味着这是一个考试场景，所以00:07:20.260 --> 00:07:22.625他们比书本更可能打开考试，00:07:22.625 --> 00:07:24.400除非是开卷考试。00:07:24.400 --> 00:07:26.830嗯，但我认为，总的来说，应该是考试。00:07:26.830 --> 00:07:29.890所以，我们在这里看到的问题是，在训练语料库中，00:07:29.890 --> 00:07:31.240学生们正在开放的事实00:07:31.240 --> 00:07:33.990这意味着它更可能是书而不是考试。00:07:33.990 --> 00:07:36.305因为总的来说，书本比考试更常见。00:07:36.305 --> 00:07:38.565但是如果我们知道上下文是，00:07:38.565 --> 00:07:41.078学监和时钟，然后应该是考试。00:07:41.078 --> 00:07:44.240所以，我在这里强调的是我们简化假设的一个问题。00:07:44.240 --> 00:07:45.860如果我们抛弃太多的上下文，00:07:45.860 --> 00:07:50.455那么，如果我们保持上下文的话，我们就不如预测单词那么好了。00:07:50.455 --> 00:07:54.690可以。所以，这是n-gram语言模型的一个问题。00:07:54.690 --> 00:07:56.810嗯，还有其他问题。00:07:56.810 --> 00:08:00.470那么，呃，这里又是你以前看到的方程。00:08:00.470 --> 00:08:01.880有一个问题我们会打电话给你00:08:01.880 --> 00:08:05.465稀疏性问题是如果上面的数字，00:08:05.465 --> 00:08:08.380分子，如果计数等于零怎么办？00:08:08.380 --> 00:08:11.210那么，如果对于某个特定的单词w，00:08:11.210 --> 00:08:14.450学生们打开的“W”一词在数据中从未出现过。00:08:14.450 --> 00:08:17.240例如，假设学生打开培养皿，00:08:17.240 --> 00:08:19.880非常罕见，而且从未出现在数据中，00:08:19.880 --> 00:08:24.355这意味着我们下一个单词被培养皿的概率是零。00:08:24.355 --> 00:08:27.390这很糟糕，因为这可能很少见，但事实上，00:08:27.390 --> 00:08:29.385一个有效的场景，对吗？00:08:29.385 --> 00:08:31.090例如，如果你是一个生物系的学生。00:08:31.090 --> 00:08:34.085所以，这是个问题，我们称之为稀疏性问题，00:08:34.085 --> 00:08:37.790因为问题是如果我们从未在培训数据中看到过事件发生，00:08:37.790 --> 00:08:41.485然后我们的模型将零概率分配给那个事件。00:08:41.485 --> 00:08:46.415所以，这个问题的一个部分解决方案是，也许我们应该增加一个小的delta，00:08:46.415 --> 00:08:48.290小数字增量到计数，00:08:48.290 --> 00:08:50.420对于词汇表中的每一个词。00:08:50.420 --> 00:08:53.920然后这样，接下来的每一个可能的词，00:08:53.920 --> 00:08:56.250至少有一些小概率。00:08:56.250 --> 00:08:59.089所以，培养皿的概率很小，00:08:59.089 --> 00:09:02.410但是，其他所有可能是坏选择的词也会是这样。00:09:02.410 --> 00:09:05.580所以，这个技术叫做平滑，因为这个想法是，00:09:05.580 --> 00:09:06.945你从一个非常，呃，00:09:06.945 --> 00:09:10.050稀疏概率分布，几乎处处为零，00:09:10.050 --> 00:09:11.550有几个尖峰，00:09:11.550 --> 00:09:13.445呃，作为我们见过的N克，00:09:13.445 --> 00:09:16.100从这一点到更平滑的概率分布00:09:16.100 --> 00:09:19.615每件事都有一个小概率。00:09:19.615 --> 00:09:24.270因此，第二个稀疏性问题可能比第一个问题更严重，00:09:24.270 --> 00:09:28.130如果分母中的数字是零，会发生什么？00:09:28.130 --> 00:09:30.200在我们的例子中，这意味着，00:09:30.200 --> 00:09:34.655如果我们从来没有在训练数据中看到过“学生打开他们的”三角图呢？00:09:34.655 --> 00:09:38.480如果发生这种情况，那么我们甚至无法计算00:09:38.480 --> 00:09:42.820因为我们以前从来没有见过这样的上下文。00:09:42.820 --> 00:09:45.825所以，一个可能的解决办法是00:09:45.825 --> 00:09:48.450如果在语料库中找不到“学生打开他们的”00:09:48.450 --> 00:09:51.940然后你就应该对最后两个词进行调整，00:09:51.940 --> 00:09:53.545而不是最后三个字。00:09:53.545 --> 00:09:55.900所以，现在你看到的时候，00:09:55.900 --> 00:09:58.460嗯，“打开他们的”看看接下来会发生什么。00:09:58.460 --> 00:10:01.350所以，这被取消，因为在这个失败的案例中，00:10:01.350 --> 00:10:04.025因为当你没有4-gram语言模型的数据时，00:10:04.025 --> 00:10:06.020你要退回到一个三角语言模型。00:10:06.020 --> 00:10:09.510现在有什么问题吗？00:10:12.310 --> 00:10:17.570可以。还有一件事要注意的是这些稀疏性问题00:10:17.570 --> 00:10:22.100如果你增加n，情况会更糟。如果你在n-gram语言模型中使n变大，00:10:22.100 --> 00:10:23.870你可能想这样做，例如，00:10:23.870 --> 00:10:26.390你可能会想，呃，我想有一个更大的背景，00:10:26.390 --> 00:10:28.565所以我可以注意到00:10:28.565 --> 00:10:30.890很久以前的事了，这会让它成为更好的预测器。00:10:30.890 --> 00:10:33.275所以，你可能认为把N变大是个好主意。00:10:33.275 --> 00:10:36.410但问题是，如果你这样做，那么稀疏性问题会变得更糟。00:10:36.410 --> 00:10:37.700因为，假设你说，00:10:37.700 --> 00:10:39.215我想要一个10克的语言模型。00:10:39.215 --> 00:10:40.910问题是你要数数，00:10:40.910 --> 00:10:43.480你经常看到9克和10克的过程。00:10:43.480 --> 00:10:45.485但是9克和10克，有这么多，00:10:45.485 --> 00:10:47.615你感兴趣的那个可能从未发生过，00:10:47.615 --> 00:10:51.155在你的训练数据中，这意味着整个事情变得不正常。00:10:51.155 --> 00:10:55.680所以，在实践中，我们通常不能有大于五的。00:10:56.170 --> 00:10:58.490可以。所以，那是，呃，00:10:58.490 --> 00:11:00.875n-gram语言模型的两个稀疏性问题。00:11:00.875 --> 00:11:02.770存储有问题。00:11:02.770 --> 00:11:04.710如果我们看这个方程，00:11:04.710 --> 00:11:06.780你得想想你需要什么00:11:06.780 --> 00:11:09.365存储以便使用N-gram语言模型。00:11:09.365 --> 00:11:12.020你需要储存这个号码，00:11:12.020 --> 00:11:14.090对于你观察到的所有n克00:11:14.090 --> 00:11:17.215当你通过训练语料库计算它们的时候。00:11:17.215 --> 00:11:19.440问题是，随着n的增加，00:11:19.440 --> 00:11:23.480然后你必须储存和计数的N克数就会增加。00:11:23.480 --> 00:11:27.515因此，增加n的另一个问题是，模型的大小，00:11:27.515 --> 00:11:30.750或者你的N克模型，呃，变大了。00:11:31.490 --> 00:11:37.215好的，那么N-gram语言模型在实践中。让我们来看一个例子。00:11:37.215 --> 00:11:42.540实际上，您可以在170万字的语料库上构建一个简单的三角语言模型，00:11:42.540 --> 00:11:44.325呃，几秒钟后，在你的笔记本上。00:11:44.325 --> 00:11:46.140事实上，我做这个的语料库00:11:46.140 --> 00:11:47.970是你在第一次任务中遇到的那个。00:11:47.970 --> 00:11:49.605这是路透社的语料库，00:11:49.605 --> 00:11:51.180嗯，商业和金融新闻。00:11:51.180 --> 00:11:52.380所以，如果你想自己动手，00:11:52.380 --> 00:11:55.005稍后您可以按照幻灯片底部的链接进行操作。00:11:55.005 --> 00:11:57.000所以，呃，这是，呃，00:11:57.000 --> 00:11:59.280我在几秒钟内在笔记本电脑上运行的东西。00:11:59.280 --> 00:12:02.790所以我今天给了它一个大人物的背景，00:12:02.790 --> 00:12:06.480然后我问三角语言模型下一个单词可能是什么。00:12:06.480 --> 00:12:09.855所以，语言模型说，最可能出现的单词是00:12:09.855 --> 00:12:13.455公司、银行、普莱斯、意大利、阿联酋等。00:12:13.455 --> 00:12:17.640所以，只要看看这些不同单词的概率，00:12:17.640 --> 00:12:19.590嗯，你可以看到有一个稀疏的问题。00:12:19.590 --> 00:12:21.840例如，前两个最可能的单词00:12:21.840 --> 00:12:24.720完全相同的概率和原因是，00:12:24.720 --> 00:12:26.760这个数字是4比26。00:12:26.760 --> 00:12:28.800所以这些是很小的整数，呃，00:12:28.800 --> 00:12:30.270意思是我们只看到，呃，00:12:30.270 --> 00:12:33.000今天公司和银行各四次。00:12:33.000 --> 00:12:34.560这是一个例子00:12:34.560 --> 00:12:37.290稀疏性问题，因为总的来说，这些都是非常低的计数，00:12:37.290 --> 00:12:39.165我们没见过这么多不同的，呃，00:12:39.165 --> 00:12:40.500此活动的版本，00:12:40.500 --> 00:12:43.885所以我们没有一个非常粒度的概率分布。00:12:43.885 --> 00:12:46.385但不管怎样，忽略了稀疏性问题，00:12:46.385 --> 00:12:47.765总的来说，00:12:47.765 --> 00:12:50.640这些建议看起来很合理。00:12:52.600 --> 00:12:55.670所以你可以使用语言模型00:12:55.670 --> 00:12:58.305生成文本，这就是您要做的。00:12:58.305 --> 00:13:00.735我们假设你已经有了前两个词，呃，00:13:00.735 --> 00:13:04.560你在这个条件下，问你的语言模型下一步会发生什么。00:13:04.560 --> 00:13:07.305所以考虑到这些词的概率分布，00:13:07.305 --> 00:13:08.850你可以从中取样，也就是说，00:13:08.850 --> 00:13:11.865选择一些有关联概率的词。00:13:11.865 --> 00:13:14.235所以让我们假设这给了我们“价格”这个词。00:13:14.235 --> 00:13:17.730那么价格就是你的下一个词，然后你只需要在最后两个词上加条件，00:13:17.730 --> 00:13:20.385在这个例子中，现在是价格。00:13:20.385 --> 00:13:23.790所以现在你得到了一个新的概率分布，你可以继续这个过程，00:13:23.790 --> 00:13:27.960嗯，取样，然后再次调节，然后取样。00:13:27.960 --> 00:13:30.150所以如果你做的足够久，00:13:30.150 --> 00:13:31.350你会得到一段文字，00:13:31.350 --> 00:13:33.690所以这是我在00:13:33.690 --> 00:13:37.005我用这个三角语言模型运行这个生成过程。00:13:37.005 --> 00:13:40.260所以它说，“今天每吨黄金的价格，00:13:40.260 --> 00:13:43.260鞋楦和鞋业的生产，00:13:43.260 --> 00:13:46.230银行在考虑和拒绝之后进行了干预。00:13:46.230 --> 00:13:49.365国际货币基金组织要求重建枯竭的欧洲股市，00:13:49.365 --> 00:13:52.8109月30日结束的初选76股算一股。00:13:52.810 --> 00:13:55.250可以。那么，呃，我们对这篇文章怎么看？00:13:55.250 --> 00:13:59.195我们觉得很好？我们，呃，惊讶吗？00:13:59.195 --> 00:14:02.370嗯，我想说在某些方面是好的，00:14:02.370 --> 00:14:04.620你知道，这有点奇怪的语法，00:14:04.620 --> 00:14:07.860主要是，呃，有点停顿，00:14:07.860 --> 00:14:09.150但是你肯定会这么说，00:14:09.150 --> 00:14:10.500这真的没有任何意义。00:14:10.500 --> 00:14:12.180很不连贯。00:14:12.180 --> 00:14:14.580我们不应该惊讶它是不连贯的我00:14:14.580 --> 00:14:17.715想想，如果你记得这是一个三角语言模型，00:14:17.715 --> 00:14:20.265它只记得最后一口井，00:14:20.265 --> 00:14:22.635三个或两个词取决于你如何看待它。00:14:22.635 --> 00:14:24.510很明显我们需要考虑00:14:24.510 --> 00:14:27.990如果我们想很好地模拟语言，一次超过三个单词。00:14:27.990 --> 00:14:32.265但是我们已经知道，增加n会使稀疏性问题变得更糟，00:14:32.265 --> 00:14:38.370n-gram语言模型，它还增加了模型大小。这是个问题吗？00:14:38.370 --> 00:14:40.320怎么[听不见][噪音]00:14:40.320 --> 00:14:43.380所以问题是，n-gram语言模型如何知道何时使用逗号。00:14:43.380 --> 00:14:45.150呃，所以你可以，00:14:45.150 --> 00:14:50.400[杂音]决定逗号和其他标点符号只是另一种单词，00:14:50.400 --> 00:14:51.705那是井还是记号？00:14:51.705 --> 00:14:54.510然后，对于语言模型来说，这并没有什么区别。00:14:54.510 --> 00:14:57.705它只是用来作为另一个可能的世界，嗯，可以预测，00:14:57.705 --> 00:14:59.445这就是为什么我们有奇怪的间隔，00:14:59.445 --> 00:15:01.770逗号是因为它本质上被视为一个单独的词。00:15:01.770 --> 00:15:06.135[噪音]好的。00:15:06.135 --> 00:15:09.195所以这门课叫做NLP，有深度的学习。00:15:09.195 --> 00:15:12.765所以你可能在想我们如何建立一个神经语言模型？00:15:12.765 --> 00:15:15.450那么，让我们简单回顾一下，呃，万一你忘了。00:15:15.450 --> 00:15:17.940记住，语言模型需要00:15:17.940 --> 00:15:20.760输入是从x1到xt的一系列单词，00:15:20.760 --> 00:15:26.290然后它输出下一个单词可能是xt加1的概率分布。00:15:27.470 --> 00:15:32.070好吧，当我们考虑到我们在这门课上遇到的神经模型是什么时候。00:15:32.070 --> 00:15:34.545呃，我们已经见过基于窗口的神经模型。00:15:34.545 --> 00:15:36.780在第三课，我们看到了你如何申请00:15:36.780 --> 00:15:40.035一种基于窗口的神经模型，用于命名实体识别。00:15:40.035 --> 00:15:43.050因此，在这种情况下，你需要借助某种窗口，00:15:43.050 --> 00:15:46.125不管这个例子中哪个是巴黎，然后，呃，00:15:46.125 --> 00:15:48.780你得到了这些的单词嵌入，将它们连接起来00:15:48.780 --> 00:15:52.890一些层次，然后你就可以决定巴黎是一个位置，而不是，00:15:52.890 --> 00:15:55.425你知道，一个人或一个组织。00:15:55.425 --> 00:15:57.900这是我们在第三课中所看到的。00:15:57.900 --> 00:16:03.795我们如何将这样的模型应用于语言建模？所以你可以这样做。00:16:03.795 --> 00:16:06.930下面是一个固定窗口神经语言模型的例子。00:16:06.930 --> 00:16:09.420所以，再一次，我们有某种背景00:16:09.420 --> 00:16:12.060也就是说，当学监开始计时时，学生们打开了他们的钟，00:16:12.060 --> 00:16:15.225嗯，我们想知道接下来会有什么消息。00:16:15.225 --> 00:16:18.450所以我们必须做一个和以前类似的简化假设。00:16:18.450 --> 00:16:21.255因为这是一个固定大小的窗户，00:16:21.255 --> 00:16:25.500我们必须放弃上下文，除了我们要调节的窗口。00:16:25.500 --> 00:16:29.070所以假设我们的固定窗户是四号的。00:16:29.070 --> 00:16:34.390所以我们要做的是类似于，啊，纳模型。00:16:34.390 --> 00:16:38.400我们将用一个热向量来表示这些词，00:16:38.400 --> 00:16:42.745然后我们会用它们来查找这些单词的嵌入，00:16:42.745 --> 00:16:44.895呃，嵌入查找矩阵。00:16:44.895 --> 00:16:48.075然后我们得到所有嵌入的单词e，1，2，3，4，00:16:48.075 --> 00:16:51.270然后我们把它们连接在一起得到e。00:16:51.270 --> 00:16:55.215一个线性层和一个非线性函数f得到某种隐藏层，00:16:55.215 --> 00:16:57.720然后我们把它穿过另一个线性层00:16:57.720 --> 00:17:01.860SoftMax函数，现在我们有一个输出概率分布y hat。00:17:01.860 --> 00:17:05.925在我们的例子中，因为我们试图预测下一个词是什么，啊，啊，00:17:05.925 --> 00:17:08.430向量y的长度为v，其中v是00:17:08.430 --> 00:17:10.020词汇表和它将包含00:17:10.020 --> 00:17:12.555词汇表中所有不同单词的概率。00:17:12.555 --> 00:17:15.600所以在这里，我把它表示为一个条形图，如果你认为00:17:15.600 --> 00:17:18.690你已经按字母顺序从A到Z列出了所有单词，00:17:18.690 --> 00:17:21.300然后是不同概率的单词。00:17:21.300 --> 00:17:22.845如果一切顺利，00:17:22.845 --> 00:17:24.480那么这个语言模型应该告诉我们00:17:24.480 --> 00:17:27.930例如，接下来的一些词可能是书籍和笔记本电脑。00:17:27.930 --> 00:17:29.940所以这些都不应该是，嗯，00:17:29.940 --> 00:17:31.770对你来说不熟悉，因为你上周看到了这一切。00:17:31.770 --> 00:17:36.100我们只是将基于窗口的模型应用于不同的任务，例如语言建模。00:17:36.470 --> 00:17:38.940好吧，那是什么？00:17:38.940 --> 00:17:42.240与n-gram语言模型相比，这个模型有什么好处？00:17:42.240 --> 00:17:46.305所以，我想说的一个优点是没有稀疏性问题。00:17:46.305 --> 00:17:49.695如果你记得一个n-gram语言模型有一个稀疏性问题00:17:49.695 --> 00:17:53.205也就是说，如果你在训练中从未见过特定的N-gram，00:17:53.205 --> 00:17:55.005你不能给它分配任何概率。00:17:55.005 --> 00:17:56.445你没有任何数据。00:17:56.445 --> 00:17:59.340但是至少在这里你可以拿任何东西，比如，00:17:59.340 --> 00:18:02.1154克你想要的，你可以把它喂进，啊，00:18:02.115 --> 00:18:03.795神经网络会给你00:18:03.795 --> 00:18:06.150它认为下一个词是什么的输出分布。00:18:06.150 --> 00:18:10.245这可能不是一个好的预测，但至少它会，它会运行。00:18:10.245 --> 00:18:12.930另一个好处是你不需要储存00:18:12.930 --> 00:18:15.090所有观察到的n克数。00:18:15.090 --> 00:18:17.280所以，呃，这是一个优势，呃，00:18:17.280 --> 00:18:19.230比较你只需要存储00:18:19.230 --> 00:18:22.155词汇表中所有单词的所有向量。00:18:22.155 --> 00:18:26.085但是这个固定窗口语言模型有很多问题。00:18:26.085 --> 00:18:29.160这里还有一些问题：呃，00:18:29.160 --> 00:18:31.470一是你的固定窗口可能太小了。00:18:31.470 --> 00:18:33.885不管你的固定窗户有多大，呃，00:18:33.885 --> 00:18:35.640你可能会失去某种00:18:35.640 --> 00:18:38.490有时你会用到的有用的上下文。00:18:38.490 --> 00:18:41.745实际上，如果你想放大窗户的尺寸，00:18:41.745 --> 00:18:44.175然后你还要放大你的尺寸，00:18:44.175 --> 00:18:45.480呃，体重因素，对不起，00:18:45.480 --> 00:18:47.580你的体重表00:18:47.580 --> 00:18:49.590所以w的宽度，因为你要乘以它。00:18:49.590 --> 00:18:52.110其中e是单词嵌入的串联。00:18:52.110 --> 00:18:56.230W的宽度随着窗口大小的增加而增大。00:18:56.390 --> 00:19:01.210所以在包含真的你的窗口永远不会足够大。00:19:01.280 --> 00:19:05.460这个模型的另一个更微妙的问题是00:19:05.460 --> 00:19:08.820x1和x2，实际上窗口中的所有单词都是，00:19:08.820 --> 00:19:11.100乘以完全不同的租金权重00:19:11.100 --> 00:19:14.565为了证明这一点，你可以画张图。00:19:14.565 --> 00:19:17.610所以问题是如果你有00:19:17.610 --> 00:19:21.720你的重量矩阵，然后你有00:19:21.720 --> 00:19:26.910嵌入E的串联，我们有，呃，四个嵌入。00:19:26.910 --> 00:19:30.390所以我们有E-1，E-2，E-3，00:19:30.390 --> 00:19:33.135E_4，你乘以，呃，00:19:33.135 --> 00:19:36.615按权重矩阵连接的嵌入。00:19:36.615 --> 00:19:39.120所以你真的可以看到00:19:39.120 --> 00:19:42.449重量矩阵的四个部分，00:19:42.449 --> 00:19:45.570第一个嵌入e_1的单词是00:19:45.570 --> 00:19:48.825乘以本节中的权重，00:19:48.825 --> 00:19:53.025它完全独立于乘以e_2等的权重。00:19:53.025 --> 00:19:56.700所以问题是你00:19:56.700 --> 00:20:00.060在一个部分的权重矩阵中学习不会与其他部分共享。00:20:00.060 --> 00:20:03.985你已经四次学习了很多类似的函数。00:20:03.985 --> 00:20:07.910所以我们认为这是个问题的原因是00:20:07.910 --> 00:20:12.130处理输入词嵌入的共性。00:20:12.130 --> 00:20:14.880所以你学到了如何处理的东西，00:20:14.880 --> 00:20:18.375第三个嵌入，至少其中一些应该与所有嵌入共享。00:20:18.375 --> 00:20:21.960所以我要说的是，我们学习的效率有点低，00:20:21.960 --> 00:20:24.300所有这些不同的词都有不同的权重00:20:24.300 --> 00:20:27.970当他们之间有很多共同点的时候。有问题吗？00:20:29.840 --> 00:20:31.180这就是为什么[听不见][噪音]。00:20:31.180 --> 00:20:31.965好的-00:20:31.965 --> 00:20:36.560是的，希望-希望口头描述已经开始。00:20:38.280 --> 00:20:42.310所以，最后，我要说的是，我们最大的问题是00:20:42.310 --> 00:20:45.280这个固定大小的神经模型显然是00:20:45.280 --> 00:20:48.355需要某种能处理任何长度输入的神经结构，00:20:48.355 --> 00:20:51.070因为这里的大部分问题都是因为我们必须00:20:51.070 --> 00:20:54.920这简化了存在固定窗口的假设。00:20:56.670 --> 00:21:00.040可以。所以这激发了，呃，00:21:00.040 --> 00:21:02.590我们来介绍这个新的神经架构家族，00:21:02.590 --> 00:21:05.515它被称为循环神经网络或RNN。00:21:05.515 --> 00:21:09.100所以，这是一个简单的图表，它向你展示了最重要的，00:21:09.100 --> 00:21:11.320嗯，RNN的特点。00:21:11.320 --> 00:21:15.070我们还有一个输入序列x1，x2，00:21:15.070 --> 00:21:20.245等等，但是你可以假设这个序列有你喜欢的任意长度。00:21:20.245 --> 00:21:24.460我们的想法是，你有一系列隐藏的状态，而不仅仅是，00:21:24.460 --> 00:21:27.175例如，我们在前一个模型中所做的一个隐藏状态。00:21:27.175 --> 00:21:30.940我们有一系列隐藏的状态，它们的数量和我们输入的一样多。00:21:30.940 --> 00:21:35.440重要的是，每个隐藏状态ht都是基于00:21:35.440 --> 00:21:40.315上一个隐藏状态以及该步骤的输入。00:21:40.315 --> 00:21:44.050所以它们被称为隐藏状态的原因是因为你可以想到00:21:44.050 --> 00:21:47.425这是一种随时间变化的单一状态。00:21:47.425 --> 00:21:50.260它有点像同一事物的几个版本。00:21:50.260 --> 00:21:53.830出于这个原因，我们经常称之为时间步骤，对吗？00:21:53.830 --> 00:21:55.540所以这些从左到右的步骤，00:21:55.540 --> 00:21:57.860我们经常称之为时间步。00:21:58.950 --> 00:22:01.870所以最重要的是00:22:01.870 --> 00:22:07.210在该RNN的每个时间步上应用相同的权重矩阵w。00:22:07.210 --> 00:22:11.365这就是我们能够处理任何长度输入的原因。00:22:11.365 --> 00:22:13.930因为我们不必在每一步都有不同的重量，00:22:13.930 --> 00:22:17.990因为我们只是在每一步上应用完全相同的转换。00:22:18.870 --> 00:22:22.690因此，您还可以从RNN获得一些输出。00:22:22.690 --> 00:22:23.995所以这些帽子，00:22:23.995 --> 00:22:26.155这些是每个步骤的输出。00:22:26.155 --> 00:22:28.735它们是可选的，因为你不需要计算它们00:22:28.735 --> 00:22:31.210或者，您可以只在某些步骤上计算它们，而不在其他步骤上计算。00:22:31.210 --> 00:22:34.160这取决于你想用RNN做什么。00:22:34.920 --> 00:22:38.260可以。这是一个简单的RNN图。00:22:38.260 --> 00:22:39.850呃，我来给你详细介绍一下。00:22:39.850 --> 00:22:43.630下面介绍如何应用RNN进行语言建模。00:22:43.630 --> 00:22:48.175所以，呃，再一次，假设我们到目前为止有一些文本。00:22:48.175 --> 00:22:50.860我的课文只有四个字，00:22:50.860 --> 00:22:53.320但是你可以假设它可以是任何长度，对吗？00:22:53.320 --> 00:22:55.420它很短，因为我们无法在幻灯片上容纳更多内容。00:22:55.420 --> 00:22:58.390所以你有一些标签序列，可能有点长。00:22:58.390 --> 00:23:02.020再一次，我们将用一种热向量来表示这些00:23:02.020 --> 00:23:06.460使用它们从嵌入矩阵中查找单词embeddings。00:23:06.460 --> 00:23:10.370然后计算第一个隐藏状态h1，00:23:10.370 --> 00:23:14.300我们需要根据以前的隐藏状态和当前输入来计算它。00:23:14.300 --> 00:23:16.615我们已经有了电流输入，即e1，00:23:16.615 --> 00:23:19.570但问题是，我们从哪里得到这个第一个隐藏的状态？00:23:19.570 --> 00:23:21.160好吧，h1之前是什么？00:23:21.160 --> 00:23:24.670所以我们经常称初始隐藏状态为h0，呃，是的，00:23:24.670 --> 00:23:28.015我们称之为初始隐藏状态，它可以是你学到的东西，00:23:28.015 --> 00:23:32.065就像它是网络的一个参数，你要学会如何初始化它，00:23:32.065 --> 00:23:35.395或者你可以假设它是零向量。00:23:35.395 --> 00:23:40.495所以我们用这个公式来计算新的隐藏状态，00:23:40.495 --> 00:23:43.195电流输入也写在左边。00:23:43.195 --> 00:23:46.690所以你对前一个隐藏状态做一个线性变换00:23:46.690 --> 00:23:48.640当前输入，然后添加00:23:48.640 --> 00:23:50.919偏压，然后把它放进一个非线性，00:23:50.919 --> 00:23:52.990例如，sigmoid函数。00:23:52.990 --> 00:23:55.700这给了你一个新的隐藏状态。00:23:56.670 --> 00:23:59.470可以。所以，一旦你做到了，00:23:59.470 --> 00:24:01.480然后你可以计算下一个隐藏状态，00:24:01.480 --> 00:24:03.850可以继续这样展开网络。00:24:03.850 --> 00:24:06.025那是，嗯，是的，00:24:06.025 --> 00:24:07.450这叫做展开，因为你00:24:07.450 --> 00:24:10.270计算前一步的每一步。00:24:10.270 --> 00:24:12.160好吧。最后，如果你还记得，00:24:12.160 --> 00:24:13.330我们正在尝试进行语言建模。00:24:13.330 --> 00:24:17.530因此，我们试图预测学生们打开后，接下来应该出现哪些单词。00:24:17.530 --> 00:24:19.870所以在这里的第四步，00:24:19.870 --> 00:24:21.205我们可以用，呃，00:24:21.205 --> 00:24:22.825当前隐藏状态，h4，00:24:22.825 --> 00:24:27.430把它放进一个线性层，然后把它放进一个SoftMax函数，然后我们得到00:24:27.430 --> 00:24:32.800我们的输出分布y-hat 4是在词汇表上的分布。00:24:32.800 --> 00:24:34.720再次，希望我们能得到一些00:24:34.720 --> 00:24:38.080对下一个词可能是什么的合理估计。00:24:38.080 --> 00:24:43.210有什么问题吗？是的？00:24:43.210 --> 00:24:47.650是隐藏状态的个数，还是输入的字数？00:24:47.650 --> 00:24:50.845问题是，隐藏状态的数量是您输入的单词数量吗？00:24:50.845 --> 00:24:53.485是的，在这里，呃，是的，00:24:53.485 --> 00:24:58.405或者你可以更一般地说，隐藏状态的数量就是输入的数量。是的。00:24:58.405 --> 00:24:59.950就像N-gram模型一样，00:24:59.950 --> 00:25:05.590我们可以使用输出作为转换模型中任务突变的输入？00:25:05.590 --> 00:25:07.000是的，所以问题是，00:25:07.000 --> 00:25:08.650就像N-gram语言模型一样，00:25:08.650 --> 00:25:10.570我们可以使用输出作为下一步的输入吗？00:25:10.570 --> 00:25:12.715答案是肯定的，我马上给你看。00:25:12.715 --> 00:25:15.700还有其他问题吗？是啊。00:25:15.700 --> 00:25:17.995你在学习嵌入吗？00:25:17.995 --> 00:25:20.560问题是，你在学习嵌入技术吗？00:25:20.560 --> 00:25:21.925嗯，那是个选择。00:25:21.925 --> 00:25:23.770例如，你可以用嵌入的方式，00:25:23.770 --> 00:25:27.370预生成的嵌入，你下载并使用它们，它们被冻结，00:25:27.370 --> 00:25:28.750或者你可以下载它们，00:25:28.750 --> 00:25:30.190但是你可以对它们进行微调。00:25:30.190 --> 00:25:32.200也就是说，允许将它们作为00:25:32.200 --> 00:25:35.170或者你可以初始化它们，00:25:35.170 --> 00:25:38.560你知道，小的，呃，随机值，从零开始学习。00:25:38.560 --> 00:25:40.570还有其他问题吗？是啊。00:25:40.570 --> 00:25:43.690你说你用的是同样的三角矩阵，00:25:43.690 --> 00:25:45.490就像你做反向传播一样，00:25:45.490 --> 00:25:48.030你只会像我们一样更新吗？00:25:48.030 --> 00:25:51.080或者你会更新wh和we吗？00:25:51.080 --> 00:25:56.085所以问题是，你说我们重用矩阵，是更新我们和wh，还是只更新一个？00:25:56.085 --> 00:25:58.980所以你突然学会了我们和什么。00:25:58.980 --> 00:26:01.410我想我是在强调什么，但是是的，00:26:01.410 --> 00:26:04.090它们都是重复应用的矩阵。00:26:04.090 --> 00:26:05.500还有一个关于后支撑的问题，00:26:05.500 --> 00:26:07.675但我们稍后会在这节课中讨论这个问题。00:26:07.675 --> 00:26:12.250好的，现在继续。嗯，所以，00:26:12.250 --> 00:26:17.530这种RNN语言模型有哪些优点和缺点？00:26:17.530 --> 00:26:23.005因此，与固定窗口相比，我们可以看到一些优势。00:26:23.005 --> 00:26:28.210所以一个明显的优点是这个RNN可以处理任何长度的输入。00:26:28.210 --> 00:26:31.180另一个优点是00:26:31.180 --> 00:26:35.050理论上，步骤t可以使用许多步骤返回的信息。00:26:35.050 --> 00:26:36.730所以在我们的动机例子中，00:26:36.730 --> 00:26:38.650就在节目主持人开始计时的时候，00:26:38.650 --> 00:26:39.970学生们打开了门。00:26:39.970 --> 00:26:42.250我们认为Proctor和Clock是00:26:42.250 --> 00:26:45.340这两个很重要的提示下一步可能会发生什么。00:26:45.340 --> 00:26:47.275所以，至少在理论上，00:26:47.275 --> 00:26:49.390最后的隐藏状态00:26:49.390 --> 00:26:54.950可以访问许多步骤前输入的信息。00:26:55.350 --> 00:26:59.785另一个优点是，模型大小不会随着输入时间的延长而增大。00:26:59.785 --> 00:27:02.485所以，呃，模型的大小是固定的。00:27:02.485 --> 00:27:05.005这只是wh和we，s00:27:05.005 --> 00:27:09.400如果你计算的话，还有偏差和嵌入矩阵。00:27:09.400 --> 00:27:13.000如果你想把它应用到更多的地方，这些都不会变大，00:27:13.000 --> 00:27:17.300嗯，输入的时间更长，因为你只需要重复使用相同的权重。00:27:18.030 --> 00:27:23.995另一个优点是你在每一个时间步上都有相同的权重。00:27:23.995 --> 00:27:29.425我之前说过，固定大小的窗神经模型，00:27:29.425 --> 00:27:31.720因为它正在应用，所以效率较低00:27:31.720 --> 00:27:34.270权重矩阵的不同权重不同，00:27:34.270 --> 00:27:35.905呃，窗户上的字。00:27:35.905 --> 00:27:38.470这个RNN的优点是00:27:38.470 --> 00:27:41.650对每个输入应用完全相同的转换。00:27:41.650 --> 00:27:45.835这意味着如果它学习了处理一个输入的好方法，00:27:45.835 --> 00:27:48.010应用于序列中的每个输入。00:27:48.010 --> 00:27:50.630所以你可以看到它在这方面更有效率。00:27:51.480 --> 00:27:54.805好吧，那么这个模型的缺点是什么？00:27:54.805 --> 00:27:58.270一个是经常性的计算很慢。00:27:58.270 --> 00:27:59.995呃，就像你以前看到的，00:27:59.995 --> 00:28:03.865您必须根据以前的隐藏状态计算隐藏状态。00:28:03.865 --> 00:28:06.925这意味着你不能并行计算所有的隐藏状态。00:28:06.925 --> 00:28:08.665你必须按顺序计算它们。00:28:08.665 --> 00:28:13.120所以，特别是如果你想在一个相当长的输入序列上计算RNN，00:28:13.120 --> 00:28:16.660这意味着RNN的计算速度可能非常慢。00:28:16.660 --> 00:28:20.425RNN的另一个缺点是它不起作用，00:28:20.425 --> 00:28:24.175在实践中，要从许多步骤中获取信息是相当困难的。00:28:24.175 --> 00:28:26.290所以即使我说我们应该能记住00:28:26.290 --> 00:28:28.930学监和时钟，用来预测考试和我们的书，00:28:28.930 --> 00:28:30.430结果发现RNN，00:28:30.430 --> 00:28:32.470至少我在这节课上讲过的那些，00:28:32.470 --> 00:28:35.305不如你想象的那么好。00:28:35.305 --> 00:28:39.295嗯，我们稍后会进一步了解这两个缺点，00:28:39.295 --> 00:28:42.610我们将学习如何修复它们。00:28:42.610 --> 00:28:46.900我们现在有什么问题吗？是的。00:28:46.900 --> 00:28:48.010为什么我们假设什么是相同的？00:28:48.010 --> 00:28:51.265对不起，你能大声点吗？00:28:51.265 --> 00:28:55.900为什么我们假设wh应该是相同的？00:28:55.900 --> 00:28:59.635所以问题是，为什么你要假设wh是相同的？00:28:59.635 --> 00:29:01.450我想，这不完全是一个假设，00:29:01.450 --> 00:29:04.390在RNN的设计中，这更像是一个深思熟虑的决定。00:29:04.390 --> 00:29:06.460所以，RNN的定义是，00:29:06.460 --> 00:29:10.450在每一步上应用完全相同的权重的网络。00:29:10.450 --> 00:29:13.800所以，我想你为什么认为应该是，00:29:13.800 --> 00:29:15.225为什么这是个好主意？00:29:15.225 --> 00:29:17.520嗯，所以我说了一点为什么这是个好主意，00:29:17.520 --> 00:29:18.690这些优点，00:29:18.690 --> 00:29:23.950我想，这就是你为什么想这样做的原因。那能回答你的问题吗？00:29:24.560 --> 00:29:29.020打开他们的书，对吗？如果假设wh相同，00:29:29.020 --> 00:29:31.420你是说，呃，00:29:31.420 --> 00:29:34.660马尔可夫链，就像马尔可夫链。00:29:34.660 --> 00:29:37.780呃，传输，呃，00:29:37.780 --> 00:29:42.955打开人类情绪的转移概率，00:29:42.955 --> 00:29:44.890它们是一样的，00:29:44.890 --> 00:29:50.940但实际上是马尔可夫链。00:29:50.940 --> 00:29:56.535模型[听不见]的转移概率相同，00:29:56.535 --> 00:30:00.895所以[听不见]概率，00:30:00.895 --> 00:30:07.105这只是一个近似值，但它是另一个测试。00:30:07.105 --> 00:30:08.240可以。所以我认为[重叠]00:30:08.240 --> 00:30:10.810如果你假设wh可能是相同的，00:30:10.810 --> 00:30:14.725很好，因为你使用了很多参数，00:30:14.725 --> 00:30:20.560但这只是一个，这只是一个近似值。00:30:20.560 --> 00:30:23.410潜在的转移，呃，00:30:23.410 --> 00:30:25.660概率，不应该是一样的。特别是[重叠]00:30:25.660 --> 00:30:28.835可以。嗯，所以我认为问题是，考虑到这些00:30:28.835 --> 00:30:30.540学生们打开的词00:30:30.540 --> 00:30:32.490都是不同的，它们发生在不同的环境中，00:30:32.490 --> 00:30:35.850那么为什么我们每次都要应用相同的转换呢？00:30:35.850 --> 00:30:37.440所以这是一个很好的问题。00:30:37.440 --> 00:30:41.670我想，呃，这个想法是你在学习一个通用函数，而不仅仅是，你知道，00:30:41.670 --> 00:30:43.535如何对待学生，00:30:43.535 --> 00:30:46.090在这一个上下文中的一个词students。00:30:46.090 --> 00:30:48.520我们正在努力学习你如何00:30:48.520 --> 00:30:51.070应该处理到目前为止给出的一个词。00:30:51.070 --> 00:30:55.090到目前为止，你正在努力学习语言和上下文的一般表示法，00:30:55.090 --> 00:30:57.055这确实是一个非常困难的问题。00:30:57.055 --> 00:31:00.175嗯，我想你也提到了近似值。00:31:00.175 --> 00:31:01.780还有一件事要注意的是00:31:01.780 --> 00:31:04.570隐藏状态是向量，它们不仅仅是单个数字，对吗？00:31:04.570 --> 00:31:06.670它们是长度的向量，我不知道，500还是什么？00:31:06.670 --> 00:31:09.610因此，他们有相当大的容量来保存关于00:31:09.610 --> 00:31:13.530在不同的位置上有不同的东西。00:31:13.530 --> 00:31:15.630所以，我认为你可以00:31:15.630 --> 00:31:18.255在不同的环境中存储大量不同的信息，00:31:18.255 --> 00:31:19.830在隐藏状态的不同部分，00:31:19.830 --> 00:31:21.960但它确实是一个近似值00:31:21.960 --> 00:31:24.575某种程度上限制了您可以存储的信息量。00:31:24.575 --> 00:31:26.845好的，还有其他问题吗？对。00:31:26.845 --> 00:31:29.410因为你处理的是任何一个长度的框架，00:31:29.410 --> 00:31:31.135你在训练中用多长时间？00:31:31.135 --> 00:31:35.035你用来训练的长度会影响什么？00:31:35.035 --> 00:31:39.355好吧，问题是，如果你可以输入任何长度，00:31:39.355 --> 00:31:41.950培训期间的输入长度是多少？00:31:41.950 --> 00:31:44.185所以，我想在实践中，00:31:44.185 --> 00:31:46.510您可以选择输入的时间00:31:46.510 --> 00:31:49.630培训要么基于你的数据，要么基于00:31:49.630 --> 00:31:52.615嗯，你的效率很重要，所以也许你是人为的00:31:52.615 --> 00:31:55.900剪短它。嗯，另一个问题是什么？00:31:55.900 --> 00:31:58.360呃，那又有什么用呢？00:31:58.360 --> 00:32:01.255可以。所以问题是，什么取决于你使用的长度？00:32:01.255 --> 00:32:04.075所以，不，这是优势列表中的一个优点。00:32:04.075 --> 00:32:07.165模型尺寸不会随着输入时间的延长而增大，00:32:07.165 --> 00:32:09.040因为我们只是打开RNN00:32:09.040 --> 00:32:11.245只要我们愿意，一次又一次地施加相同的重量。00:32:11.245 --> 00:32:13.930不需要有更多的重量，因为你有一个更长的输入。00:32:13.930 --> 00:32:16.795[噪音]是的。00:32:16.795 --> 00:32:24.235所以，你提到的比率是如何[听不见]单词数的。00:32:24.235 --> 00:32:28.405[噪音]你问的是大写E还是小写E？00:32:28.405 --> 00:32:29.485呃，小写E。00:32:29.485 --> 00:32:30.790可以。所以，问题是，00:32:30.790 --> 00:32:32.890我们如何选择小写es的维数？00:32:32.890 --> 00:32:34.300呃，你可以，例如，00:32:34.300 --> 00:32:37.120假设这些只是预先训练过的单词向量，就像你，00:32:37.120 --> 00:32:38.815呃，用于作业一。00:32:38.815 --> 00:32:39.715更像Word2vec。00:32:39.715 --> 00:32:41.140是啊。例如，word2vec，00:32:41.140 --> 00:32:42.610你只需下载并使用它们，00:32:42.610 --> 00:32:44.380或者你从零开始学习，在这种情况下，00:32:44.380 --> 00:32:46.930你可以在训练开始时决定这些向量的大小。00:32:46.930 --> 00:32:49.210[噪音]好的。我现在就走。00:32:49.210 --> 00:32:54.895[噪音]所以，我们已经了解了RNN语言模型是什么，我们也已经了解了你会怎么做，00:32:54.895 --> 00:32:56.845嗯，向前跑一步，但问题是，00:32:56.845 --> 00:32:59.080你将如何训练RNN语言模型？00:32:59.080 --> 00:33:02.230你会怎么学？[噪音]00:33:02.230 --> 00:33:03.850所以，像往常一样，在机器学习中，00:33:03.850 --> 00:33:06.670我们的答案是，你会得到大量的文本，00:33:06.670 --> 00:33:11.230我们称之为单词x1到x大写t的序列，所以，00:33:11.230 --> 00:33:15.115将单词序列输入RNN语言模型，然后，00:33:15.115 --> 00:33:19.615我们的想法是，计算每个步骤t的输出分布y-hat t。00:33:19.615 --> 00:33:21.700我知道我在上一张照片上展示的，呃，00:33:21.700 --> 00:33:23.560幻灯片[噪音]只显示我们在最后一步做，00:33:23.560 --> 00:33:26.140但其想法是，你可以在每一步上计算这个。00:33:26.140 --> 00:33:28.420所以，这意味着你实际上在预测00:33:28.420 --> 00:33:31.000每一步下一个单词的概率。00:33:31.000 --> 00:33:33.130[噪音]好的。00:33:33.130 --> 00:33:35.515所以，一旦你这样做了，你就可以定义损失函数，00:33:35.515 --> 00:33:37.120你现在应该很熟悉了。00:33:37.120 --> 00:33:39.190这是[噪音]之间的交叉熵00:33:39.190 --> 00:33:43.915我们预测的概率分布y-hat t和真的，呃，00:33:43.915 --> 00:33:47.260分发，这是Y-Hat-抱歉，只是YT，00:33:47.260 --> 00:33:49.570这是一个热向量，呃，00:33:49.570 --> 00:33:51.055代表真正的下一个[噪音]单词，00:33:51.055 --> 00:33:52.495是xt加1。00:33:52.495 --> 00:33:54.490所以，正如你以前看到的，这个，呃，00:33:54.490 --> 00:33:57.100这两个向量之间的交叉熵[噪声]可以写出来。00:33:57.100 --> 00:34:00.640也作为负对数概率。00:34:00.640 --> 00:34:05.635最后，如果你平均每一步的交叉熵损失，呃，00:34:05.635 --> 00:34:08.740语料库时间步骤t中的每个t，然后，00:34:08.740 --> 00:34:11.800呃，这会让你在整个训练中损失惨重。00:34:11.800 --> 00:34:16.360[噪音]好的。00:34:16.360 --> 00:34:18.475所以，为了更清楚地描述这一点，00:34:18.475 --> 00:34:20.080呃，假设我们的语料库是，00:34:20.080 --> 00:34:21.370学生们开始考试，00:34:21.370 --> 00:34:23.020等等，它持续了很长时间。00:34:23.020 --> 00:34:24.550那么，我们要做的是，00:34:24.550 --> 00:34:26.980我们会对这段文字进行RNN，然后，00:34:26.980 --> 00:34:30.535在每一步中，我们都会预测概率[噪声]分布y-hats，00:34:30.535 --> 00:34:31.780然后，从每一个，00:34:31.780 --> 00:34:33.310你可以计算你的损失是多少，00:34:33.310 --> 00:34:36.400这是JT，然后，呃，在第一步，00:34:36.400 --> 00:34:38.965损失将是下一个单词的负对数概率，00:34:38.965 --> 00:34:40.060在这个例子中，00:34:40.060 --> 00:34:42.040学生，[噪音]等等。00:34:42.040 --> 00:34:45.070每一个都是下一个单词的负对数概率。00:34:45.070 --> 00:34:47.515[噪音]然后，一旦你计算了所有这些，00:34:47.515 --> 00:34:49.585你可以把它们[噪音]加起来平均，00:34:49.585 --> 00:34:51.160然后，这给了你最后的损失。00:34:51.160 --> 00:34:56.260[噪音]好的。所以，这里有个警告。00:34:56.260 --> 00:34:59.935嗯，计算整个语料库的损失和梯度，00:34:59.935 --> 00:35:02.350所有这些词x1到x大写t也是00:35:02.350 --> 00:35:04.840昂贵的[噪音]因为你的语料库可能真的很大。00:35:04.840 --> 00:35:07.810[噪音]所以，嗯，就像一个学生之前问的，00:35:07.810 --> 00:35:10.555实际上，你认为你的顺序是什么？00:35:10.555 --> 00:35:12.580所以，在实践中，你可能认为你的顺序，呃，00:35:12.580 --> 00:35:14.590比如一个句子或一个文件，00:35:14.590 --> 00:35:17.270一些较短的文本单位。00:35:17.430 --> 00:35:20.890所以，呃，你要做的另一件事就是，如果你记得的话，00:35:20.890 --> 00:35:23.785随机梯度下降允许您计算梯度00:35:23.785 --> 00:35:26.980对于小数据块而不是整个语料库。00:35:26.980 --> 00:35:29.275所以，在实践中，如果你在训练一种语言模式，00:35:29.275 --> 00:35:32.830实际上你可能要做的是计算一个句子的损失，00:35:32.830 --> 00:35:35.290但这实际上是一批句子，然后，00:35:35.290 --> 00:35:37.945你计算出这批句子的梯度，00:35:37.945 --> 00:35:39.760更新体重，然后重复。00:35:39.760 --> 00:35:46.405有什么问题吗？[噪音]好的。00:35:46.405 --> 00:35:48.040所以，呃，移到背面。00:35:48.040 --> 00:35:51.055别担心，不会有像上周那样多的反弹，00:35:51.055 --> 00:35:53.230但是，呃，这里有个有趣的问题，对吧？00:35:53.230 --> 00:35:55.899所以，呃，RNN的特点是00:35:55.899 --> 00:35:58.975是它们重复应用相同的权重矩阵。00:35:58.975 --> 00:36:00.280所以，问题是，00:36:00.280 --> 00:36:02.215[噪音]损失函数的导数是多少，00:36:02.215 --> 00:36:03.610假设，在步骤t上？00:36:03.610 --> 00:36:08.635这个损失对重复的重量矩阵wh的导数是多少？00:36:08.635 --> 00:36:13.570答案是损失的导数，呃，00:36:13.570 --> 00:36:16.390相对于重复权重的梯度是00:36:16.390 --> 00:36:19.780每次出现的梯度之和，00:36:19.780 --> 00:36:21.355这个方程就是这么说的。00:36:21.355 --> 00:36:25.615在右边，用垂直线表示的符号，我是说，00:36:25.615 --> 00:36:30.670当出现在第i步时，损失相对于Wh的导数。00:36:30.670 --> 00:36:32.770可以。那么，为什么是这样？00:36:32.770 --> 00:36:35.260[噪音]呃，为了说明这是真的，00:36:35.260 --> 00:36:37.840呃，[噪音]我会提醒你多变量链规则。00:36:37.840 --> 00:36:42.535这是汗学院关于多变量链规则的文章的截图，00:36:42.535 --> 00:36:44.440我建议你去看看00:36:44.440 --> 00:36:46.630想了解更多因为这很容易理解。00:36:46.630 --> 00:36:48.220嗯，它说的是，00:36:48.220 --> 00:36:52.045给定一个函数f[噪声]，它取决于x和y，00:36:52.045 --> 00:36:56.140它们都是变量t的函数，00:36:56.140 --> 00:36:59.430如果你想得到f对t的导数，00:36:59.430 --> 00:37:04.380然后你需要分别在x和y上做链式规则，然后把它们加起来。00:37:04.380 --> 00:37:07.020[噪音]所以，这是多变量链规则，00:37:07.020 --> 00:37:10.510[噪音]如果我们把这个应用到我们的场景中00:37:10.510 --> 00:37:14.889损失的导数jt相对于我们的重量矩阵wh，00:37:14.889 --> 00:37:19.300然后你可以把它看作一种图表[噪音]，其中wh有，呃，00:37:19.300 --> 00:37:22.810与所有这些世上的个体形象的关系，00:37:22.810 --> 00:37:23.860但这只是一种简单的关系，00:37:23.860 --> 00:37:25.495这只是平等，然后，00:37:25.495 --> 00:37:29.690wh的每一个出现都以不同的方式影响损失。00:37:29.690 --> 00:37:34.080那么，如果我们应用多变量链规则，00:37:34.080 --> 00:37:37.470然后它说损失的导数00:37:37.470 --> 00:37:41.190wh是这些链式法则的总和，00:37:41.190 --> 00:37:45.600但右边的表达式只是一个，因为它是平等关系，00:37:45.600 --> 00:37:50.480[噪音]然后，这就给出了我在上一张幻灯片上写的方程式。00:37:50.480 --> 00:37:55.240所以，这是一个证明，为什么损失的导数00:37:55.240 --> 00:38:00.565关于循环矩阵，是每次出现的导数之和。00:38:00.565 --> 00:38:03.190可以。所以，假设你相信我，也就是说，00:38:03.190 --> 00:38:04.555如何计算，呃，00:38:04.555 --> 00:38:06.475相对于重复重量的梯度。00:38:06.475 --> 00:38:08.440所以，还有一个问题是，00:38:08.440 --> 00:38:10.720实际中我们如何计算这个？00:38:10.720 --> 00:38:16.660[noise]所以，答案是你要做backprop来计算这个和，00:38:16.660 --> 00:38:19.390嗯，向后，从右到左，嗯，00:38:19.390 --> 00:38:23.590通过RNN，你将积累这个总数。00:38:23.590 --> 00:38:24.940所以，重要的是，00:38:24.940 --> 00:38:28.435你不应该分别计算这些东西，00:38:28.435 --> 00:38:30.880你应该通过累计来计算它们，比如，00:38:30.880 --> 00:38:34.360每一个都可以用前一个的形式来计算。00:38:34.360 --> 00:38:39.130[noise]所以，这个计算这些的算法，00:38:39.130 --> 00:38:41.320呃，每个梯度00:38:41.320 --> 00:38:44.305前一个被称为随时间的反向传播。00:38:44.305 --> 00:38:47.650而且，嗯，我一直认为这听起来比科幻小说要高得多。00:38:47.650 --> 00:38:49.030听起来像是时间旅行之类的，00:38:49.030 --> 00:38:50.560但其实很简单。00:38:50.560 --> 00:38:53.290呃，这就是你给的名字00:38:53.290 --> 00:38:57.290将反prop算法应用于循环神经网络。00:38:57.960 --> 00:39:02.350有什么问题吗？是的。[噪音]00:39:02.350 --> 00:39:07.240所以，似乎你如何分解这些批次关系到你的最终结果。00:39:07.240 --> 00:39:15.700[听不见]。00:39:15.700 --> 00:39:21.460所以，如果你把它分成更多[听不见]。00:39:21.460 --> 00:39:23.605可以。所以问题是，嗯，当然，00:39:23.605 --> 00:39:27.865你决定如何分解你的批次会影响你学习的方式，对吗？00:39:27.865 --> 00:39:29.560因为如果你选择，呃，00:39:29.560 --> 00:39:31.660一组数据将成为您的批处理，对吧，00:39:31.660 --> 00:39:33.880您将基于此进行更新，然后，00:39:33.880 --> 00:39:36.760你只能根据你从那里去的地方[噪音]来更新下一个。00:39:36.760 --> 00:39:38.950所以，如果您决定将不同的数据放入批处理中，00:39:38.950 --> 00:39:40.495那么你就可以迈出不同的一步了。00:39:40.495 --> 00:39:42.910所以，这是真的，[噪音]这就是为什么00:39:42.910 --> 00:39:45.910随机梯度下降只是00:39:45.910 --> 00:39:49.660真正的梯度下降，因为你计算的梯度00:39:49.660 --> 00:39:53.950对于一个批次而言，只是相对于00:39:53.950 --> 00:39:56.095呃，整个语料库的损失。00:39:56.095 --> 00:39:58.165所以，是的，它确实是一个近似值00:39:58.165 --> 00:40:00.580你选择如何将你的数据进行批量处理也很重要，00:40:00.580 --> 00:40:03.040这就是为什么，例如，整理数据是个好主意，00:40:03.040 --> 00:40:05.575每一个时代，改变它是一个好主意。00:40:05.575 --> 00:40:09.130但是SGD的核心理念是00:40:09.130 --> 00:40:12.085它应该是一个足够好的近似值，在许多步骤中，00:40:12.085 --> 00:40:14.740你会，呃，尽量减少你的损失。00:40:14.740 --> 00:40:33.010[噪音]还有其他问题吗？[噪音]是的。00:40:33.010 --> 00:40:35.410[噪音]所以，是，呃，是问题，00:40:35.410 --> 00:40:37.180当你计算向前推进时，00:40:37.180 --> 00:40:40.345你开始计算后向概率吗，甚至，比如说，在损失之前？00:40:40.345 --> 00:40:41.620这就是问题吗？[噪音]00:40:41.620 --> 00:40:42.325对。00:40:42.325 --> 00:40:45.640我不这么认为，对吧？因为你需要知道损失是什么00:40:45.640 --> 00:40:49.030计算损失对某事物的导数。00:40:49.030 --> 00:40:50.560所以，我认为你应该走到最后。00:40:50.560 --> 00:40:51.760所以，如果我们假设简单，00:40:51.760 --> 00:40:54.490在几个步骤的最后，只有一个损失，00:40:54.490 --> 00:40:55.585那你就得走到尽头，00:40:55.585 --> 00:40:59.365在计算导数之前先计算损失。00:40:59.365 --> 00:41:02.200但是我想你，你，你可以计算2的导数，00:41:02.200 --> 00:41:04.240一个事物相对另一个事物的某种相邻事物。00:41:04.240 --> 00:41:05.470[重叠]但是，是的。[噪音]00:41:05.470 --> 00:41:07.780在你前进的过程中，你需要跟踪什么，00:41:07.780 --> 00:41:13.720你将拥有的（听不见的）最终会得到损失。[听不见]00:41:13.720 --> 00:41:15.865对.所以，当你向前推进时，00:41:15.865 --> 00:41:19.660你当然必须坚持所有的干预因素。00:41:19.660 --> 00:41:20.680[噪音]好的。我现在就走。00:41:20.680 --> 00:41:24.790嗯，那是个数学难题，但是，00:41:24.790 --> 00:41:27.130嗯，现在，我们开始生成文本，00:41:27.130 --> 00:41:28.675之前有人问过。00:41:28.675 --> 00:41:32.965所以，嗯，就像我们使用n-gram语言模型生成文本一样，00:41:32.965 --> 00:41:36.115您还可以使用RNN语言模型生成文本，00:41:36.115 --> 00:41:38.650嗯，通过同样的重复取样技术。00:41:38.650 --> 00:41:41.050嗯，那么，这是一张如何工作的图片。00:41:41.050 --> 00:41:43.990你如何从你最初的隐藏状态开始H0，呃，00:41:43.990 --> 00:41:46.330我们可以把它作为00:41:46.330 --> 00:41:49.060模型或者我们初始化为零，或者类似的。00:41:49.060 --> 00:41:51.340那么，假设我们有第一个词my，00:41:51.340 --> 00:41:54.235我们假设我，嗯，把它提供给模型。00:41:54.235 --> 00:41:57.235那么，使用输入和初始隐藏状态，00:41:57.235 --> 00:41:59.200你可以得到我们的第一个隐藏状态h1。00:41:59.200 --> 00:42:01.555然后，我们可以计算，呃，00:42:01.555 --> 00:42:04.765概率分布，下一个是什么，00:42:04.765 --> 00:42:07.435然后我们可以用这个分布来取样一些单词。00:42:07.435 --> 00:42:09.385所以假设我们选取了“最爱”这个词。00:42:09.385 --> 00:42:14.200所以，我们的想法是在下一步使用输出的单词作为输入。00:42:14.200 --> 00:42:16.960所以，我们把最喜欢的加入RNN的第二步，00:42:16.960 --> 00:42:18.220我们得到了一个新的隐藏状态，00:42:18.220 --> 00:42:20.784我们再次得到一个新的概率分布，00:42:20.784 --> 00:42:22.885从中我们可以看到一个新词。00:42:22.885 --> 00:42:25.675所以，我们可以一次又一次地继续这个过程，00:42:25.675 --> 00:42:27.685这样我们就可以生成一些文本。00:42:27.685 --> 00:42:29.500所以，呃，这里我们已经生成了文本，00:42:29.500 --> 00:42:30.760我最喜欢的季节是春天，00:42:30.760 --> 00:42:34.070我们想走多久就走多久。00:42:36.060 --> 00:42:39.130好吧，那么，呃，让我们玩一玩。00:42:39.130 --> 00:42:41.395呃，你可以生成，00:42:41.395 --> 00:42:43.885使用RNN语言模型的文本。00:42:43.885 --> 00:42:48.070如果你在任何文本上训练RNN语言模型，00:42:48.070 --> 00:42:51.340然后您可以使用它来生成该样式的文本。00:42:51.340 --> 00:42:53.380事实上，这已经成为一种00:42:53.380 --> 00:42:55.780你可能见过的网络幽默类型。00:42:55.780 --> 00:42:57.595比如说，00:42:57.595 --> 00:43:00.925这里有一个针对奥巴马演讲的RNN语言模型，00:43:00.925 --> 00:43:03.100我在网上的一篇博文中找到了这个。00:43:03.100 --> 00:43:07.120这里是RNN语言模型生成的文本。00:43:07.120 --> 00:43:11.350“美国将加大应对新挑战的成本，00:43:11.350 --> 00:43:15.520美国人民将分享我们制造问题的事实。00:43:15.520 --> 00:43:19.630他们被袭击了，不得不这么说00:43:19.630 --> 00:43:24.190在战争的最后几天，我不能完成所有的任务。”00:43:24.190 --> 00:43:27.130[笑声]好的。00:43:27.130 --> 00:43:30.205所以，如果我们看看这个00:43:30.205 --> 00:43:32.230尤其是想想做了什么00:43:32.230 --> 00:43:34.570文本看起来就像我们从n-gram语言模型中得到的，00:43:34.570 --> 00:43:36.160关于黄金的价格。00:43:36.160 --> 00:43:39.715嗯，我想说这比那好得多。00:43:39.715 --> 00:43:41.620总体来说，它似乎更流畅。00:43:41.620 --> 00:43:43.690呃，我想说它有更多的00:43:43.690 --> 00:43:48.535一个持续的环境，在这种情况下，一次延长时间是有意义的，00:43:48.535 --> 00:43:51.666我想说这听起来也完全像奥巴马。00:43:51.666 --> 00:43:53.035所有这些都很好，00:43:53.035 --> 00:43:55.735但你可以看到，总体来说，它仍然很不连贯，00:43:55.735 --> 00:43:58.930就像我一样――读起来很困难，因为它没有真正意义，对吧？00:43:58.930 --> 00:44:00.130所以我必须仔细读单词。00:44:00.130 --> 00:44:02.890嗯，所以，是的，我想这场演出00:44:02.890 --> 00:44:06.310使用RNN生成文本可以获得一些进展，但是，00:44:06.310 --> 00:44:09.610嗯，离人类水平很远。下面是更多的例子。00:44:09.610 --> 00:44:13.285嗯，这里有一个RNN语言模型，是在哈利波特的书上训练的。00:44:13.285 --> 00:44:17.095这就是它所说的。”“对不起。”哈利惊慌失措地喊道。00:44:17.095 --> 00:44:19.600“我把扫帚留在伦敦。”是吗？00:44:19.600 --> 00:44:21.880“不知道。”几乎没有头的尼克说，00:44:21.880 --> 00:44:23.740在塞德里克附近低抛，00:44:23.740 --> 00:44:26.980从哈利肩上拿着最后一点蜜糖。00:44:26.980 --> 00:44:29.290为了回答他，公共休息室就在上面，00:44:29.290 --> 00:44:33.025四只手拿着一个闪亮的旋钮，蜘蛛感觉不到。00:44:33.025 --> 00:44:34.855他也到了队里。”00:44:34.855 --> 00:44:38.065所以，我再说一遍，这是相当流利的。00:44:38.065 --> 00:44:40.000听起来很像《哈利波特》的书。00:44:40.000 --> 00:44:41.710事实上，它的作用给我留下了很深的印象00:44:41.710 --> 00:44:44.170听起来像是哈利波特书中的声音。00:44:44.170 --> 00:44:46.510你甚至有一些性格特征，00:44:46.510 --> 00:44:50.395我想说哈利这个角色经常在书中惊慌失措，所以这似乎是对的。00:44:50.395 --> 00:44:54.520嗯，[笑声]但是我们有一些不好的东西，00:44:54.520 --> 00:44:58.660例如，第二段中的一个很长的句子很难阅读。00:44:58.660 --> 00:45:01.490呃，你有些毫无意义的东西。00:45:01.490 --> 00:45:03.195我不知道糖浆的魅力是什么。00:45:03.195 --> 00:45:04.890听起来很好吃，但我不认为是真的，00:45:04.890 --> 00:45:07.790嗯，总的来说，这简直是胡说八道。00:45:07.790 --> 00:45:12.865这是另一个例子。这里有一个RNN语言模型，它是在食谱上训练的。00:45:12.865 --> 00:45:16.000所以，呃，[笑声]这个很奇怪，00:45:16.000 --> 00:45:18.565标题是“巧克力农场烧烤”，00:45:18.565 --> 00:45:20.950里面有帕尔马干酪，00:45:20.950 --> 00:45:25.555椰奶，鸡蛋，食谱上说，把每一个意大利面放在一层一层的块状物上，00:45:25.555 --> 00:45:29.500将混合物放入中等大小的烤箱中，用文火炖至牢固。00:45:29.500 --> 00:45:31.210热的酒体新鲜，00:45:31.210 --> 00:45:32.575芥末橙和奶酪。00:45:32.575 --> 00:45:35.815把奶酪和盐放在一个大平底锅里，把面团搅拌在一起；00:45:35.815 --> 00:45:38.140加入配料，加入巧克力和胡椒粉。00:45:38.140 --> 00:45:41.635[笑声]嗯，我认为有一件事00:45:41.635 --> 00:45:45.340这里的食谱比散文更清楚，00:45:45.340 --> 00:45:49.405是不是无法记起（噪音）总体上发生了什么，对吗？00:45:49.405 --> 00:45:53.020因为你能说的菜谱很有挑战性，因为你需要记住00:45:53.020 --> 00:45:57.100你要做的东西的标题，在本例中是巧克力农场烧烤，00:45:57.100 --> 00:45:59.470实际上，你需要，你知道，到最后把它做好。00:45:59.470 --> 00:46:01.060嗯，你还需要记住原料是什么00:46:01.060 --> 00:46:02.500一开始你用过吗？00:46:02.500 --> 00:46:05.230在食谱中，如果你做了一些东西然后把它放进烤箱，00:46:05.230 --> 00:46:07.720你以后要把它拿出来，对吧？00:46:07.720 --> 00:46:09.400所以，显然不是真的00:46:09.400 --> 00:46:11.890记住总体上发生的事情或它试图做的事情，00:46:11.890 --> 00:46:13.915它似乎只是产生一种00:46:13.915 --> 00:46:17.785一般的配方句子，并按随机顺序排列。00:46:17.785 --> 00:46:20.635嗯，但是，我的意思是，我们可以看到它相当流利，00:46:20.635 --> 00:46:23.350它在语法上是正确的，听起来像是一个食谱。00:46:23.350 --> 00:46:25.855呃，但问题是这只是胡说八道。00:46:25.855 --> 00:46:28.300例如，把混合物塑造成00:46:28.300 --> 00:46:31.345适度的烤箱是语法上的，但它没有任何意义。00:46:31.345 --> 00:46:33.295好的，最后一个例子。00:46:33.295 --> 00:46:37.510所以，这里有一个RNN语言模型，它训练了绘画颜色名称。00:46:37.510 --> 00:46:41.200这是一个字符级语言模型的例子，因为00:46:41.200 --> 00:46:44.845它预测的是下一个角色而不是下一个单词。00:46:44.845 --> 00:46:47.650这就是它能想出新词的原因。00:46:47.650 --> 00:46:49.840另一个需要注意的是，这个语言模型00:46:49.840 --> 00:46:52.090受过训练，以某种输入为条件。00:46:52.090 --> 00:46:55.780所以这里输入的是颜色本身，我想用三个数字来表示，00:46:55.780 --> 00:46:57.145这可能是RGB数字。00:46:57.145 --> 00:47:00.925它产生了一些颜色的名称。00:47:00.925 --> 00:47:02.140我觉得这很有趣。00:47:02.140 --> 00:47:04.060我最喜欢的是臭豆子，00:47:04.060 --> 00:47:05.140在右下角。00:47:05.140 --> 00:47:07.930[笑声]嗯，这很有创意，00:47:07.930 --> 00:47:10.210[笑声]我觉得这些听起来有点像00:47:10.210 --> 00:47:13.360喜欢油漆颜色，但常常很奇怪。00:47:13.360 --> 00:47:20.570[笑声]爆炸光也很好。00:47:20.910 --> 00:47:23.500所以，呃，你会了解更多关于00:47:23.500 --> 00:47:25.765未来讲座中的角色级语言模型，00:47:25.765 --> 00:47:28.870你还将进一步了解如何调节语言模型00:47:28.870 --> 00:47:32.440基于某种输入，比如颜色，嗯，代码。00:47:32.440 --> 00:47:34.330所以，这些很有趣，00:47:34.330 --> 00:47:35.890呃，但我想警告你。00:47:35.890 --> 00:47:38.920嗯，你会在网上找到很多这样的文章，00:47:38.920 --> 00:47:40.585嗯，经常有头条新闻，比如，00:47:40.585 --> 00:47:43.000“我们强迫机器人观看，你知道，00:47:43.000 --> 00:47:46.7051000小时的科幻电影，它写了一个剧本，“差不多。00:47:46.705 --> 00:47:50.800嗯，所以，我的建议是你必须用一小撮盐吃这些，因为通常，00:47:50.800 --> 00:47:53.080呃，人们上网的例子是00:47:53.080 --> 00:47:55.375人类选择的手是最有趣的例子。00:47:55.375 --> 00:47:58.660就像我认为我今天展示的所有例子都是手工选择的00:47:58.660 --> 00:48:02.200被人类当作RNN提出的最有趣的例子。00:48:02.200 --> 00:48:05.455在某些情况下，它们甚至可能被人类编辑过。00:48:05.455 --> 00:48:08.560所以，嗯，是的，当你看这些例子的时候，你需要有点怀疑。00:48:08.560 --> 00:48:10.195[重叠]是的。00:48:10.195 --> 00:48:12.925所以，呃，在《哈利波特一号》中，00:48:12.925 --> 00:48:16.630有一个开场白，然后有一个收场白。00:48:16.630 --> 00:48:18.745所以，就像你期待RNN一样，00:48:18.745 --> 00:48:22.000就像当它把开场白写出来，并且不断地放更多的字，00:48:22.000 --> 00:48:28.825你预计收盘价的概率会随着你的去向增加还是减少？00:48:28.825 --> 00:48:31.150这是个很好的问题。所以，呃，00:48:31.150 --> 00:48:32.515问题是，呃，00:48:32.515 --> 00:48:34.450我们注意到在《哈利波特》的例子中，00:48:34.450 --> 00:48:36.295有一些开放式报价和一些封闭式报价。00:48:36.295 --> 00:48:38.410看起来这个模型没搞砸，对吧？00:48:38.410 --> 00:48:40.075所有这些开放式报价和封闭式报价，00:48:40.075 --> 00:48:41.815嗯，在正确的地方。00:48:41.815 --> 00:48:44.455所以，问题是，我们是否希望模型00:48:44.455 --> 00:48:48.775在“报价”段落中给出的报价关闭概率更高？00:48:48.775 --> 00:48:51.115所以，我应该说是的，而且00:48:51.115 --> 00:48:54.220这是最重要的――主要是解释为什么这项工作。00:48:54.220 --> 00:48:56.500嗯，有一些非常有趣的尝试工作00:48:56.500 --> 00:48:58.540看看隐藏的状态，呃，00:48:58.540 --> 00:49:01.345语言模型来看看它是否在跟踪诸如，00:49:01.345 --> 00:49:03.610我们是在开盘价还是在收盘价？00:49:03.610 --> 00:49:06.430有一些有限的证据表明00:49:06.430 --> 00:49:09.370也许在隐藏状态中有一些神经元，00:49:09.370 --> 00:49:10.900他们在跟踪一些事情，比如，00:49:10.900 --> 00:49:12.550我们目前是否在报价中？00:49:12.550 --> 00:49:13.855[噪音]。是啊。00:49:13.855 --> 00:49:18.370所以，就像你认为随着向右（重叠）的增加，概率会增加吗？00:49:18.370 --> 00:49:22.270所以，问题是，随着引文篇幅的延长，00:49:22.270 --> 00:49:23.740你认为优先还是00:49:23.740 --> 00:49:26.770输出闭式报价的概率应该增加？00:49:26.770 --> 00:49:28.045嗯，我不知道。00:49:28.045 --> 00:49:31.420也许吧。嗯，我想那很好，00:49:31.420 --> 00:49:32.980因为你不想要一个无限的引用，00:49:32.980 --> 00:49:35.650呃，但如果不是这样的话我不会感到惊讶的。00:49:35.650 --> 00:49:39.400就像我不会惊讶，如果其他一些更糟糕的语言模式，00:49:39.400 --> 00:49:41.395刚刚打开报价单，但从未关闭。00:49:41.395 --> 00:49:44.815呃，还有其他问题吗？是啊。00:49:44.815 --> 00:49:47.605W公制的尺寸是多少？00:49:47.605 --> 00:49:50.710可以。那么，问题是w度量的维度是什么？00:49:50.710 --> 00:49:52.480所以我们要回到网上。00:49:52.480 --> 00:49:55.900嗯，好吧。你在问我关于“W”或“W”或其他什么？00:49:55.900 --> 00:49:56.610是啊。00:49:56.610 --> 00:49:58.960所以，我们会00:49:58.960 --> 00:50:01.435呃，如果我们说隐藏的尺寸是N，00:50:01.435 --> 00:50:07.240那么w_h将是n乘n，如果我们假设嵌入件的尺寸为d，00:50:07.240 --> 00:50:08.635然后我们会，呃，00:50:08.635 --> 00:50:12.550也许是N，N，D。00:50:12.550 --> 00:50:19.990那能回答你的问题吗？[噪音]呃，00:50:19.990 --> 00:50:23.380关于生成还有其他问题吗？是的。00:50:23.380 --> 00:50:28.030那么，你说《哈利波特》中有一个长句子？00:50:28.030 --> 00:50:28.425是啊。00:50:28.425 --> 00:50:33.640在这个手写规则中，将RNN和LIKE结合起来是否有点实际？00:50:33.640 --> 00:50:35.395对不起的。结合起来是否可行？-00:50:35.395 --> 00:50:37.810有手写规则的书面清单的RNN。00:50:37.810 --> 00:50:38.830[重叠]00:50:38.830 --> 00:50:39.880可以。是啊。这是个很好的问题。00:50:39.880 --> 00:50:42.220所以问题是，它是否曾经实用00:50:42.220 --> 00:50:44.980结合RNN和手写规则列表？00:50:44.980 --> 00:50:49.285例如，不要让你的句子比这许多单词长。00:50:49.285 --> 00:50:50.530嗯，是的。00:50:50.530 --> 00:50:54.070我想说这可能是可行的，尤其是如果你对，呃，00:50:54.070 --> 00:50:56.260确保某些不好的事情不会发生，00:50:56.260 --> 00:51:01.900你可能会应用一些黑客规则，比如“是的”，迫使它早点结束。00:51:01.900 --> 00:51:03.580我是说，好吧。所以这就是所谓的光束搜索00:51:03.580 --> 00:51:05.335我们将在稍后的讲座中了解到，00:51:05.335 --> 00:51:06.640基本上不仅仅是，00:51:06.640 --> 00:51:09.340嗯，在每一步中选择一个单词并继续。00:51:09.340 --> 00:51:12.325它探索了许多不同的选择，你可以生成的单词。00:51:12.325 --> 00:51:14.410你可以在上面应用一些规则00:51:14.410 --> 00:51:16.540如果你有很多不同的选择，00:51:16.540 --> 00:51:18.250那你就可以摆脱00:51:18.250 --> 00:51:21.265如果你不喜欢它们，因为它们违反了你的一些规则。00:51:21.265 --> 00:51:28.340但是，嗯，这可能很难做到。还有其他问题吗？00:51:29.490 --> 00:51:38.380可以。嗯，我们已经讨论过从语言模型生成。00:51:38.380 --> 00:51:40.630呃，很不幸，你不能只用00:51:40.630 --> 00:51:44.140生成作为语言模型的评估指标。00:51:44.140 --> 00:51:47.245你确实需要某种，嗯，可测量的度量标准。00:51:47.245 --> 00:51:52.015因此，语言模型的标准评估指标被称为困惑。00:51:52.015 --> 00:51:54.250困惑的定义是00:51:54.250 --> 00:51:58.480基于语言模型的语料库逆概率。00:51:58.480 --> 00:52:02.200所以，如果你看它，你会发现这就是这个公式的意思。00:52:02.200 --> 00:52:04.075它的意思是每一个，呃，00:52:04.075 --> 00:52:07.555单词xt，小写t，在语料库中，00:52:07.555 --> 00:52:10.420我们正在计算这个词的概率00:52:10.420 --> 00:52:13.630到目前为止，一切都是如此，但它的反方向是一个。00:52:13.630 --> 00:52:16.600最后，当标准化这个大的00:52:16.600 --> 00:52:19.960嗯，按字数计算的产品，00:52:19.960 --> 00:52:23.995这就是Capital T.我们之所以这么做是因为如果我们不这么做，00:52:23.995 --> 00:52:28.195然后随着语料库的增大，困惑会越来越小。00:52:28.195 --> 00:52:31.070所以我们需要用这个因素来规范化。00:52:31.140 --> 00:52:33.910所以，你可以给你看这个，呃，00:52:33.910 --> 00:52:38.470困惑度等于交叉熵损失的指数jθ。00:52:38.470 --> 00:52:41.470所以如果你记得交叉熵损失，θ是，呃，00:52:41.470 --> 00:52:44.305我们用来训练语言模型的训练目标。00:52:44.305 --> 00:52:46.555而且，嗯，通过重新安排一些事情，00:52:46.555 --> 00:52:50.890你可以看到困惑实际上是交叉熵的指数。00:52:50.890 --> 00:52:52.750这是件好事，呃，00:52:52.750 --> 00:52:55.750因为如果我们训练语言模型00:52:55.750 --> 00:52:58.900使交叉熵损失最小化，00:52:58.900 --> 00:53:04.070然后你也在训练它来优化困惑。00:53:04.800 --> 00:53:08.860所以你应该记住，越低的困惑越好，00:53:08.860 --> 00:53:12.640因为困惑是语料库的逆概率。00:53:12.640 --> 00:53:17.965所以，如果你想让你的语言模型为语料库分配高概率，对吗？00:53:17.965 --> 00:53:21.470那就意味着你想降低困惑。00:53:21.600 --> 00:53:28.480呃，有问题吗？[噪音]好的。00:53:28.480 --> 00:53:36.220嗯，所以RNN近年来在改善困惑方面相当成功。00:53:36.220 --> 00:53:39.880这是最近的结果表，00:53:39.880 --> 00:53:43.630嗯，Facebook关于RNN语言模型的研究论文。00:53:43.630 --> 00:53:46.600而且，呃，你不必理解这张桌子的所有细节，00:53:46.600 --> 00:53:48.055但它告诉你的是，00:53:48.055 --> 00:53:50.785在上面，我们有N克语言模型。00:53:50.785 --> 00:53:52.240以及后面的各种，00:53:52.240 --> 00:53:55.735我们有一些日益复杂和庞大的RNN。00:53:55.735 --> 00:53:58.945你可以看到困惑的数字在减少，00:53:58.945 --> 00:54:00.475因为越低越好。00:54:00.475 --> 00:54:02.770所以RNN非常适合00:54:02.770 --> 00:54:06.320在过去的几年里建立更有效的语言模型。00:54:08.910 --> 00:54:11.695可以。所以缩小一点，00:54:11.695 --> 00:54:13.120你可能在想，呃，00:54:13.120 --> 00:54:15.460为什么我要关心语言建模？00:54:15.460 --> 00:54:17.350为什么很重要？我会说有00:54:17.350 --> 00:54:19.735语言建模之所以重要有两个主要原因。00:54:19.735 --> 00:54:21.160呃，所以第一个是，00:54:21.160 --> 00:54:23.620语言建模是一项基准任务，00:54:23.620 --> 00:54:26.770帮助我们衡量我们在理解语言方面的进展。00:54:26.770 --> 00:54:28.540因此，您可以将语言建模视为00:54:28.540 --> 00:54:31.990一个相当普遍的语言理解任务，对吧？00:54:31.990 --> 00:54:35.425因为预测任何一个词旁边的词，00:54:35.425 --> 00:54:37.795任何一种，呃，通用文本。00:54:37.795 --> 00:54:40.975嗯，这是一个非常困难和普遍的问题。00:54:40.975 --> 00:54:43.330为了擅长语言建模，00:54:43.330 --> 00:54:45.340你必须了解很多事情，对吗？00:54:45.340 --> 00:54:46.780你必须理解语法，00:54:46.780 --> 00:54:48.115你必须理解语法，00:54:48.115 --> 00:54:49.615你必须明白，00:54:49.615 --> 00:54:51.115嗯，逻辑和推理。00:54:51.115 --> 00:54:52.570你必须了解00:54:52.570 --> 00:54:53.845你知道，现实世界的知识。00:54:53.845 --> 00:54:55.720你必须了解很多事情才能00:54:55.720 --> 00:54:57.970能够正确进行语言建模。00:54:57.970 --> 00:54:59.530所以，我们关心它的原因是00:54:59.530 --> 00:55:02.350基准测试任务是因为如果您能够构建模型，00:55:02.350 --> 00:55:05.050哪种语言模式比之前的模式更好，00:55:05.050 --> 00:55:07.930那么你一定在上取得了一些进展00:55:07.930 --> 00:55:11.620至少是自然语言理解的一些子成分。00:55:11.620 --> 00:55:14.470所以，还有一个更具体的原因00:55:14.470 --> 00:55:16.930关注语言建模是00:55:16.930 --> 00:55:19.990许多NLP任务，特别是涉及00:55:19.990 --> 00:55:23.560生成文本或估计文本的概率。00:55:23.560 --> 00:55:25.675这里有很多例子。00:55:25.675 --> 00:55:27.220一种是预测性打字。00:55:27.220 --> 00:55:29.170这就是我们在讲座开始时展示的例子00:55:29.170 --> 00:55:31.450在手机上打字或在谷歌上搜索。00:55:31.450 --> 00:55:35.185呃，这对那些有运动障碍的人也很有用，00:55:35.185 --> 00:55:39.595因为它们是帮助人们用更少的动作交流的系统。00:55:39.595 --> 00:55:41.920另一个例子是语音识别。00:55:41.920 --> 00:55:43.600所以，在语音识别中00:55:43.600 --> 00:55:45.820一个人说话的录音00:55:45.820 --> 00:55:49.975而且常常有点吵，很难理解他们在说什么，你需要，00:55:49.975 --> 00:55:51.700呃，想想他们说了什么。00:55:51.700 --> 00:55:55.300所以这是一个例子，你必须估计不同的概率，00:55:55.300 --> 00:55:58.210呃，他们本可以说的话有不同的选择。00:55:58.210 --> 00:56:00.445同样，手写识别，00:56:00.445 --> 00:56:02.410有很多噪音的例子吗？00:56:02.410 --> 00:56:05.470你必须弄清楚那个人想说什么。00:56:05.470 --> 00:56:07.810嗯，拼写和语法修正还没有完成00:56:07.810 --> 00:56:10.705另一个例子是试图弄清楚某人的意思。00:56:10.705 --> 00:56:12.340这意味着你真的明白00:56:12.340 --> 00:56:14.695很可能是他们说了不同的话。00:56:14.695 --> 00:56:19.555一个有趣的应用程序是作者身份识别。00:56:19.555 --> 00:56:22.480所以假设你有一段文字，你想00:56:22.480 --> 00:56:25.495找出可能是谁写的，也许是你写的，00:56:25.495 --> 00:56:29.830嗯，好几个不同的作者，你有不同作者写的文章。00:56:29.830 --> 00:56:31.285例如，你可以00:56:31.285 --> 00:56:34.720在每个不同作者的文本上训练一个单独的语言模型。00:56:34.720 --> 00:56:36.160然后，因为，记住，00:56:36.160 --> 00:56:39.805语言模型可以告诉你给定文本的概率。00:56:39.805 --> 00:56:42.430然后你可以问所有不同的语言模式，00:56:42.430 --> 00:56:45.790嗯，文本和问题的可能性有多大，00:56:45.790 --> 00:56:49.720如果某个作者的语言模型说有可能00:56:49.720 --> 00:56:55.000这意味着文本、文本和问题更有可能是作者写的。00:56:55.000 --> 00:56:57.820嗯，其他例子包括机器翻译。00:56:57.820 --> 00:56:59.200这是一个巨大的，呃，00:56:59.200 --> 00:57:01.390语言模型的应用，00:57:01.390 --> 00:57:03.565因为这都是关于生成文本。00:57:03.565 --> 00:57:05.740嗯，类似地，总结是00:57:05.740 --> 00:57:09.280在给定一些输入文本的情况下，我们需要生成一些文本的任务。00:57:09.280 --> 00:57:11.185嗯，还有对话，00:57:11.185 --> 00:57:14.980并非所有对话代理都是RNN语言模型，但您可以00:57:14.980 --> 00:57:19.285构建一个对话代理，使用RNN语言模型生成文本。00:57:19.285 --> 00:57:21.560还有更多的例子。00:57:21.560 --> 00:57:25.360有什么问题吗？[笑声]是的。00:57:25.360 --> 00:57:47.875所以，我知道[听不见]00:57:47.875 --> 00:57:49.945好问题。所以，问题是，00:57:49.945 --> 00:57:51.475呃，对于其中一些例子，呃，00:57:51.475 --> 00:57:55.315例如语音识别或[噪音]图像字幕，00:57:55.315 --> 00:57:59.290输入的是音频或图像或不是文本的东西，对吗？00:57:59.290 --> 00:58:01.780所以，你不能用我们之前讨论过的方式来代表它。00:58:01.780 --> 00:58:04.180在这些例子中，00:58:04.180 --> 00:58:06.460您将有一些方法来表示输入，00:58:06.460 --> 00:58:08.725某种编码音频或图像的方法。00:58:08.725 --> 00:58:13.315呃，我现在从语言模型的角度提出它的原因是，这就是输入，00:58:13.315 --> 00:58:15.685但是您使用语言模型来获取输出，对吗？00:58:15.685 --> 00:58:17.170所以，语言模型[noise]生成00:58:17.170 --> 00:58:19.345我们之前看到的输出，呃，00:58:19.345 --> 00:58:22.120但我们稍后将进一步了解这些条件语言（噪声）模型。00:58:22.120 --> 00:58:25.090[噪音]还有人吗？00:58:25.090 --> 00:58:29.020[噪音]好的。00:58:29.020 --> 00:58:32.965[噪音]那么，呃，这是一个总结。00:58:32.965 --> 00:58:36.730如果我在演讲中找不到你，呃，或者你累了，00:58:36.730 --> 00:58:38.770嗯，现在是重新开始的好时机00:58:38.770 --> 00:58:41.050因为事情会变得更容易接近。00:58:41.050 --> 00:58:43.045可以。下面是我们今天所做的回顾。00:58:43.045 --> 00:58:46.210语言模型是预测下一个单词的系统，00:58:46.210 --> 00:58:48.460[噪音]和循环神经网络，00:58:48.460 --> 00:58:50.590是个新家庭，哦，对我们来说是个新家庭，00:58:50.590 --> 00:58:53.710一类接受顺序输入的神经网络。00:58:53.710 --> 00:58:57.175任何长度，在每一步上施加相同的重量，00:58:57.175 --> 00:58:59.620它可以选择性地在00:58:59.620 --> 00:59:02.020每一步或某些步骤，或没有任何步骤。00:59:02.020 --> 00:59:04.945[噪音]所以，不要混淆。00:59:04.945 --> 00:59:08.305循环神经网络与语言模型不同。00:59:08.305 --> 00:59:12.970嗯，我们今天已经看到RNN是构建语言模型的好方法，但是实际上，00:59:12.970 --> 00:59:15.010事实证明你可以使用RNN，00:59:15.010 --> 00:59:17.710呃，很多其他不同的东西不是语言建模。00:59:17.710 --> 00:59:19.840[噪音]这里有几个例子。00:59:19.840 --> 00:59:24.085[噪音]嗯，你可以用RNN做标记任务。00:59:24.085 --> 00:59:26.320因此，标记任务的一些例子是00:59:26.320 --> 00:59:29.260语音标记和命名实体识别的一部分。00:59:29.260 --> 00:59:32.590所以，这里的图片是语音标记的一部分，这就是任务。00:59:32.590 --> 00:59:35.245我们有一些输入文本，比如，00:59:35.245 --> 00:59:37.645惊吓的猫打翻了花瓶，00:59:37.645 --> 00:59:39.385你的工作是，00:59:39.385 --> 00:59:42.085给每个单词加上词性标签或标签。00:59:42.085 --> 00:59:45.160例如，cat是一个名词，knocked是一个动词。00:59:45.160 --> 00:59:48.205因此，您可以使用RNN在中执行此任务，00:59:48.205 --> 00:59:50.350就像我们想象的那样，你，呃，00:59:50.350 --> 00:59:52.720将文本输入RNN，[噪声]然后，00:59:52.720 --> 00:59:53.905在RNN的每一步上，00:59:53.905 --> 00:59:55.705你，呃，有输出，00:59:55.705 --> 00:59:57.790可能是关于什么，呃，00:59:57.790 --> 01:00:01.775标记你认为它是，然后，呃，你可以这样标记它。01:00:01.775 --> 01:00:04.050然后，对于命名实体识别，01:00:04.050 --> 01:00:05.190就这些，嗯，01:00:05.190 --> 01:00:08.085用它们的命名实体类型标记每个单词。01:00:08.085 --> 01:00:11.820所以，你也是这样做的。[噪音]好的。01:00:11.820 --> 01:00:13.470还有一件事你可以用RNN，01:00:13.470 --> 01:00:16.200嗯，你可以用它们来进行句子分类。01:00:16.200 --> 01:00:19.080所以，句子分类只是一个通用术语01:00:19.080 --> 01:00:22.170任何你想做句子或其他文字的任务，01:00:22.170 --> 01:00:24.945然后，你想把它分为几个类中的一个。01:00:24.945 --> 01:00:28.120所以，一个例子就是情绪分类。01:00:28.120 --> 01:00:30.400嗯，情绪分类是当你有某种01:00:30.400 --> 01:00:32.680输入文本，例如，总的来说，01:00:32.680 --> 01:00:34.510我很喜欢这部电影，然后，01:00:34.510 --> 01:00:35.770你想把它归类为01:00:35.770 --> 01:00:38.095积极或消极或[噪音]中立的情绪。01:00:38.095 --> 01:00:40.090所以，在这个例子中，这是一种积极的情绪。01:00:40.090 --> 01:00:45.400[噪音]所以，你可以用RNN来处理这个任务的一种方法是，呃，01:00:45.400 --> 01:00:49.450您可以使用RNN对文本进行编码，然后，01:00:49.450 --> 01:00:53.350你真正想要的是某种句子编码，这样你01:00:53.350 --> 01:00:57.265可以输出句子的标签，对吗？01:00:57.265 --> 01:00:59.680如果你有一个向量01:00:59.680 --> 01:01:02.965表示句子，而不是所有这些单独的向量。01:01:02.965 --> 01:01:04.870那么，你会怎么做？01:01:04.870 --> 01:01:07.000如何从RNN中获取句子编码？01:01:07.000 --> 01:01:10.540你能做的一件事就是，01:01:10.540 --> 01:01:14.290您可以使用最后的隐藏状态作为句子编码。01:01:14.290 --> 01:01:18.460所以，嗯，你认为这是个好主意的原因是，01:01:18.460 --> 01:01:19.810例如，在RNN中，01:01:19.810 --> 01:01:22.675我们认为，最终的隐藏状态是，01:01:22.675 --> 01:01:25.735嗯，这是你用来预测接下来会发生什么的事情，对吧？01:01:25.735 --> 01:01:28.300所以，我们假设最终的隐藏状态包含01:01:28.300 --> 01:01:31.465关于到目前为止所有文本的信息，对吗？01:01:31.465 --> 01:01:34.990因此，出于这个原因，你可能会认为这是一个很好的句子编码，01:01:34.990 --> 01:01:36.460我们可以用噪音来预测，01:01:36.460 --> 01:01:39.040这句话是什么感想？01:01:39.040 --> 01:01:41.350事实证明，通常情况下，这是一种更好的方法，01:01:41.350 --> 01:01:42.595通常是更有效的方法，01:01:42.595 --> 01:01:46.240是做一些事情，比如可能取元素的最大值，或者01:01:46.240 --> 01:01:50.080所有这些隐藏状态的元素含义，以获得句子编码，01:01:50.080 --> 01:01:52.345嗯，[噪音]还有，呃，01:01:52.345 --> 01:01:54.640这比使用最终隐藏状态更有效。01:01:54.640 --> 01:01:58.490[噪音]嗯，还有一些更高级的事情你也可以做。01:01:59.310 --> 01:02:02.215可以。[噪音]另一件事你可以用RNN01:02:02.215 --> 01:02:05.335是一个通用编码器模块。01:02:05.335 --> 01:02:08.470呃，这里有个回答问题的例子，01:02:08.470 --> 01:02:10.480但实际上，RNN的概念是01:02:10.480 --> 01:02:15.085通用编码器模块是非常常见的[噪声]并在许多不同的情况下使用，01:02:15.085 --> 01:02:17.590嗯，NLP的深度学习[噪音]架构。01:02:17.590 --> 01:02:21.175[噪音]那么，这里有一个回答问题的例子。01:02:21.175 --> 01:02:23.410呃，那么，假设，任务是，01:02:23.410 --> 01:02:24.670你有某种背景，01:02:24.670 --> 01:02:26.110在这种情况下，01:02:26.110 --> 01:02:29.365是关于贝多芬的维基百科文章，然后，01:02:29.365 --> 01:02:31.210你有个问题要问，01:02:31.210 --> 01:02:33.070贝多芬是什么国籍的？01:02:33.070 --> 01:02:36.400嗯，这实际上是从球队的挑战中得到的，01:02:36.400 --> 01:02:38.680这是默认最终项目的主题。01:02:38.680 --> 01:02:41.770所以，嗯，如果你选择做-做默认的最终项目，01:02:41.770 --> 01:02:44.950你将要建立解决这个问题的系统。01:02:44.950 --> 01:02:49.930所以，你可能会用RNN来处理这个问题，01:02:49.930 --> 01:02:51.970贝多芬是什么国籍的？01:02:51.970 --> 01:02:56.215然后，你可以利用这些隐藏的状态，呃，01:02:56.215 --> 01:03:00.280问题的RNN表示问题。01:03:00.280 --> 01:03:03.580我在这里故意含糊不清地说接下来会发生什么，01:03:03.580 --> 01:03:05.200但你的想法是你有[噪音]01:03:05.200 --> 01:03:08.500背景和问题都将以某种方式得到满足，01:03:08.500 --> 01:03:10.900也许你也会在上下文中使用RNN，01:03:10.900 --> 01:03:14.485为了得到你的答案，你会有更多的神经架构，01:03:14.485 --> 01:03:15.895这是德语。01:03:15.895 --> 01:03:21.355所以，这里的重点是RNN作为问题的编码器，01:03:21.355 --> 01:03:23.920也就是说，你从跑步中得到的隐藏状态01:03:23.920 --> 01:03:26.650问题的RNN代表问题。01:03:26.650 --> 01:03:31.810[噪音]呃，编码器是一个更大的神经系统的一部分，01:03:31.810 --> 01:03:33.940[噪音]这就是隐藏状态本身01:03:33.940 --> 01:03:36.295你感兴趣的，因为它们包含信息。01:03:36.295 --> 01:03:38.140所以，你可以，嗯，拿走，01:03:38.140 --> 01:03:39.700嗯，元素的最大值或平均值，01:03:39.700 --> 01:03:41.005就像我们在上一张幻灯片中显示的那样，01:03:41.005 --> 01:03:44.170为了得到问题的一个向量，但通常不会这样做。01:03:44.170 --> 01:03:48.160通常，你会，呃，做一些其他直接使用隐藏状态的事情。01:03:48.160 --> 01:03:53.440所以，这里的一般观点是RNN作为一种表示的方法非常强大，01:03:53.440 --> 01:03:54.925一系列文字，01:03:54.925 --> 01:03:57.710呃，为了进一步计算。01:03:58.170 --> 01:04:02.935可以。最后一个例子。所以，再次回到RNN语言模型，[噪音]呃，01:04:02.935 --> 01:04:04.570它们可以用来生成文本，01:04:04.570 --> 01:04:07.300有很多不同的应用程序。01:04:07.300 --> 01:04:11.020例如，语音识别，呃，你会有你的输入，01:04:11.020 --> 01:04:13.345哪一个是音频，正如一个学生之前所问的，01:04:13.345 --> 01:04:15.865这将，呃，以某种方式代表，01:04:15.865 --> 01:04:19.480然后，呃，也许你会对它进行神经编码，然后，01:04:19.480 --> 01:04:22.615使用RNN语言模型生成输出，01:04:22.615 --> 01:04:24.354在这种情况下，它将是一个转录。01:04:24.354 --> 01:04:26.275关于录音的内容。01:04:26.275 --> 01:04:28.030所以，你会有一些调节的方法，01:04:28.030 --> 01:04:29.830我们要多谈谈这是怎么运作的，呃，01:04:29.830 --> 01:04:31.780在以后的讲座中，你有办法01:04:31.780 --> 01:04:35.230在输入上调整RNN语言模型。01:04:35.230 --> 01:04:38.920所以，您将使用它生成文本，[噪声]，在本例中，01:04:38.920 --> 01:04:41.335可能是说，天气怎么样，01:04:41.335 --> 01:04:44.590问号。[重叠][噪音]01:04:44.590 --> 01:04:54.220是啊。[噪音]01:04:54.220 --> 01:04:58.120在语音识别中，[听不见]。01:04:58.120 --> 01:05:00.100可以。所以，问题是，在语音识别中，01:05:00.100 --> 01:05:02.755我们经常用单词错误率来评估，01:05:02.755 --> 01:05:04.690但你会用困惑来评价吗？01:05:04.690 --> 01:05:07.690[噪音]嗯，我其实对此不太了解。你知道吗，克里斯，01:05:07.690 --> 01:05:09.250他们用什么，呃，01:05:09.250 --> 01:05:15.010语音识别作为评估指标？[噪音]01:05:15.010 --> 01:05:23.590[听不见]字错误率[听不见]。01:05:23.590 --> 01:05:25.375答案是，你经常使用WER，01:05:25.375 --> 01:05:27.550嗯，为了逃避，但你也可能会使用困惑。01:05:27.550 --> 01:05:29.500是啊。还有其他问题吗？01:05:29.500 --> 01:05:35.575[噪音]好的。所以，嗯，01:05:35.575 --> 01:05:38.350这是一个条件语言模型的例子，01:05:38.350 --> 01:05:39.970它被称为条件语言模型01:05:39.970 --> 01:05:41.725因为我们有语言模型组件，01:05:41.725 --> 01:05:44.740但最重要的是，我们正在根据某种输入对其进行调节。01:05:44.740 --> 01:05:48.580所以，不像《哈利波特》中有趣的例子，我们只是，呃，01:05:48.580 --> 01:05:51.460基本上无条件地生成文本，01:05:51.460 --> 01:05:52.750我们根据培训数据进行培训，然后，01:05:52.750 --> 01:05:54.820我们刚开始用某种随机的种子01:05:54.820 --> 01:05:56.305然后，它无条件地生成。01:05:56.305 --> 01:05:58.540这被称为条件语言模型01:05:58.540 --> 01:06:01.615因为我们需要一些输入条件。01:06:01.615 --> 01:06:05.980嗯，机器翻译也是一个条件语言模型的例子。01:06:05.980 --> 01:06:07.780我们将在01:06:07.780 --> 01:06:09.520下周关于机器翻译的讲座。01:06:09.520 --> 01:06:12.895[噪音]好的。还有问题吗？01:06:12.895 --> 01:06:14.320我想你有点多余的时间。01:06:14.320 --> 01:06:17.665[噪音]是的。01:06:17.665 --> 01:06:20.350我有一个关于RNN的问题。01:06:20.350 --> 01:06:25.345[噪音]人们有没有把RNN结合起来？01:06:25.345 --> 01:06:27.220呃，建筑模式，01:06:27.220 --> 01:06:29.965嗯，有其他神经网络吗？01:06:29.965 --> 01:06:31.885说，你有，嗯，你知道，01:06:31.885 --> 01:06:34.285n以前可以做任何事情的层，01:06:34.285 --> 01:06:35.410在你的网络末端，01:06:35.410 --> 01:06:36.880你想让他们跑过去，01:06:36.880 --> 01:06:39.160嗯，五个重复层。01:06:39.160 --> 01:06:40.810人们会像那样混搭吗？01:06:40.810 --> 01:06:42.190或者这些，呃，[听不见]。[噪音]01:06:42.190 --> 01:06:46.090呃，问题是，01:06:46.090 --> 01:06:48.580您是否曾经将RNN与其他类型的体系结构结合在一起？01:06:48.580 --> 01:06:49.870所以，我认为答案是肯定的。01:06:49.870 --> 01:06:51.595[噪音]呃，你可能[噪音]你知道，呃，01:06:51.595 --> 01:06:55.210你可能有其他类型的架构，呃，01:06:55.210 --> 01:06:58.540为了产生将作为RNN输入的向量，01:06:58.540 --> 01:07:00.280或者您可以使用RNN的输出01:07:00.280 --> 01:07:03.320[噪音]并将其输入不同类型的神经网络。01:07:06.390 --> 01:07:08.620所以，是的。[噪音]还有其他问题吗？01:07:08.620 --> 01:07:11.820[噪音]好的。01:07:11.820 --> 01:07:15.510嗯，那么，在我们结束之前，呃，我有一个关于术语的注释。01:07:15.510 --> 01:07:17.490呃，当你读报纸的时候，01:07:17.490 --> 01:07:20.915你可能经常会发现这个短语香草RNN，01:07:20.915 --> 01:07:23.065当你看到“香草RNN”这个短语时，01:07:23.065 --> 01:07:24.535这通常意味着，呃，01:07:24.535 --> 01:07:26.905本课描述的RNN。01:07:26.905 --> 01:07:30.460所以，这些被称为香草RNN的原因是01:07:30.460 --> 01:07:34.765因为实际上还有其他更复杂的RNN口味。01:07:34.765 --> 01:07:38.005例如，有GRU和LSTM，01:07:38.005 --> 01:07:40.330下周我们会了解这两种情况。01:07:40.330 --> 01:07:42.610还有一件事我们下周要了解01:07:42.610 --> 01:07:45.085[噪音]实际上你可以得到一些多层RNN，01:07:45.085 --> 01:07:48.250也就是说，您将多个RNN堆叠在一起。01:07:48.250 --> 01:07:50.935[噪音]所以，呃，你要了解这些，01:07:50.935 --> 01:07:53.875但我们希望在课程结束时，01:07:53.875 --> 01:07:56.905你将能够阅读一篇研究论文并看到一个短语01:07:56.905 --> 01:08:01.150具有剩余连接和自我关注的堆叠双向LSTM，01:08:01.150 --> 01:08:02.680你就知道那是什么了。01:08:02.680 --> 01:08:04.840[噪音]所有的配料都是RNN。01:08:04.840 --> 01:08:07.840[笑声]好吧。谢谢您。今天就到此为止。01:08:07.840 --> 01:08:15.910下一次-下一次01:08:15.910 --> 01:08:18.340我们正在学习问题[噪音]和花哨的RNN。01:08:18.340 --> 01:08:24.770
[NOISE]

