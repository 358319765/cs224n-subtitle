WEBVTT
Kind: captions
Language: en

00:00:04.700 --> 00:00:07.140大家好。我是艾比。00:00:07.140 --> 00:00:08.355如果你上周不在这里，00:00:08.355 --> 00:00:10.190我是这门课的助教组长。00:00:10.190 --> 00:00:13.325这是我三次演讲中的第二次00:00:13.325 --> 00:00:17.180将提供RNN和相关主题。00:00:17.360 --> 00:00:20.430可以。所以，欢迎来到第四周。00:00:20.430 --> 00:00:22.960今天，我们将学习消失梯度，00:00:22.960 --> 00:00:25.120以及一些更复杂的RNN类型。00:00:25.120 --> 00:00:26.815所以，在我们开始之前，00:00:26.815 --> 00:00:28.000我有几个通知。00:00:28.000 --> 00:00:33.150呃，第一个公告是今天发布了任务4，00:00:33.150 --> 00:00:35.835下星期四，而不是星期二，00:00:35.835 --> 00:00:39.795所以这意味着你有两天的时间做这件事，比你在其他家庭工作的时间多。00:00:39.795 --> 00:00:41.410原因是作业四00:00:41.410 --> 00:00:43.360到目前为止，可能比其他家庭作业更多的工作，00:00:43.360 --> 00:00:45.415所以不要惊讶。00:00:45.415 --> 00:00:48.575任务四是关于神经机器翻译的。00:00:48.575 --> 00:00:52.135嗯，这周我们要在周四的讲座上了解NMT。00:00:52.135 --> 00:00:54.270而且，呃，这真的很令人兴奋，00:00:54.270 --> 00:00:57.600因为实际上CS 224以前从未有过NMT任务，00:00:57.600 --> 00:00:58.740所以今年都是新的一年，00:00:58.740 --> 00:01:02.420你将是第一年的学生，他们将要做一个NMT作业。00:01:02.420 --> 00:01:04.910呃，还有一些不同的地方00:01:04.910 --> 00:01:07.610任务四是你将要使用的是Azure，也就是，呃，00:01:07.610 --> 00:01:09.170云计算服务，00:01:09.170 --> 00:01:13.145为了用GPU在虚拟机上训练NMT系统。00:01:13.145 --> 00:01:17.365而且，呃，为了在合理的时间内做到这一点，这是必要的。00:01:17.365 --> 00:01:19.250所以，我有一个警告，就是，00:01:19.250 --> 00:01:20.930如果你是一个可能没有的人，啊，00:01:20.930 --> 00:01:24.145学到了-在远程机器上工作的很多经验，00:01:24.145 --> 00:01:26.360例如，如果您不太熟悉ssh，00:01:26.360 --> 00:01:28.880或者TMUX，或者远程文本编辑，00:01:28.880 --> 00:01:31.580那么我建议你为第四项任务安排一些额外的时间，00:01:31.580 --> 00:01:34.945因为这可能需要你一点时间来建立和适应。00:01:34.945 --> 00:01:37.280所以，我再次强调，00:01:37.280 --> 00:01:40.250一定要在第四个任务上早点开始，因为，呃，00:01:40.250 --> 00:01:43.775在您的虚拟机上运行NMT系统大约需要4个小时，00:01:43.775 --> 00:01:47.230所以你真的不能在前一天晚上启动它，并且期望它能准时到达。00:01:47.230 --> 00:01:51.245嗯，作业四比作业三复杂得多。00:01:51.245 --> 00:01:55.250所以，呃，如果你觉得任务三简单的话，不要误会安全感。00:01:55.250 --> 00:02:00.640嗯，所以星期四关于NMT的幻灯片今天在网站上准备好了，00:02:00.640 --> 00:02:02.610所以你今天甚至可以开始看它，如果你00:02:02.610 --> 00:02:05.910想要-如果你想早点开始作业四。00:02:05.910 --> 00:02:08.310呃，那么，我还有一些通知，呃，00:02:08.310 --> 00:02:10.080关于项目，呃，00:02:10.080 --> 00:02:11.860下周的讲座将是关于项目的。00:02:11.860 --> 00:02:14.630所以，你会听到，呃，问答，00:02:14.630 --> 00:02:16.445以及默认的最终项目，00:02:16.445 --> 00:02:18.620然后你也会得到一些关于如何做的提示，00:02:18.620 --> 00:02:21.205嗯，选择并定义自己的自定义项目。00:02:21.205 --> 00:02:24.180所以，如果你本周没有考虑一个项目，那没关系。00:02:24.180 --> 00:02:27.130你可以推迟到下周再开始考虑这件事。00:02:27.130 --> 00:02:29.630但是如果你是一个已经在考虑你的项目的人，00:02:29.630 --> 00:02:32.200例如，如果你想选择你的定制项目，00:02:32.200 --> 00:02:34.295然后你应该查看网站的项目页面，00:02:34.295 --> 00:02:36.230因为它有很多关于，呃，00:02:36.230 --> 00:02:39.170如何选择你的项目，还有一些灵感。00:02:39.170 --> 00:02:41.645其中包括-我们收集了一些，呃，00:02:41.645 --> 00:02:44.650斯坦福人工智能实验室不同成员的项目想法。00:02:44.650 --> 00:02:48.155所以，这些是教师、博士生和博士后，00:02:48.155 --> 00:02:49.760有想法的人，呃，00:02:49.760 --> 00:02:51.560他们想要的NLP深度学习项目00:02:51.560 --> 00:02:54.150像你这样的CS224N学生去工作。00:02:54.150 --> 00:02:57.095所以，尤其是，如果你想以后再做研究，00:02:57.095 --> 00:02:59.180这真是个好机会，呃，00:02:59.180 --> 00:03:00.905在斯坦福人工智能实验室工作，00:03:00.905 --> 00:03:03.270也许还能得到一些指导。00:03:03.700 --> 00:03:06.840可以。下面是一个概述。00:03:06.840 --> 00:03:10.080上周，我们了解了神经网络的复发，00:03:10.080 --> 00:03:12.725嗯，我们了解了为什么它们对语言建模非常好。00:03:12.725 --> 00:03:15.335今天，我们将了解RNN的一些问题，00:03:15.335 --> 00:03:16.880我们将学习如何修复它们。00:03:16.880 --> 00:03:22.035这将激励我们学习一些更复杂的RNN变体。00:03:22.035 --> 00:03:24.300然后，呃，下周四的讲座，00:03:24.300 --> 00:03:27.570我们会有更多的基于应用程序的内容，00:03:27.570 --> 00:03:29.750所以我们将学习神经机器翻译，00:03:29.750 --> 00:03:31.700这是一项非常重要的任务，00:03:31.700 --> 00:03:33.830NLP和深入学习，尤其是，00:03:33.830 --> 00:03:37.435我们将学习这个叫做顺序到顺序的体系结构。00:03:37.435 --> 00:03:40.080但更详细的说，00:03:40.080 --> 00:03:41.640今天的讲座，首先，00:03:41.640 --> 00:03:43.650我们将学习消失梯度问题。00:03:43.650 --> 00:03:46.220这会激励我们学习两种新的00:03:46.220 --> 00:03:48.890RNN称之为长期短期记忆，00:03:48.890 --> 00:03:50.855以及门控循环单元。00:03:50.855 --> 00:03:52.940我们还将学习其他类型的00:03:52.940 --> 00:03:55.640对消失梯度问题的各种修正，00:03:55.640 --> 00:03:57.190或者爆炸梯度问题。00:03:57.190 --> 00:03:58.370呃，特别是，00:03:58.370 --> 00:03:59.790我们将学习梯度裁剪，00:03:59.790 --> 00:04:02.190这很简单，但很重要。00:04:02.190 --> 00:04:04.710呃，我们还将学习跳过连接，00:04:04.710 --> 00:04:07.080这是一种全新的神经结构，00:04:07.080 --> 00:04:08.150它试图，呃，00:04:08.150 --> 00:04:09.350解决消失梯度问题。00:04:09.350 --> 00:04:11.640[噪音]然后，在演讲结束时，00:04:11.640 --> 00:04:14.210我们将学习一些更奇特的RNN变体，比如，00:04:14.210 --> 00:04:17.780双向RN-RNN，它们不只是从左到右，00:04:17.780 --> 00:04:18.980而且从右到左，00:04:18.980 --> 00:04:21.740我们将学习多层RNN。00:04:21.740 --> 00:04:24.880当你把多个RNN堆叠在一起的时候。00:04:24.880 --> 00:04:27.560所以，今天有很多重要的定义。00:04:27.560 --> 00:04:29.870嗯，那么，你会发现00:04:29.870 --> 00:04:31.160这个讲座对00:04:31.160 --> 00:04:34.080作业四，可能也适用于你的项目。00:04:35.450 --> 00:04:40.250可以。那么，让我们开始考虑消失梯度。00:04:40.250 --> 00:04:42.350呃，我们有一个RNN，00:04:42.350 --> 00:04:44.390比如说，有四个台阶，00:04:44.390 --> 00:04:46.955假设我们有某种损失，呃，00:04:46.955 --> 00:04:50.915这是根据四种隐藏状态计算出来的。00:04:50.915 --> 00:04:56.300那么，假设我们有兴趣问一下，这个损失的导数是什么？00:04:56.300 --> 00:04:58.340关于隐藏状态，00:04:58.340 --> 00:05:00.865呃，h1，第一个隐藏状态？00:05:00.865 --> 00:05:03.390所以，我代表这个，呃，00:05:03.390 --> 00:05:05.720蓝色箭头符号表示我们00:05:05.720 --> 00:05:09.070使梯度向后流动以完成此操作。00:05:09.070 --> 00:05:11.750如果我们对这个梯度感兴趣，00:05:11.750 --> 00:05:13.850我们可以应用链式法则，然后说，“好吧，00:05:13.850 --> 00:05:15.035它是，呃，00:05:15.035 --> 00:05:17.560损失相对于h2的梯度，00:05:17.560 --> 00:05:20.975然后是h2相对于h1的梯度。”00:05:20.975 --> 00:05:23.670然后，同样地，我们可以分解它00:05:23.670 --> 00:05:27.255再次使用链式法则，我们可以再做一次。00:05:27.255 --> 00:05:31.790我们在这里做的是，分解我们感兴趣的梯度，00:05:31.790 --> 00:05:35.375这些中间梯度的产物。00:05:35.375 --> 00:05:39.290特别是，我们看到了所有这些，ht减去1，00:05:39.290 --> 00:05:41.975呃，隐藏状态的相邻梯度。00:05:41.975 --> 00:05:43.910所以，我想问你的是，00:05:43.910 --> 00:05:47.130如果这些梯度很小怎么办？00:05:47.130 --> 00:05:49.020考虑到他们很多，00:05:49.020 --> 00:05:52.090呃，如果它们的数量很小怎么办？00:05:52.130 --> 00:05:56.865那么，消失梯度问题的整体问题，00:05:56.865 --> 00:05:59.340当这些梯度很小的时候，00:05:59.340 --> 00:06:02.525然后我们的整体梯度会越来越小，00:06:02.525 --> 00:06:04.375随着它的进一步传播。00:06:04.375 --> 00:06:09.150因为累积梯度是所有这些中间梯度的乘积。00:06:09.150 --> 00:06:11.275当你用某个小东西乘以某个小东西时，00:06:11.275 --> 00:06:12.970然后整个事情变小了。00:06:12.970 --> 00:06:15.220所以，这就是我在这里所代表的，呃，00:06:15.220 --> 00:06:18.230越来越小的蓝色箭头向后移动。00:06:18.290 --> 00:06:21.565这就是消失梯度问题的一般概念。00:06:21.565 --> 00:06:23.735这里有一个更正式的定义。00:06:23.735 --> 00:06:25.905所以，如果你记得上次，00:06:25.905 --> 00:06:28.095如果我们有空RNN，00:06:28.095 --> 00:06:30.490那么隐藏状态ht是00:06:30.490 --> 00:06:33.490作为先前隐藏状态ht减去1的函数计算，00:06:33.490 --> 00:06:34.810以及电流输入xt。00:06:34.810 --> 00:06:37.880呃，所以你可能记得上节课我们00:06:37.880 --> 00:06:41.075说xt是表示单词的一个热向量，00:06:41.075 --> 00:06:42.620然后是嵌入。00:06:42.620 --> 00:06:44.060呃，这节课我们要讲，00:06:44.060 --> 00:06:45.110呃，把那些细节处理掉，00:06:45.110 --> 00:06:47.060我们只是抽象地思考00:06:47.060 --> 00:06:49.455具有某种输入xt的RNN，00:06:49.455 --> 00:06:51.210xt是任何一种向量。00:06:51.210 --> 00:06:52.260可能是密度向量，00:06:52.260 --> 00:06:53.925但你知道，这可能是文字，也可能不是。00:06:53.925 --> 00:06:55.500可能是热的，也可能是浓的。00:06:55.500 --> 00:06:57.810嗯，但这只是输入。00:06:57.810 --> 00:07:00.210所以，这就是，呃，00:07:00.210 --> 00:07:03.365我们上次学习的香草RNN的定义。00:07:03.365 --> 00:07:05.930这意味着，ht的导数，00:07:05.930 --> 00:07:08.480步骤t上的隐藏状态相对于前一个隐藏状态，00:07:08.480 --> 00:07:10.075呃，这是这个表达方式吗？00:07:10.075 --> 00:07:13.880呃，这只是链式法则的一个应用，而且，00:07:13.880 --> 00:07:16.010如果你看得够久或回头看00:07:16.010 --> 00:07:18.590你会看到的，嗯，这是有道理的。00:07:18.590 --> 00:07:20.585所以，特别是，我们，嗯，00:07:20.585 --> 00:07:23.210最后乘以wh，呃，00:07:23.210 --> 00:07:27.400因为里面有wh和ht减去1的乘法。00:07:27.400 --> 00:07:30.590可以。所以，如果你记得，在上一张幻灯片上，00:07:30.590 --> 00:07:33.650我们在考虑损失的梯度是多少，00:07:33.650 --> 00:07:34.910我想说的是，00:07:34.910 --> 00:07:36.980关于隐藏状态hj，00:07:36.980 --> 00:07:38.795在前面的步骤j。00:07:38.795 --> 00:07:41.950也许J比我早了好几步。00:07:41.950 --> 00:07:44.235所以，我们现在可以写这个了，00:07:44.235 --> 00:07:46.130呃，用下面的方法。00:07:46.130 --> 00:07:48.455所以只要应用链式法则，00:07:48.455 --> 00:07:51.365首先，我们要说的是，我们感兴趣的这个导数00:07:51.365 --> 00:07:54.920可以分解为步骤i的导数，00:07:54.920 --> 00:07:56.270这是最后一步，00:07:56.270 --> 00:08:00.265然后做所有这些相邻隐藏状态的中间梯度。00:08:00.265 --> 00:08:04.100所以，第一张幻灯片和我们刚才看到的完全一样，00:08:04.100 --> 00:08:08.160嗯，这张照片，上一张幻灯片上的图表。00:08:08.300 --> 00:08:11.435可以。然后，考虑到我们知道什么是，呃，00:08:11.435 --> 00:08:14.030DHT减去1，00:08:14.030 --> 00:08:15.050啊，再往下看，00:08:15.050 --> 00:08:16.775然后我们可以用它来代替。00:08:16.775 --> 00:08:19.370所以，我们发现这个整体梯度00:08:19.370 --> 00:08:21.795特别是对00:08:21.795 --> 00:08:23.445有这个词，呃，00:08:23.445 --> 00:08:26.790重量矩阵，它是，呃，00:08:26.790 --> 00:08:29.020乘以自身，我减去j次，00:08:29.020 --> 00:08:32.535因为我减去j，在，呃，00:08:32.535 --> 00:08:33.825步骤j和步骤i，00:08:33.825 --> 00:08:37.520这就是我们用这个梯度移动的距离。00:08:37.520 --> 00:08:39.755所以，这里的大问题是，00:08:39.755 --> 00:08:42.665如果这个重量矩阵wh很小，00:08:42.665 --> 00:08:45.445然后这个词会变得非常小，00:08:45.445 --> 00:08:49.960指数小，因为i和j进一步分开。00:08:49.960 --> 00:08:53.740所以，更详细的说，00:08:53.740 --> 00:08:55.150我们可以考虑，呃，00:08:55.150 --> 00:08:58.360所有这些矩阵的l2矩阵范数，对吗？00:08:58.360 --> 00:09:03.840还有，呃，作为-作为-作为-作为-呃，作为-对不起。00:09:03.840 --> 00:09:06.405我——这是一个众所周知的事实，00:09:06.405 --> 00:09:08.340呃，L2规范你有这个，嗯，00:09:08.340 --> 00:09:10.605不平等，这就是，呃，00:09:10.605 --> 00:09:12.450产品规格00:09:12.450 --> 00:09:15.740有些矩阵小于或等于矩阵范数的乘积。00:09:15.740 --> 00:09:19.705特别是，我们看到了我们感兴趣的梯度的范数，00:09:19.705 --> 00:09:21.905小于或等于，呃，00:09:21.905 --> 00:09:26.690乘积i减去j，乘以重量矩阵wh的范数。00:09:26.690 --> 00:09:30.415所以，这就是我们所说的，当我们担心什么是小的时候，00:09:30.415 --> 00:09:34.180因为如果它很小，那么左边的东西就必须是指数级的小。00:09:34.180 --> 00:09:36.000特别是在这个问题上，00:09:36.000 --> 00:09:37.065呃，纸上写的，呃，00:09:37.065 --> 00:09:40.080如果你感兴趣的话，你可以看看底部，嗯，呃，00:09:40.080 --> 00:09:41.960Pascanu等人证明00:09:41.960 --> 00:09:46.265权重矩阵wh的最大特征值小于1，00:09:46.265 --> 00:09:49.910然后左边的梯度会呈指数收缩。00:09:49.910 --> 00:09:52.385你可以直观地看到为什么这是真的。00:09:52.385 --> 00:09:53.960所以如果，你知道，作为一个简化的假设，00:09:53.960 --> 00:09:56.090我们假设wh不是矩阵，00:09:56.090 --> 00:09:58.765但只是一个标量，它只是一个数字，00:09:58.765 --> 00:10:01.760那你就知道为什么如果这个数字大于1，00:10:01.760 --> 00:10:03.350然后整个事情就要爆炸了。00:10:03.350 --> 00:10:04.955如果这个数字小于一，00:10:04.955 --> 00:10:06.305然后它会收缩00:10:06.305 --> 00:10:09.070一次又一次地用同一个数字乘。00:10:09.070 --> 00:10:12.710呃，你可以去看看报纸了解更多细节，00:10:12.710 --> 00:10:14.960但是这里，呃，边界是一个，00:10:14.960 --> 00:10:17.725部分原因是我们有乙状结肠的非线性。00:10:17.725 --> 00:10:21.140根据我们所知道的范围，00:10:21.140 --> 00:10:24.300呃，乙状结肠功能正常。00:10:24.490 --> 00:10:29.400所以，呃，这告诉你为什么00:10:29.400 --> 00:10:31.020wh矩阵很小，00:10:31.020 --> 00:10:32.600或者如果它的最大特征值很小，00:10:32.600 --> 00:10:34.280然后我们会得到消失梯度。00:10:34.280 --> 00:10:35.930同样的，如果你看了报纸，00:10:35.930 --> 00:10:38.140你可以看到有类似的证据，呃，00:10:38.140 --> 00:10:40.975如果最大特征值大于1，00:10:40.975 --> 00:10:43.000有爆炸的梯度。00:10:43.000 --> 00:10:45.145所以当坡度越来越大的时候，00:10:45.145 --> 00:10:47.240当你进一步后退时。00:10:48.270 --> 00:10:52.075可以。希望我能说服你00:10:52.075 --> 00:10:55.240消失梯度是一种发生在我们规范中的现象。00:10:55.240 --> 00:10:57.475但我还没说为什么这是个问题。00:10:57.475 --> 00:11:00.340那么，我们为什么要把这看作一件坏事呢？00:11:00.340 --> 00:11:02.515如果坡度越来越大，00:11:02.515 --> 00:11:04.570或者像你的后俯那样越来越小？00:11:04.570 --> 00:11:08.795这是，呃，这是一张图片，可以说明为什么这是件坏事。00:11:08.795 --> 00:11:10.500所以，呃，和以前一样，00:11:10.500 --> 00:11:11.790假设我们正在考虑，00:11:11.790 --> 00:11:14.130损失的导数是多少00:11:14.130 --> 00:11:16.740关于第一个隐藏状态的第四步？00:11:16.740 --> 00:11:18.270我们现在的情况是00:11:18.270 --> 00:11:21.560当它向后移动时，梯度越来越小。00:11:21.560 --> 00:11:24.820但是，想想，我们说的梯度是什么00:11:24.820 --> 00:11:28.270第二步的损失也与第一隐藏状态有关。00:11:28.270 --> 00:11:30.745所以我用橙色箭头表示。00:11:30.745 --> 00:11:32.635我想说的是，00:11:32.635 --> 00:11:37.765是近距离梯度信号的大小，00:11:37.765 --> 00:11:42.175比远处梯度信号的幅度大得多。00:11:42.175 --> 00:11:45.790这意味着当你更新你的模型权重时，00:11:45.790 --> 00:11:48.520你从附近传来的信号会00:11:48.520 --> 00:11:50.770比远处的信号大很多，00:11:50.770 --> 00:11:52.585基本上你只会学习，00:11:52.585 --> 00:11:54.610你只会优化00:11:54.610 --> 00:11:57.535这些邻近效应而不是长期效应。00:11:57.535 --> 00:12:02.200所以你会失去长期效果，呃，在里面，00:12:02.200 --> 00:12:07.385附近的影响。有什么问题吗？00:12:07.385 --> 00:12:11.490所以，呃，在那里他们说你做了真正的更新。00:12:11.490 --> 00:12:15.235你知道，实际上有一些是多条链，而不仅仅是一条。00:12:15.235 --> 00:12:17.815所以近期应该包括它。00:12:17.815 --> 00:12:19.330对不起，最后一部分是什么？00:12:19.330 --> 00:12:22.405考虑到你00:12:22.405 --> 00:12:25.870更新不同链上的权重总和。00:12:25.870 --> 00:12:29.035可以。所以我想，啊，观察到，00:12:29.035 --> 00:12:30.250例如，考虑到这一点，00:12:30.250 --> 00:12:32.800在语言建模中，您可能会总结出多个损失。00:12:32.800 --> 00:12:35.860每一步都有损失，你把所有的损失加起来，这就是你的全部损失。00:12:35.860 --> 00:12:40.600然后，您需要更新更多关于附近损失的信息，而不是远处损失的信息。00:12:40.600 --> 00:12:41.950所以我想，嗯，是的，00:12:41.950 --> 00:12:43.750所以如果你的目标函数的设计00:12:43.750 --> 00:12:45.460是每一步的损失之和，00:12:45.460 --> 00:12:46.765那么你真的想，呃，00:12:46.765 --> 00:12:48.325所有的重量相等。00:12:48.325 --> 00:12:50.410我想，呃，我的观点是，00:12:50.410 --> 00:12:52.675什么影响，呃，00:12:52.675 --> 00:12:55.285重量矩阵在这个早期阶段的作用。00:12:55.285 --> 00:12:57.595它对附近的损失有什么影响？00:12:57.595 --> 00:13:00.250它对遥远的损失有什么影响？00:13:00.250 --> 00:13:02.875嗯，由于，呃，00:13:02.875 --> 00:13:05.290消失梯度的动力学，呃，00:13:05.290 --> 00:13:07.420问题解决了，那么，呃，00:13:07.420 --> 00:13:09.250对遥远的损失的影响00:13:09.250 --> 00:13:11.185会比附近的影响小很多。00:13:11.185 --> 00:13:15.040稍后我会给你更多的语言学例子，说明你为什么想学习，00:13:15.040 --> 00:13:17.065嗯，远处的连接线。00:13:17.065 --> 00:13:18.190所以本质上问题是，00:13:18.190 --> 00:13:20.260在你想学习这种联系的情况下00:13:20.260 --> 00:13:23.125在较早发生的事情和较晚发生的事情之间，00:13:23.125 --> 00:13:25.090那你就不能学会这种联系了。00:13:25.090 --> 00:13:28.330嗯，那么我们马上就会看到一些激励人心的例子。00:13:28.330 --> 00:13:36.130还有其他问题吗？是啊？00:13:36.130 --> 00:13:39.970嗯，我有点困惑了，你为什么说像DH，DJ DH。00:13:39.970 --> 00:13:46.509呃，这就像h参数，我们要去吗？-00:13:46.509 --> 00:13:46.835是啊。00:13:46.835 --> 00:13:47.020从-00:13:47.020 --> 00:13:48.685可以。这是个很好的问题。00:13:48.685 --> 00:13:52.510所以你问我们为什么对DJ感兴趣00:13:52.510 --> 00:13:56.935假设我们不更新h.h是一个激活而不是一个权重。00:13:56.935 --> 00:14:00.520嗯，所以我们考虑这个的原因，00:14:00.520 --> 00:14:04.330因为当你想到什么是DW的DJ时，00:14:04.330 --> 00:14:05.800这是我们要更新的内容。00:14:05.800 --> 00:14:10.300从某种意义上说，这总是由卫生部的DJ来衡量的，对吗？00:14:10.300 --> 00:14:12.040所以如果我们考虑w，你知道，00:14:12.040 --> 00:14:13.615它的作用，呃，00:14:13.615 --> 00:14:15.550从H_1到H_2的传输，00:14:15.550 --> 00:14:21.355然后，在该位置的dj4 by w必须通过dh_2的dj4。00:14:21.355 --> 00:14:23.620所以如果我们得到消失梯度，00:14:23.620 --> 00:14:25.795呃，随着我们进一步传播，00:14:25.795 --> 00:14:27.190那就有点像瓶颈了。00:14:27.190 --> 00:14:29.740那么你肯定会有消失梯度，因为它们会影响，呃，00:14:29.740 --> 00:14:31.540这里的递推矩阵，00:14:31.540 --> 00:14:35.540实际上是应用于输入的矩阵。00:14:38.880 --> 00:14:41.990可以。我要走了。00:14:42.150 --> 00:14:46.300呃，另一种解释梯度消失的方法是个问题，00:14:46.300 --> 00:14:49.435你可以把它看成，呃，梯度。00:14:49.435 --> 00:14:53.185你可以把它看作是对过去对未来影响的一种衡量。00:14:53.185 --> 00:14:54.760所以我们已经谈过了。00:14:54.760 --> 00:14:57.460呃，梯度就像是说，如果我改变，呃，00:14:57.460 --> 00:14:59.575这个重量或者这个激活有点，00:14:59.575 --> 00:15:03.010那么它在未来对这件事有多大的影响，又有多大。00:15:03.010 --> 00:15:08.650因此，特别是，如果我们的梯度在更长的距离内变得越来越小，00:15:08.650 --> 00:15:12.130假设从步骤t到步骤t加n，00:15:12.130 --> 00:15:15.790那么我们就无法判断是否在两种情况中的一种。00:15:15.790 --> 00:15:18.970所以第一种情况可能是00:15:18.970 --> 00:15:22.165数据中的步骤t和步骤t加上n。00:15:22.165 --> 00:15:24.715所以也许我们正在学习一项任务，00:15:24.715 --> 00:15:27.190在这项任务中确实没有收集，呃，00:15:27.190 --> 00:15:29.080关系00:15:29.080 --> 00:15:31.510在步骤t和步骤t之间学习00:15:31.510 --> 00:15:33.580步骤t加n。所以没有什么可以做的。00:15:33.580 --> 00:15:35.980学会了，事实上应该是正确的，00:15:35.980 --> 00:15:38.800你知道，这两件事的梯度很小。00:15:38.800 --> 00:15:41.620但第二种可能性是，是的，00:15:41.620 --> 00:15:44.965这是数据和任务中这两件事之间的真正联系。00:15:44.965 --> 00:15:47.650理想情况下，我们应该学习这种联系。00:15:47.650 --> 00:15:52.480嗯，但是我们的模型中有错误的参数来捕捉这个东西，00:15:52.480 --> 00:15:54.115所以这就是为什么，00:15:54.115 --> 00:15:55.165梯度很小。00:15:55.165 --> 00:15:57.340因为模型不认为它们是连接的。00:15:57.340 --> 00:16:00.940所以我们并没有了解到这两件事之间真正的依赖性。00:16:00.940 --> 00:16:03.970消失梯度问题的问题是，00:16:03.970 --> 00:16:05.740在这种情况下我们无法判断，00:16:05.740 --> 00:16:07.795我们处于这两种情况中的哪一种。00:16:07.795 --> 00:16:09.820可以。所以这都是理论上的。00:16:09.820 --> 00:16:11.365我认为这个例子应该让它更进一步，00:16:11.365 --> 00:16:14.590更清楚为什么消失梯度问题是坏的。00:16:14.590 --> 00:16:17.695上周我们学习了RNN语言模型。00:16:17.695 --> 00:16:20.680如果你还记得语言建模是一项任务，你有一些00:16:20.680 --> 00:16:24.010然后你就在试着预测下一个单词是什么。00:16:24.010 --> 00:16:25.630这是一段文字。00:16:25.630 --> 00:16:28.420上面写着，“嗯，”当她试图打印她的票时，00:16:28.420 --> 00:16:30.310她发现打印机的墨粉用完了。00:16:30.310 --> 00:16:32.665她去文具店买了更多的墨粉。00:16:32.665 --> 00:16:34.150价格太高了。00:16:34.150 --> 00:16:36.115将碳粉装入打印机后，00:16:36.115 --> 00:16:38.110她终于打印了她，“和00:16:38.110 --> 00:16:40.524有人能喊出你认为下一个该说什么吗？00:16:40.524 --> 00:16:41.395门票。00:16:41.395 --> 00:16:42.790门票。是的，没错。00:16:42.790 --> 00:16:44.920所以这对你来说很容易，因为，呃，00:16:44.920 --> 00:16:47.275从逻辑上讲，如果这是她想做的事情，00:16:47.275 --> 00:16:51.055这就是她要做的事情，一旦她走了一整条路去买墨粉。00:16:51.055 --> 00:16:53.830嗯，问题是，00:16:53.830 --> 00:16:57.400RNN语言模型能很容易地回答这个问题吗？00:16:57.400 --> 00:17:00.429他们会在这个特定的语言建模示例上做得很好吗？00:17:00.429 --> 00:17:04.060因此，对于一个RNN语言模型来说，要在这类示例中表现出色，00:17:04.060 --> 00:17:07.360然后他们需要从培训数据中的此类示例中学习。00:17:07.360 --> 00:17:09.745所以如果它解决了训练数据中的例子，00:17:09.745 --> 00:17:12.475然后RNN语言模型将需要为依赖关系建模。00:17:12.475 --> 00:17:14.560学习外观之间的联系00:17:14.560 --> 00:17:17.065在第七步的早期，00:17:17.065 --> 00:17:20.200最后是目标词的罚单。00:17:20.200 --> 00:17:22.945但是如果我们有消失梯度问题，00:17:22.945 --> 00:17:25.855然后这些梯度，如果他们知道步骤，00:17:25.855 --> 00:17:27.955关于早期步骤的最后一步，00:17:27.955 --> 00:17:29.380因为它很小，00:17:29.380 --> 00:17:31.120这是一段相当长的距离，对吧？00:17:31.120 --> 00:17:33.310这意味着模型将无法00:17:33.310 --> 00:17:36.040很容易或者完全了解这种依赖性。00:17:36.040 --> 00:17:39.340所以，如果模型在训练过程中不能学会这种依赖性，00:17:39.340 --> 00:17:41.590那么这个模型就无法预测00:17:41.590 --> 00:17:44.845在测试时类似的长距离依赖。00:17:44.845 --> 00:17:47.545好吧，这是另一个例子。00:17:47.545 --> 00:17:50.200嗯，这是一段文字。00:17:50.200 --> 00:17:52.780嗯，课文上写着，这不是一个完整的句子。00:17:52.780 --> 00:17:54.160这只是一个部分句子。00:17:54.160 --> 00:17:56.935上面写着，书的作者，空白。00:17:56.935 --> 00:17:58.570我给你两个选择。00:17:58.570 --> 00:18:03.325要么是书的作者，要么是书的作者。00:18:03.325 --> 00:18:07.480所以，呃，再次大声喊出你认为是，是还是是？00:18:07.480 --> 00:18:08.200是。00:18:08.200 --> 00:18:10.930是的，没错。所以，呃，正确的答案，00:18:10.930 --> 00:18:13.435一个正确的句子可能是，00:18:13.435 --> 00:18:15.700呃，这本书的作者正在计划续集。00:18:15.700 --> 00:18:18.910我想不出一个续集，这本书的作者是，00:18:18.910 --> 00:18:21.910这是，呃，语法正确的。00:18:21.910 --> 00:18:24.655所以我提出这个例子的原因，00:18:24.655 --> 00:18:27.490因为这显示了，呃，00:18:27.490 --> 00:18:28.600有两件事叫，呃，00:18:28.600 --> 00:18:32.725句法近因和顺序近因。00:18:32.725 --> 00:18:36.790因此，句法新进是指00:18:36.790 --> 00:18:40.780为了正确预测下一个单词应该是大于等于，00:18:40.780 --> 00:18:45.370在这里，单词writer是一种句法上相近的单词。00:18:45.370 --> 00:18:48.640所以我们说这本书的作者是因为它是作者。00:18:48.640 --> 00:18:51.310所以你可以把它看作是一个文字撰稿人，00:18:51.310 --> 00:18:53.425在句法上很接近。00:18:53.425 --> 00:18:55.975因为如果您查看依赖关系路径，00:18:55.975 --> 00:18:59.050那棵树上就会有一条很短的路。00:18:59.050 --> 00:19:05.155相比之下，序列最近性是，00:19:05.155 --> 00:19:10.960呃，简单的概念就是句子中的单词是多么的接近，就像一系列的单词。00:19:10.960 --> 00:19:12.534在这个例子中，00:19:12.534 --> 00:19:15.865书和书，都是按顺序排列的，因为它们彼此紧挨着。00:19:15.865 --> 00:19:18.475所以我提出这个问题的原因是，00:19:18.475 --> 00:19:22.660第二个可能是不正确的，但这是一个诱人的选择。00:19:22.660 --> 00:19:26.590因为如果你只关注最近发生的事情，00:19:26.590 --> 00:19:29.155嗯，那你可能会分心思考，00:19:29.155 --> 00:19:31.090“哦，这些书，听起来不错。”00:19:31.090 --> 00:19:35.500所以这里的问题是RNN语言模型00:19:35.500 --> 00:19:41.170在顺序近因学习上优于sicta-句法近因。00:19:41.170 --> 00:19:42.655部分到期，00:19:42.655 --> 00:19:44.455由于消失梯度问题。00:19:44.455 --> 00:19:47.290因为特别是，如果你在句法上，00:19:47.290 --> 00:19:49.765嗯，相关的词其实有点遥远，00:19:49.765 --> 00:19:54.355那么，从语法上讲最近的单词中获取信息可能会变得非常困难，00:19:54.355 --> 00:19:58.390尤其是如果有很多强信号从连续最近的字。00:19:58.390 --> 00:20:03.520所以，有一些论文表明RNN语言模型会产生这种错误，00:20:03.520 --> 00:20:05.200说是，而不是是。00:20:05.200 --> 00:20:08.440呃，他们犯这种错误的次数比你想的要多，呃，00:20:08.440 --> 00:20:11.860尤其是如果你有很多这样让人分心的词，比如书，00:20:11.860 --> 00:20:14.380介于你想预测的词之间00:20:14.380 --> 00:20:17.900你应该说的真话。00:20:19.470 --> 00:20:27.490好的，有什么问题吗？好吧，继续。00:20:27.490 --> 00:20:31.780所以，我们简单地提到，爆炸梯度，呃，是一个问题。00:20:31.780 --> 00:20:34.960所以，我简单地解释一下，为什么爆炸梯度是一个问题，00:20:34.960 --> 00:20:36.970为什么，呃，它看起来像什么？00:20:36.970 --> 00:20:40.015[噪音]所以，爆炸梯度是个问题的原因，00:20:40.015 --> 00:20:42.460如果你还记得SGD就是这样工作的。00:20:42.460 --> 00:20:44.860呃，我们说模型的新参数，00:20:44.860 --> 00:20:46.495我们用θ表示，00:20:46.495 --> 00:20:48.430等于老房子，00:20:48.430 --> 00:20:50.620然后你朝着00:20:50.620 --> 00:20:54.040负梯度是因为你想把J的损失降到最小。00:20:54.040 --> 00:20:58.050问题是如果你的梯度变大了，呃，00:20:58.050 --> 00:21:01.890然后你的SGD更新步骤也会变得非常大。00:21:01.890 --> 00:21:03.690所以，你将迈出一大步，00:21:03.690 --> 00:21:07.170你会彻底改变你的模型参数，θ。00:21:07.170 --> 00:21:10.705这意味着你最终会得到一些坏的更新。00:21:10.705 --> 00:21:13.075我们最后走了太大的一步。00:21:13.075 --> 00:21:15.580我们改变的参数太多了。00:21:15.580 --> 00:21:16.780这意味着，呃，00:21:16.780 --> 00:21:18.145我们迈出了一大步，00:21:18.145 --> 00:21:19.840最后我们会有一些，呃，00:21:19.840 --> 00:21:21.940参数实际上非常差的区域。00:21:21.940 --> 00:21:25.450嗯，举个例子-例如，00:21:25.450 --> 00:21:27.805他们的损失可能比以前大得多。00:21:27.805 --> 00:21:29.860所以，在最坏的情况下，00:21:29.860 --> 00:21:32.755这通常表现为看到，呃，00:21:32.755 --> 00:21:37.920无穷大或纳米，而不是在你的网络中训练时的数字。00:21:37.920 --> 00:21:41.485[噪音]所以，这可能发生，因为如果你迈出这么大的一步00:21:41.485 --> 00:21:45.445也许你更新了很多参数，现在它们是无穷大的，00:21:45.445 --> 00:21:47.290或者减去无穷大之类的，00:21:47.290 --> 00:21:50.245那么你的激活中也会有所有这些无穷大，00:21:50.245 --> 00:21:52.195然后你所有的损失都将是无限的，00:21:52.195 --> 00:21:54.385整个事情根本不起作用。00:21:54.385 --> 00:21:56.170所以，当这种情况发生的时候很烦人，00:21:56.170 --> 00:21:58.465不幸的是，这种情况经常发生。00:21:58.465 --> 00:22:00.355如果是这样，那么你必须00:22:00.355 --> 00:22:02.590从之前的某个检查点重新开始培训00:22:02.590 --> 00:22:04.480有奶奶和无穷大因为00:22:04.480 --> 00:22:06.580不可能把它从新的状态中拯救出来。00:22:06.580 --> 00:22:10.900[噪音]那么，这个爆炸梯度问题的解决方案是什么？00:22:10.900 --> 00:22:13.300[噪音]嗯，解决方案其实很不错00:22:13.300 --> 00:22:16.315很简单，这就是所谓的梯度剪辑技术。00:22:16.315 --> 00:22:18.580所以，梯度裁剪的主要思想，00:22:18.580 --> 00:22:21.610[噪音]是指如果你的坡度标准是00:22:21.610 --> 00:22:25.600大于某个阈值，该阈值是您选择的超参数。00:22:25.600 --> 00:22:29.110嗯，然后你想缩小这个梯度，00:22:29.110 --> 00:22:32.035嗯，在应用SGD更新之前。00:22:32.035 --> 00:22:35.410所以，直觉是，你仍然会朝着同一个方向迈出一步。00:22:35.410 --> 00:22:37.030但你要确保这是一个小步骤。00:22:37.030 --> 00:22:38.950[噪音]那么，这里，嗯，00:22:38.950 --> 00:22:41.995我有一些伪代码的截图00:22:41.995 --> 00:22:43.375相关文件，呃，00:22:43.375 --> 00:22:46.180提出了梯度削波，或者至少是某种形式的梯度削波。00:22:46.180 --> 00:22:48.640[噪音]而且，嗯，这很简单，你可以看到。00:22:48.640 --> 00:22:51.310这是向量，它是，00:22:51.310 --> 00:22:54.475关于前提的误差的导数，00:22:54.475 --> 00:22:56.770它的意思是如果00:22:56.770 --> 00:23:00.070这个梯度比阈值大，然后你只需要缩小它。00:23:00.070 --> 00:23:03.429但需要注意的是，它仍然指向同一个方向，00:23:03.429 --> 00:23:06.290只是小步而已。00:23:06.420 --> 00:23:10.615所以，这里有一张图片来展示这在实践中是如何实现的。00:23:10.615 --> 00:23:13.105这是一张图表，00:23:13.105 --> 00:23:16.255深入学习的教科书，也链接在[噪音]网站上。00:23:16.255 --> 00:23:19.615所以，这里发生的是，呃，00:23:19.615 --> 00:23:23.200这里的图片是一个简单RNN的损失面。00:23:23.200 --> 00:23:27.250所以，他们做了一个非常简单的RNN，而不是，00:23:27.250 --> 00:23:29.275作为隐藏状态的向量序列，00:23:29.275 --> 00:23:32.545它只是假设每个隐藏状态只是一个标量。00:23:32.545 --> 00:23:35.020所以，这意味着，不是有一个重量矩阵，w，00:23:35.020 --> 00:23:36.490偏压向量，b，00:23:36.490 --> 00:23:38.605有一个标量w和一个标量b。00:23:38.605 --> 00:23:42.550所以，这就是为什么在图片中，你只有二维的参数空间。00:23:42.550 --> 00:23:45.805然后，z轴就是你的，就是你的损失。00:23:45.805 --> 00:23:47.515所以在这里，高损失是，00:23:47.515 --> 00:23:50.335是坏的，低损失是好的。00:23:50.335 --> 00:23:52.825所以，呃，在这张照片里，00:23:52.825 --> 00:23:56.890你有这样的悬崖，对吧，你有这么陡峭的悬崖面，00:23:56.890 --> 00:23:59.290呃，损失变化很快。00:23:59.290 --> 00:24:03.715[噪音]而且这个悬崖非常危险，因为它有陡峭的斜坡。00:24:03.715 --> 00:24:06.160你可能会冒着承受巨大压力的危险，00:24:06.160 --> 00:24:09.970[噪音]嗯，更新步骤，因为你所在的区域有一个很陡的坡度。00:24:09.970 --> 00:24:12.505[噪音]所以，在左边，00:24:12.505 --> 00:24:17.410如果没有梯度剪切，可能会发生什么情况。00:24:17.410 --> 00:24:19.330[噪音]所以，在左边，呃，00:24:19.330 --> 00:24:22.615你可以看到你从悬崖底部开始，00:24:22.615 --> 00:24:25.465你有一个F-A-SI-一些小的更新。00:24:25.465 --> 00:24:28.150然后，特别是因为你00:24:28.150 --> 00:24:30.760看，在它从悬崖上掉下来之前，有一种小的倾斜。00:24:30.760 --> 00:24:32.590所以，这是真正的局部最小值，00:24:32.590 --> 00:24:36.145你想达到的最佳状态是，在那条小水沟的底部。00:24:36.145 --> 00:24:40.375而且，嗯，开始的时候离那条沟的边缘很近，00:24:40.375 --> 00:24:42.670然后有一个负的梯度进入。00:24:42.670 --> 00:24:45.775但不幸的是，更新类超调，00:24:45.775 --> 00:24:47.785最后它离悬崖很远。00:24:47.785 --> 00:24:50.470所以现在情况不好，更新得不好，00:24:50.470 --> 00:24:52.930现在它的损失比以前大得多。00:24:52.930 --> 00:24:54.655现在它在悬崖上了。00:24:54.655 --> 00:24:56.485同样，它测量梯度，00:24:56.485 --> 00:24:58.180坡度很陡，对吗？00:24:58.180 --> 00:24:59.425梯度很大。00:24:59.425 --> 00:25:01.360所以，当需要一个，呃，00:25:01.360 --> 00:25:03.115根据梯度更新，00:25:03.115 --> 00:25:04.300因为梯度太大了，00:25:04.300 --> 00:25:05.905这需要迈出一大步。00:25:05.905 --> 00:25:08.020那就是右边的那个。00:25:08.020 --> 00:25:09.580你可以看到右面的台阶。00:25:09.580 --> 00:25:12.190所以，这也是一个非常糟糕的更新，因为它只是00:25:12.190 --> 00:25:15.310对某些人来说，可能是相当随机的，00:25:15.310 --> 00:25:17.635嗯，W和B的配置。00:25:17.635 --> 00:25:20.740所以，在左边，你可以看到如果你服用00:25:20.740 --> 00:25:24.825这些非常大的台阶是因为你所在的地区坡度非常陡。00:25:24.825 --> 00:25:26.520所以，相比之下，在右边，00:25:26.520 --> 00:25:29.610你可以看到如果你有一个梯度剪辑会发生什么。00:25:29.610 --> 00:25:32.550[噪音][噪音]而且，嗯，它不那么激烈，对吧？00:25:32.550 --> 00:25:35.775你有一个类似的模式，它需要进入沟渠几步，00:25:35.775 --> 00:25:37.745最后有点从悬崖上掉下来，00:25:37.745 --> 00:25:39.835但不要太多，因为梯度被剪掉了。00:25:39.835 --> 00:25:42.400然后，在悬崖上，又有一个非常陡峭的斜坡，00:25:42.400 --> 00:25:45.490但它没有走这么大的一步，因为梯度又被剪掉了，00:25:45.490 --> 00:25:47.095这样它就会恢复正常。00:25:47.095 --> 00:25:51.085所以，你可以用这个梯度裁剪方法看到这一点。00:25:51.085 --> 00:25:53.215你有一个更安全的更新规则，00:25:53.215 --> 00:25:54.310在你不想带走的地方，00:25:54.310 --> 00:25:57.355任何一个疯狂的大步，你都更有可能找到，00:25:57.355 --> 00:25:59.590沟渠底部的实际最小值。00:25:59.590 --> 00:26:02.245[噪音]我想之前有个问题。00:26:02.245 --> 00:26:03.940这里有问题吗？[噪音]00:26:03.940 --> 00:26:05.380我只想看看它的价值。[噪音][噪音]00:26:05.380 --> 00:26:07.870可以。还有人吗？00:26:07.870 --> 00:26:09.460[噪音]00:26:09.460 --> 00:26:21.190是啊？00:26:21.190 --> 00:26:21.460[噪音][听不见]00:26:21.460 --> 00:26:23.005所以，问题是，在作业3中，00:26:23.005 --> 00:26:26.320你看到了原子优化算法，00:26:26.320 --> 00:26:27.925这就是动量，00:26:27.925 --> 00:26:31.015它本质上说类似于物理动量，00:26:31.015 --> 00:26:35.755在现实世界中，如果你在同一个方向旅行了一段时间，00:26:35.755 --> 00:26:39.190然后你可以采取更大的步骤，00:26:39.190 --> 00:26:41.320我想，如果你最近改变了方向，00:26:41.320 --> 00:26:42.835然后你应该采取较小的步骤。00:26:42.835 --> 00:26:47.620我认为还有另一个元素，你可以除以某个因子。00:26:47.620 --> 00:26:49.600[噪音]所以，这是一个类似的想法。00:26:49.600 --> 00:26:51.190我想这是一个不同的标准，对吧？00:26:51.190 --> 00:26:54.430所以，他们的共同点是，这是一种判断何时00:26:54.430 --> 00:26:58.045放大或缩小更新步骤的大小。00:26:58.045 --> 00:27:00.190嗯，我认为它们是基于不同的概念00:27:00.190 --> 00:27:03.160你什么时候应该采取更大的步骤，什么时候应该采取更小的步骤。00:27:03.160 --> 00:27:05.020你什么时候应该小心谨慎？00:27:05.020 --> 00:27:07.510所以，我想这里的标准是不同的。00:27:07.510 --> 00:27:09.880这是一个简单的标准，如果真的很陡，00:27:09.880 --> 00:27:13.250那就小心点。是啊。另一个问题？00:27:26.730 --> 00:27:31.885嗯，所以[听不见]。[噪音]00:27:31.885 --> 00:27:34.060可以。所以问题是，00:27:34.060 --> 00:27:37.150这类似于某种形式的正规化吗？00:27:37.150 --> 00:27:39.460所以，我想，是的，有一些共同点。00:27:39.460 --> 00:27:43.690例如，L2正则化表示，例如，00:27:43.690 --> 00:27:49.405你的权重矩阵有一个小的二级范数，对吗？00:27:49.405 --> 00:27:51.340这个想法是你试图阻止00:27:51.340 --> 00:27:53.920你的模型通过，嗯，00:27:53.920 --> 00:27:57.190有一种约束说你必须保持你的体重相当简单，00:27:57.190 --> 00:27:59.140你知道，那就是把它们放在小地方。00:27:59.140 --> 00:28:01.390所以，我想我们的关系是00:28:01.390 --> 00:28:03.490说我们不希望梯度的范数太大。00:28:03.490 --> 00:28:07.500啊，我不知道这是否与过度装修有关。00:28:07.500 --> 00:28:09.495嗯，我想我得仔细考虑一下，00:28:09.495 --> 00:28:14.175但我想这是一个类似的约束，你正在放置。00:28:14.175 --> 00:28:16.440可以。我现在就走。00:28:16.440 --> 00:28:19.660呃，我们已经谈过了00:28:19.660 --> 00:28:23.080关于如何用梯度裁剪解决爆炸梯度问题，00:28:23.080 --> 00:28:26.740但我们还没有讨论如何解决消失梯度问题。00:28:26.740 --> 00:28:29.410所以，嗯，简单地说，00:28:29.410 --> 00:28:34.270我认为描述RNN中消失梯度问题的一种方法是00:28:34.270 --> 00:28:39.620对于RNN来说，要学会在许多时间步骤中保存信息太困难了。00:28:39.620 --> 00:28:41.340因此，在我们的示例中，使用打印00:28:41.340 --> 00:28:44.475我记得是她要打印的票，00:28:44.475 --> 00:28:48.420你可以认为RNN语言模型很难正确地表达00:28:48.420 --> 00:28:52.260预测罚单，因为在某种程度上，RNN语言模型很难，00:28:52.260 --> 00:28:56.345嗯，学会保留门票信息，以后再使用。00:28:56.345 --> 00:28:58.900如果你看这个方程00:28:58.900 --> 00:29:01.630对于香草RNN以及我们如何计算隐藏状态，呃，00:29:01.630 --> 00:29:03.955根据之前的隐藏状态和输入，00:29:03.955 --> 00:29:07.495你可以看到隐藏状态在某种程度上不断被重写。00:29:07.495 --> 00:29:09.910它总是根据这些计算的，呃，00:29:09.910 --> 00:29:11.650线性变换和，00:29:11.650 --> 00:29:13.105你知道，非线性。00:29:13.105 --> 00:29:15.010所以，这并不容易00:29:15.010 --> 00:29:17.965将信息从一个隐藏状态保存到另一个隐藏状态，00:29:17.965 --> 00:29:21.145特别是，因为我们正把它通过这个非线性函数。00:29:21.145 --> 00:29:26.980所以，这促使我们去问，一个有着独立记忆的RNN怎么办？00:29:26.980 --> 00:29:31.510如果我们有一些单独的地方来存储我们以后要使用的信息，00:29:31.510 --> 00:29:34.630那么这会使我们的RNN更容易吗？00:29:34.630 --> 00:29:38.290学会在许多时间步骤中保存信息？00:29:38.290 --> 00:29:45.205所以，这是LSTMS或长期短期记忆RNN背后的激励思想。00:29:45.205 --> 00:29:51.550所以，这里的想法是LSTM是RNN的一种，它是在1997年提出的。00:29:51.550 --> 00:29:53.335这个想法是，呃，00:29:53.335 --> 00:29:56.800这是作为消失梯度问题的一个显式解决方案提出的。00:29:56.800 --> 00:30:00.280[噪音]所以，这里的主要区别之一是00:30:00.280 --> 00:30:03.880在每一个步骤t，而不是仅仅有一个隐藏的状态h_t，00:30:03.880 --> 00:30:08.320我们既有隐藏态H_t，也有细胞态C_t。00:30:08.320 --> 00:30:12.085这两个都是相同长度的向量，00:30:12.085 --> 00:30:15.430n，这里的想法是，细胞是用来00:30:15.430 --> 00:30:20.110存储我们的长期信息，存储在内存单元上。00:30:20.110 --> 00:30:23.605另一个非常重要的事情是LSTM可以00:30:23.605 --> 00:30:26.980擦除并写入[噪声]并从单元中读取信息。00:30:26.980 --> 00:30:29.995所以，你觉得这有点像电脑内存，00:30:29.995 --> 00:30:33.715你可以做这些操作，读写和擦除，00:30:33.715 --> 00:30:37.090嗯，这就是你保存信息的方式。00:30:37.090 --> 00:30:39.220[噪音]。00:30:39.220 --> 00:30:43.254另一个非常重要的事情是LSTM的决定方式，00:30:43.254 --> 00:30:45.490它是否想擦除、写入、读取，00:30:45.490 --> 00:30:48.565信息，决定多少信息，00:30:48.565 --> 00:30:51.430嗯，这都是由这些[噪音]门控制的。00:30:51.430 --> 00:30:56.560所以，这个想法是[噪音]门本身也是长度n的向量，00:30:56.560 --> 00:30:59.590我们的想法是在每一个时间点，00:30:59.590 --> 00:31:04.960这些门的每一个元素都是向量，介于0和1之间。00:31:04.960 --> 00:31:10.165所以这里，一个代表打开的门，零代表关闭的门，00:31:10.165 --> 00:31:12.715你可以在两者之间的任何地方拥有价值观。00:31:12.715 --> 00:31:15.475所以，我们将在下一张幻灯片中确定的总体想法，00:31:15.475 --> 00:31:17.770但总的来说，如果门是开着的，00:31:17.770 --> 00:31:20.590代表某种信息被传递，00:31:20.590 --> 00:31:21.670如果门是关着的，00:31:21.670 --> 00:31:24.320它[噪音]意味着信息无法传递。00:31:24.360 --> 00:31:28.705可以。最后一件非常重要的事情就是门是动态的。00:31:28.705 --> 00:31:32.950它们不只是为整个序列设置一个常量。00:31:32.950 --> 00:31:34.330[噪音]嗯，它们是动态的，00:31:34.330 --> 00:31:36.790这意味着它们在每个时间点上都是不同的，00:31:36.790 --> 00:31:41.200价值是决定它们是开放的还是封闭的，以何种方式，00:31:41.200 --> 00:31:45.100[噪声]嗯，这是根据当前上下文计算的。00:31:45.100 --> 00:31:46.945可以。这是，嗯，00:31:46.945 --> 00:31:50.125下面是LSTM的-方程式，可能会更清楚。00:31:50.125 --> 00:31:54.160所以，呃，假设我们有一个I输入x的序列，我们00:31:54.160 --> 00:31:58.435要计算隐藏状态h_t和单元格状态c_t的序列。00:31:58.435 --> 00:32:02.740所以，这就是Timestep T.呃，00:32:02.740 --> 00:32:07.210这个过程方程向你展示了我之前讨论过的三个门。00:32:07.210 --> 00:32:09.910所以，第一个被称为遗忘之门。00:32:09.910 --> 00:32:14.545我们的想法是，这个控制着什么是被保留的，什么是被遗忘的，00:32:14.545 --> 00:32:18.130嗯，从前一个单元状态，前一个内存。00:32:18.130 --> 00:32:22.510你可以看到这个遗忘门是基于，呃，00:32:22.510 --> 00:32:26.465前一个隐藏状态h_t减去1和当前输入x_t。00:32:26.465 --> 00:32:29.110嗯，这就是我说的00:32:29.110 --> 00:32:31.930动态的，它是基于当前上下文计算的。00:32:31.930 --> 00:32:36.190[噪音]嗯，你也可以看到它是用，00:32:36.190 --> 00:32:37.390呃，乙状结肠功能，00:32:37.390 --> 00:32:39.920这意味着它在0到1之间。00:32:39.920 --> 00:32:43.050可以。下一个门叫做输入门，00:32:43.050 --> 00:32:48.645这个控制着新单元内容的哪些部分被写入单元。00:32:48.645 --> 00:32:52.020所以，这里的想法是你有这个-这个记忆细胞，这个，嗯，00:32:52.020 --> 00:32:57.235像ho-how这样的控制，你可以写什么到内存单元。00:32:57.235 --> 00:32:59.995可以。最后一道叫做上大门。00:32:59.995 --> 00:33:01.780所以，这个控制着，呃，00:33:01.780 --> 00:33:04.930单元的哪些部分输出到隐藏状态，00:33:04.930 --> 00:33:08.620[noise]这样你就可以把它看作是一种读函数，对吧？00:33:08.620 --> 00:33:10.090我们要从00:33:10.090 --> 00:33:12.685我们的记忆细胞会进入我们隐藏的状态，00:33:12.685 --> 00:33:14.050这个门会控制这个。00:33:14.050 --> 00:33:16.990[噪音]好的。00:33:16.990 --> 00:33:22.060[噪音]嗯，是的，这只是我们之前提到的乙状结肠功能。00:33:22.060 --> 00:33:25.870好吧。下一组方程展示了我们如何使用这些门。00:33:25.870 --> 00:33:28.120[噪音]所以，第一行，呃，00:33:28.120 --> 00:33:29.335你可以考虑这个，呃，00:33:29.335 --> 00:33:32.095作为新的[噪音]单元内容。00:33:32.095 --> 00:33:34.960所以，呃，这是你想写给手机的新内容，00:33:34.960 --> 00:33:37.540[噪音]这也是根据，呃，00:33:37.540 --> 00:33:39.490您以前的隐藏状态和当前输入，00:33:39.490 --> 00:33:41.920这是通过你的棕色H非线性。00:33:41.920 --> 00:33:45.250所以，呃，这是-主要内容00:33:45.250 --> 00:33:49.570您是基于上下文进行计算的，您希望将其写入内存中。00:33:49.570 --> 00:33:54.550所以，在下一行，我们将使用00:33:54.550 --> 00:34:00.070忘记门有选择地忘记了以前的一些信息，00:34:00.070 --> 00:34:01.930[噪音]嗯，记忆细胞。00:34:01.930 --> 00:34:04.780你可以看到我们在做这些元素导向的产品，00:34:04.780 --> 00:34:06.340这就是小圆圈。00:34:06.340 --> 00:34:08.950所以，我们的想法是，如果你记得00:34:08.950 --> 00:34:11.980一个在0到1之间充满值的向量，00:34:11.980 --> 00:34:14.380当你在F和00:34:14.380 --> 00:34:16.885前一个单元格状态c_t减去1，00:34:16.885 --> 00:34:19.000那么你真正要做的就是掩盖00:34:19.000 --> 00:34:21.865从以前的隐藏状态中提取一些信息。00:34:21.865 --> 00:34:23.680抱歉，没有。以前的手机状态。00:34:23.680 --> 00:34:26.155所以，当f是一时，00:34:26.155 --> 00:34:27.805然后你在复制信息，00:34:27.805 --> 00:34:30.340但当f为零时，你就摆脱了这些信息，00:34:30.340 --> 00:34:32.840你是在擦除还是忘记它。00:34:33.930 --> 00:34:37.030可以。然后这个方程的另一半，00:34:37.030 --> 00:34:39.550嗯，我乘以c tilde t，呃，00:34:39.550 --> 00:34:41.500这就是输入门控制00:34:41.500 --> 00:34:44.215新单元格内容的哪些部分将被写入，00:34:44.215 --> 00:34:46.910写入、写入单元。00:34:47.130 --> 00:34:50.095可以。最后我们要做的就是，00:34:50.095 --> 00:34:52.900让细胞通过棕褐色的H，00:34:52.900 --> 00:34:55.390这只是增加了另一个非线性，00:34:55.390 --> 00:34:56.530然后你通过那个00:34:56.530 --> 00:34:59.380输出门，这给了你隐藏的状态。00:34:59.380 --> 00:35:02.950所以，在LSTM中，我们经常认为隐藏状态是，00:35:02.950 --> 00:35:05.095呃，就像RNN的输出。00:35:05.095 --> 00:35:07.570原因是你的观点00:35:07.570 --> 00:35:09.910细胞状态是这样的00:35:09.910 --> 00:35:12.985外部一般无法访问的内存，00:35:12.985 --> 00:35:15.280但隐藏的状态是你00:35:15.280 --> 00:35:17.905把模型的下一部分传下去。00:35:17.905 --> 00:35:20.620所以，这就是为什么我们把它看作是模型的输出。00:35:20.620 --> 00:35:24.520[噪音]嗯，这是，是的，00:35:24.520 --> 00:35:26.410X只是提醒-有-圆圈是00:35:26.410 --> 00:35:29.095元素方面的产品，这就是我们应用门的方式。00:35:29.095 --> 00:35:30.910呃，有人对此有什么问题吗？00:35:30.910 --> 00:35:40.990[噪音]。00:35:40.990 --> 00:35:43.960可以。[噪音]嗯，作为提醒，00:35:43.960 --> 00:35:46.420所有这些都是一些相同长度n的向量。00:35:46.420 --> 00:35:49.510[噪音]好的。00:35:49.510 --> 00:35:52.914所以，有些人从图表中学到的比方程式好，00:35:52.914 --> 00:35:55.660这里有一个同样想法的示意图。00:35:55.660 --> 00:35:57.895所以，这是一个很好的博客图表，00:35:57.895 --> 00:35:59.665呃，克里斯·奥拉关于LSTM的报道，00:35:59.665 --> 00:36:01.420如果你想的话，那是个很好的开始00:36:01.420 --> 00:36:04.450直观地了解LSTM是什么。00:36:04.450 --> 00:36:06.505所以，在这个图表中，呃，00:36:06.505 --> 00:36:09.055绿色框代表时间步骤，00:36:09.055 --> 00:36:12.550嗯，让我们放大中间的那个，看看这里发生了什么。00:36:12.550 --> 00:36:14.860所以，在一个时间步内，00:36:14.860 --> 00:36:17.650您可以看到，此图显示的内容与00:36:17.650 --> 00:36:20.755上一张幻灯片显示了这六个方程式。00:36:20.755 --> 00:36:25.810我们要做的第一件事就是使用，呃，电流输入x00:36:25.810 --> 00:36:29.350在底部，前一个隐藏状态h_t减去左边的那个，00:36:29.350 --> 00:36:31.420我们可以用它来计算忘记门。00:36:31.420 --> 00:36:34.720[噪音]你可以看到f_t在那箭头上。00:36:34.720 --> 00:36:39.385然后你把忘记门应用到前一个，呃，手机，00:36:39.385 --> 00:36:42.970这和忘记上次的一些-细胞内容是一样的。00:36:42.970 --> 00:36:44.695[噪音]好的。00:36:44.695 --> 00:36:47.290然后，你可以计算输入门，呃，00:36:47.290 --> 00:36:50.155计算方法和忘记门差不多。00:36:50.155 --> 00:36:55.240然后使用输入门来决定这其中的哪些部分，00:36:55.240 --> 00:36:58.765呃，新的手机内容被写入手机，00:36:58.765 --> 00:37:00.565这就给了你细胞计数。00:37:00.565 --> 00:37:04.240在这里，你可以看到你计算了输入GA-输入门和00:37:04.240 --> 00:37:08.770新的内容，然后使用它将其选入并写入单元。00:37:08.770 --> 00:37:10.600所以，现在我们有了新的手机00:37:10.600 --> 00:37:15.370然后我们要做的最后一件事就是计算新的输出门。00:37:15.370 --> 00:37:19.780最后，使用输出门选择00:37:19.780 --> 00:37:25.090你将要阅读的单元格内容并将其置于新的隐藏状态。00:37:25.090 --> 00:37:27.310所以，那是，那是，呃，那是00:37:27.310 --> 00:37:30.440和我们在上一张幻灯片上看到的方程一样。00:37:32.400 --> 00:37:35.035可以。所以，这就是LSTM。00:37:35.035 --> 00:37:37.510嗯，有问题吗？00:37:37.510 --> 00:37:50.110有什么重要性[噪音][听不见]00:37:50.110 --> 00:37:52.840问题是，为什么我们要用棕褐色的H呢？00:37:52.840 --> 00:37:55.720在这张幻灯片的最后一个方程上？00:37:55.720 --> 00:38:01.120为什么我们要在应用输出门之前对单元进行tan h h规划？00:38:01.120 --> 00:38:05.800让我们看看。嗯。00:38:05.800 --> 00:38:10.330是啊。所以，你的问题是，细胞，00:38:10.330 --> 00:38:19.330我不确定，新的手机内容已经被晒黑了。00:38:19.330 --> 00:38:21.580所以，我想A-A-一般的答案是00:38:21.580 --> 00:38:23.980在某种程度上给予更多的表现力，00:38:23.980 --> 00:38:26.410这不仅仅是申请00:38:26.410 --> 00:38:31.420按顺序谭H是因为你有门在中间。00:38:31.420 --> 00:38:34.240嗯，我想一定有原因，00:38:34.240 --> 00:38:36.445类似于你申请的时候-申请00:38:36.445 --> 00:38:39.640一个线性层，在下一个线性层之前，你不会有非线性。00:38:39.640 --> 00:38:43.075我想也许我们把这些情况看作是一种线性层？00:38:43.075 --> 00:38:44.920我不确定。我去查一下。00:38:44.920 --> 00:38:49.450[噪音]好的。00:38:49.450 --> 00:38:52.075所以，呃，这是LSTM。00:38:52.075 --> 00:38:54.220如果你还记得的话，00:38:54.220 --> 00:38:55.795我们-哦，有问题吗？00:38:55.795 --> 00:38:58.615是啊。为什么在遗忘之门，00:38:58.615 --> 00:39:02.185你不看前一个单元格状态，只看新的隐藏状态？00:39:02.185 --> 00:39:04.180好像你是这样的-而不是00:39:04.180 --> 00:39:07.090决定从细胞状态中忘记什么，你应该看看它。00:39:07.090 --> 00:39:09.670所以问题是，为什么是遗忘之门？00:39:09.670 --> 00:39:12.610仅为以前的隐藏状态和当前输入计算，00:39:12.610 --> 00:39:16.840为什么它不是根据CT减去1本身来计算的，对吗？00:39:16.840 --> 00:39:18.820因为你肯定想看看你想知道的事情00:39:18.820 --> 00:39:21.880你想不想忘记它？00:39:21.880 --> 00:39:24.295嗯，这是个很好的问题。00:39:24.295 --> 00:39:29.740嗯，所以，我想你会认为这是一个很好的原因是00:39:29.740 --> 00:39:33.039LSTM可能正在学习一种通用算法00:39:33.039 --> 00:39:36.730它在单元中存储不同类型信息的位置，对吗？00:39:36.730 --> 00:39:39.370所以，也许是因为它知道在细胞的这个特殊位置，00:39:39.370 --> 00:39:44.230我了解了关于这个特定语义的信息，然后在这种情况下，00:39:44.230 --> 00:39:48.295我想用还是不用，忘记还是保留。00:39:48.295 --> 00:39:51.580但是，是的，我还没有完全说服自己为什么你不想00:39:51.580 --> 00:39:55.090查看单元格本身的内容以确定。00:39:55.090 --> 00:40:02.530我想另一件要注意的事情是，从CT上读取的是ht减去1。00:40:02.530 --> 00:40:06.010所以，我想这里有一些信息，但不一定是所有的信息。00:40:06.010 --> 00:40:11.095啊，是的。00:40:11.095 --> 00:40:13.420我不确定，我想这是我需要查的另一件事。00:40:13.420 --> 00:40:18.530[噪音]还有其他问题吗？00:40:21.060 --> 00:40:26.990可以。啊，那么，这就是LSTM，00:40:27.780 --> 00:40:31.825引入LSTMS来解决消失梯度问题。00:40:31.825 --> 00:40:33.640所以，问题是，啊，00:40:33.640 --> 00:40:37.840这个架构到底是如何使消失梯度问题变得更好的？00:40:37.840 --> 00:40:41.710所以，你可以，啊，看到LSTM架构00:40:41.710 --> 00:40:45.925实际上，RNN可以更容易地在许多时间步骤中保存信息。00:40:45.925 --> 00:40:48.340所以，虽然这对00:40:48.340 --> 00:40:52.285香草RNN保护所有隐藏状态的信息，00:40:52.285 --> 00:40:54.730实际上有一个相当简单的策略00:40:54.730 --> 00:40:57.265LSTM保存信息很简单。00:40:57.265 --> 00:41:02.290所以，也就是说，如果忘记之门设置为记住每一步的每一件事，嗯，00:41:02.290 --> 00:41:05.290这是一个相当简单的策略，可以确保00:41:05.290 --> 00:41:09.820在许多时间步骤中，细胞中的信息将被无限期地保存。00:41:09.820 --> 00:41:13.105所以，我不知道这是否是一个很好的策略，无论你想做什么，00:41:13.105 --> 00:41:15.985但我的观点是至少00:41:15.985 --> 00:41:20.680LSTM通过许多步骤保存信息的一种非常简单的方法。00:41:20.680 --> 00:41:25.175正如我们所指出的，香草RNN相对来说更难做到这一点。00:41:25.175 --> 00:41:29.640所以，你可以认为这是LSTM更具能力的关键原因，00:41:29.640 --> 00:41:31.859啊，为了保存信息00:41:31.859 --> 00:41:34.820因此，对于消失梯度问题更具鲁棒性。00:41:34.820 --> 00:41:38.590啊，不过，我想你还是应该知道，LSTM没有00:41:38.590 --> 00:41:42.175必须保证我们不存在消失或爆炸梯度问题。00:41:42.175 --> 00:41:43.615你可能还有这个问题，00:41:43.615 --> 00:41:47.630但要记住的是，无论如何，要避免它是比较容易的。00:41:48.080 --> 00:41:51.420可以。所以，嗯，lstms，啊，00:41:51.420 --> 00:41:54.825已经证明对消失梯度问题更具鲁棒性，00:41:54.825 --> 00:41:57.180啊，但我要告诉你一点他们是怎么做到的00:41:57.180 --> 00:42:00.110实际上在现实生活中更成功。你有问题吗？00:42:00.110 --> 00:42:21.600是的，[听不见]00:42:21.600 --> 00:42:26.110可以。所以这是个很好的问题。00:42:26.110 --> 00:42:28.480问题是，为什么仅仅因为你00:42:28.480 --> 00:42:31.390把这些LSTM定义为正向方程，00:42:31.390 --> 00:42:33.385为什么没有消失梯度问题？00:42:33.385 --> 00:42:36.280为什么-逻辑是关于，啊，00:42:36.280 --> 00:42:40.060链式法则是不是越来越小或越来越大不适用？00:42:40.060 --> 00:42:43.795所以，我认为关键是，嗯，00:42:43.795 --> 00:42:45.715在香草RNN中，00:42:45.715 --> 00:42:47.920隐藏状态有点像瓶颈，对吧？00:42:47.920 --> 00:42:50.620就像所有的梯度都必须通过它们。00:42:50.620 --> 00:42:52.360所以，如果这个梯度很小，00:42:52.360 --> 00:42:54.595所有下游坡度都很小，00:42:54.595 --> 00:42:58.405但是在这里你可以把细胞看作是00:42:58.405 --> 00:43:00.820至少在00:43:00.820 --> 00:43:04.300忘记门是用来记住事情的，00:43:04.300 --> 00:43:07.330嗯，那有点像一个快捷连接，00:43:07.330 --> 00:43:11.020如果你将忘记门设置为记住事情，那么这个单元将保持不变。00:43:11.020 --> 00:43:14.080所以，如果细胞基本上保持不变，00:43:14.080 --> 00:43:17.530那你就不会了，00:43:17.530 --> 00:43:20.200啊，通过细胞的消失梯度。00:43:20.200 --> 00:43:22.510所以，这意味着从00:43:22.510 --> 00:43:25.465未来事物相对于过去事物的梯度，00:43:25.465 --> 00:43:27.760有一个潜在的途径，梯度00:43:27.760 --> 00:43:30.865穿过不一定会消失的细胞。00:43:30.865 --> 00:43:32.860因此，我还有一个问题。00:43:32.860 --> 00:43:33.120嗯。00:43:33.120 --> 00:43:49.830因为我们有捷径[听不见]00:43:49.830 --> 00:43:53.680所以我认为问题是，你如何检查梯度是否正确00:43:53.680 --> 00:43:57.895现在有多条路线供信息旅行？00:43:57.895 --> 00:43:58.450正确的。00:43:58.450 --> 00:44:01.780所以，我想这和我们上次谈的有些关系00:44:01.780 --> 00:44:04.660关于什么是什么的多变量链规则00:44:04.660 --> 00:44:08.815损失对重复权矩阵的导数，我们看到，00:44:08.815 --> 00:44:10.675如果有多条路线，那么00:44:10.675 --> 00:44:13.330多变量链规则表示你加上了梯度。00:44:13.330 --> 00:44:16.675所以，如果你的问题是如何正确地做微积分并确保它是正确的，00:44:16.675 --> 00:44:18.190我想你只是想申请一下00:44:18.190 --> 00:44:19.660多变量链规则00:44:19.660 --> 00:44:21.460比用LSTM评估复杂。00:44:21.460 --> 00:44:24.085啊，如果你用的是PyTorch 14，你不必自己动手，00:44:24.085 --> 00:44:25.780如果你想自己实现它，00:44:25.780 --> 00:44:27.535你可能会有更困难的时候。00:44:27.535 --> 00:44:30.775嗯，是的。所以，我想，是的。00:44:30.775 --> 00:44:38.560可以。好吧，那我们该怎么办？好吧。00:44:38.560 --> 00:44:41.395那么，让我们来谈谈LSTM以及它们在现实世界中是如何工作的。00:44:41.395 --> 00:44:44.950所以，在最近的一段时间里，00:44:44.950 --> 00:44:48.4902013-2015年，um-lstm开始实现00:44:48.490 --> 00:44:52.315这项艺术的结果是各种不同的任务，例如，00:44:52.315 --> 00:44:54.370手写识别，语音识别，00:44:54.370 --> 00:44:57.805机器翻译，解析，图像字幕。00:44:57.805 --> 00:44:59.380所以，在这段时间里，00:44:59.380 --> 00:45:02.020LSTMS成为许多00:45:02.020 --> 00:45:08.230这些应用领域是因为它们比普通RNN工作得更好。00:45:08.230 --> 00:45:10.705然而，在2019年的今天，00:45:10.705 --> 00:45:13.465在深入学习中，情况变化很快。00:45:13.465 --> 00:45:16.240例如，其他方法，00:45:16.240 --> 00:45:18.400变形金刚，你稍后会在课堂上学到。00:45:18.400 --> 00:45:20.410啊，在一些应用领域，00:45:20.410 --> 00:45:21.715他们似乎变成了，00:45:21.715 --> 00:45:23.935啊，主导的方法。00:45:23.935 --> 00:45:25.480所以，要研究这个问题，00:45:25.480 --> 00:45:29.350我看过WMT，它是一个机器翻译会议，00:45:29.350 --> 00:45:33.865还有人提交他们的机器翻译系统进行评估的竞争。00:45:33.865 --> 00:45:35.620我看了一下报告，00:45:35.620 --> 00:45:39.760WMT 2016年总结报告和本报告，00:45:39.760 --> 00:45:41.125
I did a quick Ctrl+F,

00:45:41.125 --> 00:45:44.005我发现RNN这个词出现了44次。00:45:44.005 --> 00:45:46.990所以，似乎大多数参加比赛的人都在努力00:45:46.990 --> 00:45:50.800他们的MT系统基于RNN，尤其是LSTM。00:45:50.800 --> 00:45:52.945然后我看了2018年的报告，00:45:52.945 --> 00:45:55.210两年后我发现RNN00:45:55.210 --> 00:45:59.785RNN这个词只出现了9次，transformer这个词出现了63次，00:45:59.785 --> 00:46:02.155事实上，组织者指出，00:46:02.155 --> 00:46:04.330嗯，现在大多数人似乎都在使用变压器。00:46:04.330 --> 00:46:07.930所以，嗯，这表明在深度学习中事情变化很快。00:46:07.930 --> 00:46:11.350就在几年前，那东西又热又新，嗯，00:46:11.350 --> 00:46:15.655现在可能正通过其他方法。00:46:15.655 --> 00:46:17.260所以，你要了解更多关于变形金刚的知识00:46:17.260 --> 00:46:19.315后来，但我想这给了你一种00:46:19.315 --> 00:46:24.385了解LSTM当前在应用程序中的位置。00:46:24.385 --> 00:46:29.845可以。所以，我们要学习的第二种RNN是门循环单元。00:46:29.845 --> 00:46:32.605所以，幸运的是，这些比LSTM简单，00:46:32.605 --> 00:46:36.070事实上，这就是他们被提议的动机。00:46:36.070 --> 00:46:40.150他们在2014年被提议作为一种试图保留00:46:40.150 --> 00:46:45.055通过消除任何不必要的复杂性，LSTM的优势。00:46:45.055 --> 00:46:46.825所以，在一个地下室里，00:46:46.825 --> 00:46:48.520我们没有细胞状态。00:46:48.520 --> 00:46:50.470我们又有了一个隐藏的状态。00:46:50.470 --> 00:46:54.070但是它与lstms的共同之处在于00:46:54.070 --> 00:46:57.460使用门来控制信息流。00:46:57.460 --> 00:47:00.415这里是GRU的方程。00:47:00.415 --> 00:47:02.890我们从两个门开始。00:47:02.890 --> 00:47:06.430所以第一个门叫做更新门，这个00:47:06.430 --> 00:47:11.050控制哪些隐藏状态部分将被更新或保留。00:47:11.050 --> 00:47:13.750所以，你可以认为这是在玩00:47:13.750 --> 00:47:17.170忘记门和输入门在00:47:17.170 --> 00:47:24.580LSTM和它的计算方法与LSTM中的门差不多。00:47:24.580 --> 00:47:27.565第二个门叫做重设门RT，00:47:27.565 --> 00:47:30.550这个门控制着00:47:30.550 --> 00:47:34.915前一个隐藏状态将用于计算新内容。00:47:34.915 --> 00:47:37.735所以，你可以把重设门看作是一种选择00:47:37.735 --> 00:47:41.305以前隐藏状态的哪些部分有用，哪些不有用。00:47:41.305 --> 00:47:45.010所以，它将丢弃一些东西，选择一些其他的东西。00:47:45.010 --> 00:47:48.055可以。所以，这些门是如何使用的。00:47:48.055 --> 00:47:49.690嗯，这里是H蒂尔德。00:47:49.690 --> 00:47:54.400这是你可以认为它是新的隐藏状态内容和00:47:54.400 --> 00:47:56.440在这个方程中，我们正在应用00:47:56.440 --> 00:47:59.410重置门到上一个隐藏状态ht减号00:47:59.410 --> 00:48:04.150一个，然后把所有的都通过线性变换00:48:04.150 --> 00:48:07.420一个棕褐色的H，然后这给了我们新的内容00:48:07.420 --> 00:48:11.305我们要把它写到隐藏的单元。00:48:11.305 --> 00:48:15.655最后，我们新的隐藏单元将是一个组合00:48:15.655 --> 00:48:20.065啊，这个新的内容和以前的隐藏状态。00:48:20.065 --> 00:48:24.790所以，这里要注意的重要一点是，我们有一个负的u和u项。00:48:24.790 --> 00:48:27.370嗯，这有点像平衡，对吧？00:48:27.370 --> 00:48:31.079你是啊，是在00:48:31.079 --> 00:48:35.085从以前的隐藏状态保存东西，而不是写新东西。00:48:35.085 --> 00:48:36.360所以，在LSTM中，00:48:36.360 --> 00:48:39.195这是两个完全独立的门，可以是任何价值。00:48:39.195 --> 00:48:42.240这里我们有一个约束条件，就是u是平衡的。00:48:42.240 --> 00:48:44.765所以，如果你有更多的一个，你必须有更少的另一个。00:48:44.765 --> 00:48:51.160因此，这是GRU的创建者试图使LSTM更简单的一种方法。00:48:51.160 --> 00:48:54.520就是让一个门同时扮演这两个角色。00:48:54.520 --> 00:48:59.410可以。所以，这就是格鲁斯，我认为仅仅看看它就不那么明显了。00:48:59.410 --> 00:49:04.870为什么GRUS有助于消除渐变问题，因为没有明确的AH记忆00:49:04.870 --> 00:49:06.520细胞，就像在LSTM中一样。00:49:06.520 --> 00:49:10.360所以，我想看这个的方法是，嗯，格鲁斯，00:49:10.360 --> 00:49:12.160您也可以将此视为00:49:12.160 --> 00:49:15.220消失梯度问题，因为像lstms一样，00:49:15.220 --> 00:49:19.425GRUS使信息的长期保存更加容易。00:49:19.425 --> 00:49:21.240例如这里，00:49:21.240 --> 00:49:25.094如果更新门UT设置为零，00:49:25.094 --> 00:49:30.385然后我们要保持隐藏状态在每一步都是一样的。00:49:30.385 --> 00:49:33.640再说一次，这也许不是个好主意，但至少这是一个很容易的策略00:49:33.640 --> 00:49:37.315这样做是为了长距离保留信息。00:49:37.315 --> 00:49:40.420所以这有点像-就像格鲁斯是怎么做到的一样。00:49:40.420 --> 00:49:44.660RNN可能更容易长期保留信息。00:49:46.410 --> 00:49:51.490可以。所以，我们已经了解了这两种不同类型的RNN。对。00:49:51.490 --> 00:50:08.230[听不见]00:50:08.230 --> 00:50:10.105我想问题是，00:50:10.105 --> 00:50:12.790如果我们把地下室的两个门看作，呃，00:50:12.790 --> 00:50:20.440一个精确的，嗯，对LSTM中的门的类比，还是更模糊的类比。00:50:20.440 --> 00:50:22.794我想说的可能更像是一个模糊的类比00:50:22.794 --> 00:50:26.080因为这里还有其他变化，比如，00:50:26.080 --> 00:50:28.510例如，事实上没有单独的，嗯，00:50:28.510 --> 00:50:32.260内存单元，这意味着它们没有执行完全相同的功能。00:50:32.260 --> 00:50:39.895是啊。可以。所以，我们已经了解了lstms和grus，它们都是，00:50:39.895 --> 00:50:41.650嗯，更复杂的RNN形式，00:50:41.650 --> 00:50:43.705比香草更复杂。00:50:43.705 --> 00:50:45.610他们都是，00:50:45.610 --> 00:50:48.865嗯，对消失梯度问题更为强大。00:50:48.865 --> 00:50:53.950所以，嗯，知道我们应该在实践中使用哪一个会很有用？00:50:53.950 --> 00:50:55.120哪一个更成功，00:50:55.120 --> 00:50:56.770LSTM还是GRU？00:50:56.770 --> 00:50:59.770呃，所以，我-我读了一点书，看起来研究人员00:50:59.770 --> 00:51:02.890提出了许多不同类型的门控RNN。00:51:02.890 --> 00:51:04.450所以，这不仅仅是GRUS和LSTM，00:51:04.450 --> 00:51:07.585还有很多其他的论文有很多不同的变体。00:51:07.585 --> 00:51:11.845嗯，但这两个肯定是使用最广泛的。00:51:11.845 --> 00:51:15.340你可能会说这两者最大的区别00:51:15.340 --> 00:51:17.920当然，GRUS更简单00:51:17.920 --> 00:51:21.115计算速度更快，参数也更少。00:51:21.115 --> 00:51:23.500所以，这对你来说是一个实际的区别，呃，00:51:23.500 --> 00:51:27.970一个深入学习的从业者，因为如果你基于GRUS构建你的网络，00:51:27.970 --> 00:51:29.965然后向前跑会更快，00:51:29.965 --> 00:51:32.170你知道，训练得更快等等。00:51:32.170 --> 00:51:34.540因此，除此之外，似乎还有00:51:34.540 --> 00:51:38.680没有确凿的证据表明这些LSTM或GRUS中的一个，00:51:38.680 --> 00:51:42.790嗯，在很多不同的任务上总是比其他的强。00:51:42.790 --> 00:51:45.955呃，似乎经常，呃，00:51:45.955 --> 00:51:48.250有时GRU的性能和LSTM一样好，00:51:48.250 --> 00:51:51.520但在某些情况下，它们中的一个表现比另一个好。00:51:51.520 --> 00:51:53.440所以，根据经验，00:51:53.440 --> 00:51:57.190似乎lstm通常是一个不错的默认选择，首先，00:51:57.190 --> 00:51:58.840尤其是如果你的数据00:51:58.840 --> 00:52:01.150特别是长期依赖，因为有证据表明00:52:01.150 --> 00:52:05.500LSTM在远距离保存信息方面可能稍有优势。00:52:05.500 --> 00:52:07.570而且，如果你有很多训练数据，00:52:07.570 --> 00:52:09.670你可能认为LSTM是更好的选择，因为它们00:52:09.670 --> 00:52:12.340有更多的参数，这意味着，00:52:12.340 --> 00:52:15.860嗯，也许你需要更多的训练数据来学习。00:52:17.940 --> 00:52:21.700所以，经验法则是，也许你想从lstms开始00:52:21.700 --> 00:52:23.500如果你对他们的表现感到满意00:52:23.500 --> 00:52:25.675很高兴你花了多长时间来训练，然后你就坚持下去。00:52:25.675 --> 00:52:27.610但是如果你觉得你需要它更有效，00:52:27.610 --> 00:52:30.850那么也许你应该切换到GRUS看看这和性能有什么关系。00:52:30.850 --> 00:52:34.690如果速度更快。好吧。00:52:34.690 --> 00:52:36.970所以，嗯，到目前为止我们已经讨论过00:52:36.970 --> 00:52:40.720消失/爆炸梯度是RNN中经常出现的问题。00:52:40.720 --> 00:52:42.415但问题是，00:52:42.415 --> 00:52:43.735这只是RNN问题吗？00:52:43.735 --> 00:52:46.330这是否也发生在其他类型的神经网络中？00:52:46.330 --> 00:52:48.115答案是，00:52:48.115 --> 00:52:50.170不，这不仅仅是RNN问题。00:52:50.170 --> 00:52:52.390实际上，消失和爆炸梯度是00:52:52.390 --> 00:52:55.150相当重要的问题00:52:55.150 --> 00:52:58.419大多数神经结构，如前馈和卷积，00:52:58.419 --> 00:52:59.740尤其是当它们很深的时候。00:52:59.740 --> 00:53:02.410这是一个非常严重的问题，因为没有必要00:53:02.410 --> 00:53:06.655一个非常酷的神经架构，如果你不能有效地学习它，因为，00:53:06.655 --> 00:53:08.500呃，消失梯度问题。00:53:08.500 --> 00:53:13.030特别是在这些前馈和卷积网络中，00:53:13.030 --> 00:53:15.280你经常有一个渐变消失00:53:15.280 --> 00:53:18.520小的反向传播，因为链式法则，00:53:18.520 --> 00:53:20.200因为这个乘以00:53:20.200 --> 00:53:22.390所有这些不同的中间梯度或00:53:22.390 --> 00:53:25.120有时由于你选择非线性函数。00:53:25.120 --> 00:53:29.260所以，如果发生这种情况，这意味着你的-你的下层，比如说，00:53:29.260 --> 00:53:31.285卷积或前馈网络，00:53:31.285 --> 00:53:32.950它们有一个小得多的，00:53:32.950 --> 00:53:35.935嗯，梯度比高水平。00:53:35.935 --> 00:53:39.970这意味着它们在SGD期间变化非常缓慢。00:53:39.970 --> 00:53:41.290这意味着，总的来说，00:53:41.290 --> 00:53:43.990你的网络训练很慢，因为当你进行更新时，00:53:43.990 --> 00:53:47.080然后你的下层变化非常缓慢。00:53:47.080 --> 00:53:49.300所以，一个解决方案，呃，00:53:49.300 --> 00:53:51.280就像我们看到的一系列解决方案00:53:51.280 --> 00:53:53.620近年来00:53:53.620 --> 00:53:58.720对新型深度前馈或卷积架构的建议。00:53:58.720 --> 00:54:02.740他们所做的就是在网络中添加更多的直接连接。00:54:02.740 --> 00:54:04.330这个想法，00:54:04.330 --> 00:54:05.755就像我们之前说的那样，00:54:05.755 --> 00:54:08.530如果你在层之间添加所有这些直接连接，00:54:08.530 --> 00:54:12.085可能不仅仅是相邻的层，而是相隔更远的层，00:54:12.085 --> 00:54:14.680然后它使梯度更容易流动，00:54:14.680 --> 00:54:17.635你会发现培训你的整个网络会更容易。00:54:17.635 --> 00:54:19.660所以，我要给你们看一些例子00:54:19.660 --> 00:54:21.670特别是因为你很可能00:54:21.670 --> 00:54:25.705当你做项目和阅读论文的时候，会碰到这些类型的架构。00:54:25.705 --> 00:54:29.410所以，有一个例子叫做剩余连接，00:54:29.410 --> 00:54:32.515嗯，网络本身有时被称为resnet。00:54:32.515 --> 00:54:36.350我们从相关文件中得到了一个数字。00:54:36.350 --> 00:54:41.070所以，在这张图中，你有，呃，00:54:41.070 --> 00:54:42.930通常的那种你有重量层和00:54:42.930 --> 00:54:45.480一个非线性，即relu和另一个权重层。00:54:45.480 --> 00:54:49.180所以，如果你认为这个函数是x的f，啊，00:54:49.180 --> 00:54:50.695他们所做的不是简单的，啊，00:54:50.695 --> 00:54:52.540将x转换为x的f，00:54:52.540 --> 00:54:55.150他们用的是x加x的f。00:54:55.150 --> 00:54:58.435所以他们添加了这个身份跳过连接00:54:58.435 --> 00:55:01.990输入x跳过这两层，然后，00:55:01.990 --> 00:55:05.470嗯，加上两层的输出。00:55:05.470 --> 00:55:07.510所以，这是个好主意的原因，00:55:07.510 --> 00:55:10.150呃，也叫跳过连接，00:55:10.150 --> 00:55:15.280在默认情况下，身份连接是否会保留信息，对吗？00:55:15.280 --> 00:55:18.010所以，如果你想象，如果你，嗯，00:55:18.010 --> 00:55:20.110初始化网络，然后00:55:20.110 --> 00:55:22.600初始化权重层，使其具有较小的随机值，00:55:22.600 --> 00:55:24.835如果它们很小，接近于零，00:55:24.835 --> 00:55:28.885然后你会得到一个类似于嘈杂的身份函数的东西，对吗？00:55:28.885 --> 00:55:32.245所以默认情况下，您将通过所有层保存信息。00:55:32.245 --> 00:55:33.520如果你有一个很深的关系网，00:55:33.520 --> 00:55:35.110这意味着，即使经常有很多人，00:55:35.110 --> 00:55:39.410嗯，很多层，你仍然会有一些像你的原始输入。00:55:40.320 --> 00:55:44.215所以，呃，写这篇论文的人，他们展示了，呃，00:55:44.215 --> 00:55:46.660如果你没有类似的跳过连接，那么00:55:46.660 --> 00:55:49.570事实上，你可以找到深层-呃，00:55:49.570 --> 00:55:53.245深网络在某些任务上的性能比浅网络差。00:55:53.245 --> 00:55:54.955不是因为他们不够有表现力，00:55:54.955 --> 00:55:56.665但因为他们太难学了。00:55:56.665 --> 00:55:58.285所以，当你试图学习深层网络时，00:55:58.285 --> 00:55:59.950它只是没有有效地学习，结果你00:55:59.950 --> 00:56:01.795在浅网络中性能越来越差。00:56:01.795 --> 00:56:03.070所以，写这篇论文的人，00:56:03.070 --> 00:56:05.050它们表明，当添加这些跳过连接时，00:56:05.050 --> 00:56:07.105然后他们建立了深层网络，00:56:07.105 --> 00:56:10.550更有效的是，他们获得了良好的表现。00:56:12.000 --> 00:56:15.280呃，另一个例子是，这个想法00:56:15.280 --> 00:56:18.220还有一种叫做密集连接或密集网的东西。00:56:18.220 --> 00:56:19.585再说一遍，这是，呃，00:56:19.585 --> 00:56:23.965我认为在前馈或卷积环境中提出的东西。00:56:23.965 --> 00:56:26.770而且，啊，这和跳过连接差不多，但是，00:56:26.770 --> 00:56:28.030嗯，把一切都联系起来。00:56:28.030 --> 00:56:30.070所以，添加更多这些跳过连接00:56:30.070 --> 00:56:32.680从每一层到每一层都显示了这一点，00:56:32.680 --> 00:56:34.480嗯，表演得更好。00:56:34.480 --> 00:56:37.450还有，呃，最后一个我想说的，我没有照片的00:56:37.450 --> 00:56:40.210因为这就是所谓的公路连接。00:56:40.210 --> 00:56:43.180因此，这类似于剩余连接或跳过连接。00:56:43.180 --> 00:56:46.585啊，但我们的想法是，不只是添加x，00:56:46.585 --> 00:56:48.640添加你的身份，呃，连接，00:56:48.640 --> 00:56:52.060我们的想法是你要有一个控制平衡的门，嗯，00:56:52.060 --> 00:56:55.960添加身份和计算，啊，转换。00:56:55.960 --> 00:56:58.405所以，不是x加x，而是f，你知道，00:56:58.405 --> 00:56:59.995门乘以x+f，你知道，00:56:59.995 --> 00:57:02.110一减去门乘以x，类似的。00:57:02.110 --> 00:57:05.560嗯，这部作品实际上是受LSTM的启发，00:57:05.560 --> 00:57:07.510但不是将其应用于反复出现的环境，00:57:07.510 --> 00:57:11.570他们试图将其应用于前馈设置。00:57:13.860 --> 00:57:16.120可以。我现在要继续走了。00:57:16.120 --> 00:57:19.390嗯。所以，总的来说问题是，00:57:19.390 --> 00:57:20.620你知道，多少呃，00:57:20.620 --> 00:57:23.650消失和爆炸梯度是RNN设置之外的一个问题？00:57:23.650 --> 00:57:26.710我认为，嗯，重要的是，这是一个大问题00:57:26.710 --> 00:57:30.595但是您应该注意到这对于RNN来说是一个特别的问题。00:57:30.595 --> 00:57:34.000所以，嗯，RNN特别不稳定00:57:34.000 --> 00:57:37.525这主要是由于同一重量矩阵的重复乘法。00:57:37.525 --> 00:57:39.175如果你记得上次，嗯，00:57:39.175 --> 00:57:41.890RNN的特点是使其反复出现00:57:41.890 --> 00:57:44.770一次又一次地应用相同的权重矩阵。00:57:44.770 --> 00:57:47.095所以，这实际上是核心原因00:57:47.095 --> 00:57:49.865为什么它们如此容易受到消失和爆炸梯度的影响，00:57:49.865 --> 00:57:53.590啊，你可以在报纸上看到更多的信息。00:57:53.670 --> 00:57:57.730可以。所以，我知道今天有很多密集的信息，00:57:57.730 --> 00:57:59.815很多，嗯，很多符号。00:57:59.815 --> 00:58:01.690所以，如果我在任何时候失去你，这里有一个概括。00:58:01.690 --> 00:58:03.280现在是重新开始的好时机，因为它会00:58:03.280 --> 00:58:05.515可能会更容易理解。00:58:05.515 --> 00:58:07.600所以，好吧，回顾一下。我们今天学到了什么？00:58:07.600 --> 00:58:10.435嗯，我们学到的第一件事就是消失梯度问题。00:58:10.435 --> 00:58:11.830我们知道了，呃，这是什么。00:58:11.830 --> 00:58:15.580我们了解了为什么会发生这种情况，也看到了为什么对RNN不利，00:58:15.580 --> 00:58:18.085例如，RNN语言模型。00:58:18.085 --> 00:58:21.640啊，我们还学习了lstms和grus00:58:21.640 --> 00:58:25.480更复杂的RNN，它们使用门来控制信息流。00:58:25.480 --> 00:58:29.395通过这样做，它们对消失梯度问题更有弹性。00:58:29.395 --> 00:58:31.390可以。所以，如果这节课的其余部分，00:58:31.390 --> 00:58:32.740我想我们还有20分钟，00:58:32.740 --> 00:58:36.445啊，我们将学习两种更高级的RNN。00:58:36.445 --> 00:58:39.040所以，第一个是双向RNN，仅此而已00:58:39.040 --> 00:58:43.105关于信息从左到右和从右到左的流动。00:58:43.105 --> 00:58:44.905然后我们还要学习00:58:44.905 --> 00:58:49.780多层RNN，即在彼此的顶部应用多个RNN时。00:58:49.780 --> 00:58:54.100所以，我想说这两个概念都很简单。00:58:54.100 --> 00:58:56.905嗯，所以不应该太难理解。00:58:56.905 --> 00:59:00.160好吧，让我们从双向RNN开始。00:59:00.160 --> 00:59:04.225嗯，这是你在上节课结束时看到的照片。00:59:04.225 --> 00:59:05.305所以，如果你记得，00:59:05.305 --> 00:59:07.690情绪分类是当你00:59:07.690 --> 00:59:10.150像电影这样的输入句00:59:10.150 --> 00:59:15.460非常令人兴奋，你想把这归类为积极或消极的情绪。00:59:15.460 --> 00:59:19.160因此，在这个例子中，它应该被视为积极情绪。00:59:19.680 --> 00:59:23.650所以，嗯，这是一个你可以尝试的例子00:59:23.650 --> 00:59:26.860使用相当简单的RNN模型解决情绪分类。00:59:26.860 --> 00:59:29.830啊，这里我们使用RNN作为00:59:29.830 --> 00:59:32.890句子和隐藏状态代表句子。00:59:32.890 --> 00:59:35.740我们会做一些隐藏状态的组合来计算，00:59:35.740 --> 00:59:37.765我们认为这种情绪是什么。00:59:37.765 --> 00:59:40.165所以，我的问题是，如果我们看看，假设，00:59:40.165 --> 00:59:44.350与“可怕”这个词相对应的隐藏状态00:59:44.350 --> 00:59:46.420这个隐藏的状态作为单词的表示00:59:46.420 --> 00:59:49.510在句子的上下文中非常可怕。00:59:49.510 --> 00:59:53.455因此，在这种情况下，我们有时称之为隐藏状态00:59:53.455 --> 00:59:55.930一种上下文表示，因为其思想是00:59:55.930 --> 00:59:59.785在句子的上下文中对这个词的一种表示。00:59:59.785 --> 01:00:04.150所以，这里要考虑的是，这个上下文表示，01:00:04.150 --> 01:00:07.165它只包含有关左上下文的信息。01:00:07.165 --> 01:00:10.150所以，非常可怕的是，左边的上下文是单词um，01:00:10.150 --> 01:00:13.120电影是这样的，这个隐藏的状态就是01:00:13.120 --> 01:00:16.435它周围的蓝色框只看到左边的信息。01:00:16.435 --> 01:00:20.485它没有看到单词令人兴奋或感叹号的信息。01:00:20.485 --> 01:00:24.715所以，我们要问的是，正确的背景是什么？01:00:24.715 --> 01:00:28.690“可怕”的正确语境是-是多么令人兴奋和感叹号。01:00:28.690 --> 01:00:33.040我们认为合适的上下文在这里有用吗？01:00:33.040 --> 01:00:35.230我们认为这是我们想知道的吗？01:00:35.230 --> 01:00:37.405我认为在这个例子中，01:00:37.405 --> 01:00:41.695这实际上有点重要，因为我们有一个非常令人兴奋的短语。01:00:41.695 --> 01:00:44.829如果你孤立地看这个词，01:00:44.829 --> 01:00:47.005可怕的或可怕的通常意味着坏的东西，对吗？01:00:47.005 --> 01:00:50.650但是非常令人兴奋，你可以说一些好的东西，因为它只意味着非常令人兴奋。01:00:50.650 --> 01:00:53.230所以，如果你知道正确的背景，01:00:53.230 --> 01:00:56.680“令人兴奋”这个词可能很明显01:00:56.680 --> 01:00:58.900修改你对单词意思的理解01:00:58.900 --> 01:01:01.210在句子的上下文中非常可怕。01:01:01.210 --> 01:01:03.790尤其是考虑到我们正试图对情绪进行分类，01:01:03.790 --> 01:01:05.815这是-这很重要。01:01:05.815 --> 01:01:10.150所以这激发了你为什么想要得到信息01:01:10.150 --> 01:01:13.915当你做陈述的时候，从左到右。01:01:13.915 --> 01:01:16.000啊，如果你小时候，01:01:16.000 --> 01:01:18.535你父母告诉你过马路前要两头看。01:01:18.535 --> 01:01:20.620你可能会认为这和01:01:20.620 --> 01:01:22.510左边和右边的有用信息01:01:22.510 --> 01:01:24.970你想知道啊，在你做任何事情之前。01:01:24.970 --> 01:01:27.925可以。这就是动机，嗯，01:01:27.925 --> 01:01:31.900下面是双向RNN在实践中的工作原理。01:01:31.900 --> 01:01:35.065我有一种偶然的节日色彩。01:01:35.065 --> 01:01:38.755所以我们的想法是你有两个RNN。01:01:38.755 --> 01:01:42.880前面的RNN是由左向右编码的。01:01:42.880 --> 01:01:46.000另外，你还有一个反向RNN。01:01:46.000 --> 01:01:49.135这对前RNN有完全不同的权重。01:01:49.135 --> 01:01:52.660所以，反向RNN也在做同样的事情01:01:52.660 --> 01:01:56.110只不过它是从右到左对序列进行编码。01:01:56.110 --> 01:01:59.980所以，每个隐藏状态都是根据右边的那个状态来计算的。01:01:59.980 --> 01:02:02.500最后，你只需要从01:02:02.500 --> 01:02:06.700这两个RNN，然后你把它们连接在一起，你得到了你的呃，01:02:06.700 --> 01:02:09.385你最后的陈述。01:02:09.385 --> 01:02:12.025所以，特别是，如果我们现在考虑01:02:12.025 --> 01:02:16.330这个词在上下文中的上下文表现，01:02:16.330 --> 01:02:22.180嗯，这个-这个向量有来自左边和右边的信息，对吗？01:02:22.180 --> 01:02:24.235因为你有向前和向后的RNN01:02:24.235 --> 01:02:27.295分别有来自左右的信息。01:02:27.295 --> 01:02:30.190所以我们的想法是这些连接的隐藏状态，01:02:30.190 --> 01:02:34.735这些可以被视为类似于双向RNN的输出。01:02:34.735 --> 01:02:36.460就像你要用这些隐藏状态01:02:36.460 --> 01:02:38.680任何进一步的计算，然后啊，01:02:38.680 --> 01:02:40.780就是这些连接的隐藏状态01:02:40.780 --> 01:02:44.210传递到网络的下一部分。01:02:44.250 --> 01:02:48.205嗯，这里-这里是方程组，它们说的是同一件事。01:02:48.205 --> 01:02:51.790所以，你有你的前锋RNN，这里我们有啊，01:02:51.790 --> 01:02:53.995你以前可能没见过的符号01:02:53.995 --> 01:02:57.010这是一种符号，上面写着RNN，然后在括号里，01:02:57.010 --> 01:03:00.775前一个隐藏状态和输入，简单地说，你知道，01:03:00.775 --> 01:03:04.180HT是根据以前的隐藏状态和输入进行计算的。01:03:04.180 --> 01:03:08.590RNN Forward可以是香草味的、咕噜味的或LSTM味的。01:03:08.590 --> 01:03:10.810这并不重要，我们只是抽象地看待它。01:03:10.810 --> 01:03:16.104所以，你有这两个独立的RNN，01:03:16.104 --> 01:03:19.540RNN向前和RNN向后，一般来说，它们有单独的权重。01:03:19.540 --> 01:03:21.910尽管我看过一些论文，它们都有共同的权重。01:03:21.910 --> 01:03:23.875所以，有时候效果更好，01:03:23.875 --> 01:03:26.600也许当你有足够的训练数据时。01:03:26.790 --> 01:03:32.020最后，我们将这些连接的隐藏状态01:03:32.020 --> 01:03:37.610注意ht就像双向RNN的隐藏状态。01:03:38.550 --> 01:03:42.550所以，嗯，前面的图表相当笨拙。01:03:42.550 --> 01:03:44.395这是一个简化的图表。01:03:44.395 --> 01:03:46.240这可能是你要做的唯一一种图表01:03:46.240 --> 01:03:48.700从现在开始，请参阅以表示双向RNN。01:03:48.700 --> 01:03:50.770嗯，那么，我们在这里所做的就是01:03:50.770 --> 01:03:53.575使所有的水平箭头都向左右移动啊，01:03:53.575 --> 01:03:56.260表示这是一个双向RNN。01:03:56.260 --> 01:04:00.370所以，你应该假设的另一件事是这里描述的隐藏状态，01:04:00.370 --> 01:04:04.240这些红色的尝试红色的矩形与点。01:04:04.240 --> 01:04:06.580您可以假设这些是连接的转发，01:04:06.580 --> 01:04:08.590双向RNN的反向隐藏状态。01:04:08.590 --> 01:04:16.000[听不见]01:04:16.000 --> 01:04:18.415可以。所以问题是，嗯，01:04:18.415 --> 01:04:22.060你能分别训练你的前后RNN吗？01:04:22.060 --> 01:04:23.890嗯，在某种任务上，然后01:04:23.890 --> 01:04:26.665可能在它们被单独训练后将它们连接在一起，01:04:26.665 --> 01:04:28.285或者你会一起训练他们吗？01:04:28.285 --> 01:04:32.200嗯，在我看来，一起训练更常见，01:04:32.200 --> 01:04:35.230但我没有-我想我没有听说有人单独训练过他们。01:04:35.230 --> 01:04:37.270嗯，所以是的，看起来标准做法通常是01:04:37.270 --> 01:04:39.160一起训练。这有道理吗？01:04:39.160 --> 01:04:53.290[听不见]。01:04:53.290 --> 01:04:55.690那么，假设我们正在努力建造01:04:55.690 --> 01:04:59.440使用双向RNN的情感分类系统。01:04:59.440 --> 01:05:03.460那么，你所做的，也许我应该画出来，但我没有空间，是，01:05:03.460 --> 01:05:07.420你会做和你在单向RNN上做的一样的事情，呃，01:05:07.420 --> 01:05:10.045也就是说，元素y是min或max，01:05:10.045 --> 01:05:11.665嗯，得到你的句子编码。01:05:11.665 --> 01:05:16.490也许你只是这样做，但在连接的，嗯，n个州。01:05:16.950 --> 01:05:20.560可以。所以，需要注意的一点是，01:05:20.560 --> 01:05:23.020谈到双向RNN时，01:05:23.020 --> 01:05:26.935我们假设我们实际上可以访问整个输入序列。01:05:26.935 --> 01:05:28.780所以，我们假设我们有完整的句子，01:05:28.780 --> 01:05:31.565嗯，这部电影很刺激，而且，01:05:31.565 --> 01:05:34.740呃，那是一个必要的假设01:05:34.740 --> 01:05:37.935能够向前和向后运行RNN，对吗？01:05:37.935 --> 01:05:40.935嗯，有些情况下你不能假设这一点。01:05:40.935 --> 01:05:43.185例如，在语言建模中，01:05:43.185 --> 01:05:47.355根据任务的定义，您只能访问左侧上下文。01:05:47.355 --> 01:05:48.990你只知道迄今为止所说的话。01:05:48.990 --> 01:05:50.405你不知道接下来会发生什么。01:05:50.405 --> 01:05:54.070所以，你不能使用双向RNN，呃，01:05:54.070 --> 01:05:55.525做语言建模，01:05:55.525 --> 01:05:57.760以我们在这里描述的方式，因为，01:05:57.760 --> 01:05:59.815你没有完整的序列。01:05:59.815 --> 01:06:03.115但是，如果您确实可以访问整个序列。01:06:03.115 --> 01:06:05.230例如，如果你在做任何编码01:06:05.230 --> 01:06:07.495类似于情感的例子，01:06:07.495 --> 01:06:11.680嗯，那么双向性-双向性是相当强大的。01:06:11.680 --> 01:06:14.815你可能会认为这是一件好事01:06:14.815 --> 01:06:16.870因为事实证明从01:06:16.870 --> 01:06:18.805左右两边，呃，01:06:18.805 --> 01:06:23.725使学习这些更有用的上下文表示更加容易。01:06:23.725 --> 01:06:25.870因此，特别是作为01:06:25.870 --> 01:06:28.030你稍后会在课堂上学到的东西，呃，01:06:28.030 --> 01:06:30.610有一个叫伯特的模型，B-E-R-T，01:06:30.610 --> 01:06:34.330它代表来自变压器的双向编码器表示。01:06:34.330 --> 01:06:36.010这是最近的一次。01:06:36.010 --> 01:06:39.070比如，几个月前，呃，提议的系统，01:06:39.070 --> 01:06:42.460这是一个预先训练过的上下文表示系统。01:06:42.460 --> 01:06:46.450嗯，它严重依赖于双向性的概念。01:06:46.450 --> 01:06:48.760结果是双向的，呃，01:06:48.760 --> 01:06:51.565伯特的本性对它的成功非常重要。01:06:51.565 --> 01:06:53.290所以，你以后会了解更多，01:06:53.290 --> 01:06:55.990但这只是双向性给你带来的好处的一个例子01:06:55.990 --> 01:06:59.875更强大的上下文表示。01:06:59.875 --> 01:07:04.390可以。所以我们今天要讨论的最后一件事是多层RNN。01:07:04.390 --> 01:07:08.800呃，所以你可以认为RNN已经很深了01:07:08.800 --> 01:07:14.200从某种意义上说，因为你已经展开了很可能非常多次的步骤，01:07:14.200 --> 01:07:16.630你可以认为这是一种深度，对吧？01:07:16.630 --> 01:07:19.390但还有另一种方法，RNN可能很深。01:07:19.390 --> 01:07:25.210例如，如果您一个接一个地应用多个RNN，01:07:25.210 --> 01:07:28.555那么这将是一个不同的方式，使你的RNN深，01:07:28.555 --> 01:07:30.490这是，呃，01:07:30.490 --> 01:07:33.775在多层RNN后面。01:07:33.775 --> 01:07:37.315所以，你想这么做的原因是，01:07:37.315 --> 01:07:40.660这可能允许网络计算更复杂的表示。01:07:40.660 --> 01:07:43.840所以，这就是深层网络背后的逻辑。01:07:43.840 --> 01:07:45.280所以，如果你知道为什么01:07:45.280 --> 01:07:47.620更深层的是更好的，比如说卷积网络，01:07:47.620 --> 01:07:49.195那么这是一种相同的逻辑。01:07:49.195 --> 01:07:54.760也就是说，呃，你的低RNN可能在计算低级别的功能，比如，01:07:54.760 --> 01:07:56.770假设它跟踪语法，01:07:56.770 --> 01:08:01.700更高级别的RNN将计算更高级别的特性，比如语义。01:08:02.100 --> 01:08:06.775注意术语，这些有时被称为堆叠RNN。01:08:06.775 --> 01:08:09.640所以，这和你想象的一样有效。01:08:09.640 --> 01:08:13.630下面是一个多层RNN如何工作的例子。01:08:13.630 --> 01:08:15.610如果是三层的话。01:08:15.610 --> 01:08:18.115所以这是一个单向RNN，01:08:18.115 --> 01:08:20.530但它可能是双向的，01:08:20.530 --> 01:08:23.680嗯，如果你可以访问整个输入序列。01:08:23.680 --> 01:08:29.290所以，我猜，主要的是，一个RNN层的隐藏状态01:08:29.290 --> 01:08:35.390用作下一个RNN层的输入。01:08:35.400 --> 01:08:38.800嗯，有什么问题吗？01:08:38.800 --> 01:08:45.270是啊。01:08:45.270 --> 01:08:46.450[听不见]。01:08:46.450 --> 01:08:49.450这是个很好的问题。所以我认为这是关于计算顺序的问题。01:08:49.450 --> 01:08:52.390你将以什么顺序计算所有这些隐藏状态？01:08:52.390 --> 01:08:56.100我想有点灵活性吧？01:08:56.100 --> 01:08:59.640但是你可以计算所有的第一步，01:08:59.640 --> 01:09:02.295像所有的V型和电影型一样，01:09:02.295 --> 01:09:05.970或者你可以做所有RNN第1层，然后再做所有RNN第2层。01:09:05.970 --> 01:09:08.880所以，这是-我想，嗯，当你-你知道，01:09:08.880 --> 01:09:11.055调用pytorch函数来执行多层RNN，01:09:11.055 --> 01:09:13.380它将完成RNN第1层、第2层、第3层的所有操作。01:09:13.380 --> 01:09:14.580我想就是这样。01:09:14.580 --> 01:09:16.270但从逻辑上看，01:09:16.270 --> 01:09:19.000你没有理由不能用另一种方式。01:09:19.000 --> 01:09:30.190是的？[听不见]。01:09:30.190 --> 01:09:32.110是的，是的。这也是一个很好的观点。01:09:32.110 --> 01:09:35.950嗯，有人指出，如果它们是双向的，01:09:35.950 --> 01:09:37.480那么你就不再有这种灵活性了。01:09:37.480 --> 01:09:39.955你必须在第二层之前完成所有的第一层。01:09:39.955 --> 01:09:44.090是的，说得对。还有人吗？01:09:47.040 --> 01:09:52.480可以。呃，所以大部分RNN都在实践中，01:09:52.480 --> 01:09:56.065嗯，这往往表现得很好，01:09:56.065 --> 01:09:58.090呃，当我看的时候，01:09:58.090 --> 01:10:00.970基于RNN的系统在某种任务上做得很好，01:10:00.970 --> 01:10:04.330它们通常是某种多层RNN，嗯，01:10:04.330 --> 01:10:06.400但他们肯定没有01:10:06.400 --> 01:10:09.430你可能看到的深层卷积或前馈网络，01:10:09.430 --> 01:10:10.795例如，图像任务。01:10:10.795 --> 01:10:12.550所以，你知道，非常深的卷积网络，01:10:12.550 --> 01:10:14.470我想现在有几百层了，嗯，01:10:14.470 --> 01:10:16.795你肯定没有得到那么深的RNN。01:10:16.795 --> 01:10:18.790例如，嗯，01:10:18.790 --> 01:10:22.300在这篇来自谷歌的论文中，01:10:22.300 --> 01:10:25.165他们正在进行这种大型超参数搜索01:10:25.165 --> 01:10:29.740神经机器翻译以找出哪种超参数在NMT中工作得很好。01:10:29.740 --> 01:10:31.720在这篇论文中，他们发现01:10:31.720 --> 01:10:34.165对于编码器RNN来说，两到四层是最好的，01:10:34.165 --> 01:10:36.280四层是最好的解码器RNN。01:10:36.280 --> 01:10:39.430呃，下次你会发现编码器和解码器的更多含义。01:10:39.430 --> 01:10:41.275嗯，但这些数字相当小。01:10:41.275 --> 01:10:43.330尽管他们发现如果你加上这些跳过01:10:43.330 --> 01:10:45.355连接或这些密集的连接，嗯，01:10:45.355 --> 01:10:49.750这样更容易更有效地学习一些更深层次的RNN，01:10:49.750 --> 01:10:51.055比如，可能多达八层，01:10:51.055 --> 01:10:53.605但这些肯定不是几百层深。01:10:53.605 --> 01:10:55.750其中一个原因是，01:10:55.750 --> 01:10:59.095RNN往往没有其他类型的网络那么深，01:10:59.095 --> 01:11:01.675这是因为正如我们之前所评论的，01:11:01.675 --> 01:11:03.205RNN必须计算，呃，01:11:03.205 --> 01:11:05.380顺序；它们不能并行计算。01:11:05.380 --> 01:11:07.330这意味着它们计算起来相当昂贵。01:11:07.330 --> 01:11:09.519如果你有这样的深度，二维的，01:11:09.519 --> 01:11:13.675你有时间步长的深度，然后RNN层的深度是2，01:11:13.675 --> 01:11:15.160然后它变得非常，01:11:15.160 --> 01:11:17.830计算这些RNN非常昂贵。01:11:17.830 --> 01:11:19.885所以，这也是他们不深入的另一个原因。01:11:19.885 --> 01:11:23.290嗯，所以我们又提到了变形金刚。01:11:23.290 --> 01:11:25.165呃，你以后会了解变形金刚的。01:11:25.165 --> 01:11:27.280但这些，似乎，嗯，01:11:27.280 --> 01:11:30.400从我所知道的情况来看，01:11:30.400 --> 01:11:31.900这些天人们在使用什么。01:11:31.900 --> 01:11:33.580基于变压器的网络可能很深。01:11:33.580 --> 01:11:35.530所以，呃，但是举例来说，01:11:35.530 --> 01:11:38.425有24层版本和12层版本，嗯，01:11:38.425 --> 01:11:39.820诚然，这是谷歌的训练，01:11:39.820 --> 01:11:41.860它们有很大的计算能力。01:11:41.860 --> 01:11:43.600嗯，但我认为部分原因是01:11:43.600 --> 01:11:45.685这些基于变压器的网络可能很深，01:11:45.685 --> 01:11:48.220他们有很多这种跳跃式的连接。01:11:48.220 --> 01:11:49.750事实上，整个嗯，01:11:49.750 --> 01:11:52.570变形金刚的创新之处在于它们建立在01:11:52.570 --> 01:11:57.520跳过连接。好的，有问题吗？01:11:57.520 --> 01:12:01.450我们快结束了。可以。好吧。01:12:01.450 --> 01:12:04.030所以，呃，这是我们今天所学的总结。01:12:04.030 --> 01:12:05.965我知道这是很多信息。01:12:05.965 --> 01:12:11.530嗯，但我认为从今天开始有四个实际的收获，呃，01:12:11.530 --> 01:12:13.795可能对你的项目有用，01:12:13.795 --> 01:12:14.920即使你，嗯，01:12:14.920 --> 01:12:17.800呃，即使你01:12:17.800 --> 01:12:21.130他们没有发现自己很有趣，他们可能很有用。01:12:21.130 --> 01:12:24.190所以，第一个问题是LSTM非常强大。01:12:24.190 --> 01:12:25.990他们肯定比01:12:25.990 --> 01:12:27.910嗯，比瓦妮拉更强大。01:12:27.910 --> 01:12:31.450嗯，咕噜也比瓦妮拉更强大。01:12:31.450 --> 01:12:34.210唯一的区别就是01:12:34.210 --> 01:12:37.480同样，GRU比LSTM更快。01:12:37.480 --> 01:12:40.465下一个问题是，你可能应该剪掉你的梯度，01:12:40.465 --> 01:12:42.055因为如果你不修剪你的梯度，01:12:42.055 --> 01:12:47.050你有从悬崖上走下来的危险，最终会在你的模特身上找到奶奶。01:12:47.050 --> 01:12:52.105下一个技巧是，如果你能应用双向性，它是有用的。01:12:52.105 --> 01:12:56.125而且，基本上，当你有权访问整个输入序列时，01:12:56.125 --> 01:12:57.850你可以应用双向性，01:12:57.850 --> 01:12:59.790所以在默认情况下，您应该这样做。01:12:59.790 --> 01:13:04.140最后一个技巧是多层RNN非常强大。01:13:04.140 --> 01:13:06.330再说一次，如果你01:13:06.330 --> 01:13:08.355嗯，有足够的计算能力。01:13:08.355 --> 01:13:11.405但是如果你想让你的多层RNN很深，01:13:11.405 --> 01:13:13.255那么您可能需要跳过连接。01:13:13.255 --> 01:13:22.840
All right. Thanks [NOISE].

