WEBVTT
Kind: captions
Language: en

00:00:04.790 --> 00:00:11.940可以。很高兴看到每个人都回来上课。00:00:11.940 --> 00:00:14.160嗯，所以，对于LEC来说，00:00:14.160 --> 00:00:16.500今天的讲座，嗯，00:00:16.500 --> 00:00:20.040我大部分时间想做的实际上是00:00:20.040 --> 00:00:23.730进入这些想法的核心00:00:23.730 --> 00:00:28.290神经网络的反向传播算法及构造方法00:00:28.290 --> 00:00:33.690足以进行反向传播的计算图，00:00:33.690 --> 00:00:36.195神经网络训练神经网络。00:00:36.195 --> 00:00:41.330所以，总的来说，嗯，这就是我今天计划做的。00:00:41.330 --> 00:00:43.160所以，上节课结束时，00:00:43.160 --> 00:00:47.870我有点没时间了，开始咕哝着，挥手，00:00:47.870 --> 00:00:51.620嗯，做关于重量梯度的导数。00:00:51.620 --> 00:00:54.265所以，我有点想这么做，但又一次。00:00:54.265 --> 00:00:57.665所以希望它能更好地沟通。00:00:57.665 --> 00:01:04.070所以，我们要做的是，更多地讨论一些关于矩阵梯度的技巧，00:01:04.070 --> 00:01:07.835嗯，还有一个特别的问题，就是单词向量。00:01:07.835 --> 00:01:10.010然后是课堂的主要部分，00:01:10.010 --> 00:01:12.619我们将讨论反向传播算法00:01:12.619 --> 00:01:15.770以及它如何在计算图上运行。00:01:15.770 --> 00:01:18.870嗯，然后在最后一节课上，00:01:18.870 --> 00:01:22.380嗯，我不会隐瞒的，嗯，00:01:22.380 --> 00:01:26.750这有点像你应该拿的一袋杂七杂八的东西00:01:26.750 --> 00:01:32.005了解神经网络和训练神经网络。00:01:32.005 --> 00:01:33.940嗯，就像，我想，00:01:33.940 --> 00:01:38.720你知道我们梦想着人工智能的未来00:01:38.720 --> 00:01:43.670非常聪明，你可以对他们说这是数据，这是我的问题，00:01:43.670 --> 00:01:46.850去给我训练一个模型，它可能会起作用。00:01:46.850 --> 00:01:48.560嗯，在未来的世界里，00:01:48.560 --> 00:01:50.720这可能是伴随而来的[噪音]。00:01:50.720 --> 00:01:53.150这肯定是一件积极的事情00:01:53.150 --> 00:01:56.840目前在auto-ml的主题下进行了研究。00:01:56.840 --> 00:02:02.240我想问题是auto-ml是一个可扩展的解决方案还是00:02:02.240 --> 00:02:06.110自动ML技术的气候变化后果是00:02:06.110 --> 00:02:11.000很糟糕的是，有人真的认为这些低得多的权力，00:02:11.000 --> 00:02:17.135嗯，神经系统在解决某些问题上可能会更好。00:02:17.135 --> 00:02:20.240但不管怎样，不管怎样，我们都还没有真正做到。00:02:20.240 --> 00:02:22.235事实是，00:02:22.235 --> 00:02:24.320当你训练神经网络时，00:02:24.320 --> 00:02:27.860你只需要知道一大堆事情00:02:27.860 --> 00:02:31.565初始化、非线性、学习速率等。00:02:31.565 --> 00:02:34.440你知道，当我教这门课的时候00:02:34.440 --> 00:02:41.410上一次我不知怎么地认为人们会通过潜移默化来接受这个。00:02:41.410 --> 00:02:43.689如果我们给了初学者，00:02:43.689 --> 00:02:47.440把代码剪切到人身上，现在开始00:02:47.440 --> 00:02:52.760我们初始化了矩阵和设定学习速率的代码，00:02:52.760 --> 00:02:58.445通过潜移默化，人们会理解这就是你必须做的，并且去做。00:02:58.445 --> 00:03:03.859嗯，它并没有真正在课堂上教授足够的实用技巧和技巧，00:03:03.859 --> 00:03:05.840但很明显当我们00:03:05.840 --> 00:03:12.035最后一个项目时间，至少对很多人来说，渗透作用没有起作用。00:03:12.035 --> 00:03:13.789嗯，所以这次，00:03:13.789 --> 00:03:16.625[笑声]我至少想花上几分钟00:03:16.625 --> 00:03:20.510至少指出一些其他重要的事情。00:03:20.510 --> 00:03:22.729我的意思是，总的来说，00:03:22.729 --> 00:03:27.330你知道2018年的现实，深入学习，不，00:03:27.330 --> 00:03:30.510等等，现在是2019年，2019年，嗯，00:03:30.510 --> 00:03:34.430深度学习，就是深度学习仍然是一种手艺。00:03:34.430 --> 00:03:39.620你要知道很多做事情的技巧00:03:39.620 --> 00:03:42.440神经网络训练00:03:42.440 --> 00:03:46.050与您的模型无法成功工作相反。00:03:46.050 --> 00:03:50.390可以。最后一个公告，我就进去了。00:03:50.390 --> 00:03:56.345嗯，所以，我们在办公室里做了一些进一步的工作，00:03:56.345 --> 00:03:59.840我们的位置和我想有很多问题00:03:59.840 --> 00:04:04.460包括本地ICPD学生没有斯坦福大学ID的机会。00:04:04.460 --> 00:04:05.795我们必须，嗯，00:04:05.795 --> 00:04:07.880去上班时间。00:04:07.880 --> 00:04:10.670所以周四晚上的办公时间，00:04:10.670 --> 00:04:12.830嗯，那是在这节课之后，00:04:12.830 --> 00:04:14.655如果你想去谈谈，00:04:14.655 --> 00:04:16.705嗯，第二个家庭作业，嗯，00:04:16.705 --> 00:04:21.335周四晚上的办公时间将在索顿-桑顿110。00:04:21.335 --> 00:04:24.260嗯，现在我不知道桑顿在哪里。00:04:24.260 --> 00:04:29.390当我把它翻译成旧的terman附件时，我觉得更合理，00:04:29.390 --> 00:04:32.450但那可能只是显示我的年龄，因为可能没有00:04:32.450 --> 00:04:35.620你记得他们曾经是一座叫做特曼的建筑。00:04:35.620 --> 00:04:37.695所以这对你也没有帮助。00:04:37.695 --> 00:04:38.980但是你知道，00:04:38.980 --> 00:04:40.460如果你要去，对吧，00:04:40.460 --> 00:04:42.230我不知道我们面对的是哪个方向。00:04:42.230 --> 00:04:44.855如果你往那边走，我想00:04:44.855 --> 00:04:49.520如果你知道巴布亚新几内亚雕塑园在哪里，00:04:49.520 --> 00:04:54.650在你到达巴布亚新几内亚雕塑园之前，有一片开阔的草地，00:04:54.650 --> 00:04:59.695那是特曼过去住的地方，现在仍然矗立在那里的建筑是桑顿。00:04:59.695 --> 00:05:02.700嗯，桑顿，今晚110。00:05:02.700 --> 00:05:04.349我想六点半开始，00:05:04.349 --> 00:05:06.210正确的？6:30到9点。00:05:06.210 --> 00:05:11.120可以。正确的。所以，让我把上次的事做完。00:05:11.120 --> 00:05:15.195记住我们有五个单词的窗口，然后我们00:05:15.195 --> 00:05:19.700通过一个c等于wx加b的神经网络层，00:05:19.700 --> 00:05:22.160h的非线性等于x的f，00:05:22.160 --> 00:05:23.480然后我们，嗯，00:05:23.480 --> 00:05:29.150只会得到一个关于这个是否在它的中心的分数[噪音]00:05:29.150 --> 00:05:31.955像巴黎这样的命名实体00:05:31.955 --> 00:05:35.090取矢量的点积乘以隐藏层。00:05:35.090 --> 00:05:36.774所以这是我们的模型，00:05:36.774 --> 00:05:41.120然后我们想算出s的偏导数00:05:41.120 --> 00:05:45.680考虑到我们所有的变量，我们做了各种各样的案例，00:05:45.680 --> 00:05:48.110但我们还没有做的是重量，00:05:48.110 --> 00:05:50.645所有这些神经网络层的重量。00:05:50.645 --> 00:05:53.285可以。所以，链式法则，嗯，00:05:53.285 --> 00:05:58.005ds dw的部分是ds乘以hd，00:05:58.005 --> 00:06:01.695嗯，dhdz乘以dz，dw。00:06:01.695 --> 00:06:05.055好吧，如果你记得上次，00:06:05.055 --> 00:06:09.945我们对前两个做了一些计算，00:06:09.945 --> 00:06:12.125嗯，偏导数是。00:06:12.125 --> 00:06:14.780我们可以说我们可以打电话00:06:14.780 --> 00:06:19.145这些增量是我们的错误信号来自上面。00:06:19.145 --> 00:06:23.060有来自上面的错误信号的概念是00:06:23.060 --> 00:06:24.920我将回到的主要部分00:06:24.920 --> 00:06:27.020演讲和一种核心概念。00:06:27.020 --> 00:06:29.630但我们没有处理的是这个DZ，00:06:29.630 --> 00:06:35.540我们开始研究这个，我提出了论点，嗯，00:06:35.540 --> 00:06:39.050根据我们的形状约定00:06:39.050 --> 00:06:42.320这应该和我们的w矩阵的形状相同。00:06:42.320 --> 00:06:43.835所以应该是，嗯，00:06:43.835 --> 00:06:47.360和这个w矩阵在时间m上是一样的。00:06:47.360 --> 00:06:55.590所以我们想用w求出z的偏导数，和这个一样，00:06:55.590 --> 00:06:59.335嗯，[噪音]dwx加上b，dw。00:06:59.335 --> 00:07:03.065所以我们想知道这个导数是什么。00:07:03.065 --> 00:07:04.940如果这不明显，00:07:04.940 --> 00:07:09.740考虑它的一种方法是回到矩阵的这个元素00:07:09.740 --> 00:07:14.830事实上，首先要从元素的角度来考虑它应该是什么，00:07:14.830 --> 00:07:17.795一旦你想清楚它应该是什么，嗯，00:07:17.795 --> 00:07:22.090把它改写回矩阵形式给出简洁的答案。00:07:22.090 --> 00:07:26.750我们得到的是这里的输入和有偏项00:07:26.750 --> 00:07:31.730我们将矩阵乘以这个向量，得到这些。00:07:31.730 --> 00:07:34.400如果你想想那里发生了什么，00:07:34.400 --> 00:07:39.200我们得到了这个重量矩阵，对于一个特定的重量，00:07:39.200 --> 00:07:44.885权重是第一个指标，它将对应于00:07:44.885 --> 00:07:48.950隐藏层及其第二个索引将00:07:48.950 --> 00:07:53.240对应于输入向量中的一个位置。00:07:53.240 --> 00:07:56.870矩阵中的一个权重最终是00:07:56.870 --> 00:08:01.100用于计算隐藏层的一个元素的部分。00:08:01.100 --> 00:08:04.939所以，你要取的隐藏层的一个元素，嗯，00:08:04.939 --> 00:08:08.270矩阵的一行，然后乘以00:08:08.270 --> 00:08:11.600这个向量的分量，所以当偏压00:08:11.600 --> 00:08:15.050但矩阵的一个元素只是00:08:15.050 --> 00:08:19.020用于计算的一个元素之间，00:08:19.020 --> 00:08:22.150嗯，隐藏向量的一个重要元素。00:08:22.150 --> 00:08:25.170可以。所以，好吧，这意味着，嗯，00:08:25.170 --> 00:08:31.040如果我们考虑的是关于wij的偏导数是什么，00:08:31.040 --> 00:08:38.535它只对Zi有贡献，00:08:38.535 --> 00:08:42.750它只对XJ起作用。00:08:42.750 --> 00:08:44.850所以，我们最终00:08:44.850 --> 00:08:47.870我们得到了关于wij的部分信息，00:08:47.870 --> 00:08:50.570我们可以计算出，00:08:50.570 --> 00:08:53.690只是为了尊重子。00:08:53.690 --> 00:08:57.080当我们来看这个乘法的时候，00:08:57.080 --> 00:09:01.370我们要结束的是这类术语和wik times00:09:01.370 --> 00:09:04.490XK，那排有重量00:09:04.490 --> 00:09:07.685穿过向量位置的矩阵。00:09:07.685 --> 00:09:17.105所以wij的唯一使用位置是，乘以xj。00:09:17.105 --> 00:09:18.485在那一点上，00:09:18.485 --> 00:09:20.570从某种程度上来说，00:09:20.570 --> 00:09:24.440在我们做微分的一个基本变量中，00:09:24.440 --> 00:09:27.035就像我们有3个，00:09:27.035 --> 00:09:30.260嗯，我们说3x的导数是什么？00:09:30.260 --> 00:09:31.700事实上，x令人困惑，00:09:31.700 --> 00:09:32.795所以我不该这么说。00:09:32.795 --> 00:09:38.450就像我们有三个w，三个w对w的导数是什么？00:09:38.450 --> 00:09:39.890三个人，对吗？00:09:39.890 --> 00:09:44.660所以，我们这里有一个术语，它本来是w，00:09:44.660 --> 00:09:47.805将是wij乘以xj，00:09:47.805 --> 00:09:51.725它对wij的导数就是xj。00:09:51.725 --> 00:09:53.825这有道理吗？00:09:53.825 --> 00:09:55.310每个人都相信？00:09:55.310 --> 00:09:57.270[噪音]手指交叉。00:09:57.270 --> 00:10:01.920可以。对于这个矩阵的一个元素，00:10:01.920 --> 00:10:04.290我们刚从XJ出来。00:10:04.290 --> 00:10:05.715在那一点上，00:10:05.715 --> 00:10:07.910嗯，我们说，嗯，00:10:07.910 --> 00:10:14.060当然，我们想知道全矩阵的雅可比式是什么。00:10:14.060 --> 00:10:15.410如果你开始考虑它，00:10:15.410 --> 00:10:18.545这个论点适用于每个单元格。00:10:18.545 --> 00:10:20.210所以，对于每一个，00:10:20.210 --> 00:10:22.175嗯，细胞，嗯，00:10:22.175 --> 00:10:24.260W的雅各比人，00:10:24.260 --> 00:10:26.900嗯，是XJ。00:10:26.900 --> 00:10:30.530所以，这意味着，嗯，00:10:30.530 --> 00:10:34.850我们可以利用它来计算雅可比。00:10:34.850 --> 00:10:42.650所以，单个wij的导数是delta-ixj，对所有细胞都是这样的。00:10:42.650 --> 00:10:48.430所以我们想要一个雅可比矩阵，它有delta i，00:10:48.430 --> 00:10:51.560嗯，XJ在它的每个细胞里。00:10:51.560 --> 00:10:55.850我们可以通过使用外部产品来创造。00:10:55.850 --> 00:11:00.049所以，如果我们有一个三角洲的行向量，00:11:00.049 --> 00:11:03.325来自上面和一列的错误信号，00:11:03.325 --> 00:11:05.795对，我说的不对，对不起。00:11:05.795 --> 00:11:11.420如果我们有一列delta错误信号00:11:11.420 --> 00:11:17.300从上面我们有一行x转移向量，00:11:17.300 --> 00:11:21.995嗯，当我们把它们相乘时，我们得到了外积00:11:21.995 --> 00:11:26.840我们得到每个细胞的delta-ixj，这就是我们的雅可比答案，00:11:26.840 --> 00:11:29.615嗯，为了锻炼，00:11:29.615 --> 00:11:34.070嗯，我们从一开始就用的三角洲。00:11:34.070 --> 00:11:36.710可以。这个，嗯，00:11:36.710 --> 00:11:40.100我们得到这个形式，它是00:11:40.100 --> 00:11:44.855上面的误差信号和我们计算的局部梯度信号。00:11:44.855 --> 00:11:47.720这就是我们将要反复看到的模式00:11:47.720 --> 00:11:51.260这将再次利用我们的计算图。00:11:51.260 --> 00:11:53.315好吧，一切都好吗？00:11:53.315 --> 00:11:56.810可以。嗯，那么，这只是，00:11:56.810 --> 00:12:01.070嗯，这是家庭作业二。00:12:01.070 --> 00:12:03.110你应该做一些这样的事情。00:12:03.110 --> 00:12:06.845嗯，这里有一些收集到的小费，00:12:06.845 --> 00:12:10.010嗯，我希望能有所帮助。00:12:10.010 --> 00:12:13.480我的意思是跟踪你的变量和00:12:13.480 --> 00:12:16.150它们的维度非常有用，因为我们00:12:16.150 --> 00:12:19.255就可以算出事物的维度。00:12:19.255 --> 00:12:21.385你经常走到一半。00:12:21.385 --> 00:12:24.190我的意思是基本上你所做的是00:12:24.190 --> 00:12:27.100一次又一次地应用链式法则。00:12:27.100 --> 00:12:28.800它总是这样。00:12:28.800 --> 00:12:33.920嗯，但是用这种矩阵演算的方式做，链式法则的意义。00:12:33.920 --> 00:12:37.610嗯，在家庭作业中，你必须做一个SoftMax，00:12:37.610 --> 00:12:39.410我们在课堂上没有做过。00:12:39.410 --> 00:12:42.305嗯，我认为你会发现有用的东西，00:12:42.305 --> 00:12:46.985如果你想分开，SoftMax要考虑两种情况。00:12:46.985 --> 00:12:51.650第一，当你为正确的班级做准备的时候。00:12:51.650 --> 00:12:56.795然后，另一种情况是所有其他不正确的类。00:12:56.795 --> 00:12:58.505嗯，是的。00:12:58.505 --> 00:13:00.860嗯，在这个小小的推导过程中，00:13:00.860 --> 00:13:03.035我以前做过，我说得很好，00:13:03.035 --> 00:13:05.360我们来计算一个元素部分00:13:05.360 --> 00:13:08.660因为这会让我对发生的事情有所了解，00:13:08.660 --> 00:13:09.860答案是什么？00:13:09.860 --> 00:13:12.230我觉得这真是件好事00:13:12.230 --> 00:13:14.780如果你被矩阵微积分搞糊涂了。00:13:14.780 --> 00:13:16.565我有点，00:13:16.565 --> 00:13:20.660嗯，稍微跳过另一张幻灯片。00:13:20.660 --> 00:13:22.295上次说的00:13:22.295 --> 00:13:25.400我刚才谈到的形状规则00:13:25.400 --> 00:13:31.550对于家庭作业，你可以想怎么做就怎么做，00:13:31.550 --> 00:13:33.380你可以从以下方面来解决这个问题：00:13:33.380 --> 00:13:35.630你知道分子顺序雅各比人，00:13:35.630 --> 00:13:37.280如果你觉得最好的话。00:13:37.280 --> 00:13:40.130但是我们希望你给我们最后的答案00:13:40.130 --> 00:13:43.730您的作业问题遵循形状规则。00:13:43.730 --> 00:13:46.760所以，导数应该在00:13:46.760 --> 00:13:50.825与变量相同的向量矩阵，00:13:50.825 --> 00:13:54.920你正在计算你的衍生产品。00:13:54.920 --> 00:14:00.320可以。嗯，最后一点是最后一次完成这个例子，00:14:00.320 --> 00:14:01.700我想说一点，00:14:01.700 --> 00:14:05.150就是语言所发生的。00:14:05.150 --> 00:14:08.300一个答案也没什么不同。00:14:08.300 --> 00:14:12.785但另一个答案是，它们有点特殊，因为，00:14:12.785 --> 00:14:16.610你知道，我们真的有一个词向量矩阵，对吗？00:14:16.610 --> 00:14:19.535每个单词都有一个向量。00:14:19.535 --> 00:14:23.540然后你可以把它看作是一个单词向量矩阵，00:14:23.540 --> 00:14:25.310哪一行有不同的词。00:14:25.310 --> 00:14:28.760但我们实际上并没有联系上00:14:28.760 --> 00:14:33.170这个矩阵直接进入我们的分类器系统。00:14:33.170 --> 00:14:37.190相反，我们连接到分类器系统的是00:14:37.190 --> 00:14:41.630这扇窗户和那扇窗户有五个字。00:14:41.630 --> 00:14:43.490通常它们是不同的词。00:14:43.490 --> 00:14:46.145但你知道有时候会出现同样的词，00:14:46.145 --> 00:14:49.160嗯，在那个窗口的两个位置。00:14:49.160 --> 00:14:52.190因此，我们仍然可以00:14:52.190 --> 00:14:57.349完全一样，继续往下倾斜，然后说好的，00:14:57.349 --> 00:14:59.435嗯，让我们解决，嗯，00:14:59.435 --> 00:15:03.335这个词窗口向量的梯度。00:15:03.335 --> 00:15:09.755如果这些是维D的话，我们会得到这类5维的向量。00:15:09.755 --> 00:15:13.025但是，你知道我们该怎么做，00:15:13.025 --> 00:15:15.140以及我们对此所做的回答。00:15:15.140 --> 00:15:21.980我们可以把这个窗口向量分成五部分，然后说“啊哈”，00:15:21.980 --> 00:15:25.190我们有五个词向量更新。00:15:25.190 --> 00:15:30.335我们将把它们应用到向量矩阵这个词上。00:15:30.335 --> 00:15:34.565嗯，如果同一个词出现两次，00:15:34.565 --> 00:15:38.990嗯，在那个窗口中，我们实际上应用了两个更新。00:15:38.990 --> 00:15:41.810所以，它会更新两次或者可能00:15:41.810 --> 00:15:44.810实际上，你想先对它们求和，然后更新一次，但是是的，00:15:44.810 --> 00:15:46.955这是一个技术问题。00:15:46.955 --> 00:15:53.345嗯，这实际上意味着我们非常稀少00:15:53.345 --> 00:15:57.180更新单词向量矩阵，因为00:15:57.180 --> 00:16:01.655矢量矩阵一词不变，只有几行，00:16:01.655 --> 00:16:03.440嗯，将被更新。00:16:03.440 --> 00:16:07.880如果-嗯，很快我们就要来这里用手电筒做些事情了00:16:07.880 --> 00:16:11.990嗯，如果你在手电筒周围戳，它甚至还有一些特殊的东西。00:16:11.990 --> 00:16:15.440嗯，找一些像稀疏SGD这样的东西来寻找意义00:16:15.440 --> 00:16:19.100你正在做一个非常稀疏的更新。00:16:19.100 --> 00:16:24.590嗯，但是还有一件有趣的事情你应该知道。00:16:24.590 --> 00:16:26.285很多时候，嗯，00:16:26.285 --> 00:16:29.600你所做的就是我们推动00:16:29.600 --> 00:16:33.380沿着这些梯度向下进入我们的词向量。00:16:33.380 --> 00:16:35.645好吧，想法是不，00:16:35.645 --> 00:16:39.094如果我们这样做就像其他神经网络学习一样，00:16:39.094 --> 00:16:46.655我们原则上会说，用这种方式移动矢量这个词。00:16:46.655 --> 00:16:50.134因为它们在帮助确定00:16:50.134 --> 00:16:54.665在本例中命名为实体分类，因为这是我们的激励示例。00:16:54.665 --> 00:16:59.150嗯，所以你知道，比如，你可能会知道00:16:59.150 --> 00:17:04.970指定实体的一个很好的指标是落下或对不起下面的地名。00:17:04.970 --> 00:17:08.090所以，N之后你经常去伦敦、巴黎等地。00:17:08.090 --> 00:17:11.030是的，所以有一种特殊的行为00:17:11.030 --> 00:17:14.360其他介词并不是一个好的位置指示器。00:17:14.360 --> 00:17:16.040所以，可能有点嗯，00:17:16.040 --> 00:17:19.940移动它的位置，然后说这是00:17:19.940 --> 00:17:26.135良好的位置指示器，因此有助于我们的分类器更好地工作。00:17:26.135 --> 00:17:30.020所以，原则上这是好事，这是好事，00:17:30.020 --> 00:17:34.100更新单词向量以帮助您更好地执行00:17:34.100 --> 00:17:39.710一个受监督的任务，如这个命名的实体识别分类。00:17:39.710 --> 00:17:45.050但是，有一个缺点，那就是它实际上并不总是有效的。00:17:45.050 --> 00:17:47.120那么，为什么它不总是有效呢？00:17:47.120 --> 00:17:50.750好吧，假设我们正在训练分类器。00:17:50.750 --> 00:17:56.360嗯，你知道这可能是我刚做的，或者是一个SoftMax或Logistic回归。00:17:56.360 --> 00:17:59.525我们想分类，嗯，00:17:59.525 --> 00:18:02.855电影评论感情是积极的还是消极的。00:18:02.855 --> 00:18:07.730好吧，你知道，如果我们训练了我们的词汇载体，00:18:07.730 --> 00:18:13.310我们有一些向量空间，也许在向量空间，嗯，电视，00:18:13.310 --> 00:18:15.860电视和电视都很近00:18:15.860 --> 00:18:19.520因为它们基本上是相同的。00:18:19.520 --> 00:18:22.400很好，我们的矢量词很好。00:18:22.400 --> 00:18:25.250但是，假设是这样，00:18:25.250 --> 00:18:28.820在我们的分类器训练数据中。00:18:28.820 --> 00:18:32.630所以，这是我们的电影情感评论培训数据。00:18:32.630 --> 00:18:38.465我们有电视和电视这个词，但没有电视这个词。00:18:38.465 --> 00:18:40.400那么接下来会发生什么呢？00:18:40.400 --> 00:18:45.350当我们试着训练情绪分类器时，00:18:45.350 --> 00:18:51.200如果我们把梯度推回向量这个词，会发生什么？00:18:51.200 --> 00:18:58.040它将围绕我们在训练数据中看到的单词向量移动。00:18:58.040 --> 00:19:01.070但电视肯定不会动，对吧？00:19:01.070 --> 00:19:05.090因为我们只是把梯度向下推到训练数据中的单词上。00:19:05.090 --> 00:19:06.890所以，这个词毫无意义，00:19:06.890 --> 00:19:09.740所以它一直都在原来的地方。00:19:09.740 --> 00:19:13.760所以，如果我们的训练结果是文字四处移动。00:19:13.760 --> 00:19:17.525所以，这里有一个很好的词来表示负面情绪，嗯，00:19:17.525 --> 00:19:20.839如果在测试时，00:19:20.839 --> 00:19:22.400当我们运行我们的模型时，00:19:22.400 --> 00:19:25.385如果我们用电视来评价一个句子，00:19:25.385 --> 00:19:27.620它实际上会给出错误的答案。00:19:27.620 --> 00:19:32.510然而，如果我们根本没有改变矢量这个词，而且刚刚离开00:19:32.510 --> 00:19:37.610它们是我们的嵌入学习系统放置它们的地方。00:19:37.610 --> 00:19:39.500然后它会说电视，00:19:39.500 --> 00:19:42.620这个词的意思和电视或电视差不多。00:19:42.620 --> 00:19:43.850我应该同样对待它00:19:43.850 --> 00:19:47.840我的情绪分类器和它实际上会做得更好。00:19:47.840 --> 00:19:54.740所以，是否通过训练单词向量而获得的结果是有点两面性的。00:19:54.740 --> 00:19:58.025所以，这是一个总结，嗯，也就是说；00:19:58.025 --> 00:20:01.715它是双面的，实际上你应该做什么。00:20:01.715 --> 00:20:09.080所以，第一个选择是g是一个好主意，使用像00:20:09.080 --> 00:20:12.575在赋值1或中使用的单词2vec向量00:20:12.575 --> 00:20:17.110使用你现在正在做的第二作业的训练方法。00:20:17.110 --> 00:20:20.830答案几乎总是肯定的。00:20:20.830 --> 00:20:24.970原因是这个词向量训练方法00:20:24.970 --> 00:20:29.500非常容易在数十亿字的文本上运行。00:20:29.500 --> 00:20:35.750所以，我们知道训练这些模型，比如[听不见]数十亿或数百亿个单词。00:20:35.750 --> 00:20:38.315很容易做到这一点有两个原因。00:20:38.315 --> 00:20:41.960首先，因为训练算法很简单，对吗？00:20:41.960 --> 00:20:46.925也就是说，word2vec训练算法跳过grams非常简单的算法。00:20:46.925 --> 00:20:50.900第二，因为我们不需要任何昂贵的资源，00:20:50.900 --> 00:20:54.470所有或者我们需要一大堆文本文档，我们可以在上面运行它。00:20:54.470 --> 00:20:56.870所以，很容易操作，00:20:56.870 --> 00:20:59.360你知道五百亿个词。00:20:59.360 --> 00:21:03.350但是，你知道，我们不能对大多数分类器这样做00:21:03.350 --> 00:21:04.760想要建立因为如果它是00:21:04.760 --> 00:21:07.670情绪分类器或命名实体识别器，00:21:07.670 --> 00:21:10.280我们需要有标签的训练数据来训练00:21:10.280 --> 00:21:15.605我们的分类器，然后我们问某人有多少单词标记了训练数据，00:21:15.605 --> 00:21:18.800你有没有命名实体识别，他们会把这个还给你？00:21:18.800 --> 00:21:22.130一个30万字或100万字的数字，对吧。00:21:22.130 --> 00:21:24.655它的数量级要小一点。00:21:24.655 --> 00:21:27.554可以。嗯。因此，00:21:27.554 --> 00:21:30.340我们可以使用预先训练过的词向量，00:21:30.340 --> 00:21:32.795因为他们知道所有不是00:21:32.795 --> 00:21:35.705现在监督，分类培训数据。00:21:35.705 --> 00:21:38.180他们也更了解00:21:38.180 --> 00:21:40.960在培训数据中，但很少。00:21:40.960 --> 00:21:42.750所以，唯一的例外是，00:21:42.750 --> 00:21:45.490如果你有数亿字的数据，00:21:45.490 --> 00:21:50.115然后你可以从随机词向量开始，从那里开始。00:21:50.115 --> 00:21:52.775所以，这是一个很常见的例子，00:21:52.775 --> 00:21:54.450用于机器翻译，00:21:54.450 --> 00:21:56.115我们稍后在课堂上做。00:21:56.115 --> 00:21:58.415相对来说00:21:58.415 --> 00:22:03.335大语种获得数亿字的翻译文本。00:22:03.335 --> 00:22:04.635如果你想做点什么，00:22:04.635 --> 00:22:09.510像德英或汉英机器翻译系统。00:22:09.510 --> 00:22:14.320不难获得1.5亿字的翻译文本。00:22:14.320 --> 00:22:16.760所以，这是足够多的数据，00:22:16.760 --> 00:22:21.015人们通常只是从单词向量开始，00:22:21.015 --> 00:22:24.300被随机初始化并开始训练，00:22:24.300 --> 00:22:27.000嗯，他们的翻译系统。00:22:27.000 --> 00:22:29.855可以。那么第二个问题是，好的。00:22:29.855 --> 00:22:32.150我用的是预先训练过的词向量。00:22:32.150 --> 00:22:35.780嗯，当我训练我的监督分级机时，00:22:35.780 --> 00:22:40.285我应该把梯度向下推到向量这个词中并向上推，然后更新它们吗？00:22:40.285 --> 00:22:44.325这通常被称为微调矢量这个词，00:22:44.325 --> 00:22:45.900或者我不应该，00:22:45.900 --> 00:22:47.670我应该扔掉吗00:22:47.670 --> 00:22:51.090这些梯度，而不是把它们向下推到向量这个词？00:22:51.090 --> 00:22:53.710你知道，答案取决于，00:22:53.710 --> 00:22:55.470这取决于大小。00:22:55.470 --> 00:23:01.345所以，如果你只有一个很小的训练数据集，嗯，通常，00:23:01.345 --> 00:23:06.065最好是把预先训练过的单词向量当作固定的，00:23:06.065 --> 00:23:08.805嗯，根本不做任何更新。00:23:08.805 --> 00:23:11.270如果你有一个大的数据集，00:23:11.270 --> 00:23:16.620然后，您通常可以通过对单词向量进行微调来获得。00:23:16.620 --> 00:23:17.910当然，答案是，00:23:17.910 --> 00:23:19.955才算大。00:23:19.955 --> 00:23:21.850嗯，你知道，如果可以的话，00:23:21.850 --> 00:23:24.370如果你在十万字的统治下，00:23:24.370 --> 00:23:27.105几十万字，你很小。00:23:27.105 --> 00:23:29.860如果你开始有超过一百万个单词，00:23:29.860 --> 00:23:31.020那么也许你是个大块头。00:23:31.020 --> 00:23:34.265但你知道，在实践中，人们都会这样做，看看哪个数字更高，00:23:34.265 --> 00:23:36.290这就是他们坚持的。00:23:36.290 --> 00:23:39.955嗯。对.嗯，那么，那种，00:23:39.955 --> 00:23:44.755这里有一点值得强调，那就是“是的”，00:23:44.755 --> 00:23:51.980所以原则上，我们可以将这个梯度反向传播到模型中的每个变量。00:23:51.980 --> 00:23:56.650嗯，这实际上是一个定理，我们可以任意地00:23:56.650 --> 00:24:02.360决定扔掉这些梯度的任何子集，00:24:02.360 --> 00:24:07.960我们还在改进我们模型的对数可能性，好吗？00:24:07.960 --> 00:24:09.815这是不可能不一致的。00:24:09.815 --> 00:24:12.240你可以选择一些子集，然后只说00:24:12.240 --> 00:24:14.980训练那37个，把剩下的都扔掉。00:24:14.980 --> 00:24:17.389算法仍将改进，00:24:17.389 --> 00:24:19.185嗯，模型的对数可能性。00:24:19.185 --> 00:24:22.355也许不像你训练其他变量那样多，00:24:22.355 --> 00:24:24.280还有，嗯，但是是的，00:24:24.280 --> 00:24:27.145不训练任何东西实际上不会造成任何伤害。00:24:27.145 --> 00:24:32.315嗯，这也是人们不经常注意到代码中的错误的原因之一。00:24:32.315 --> 00:24:34.800因为如果你的代码有点坏00:24:34.800 --> 00:24:37.550只有一半的变量被更新，00:24:37.550 --> 00:24:40.650它似乎仍在训练和改进。00:24:40.650 --> 00:24:43.415嗯。只是做得不够好，00:24:43.415 --> 00:24:45.455如果编码正确。00:24:45.455 --> 00:24:49.185可以。嗯，所以，现在，嗯，00:24:49.185 --> 00:24:51.035有点，嗯，00:24:51.035 --> 00:24:53.730几乎显示了你的反向传播，对吗？00:24:53.730 --> 00:24:59.030所以，反向传播实际上是用一个广义的链规则来求导，00:24:59.030 --> 00:25:03.420我们用三角洲代表的另一个技巧，00:25:03.420 --> 00:25:06.860你想成为什么样的人，嗯，00:25:06.860 --> 00:25:08.560很聪明，所以，00:25:08.560 --> 00:25:12.995您可以通过重用共享的东西来最小化计算。00:25:12.995 --> 00:25:16.620嗯，但现在我想继续做的是看看我们该怎么做00:25:16.620 --> 00:25:19.890更系统地说，这就是这个想法。00:25:19.890 --> 00:25:22.540我们有一个计算图，我们要运行00:25:22.540 --> 00:25:26.380通过计算图的反向传播算法。00:25:27.170 --> 00:25:33.730这有点像抽象语法树，00:25:33.730 --> 00:25:37.085您可能在编译器类中看到的表达式树，00:25:37.085 --> 00:25:38.595或者类似的，对吧？00:25:38.595 --> 00:25:44.150所以，当我们有一个我们要计算的算术表达式时，00:25:44.150 --> 00:25:48.565我们可以在它的边树表示上使它倾斜。00:25:48.565 --> 00:25:50.990我们有x和w变量，00:25:50.990 --> 00:25:52.740我们要把它们相乘。00:25:52.740 --> 00:25:53.990这是b变量，00:25:53.990 --> 00:25:56.470我们将把它添加到前面的部分结果中。00:25:56.470 --> 00:25:59.185我们要把它通过我们的非线性F00:25:59.185 --> 00:26:01.315然后我们将它乘以u。00:26:01.315 --> 00:26:03.005这就是计算结果，00:26:03.005 --> 00:26:05.455我们在神经网络中做的。00:26:05.455 --> 00:26:08.790所以，嗯，源节点或输入，00:26:08.790 --> 00:26:12.530这棵树的内部节点是操作。00:26:12.530 --> 00:26:17.255然后我们得到了这些边，它们通过我们的计算结果。00:26:17.255 --> 00:26:20.955所以，这就是这个例子的计算图00:26:20.955 --> 00:26:25.330上节课我一直在做[噪音]。00:26:25.330 --> 00:26:28.660好吧，我们有两件事要做。00:26:28.660 --> 00:26:30.090第一个是，00:26:30.090 --> 00:26:34.315我们希望能够从这些变量开始计算，00:26:34.315 --> 00:26:36.130然后计算s是什么。00:26:36.130 --> 00:26:38.485这是非常简单的部分，00:26:38.485 --> 00:26:41.815这被称为正向传播。00:26:41.815 --> 00:26:45.800所以，正向传播只是表达式评估，00:26:45.800 --> 00:26:48.870就像在任何语言解释器的编程中一样。00:26:48.870 --> 00:26:51.220嗯，这一点都不难。00:26:51.220 --> 00:26:54.390但区别在于“嘿，00:26:54.390 --> 00:26:59.705我们要做一个学习算法“，所以我们也要做与之相反的事情。00:26:59.705 --> 00:27:04.070我们要做的就是反向传播，00:27:04.070 --> 00:27:07.805或者是反向传播或者只是反向支撑，这通常被称为，00:27:07.805 --> 00:27:10.170那就是我们想去，00:27:10.170 --> 00:27:12.210嗯，从最后一部分。00:27:12.210 --> 00:27:14.190最后一部分。00:27:14.190 --> 00:27:16.395然后在每一步，00:27:16.395 --> 00:27:18.315我们要计算00:27:18.315 --> 00:27:22.680这些偏导数，然后把它们传回图中。00:27:22.680 --> 00:27:27.210所以，这就是之前我们有错误信号的概念，对吧？00:27:27.210 --> 00:27:28.860所以，我们从这里开始，00:27:28.860 --> 00:27:32.190我们用z计算了s的一部分，00:27:32.190 --> 00:27:34.920这是关于那个的。00:27:34.920 --> 00:27:38.735这就是我们计算的误差信号，到这里为止，00:27:38.735 --> 00:27:41.940然后我们要把球传回来，开始，嗯，00:27:41.940 --> 00:27:46.010计算，嗯，嗯，更后面的梯度。00:27:46.010 --> 00:27:49.570正确的？我们开始了，嗯，就在这里，00:27:49.570 --> 00:27:54.560S乘S的部分。S乘S的部分是什么？00:27:54.560 --> 00:27:57.040一个。好的，好的。00:27:57.040 --> 00:28:00.240所以，s变化的速率就是s变化的速率。00:28:00.240 --> 00:28:02.130所以，我们从一开始，00:28:02.130 --> 00:28:07.565然后我们想知道，随着我们的前进，梯度是如何变化的。00:28:07.565 --> 00:28:14.515我们在这里做的就是为一个节点计算，00:28:14.515 --> 00:28:18.815一个节点将向它的上游梯度传递，00:28:18.815 --> 00:28:20.465这是它的错误信号。00:28:20.465 --> 00:28:26.045所以，这是我们最后的部分，f-最后的结果，00:28:26.045 --> 00:28:29.320这是我们的损失，嗯，嗯，00:28:29.320 --> 00:28:32.970变量是这些计算节点的输出。00:28:32.970 --> 00:28:35.315所以，这是S i h的一部分，这里。00:28:35.315 --> 00:28:39.340然后，我们在这里做了一些手术。00:28:39.340 --> 00:28:42.800这是非线性，但它可能是其他东西。00:28:42.800 --> 00:28:47.100所以我们要计算的是下游梯度，00:28:47.100 --> 00:28:49.700这是s乘z的部分，00:28:49.700 --> 00:28:51.825这是这个函数的输入。00:28:51.825 --> 00:28:53.320那么问题是，00:28:53.320 --> 00:28:54.845我们怎么做？00:28:54.845 --> 00:28:56.895答案是，00:28:56.895 --> 00:28:59.045当然，我们使用链式法则，对吗？00:28:59.045 --> 00:29:02.760所以，at，我们有一个局部梯度的概念。00:29:02.760 --> 00:29:06.425这里是h作为输出，00:29:06.425 --> 00:29:08.505嗯，Z是输入。00:29:08.505 --> 00:29:10.175所以，这里的函数，00:29:10.175 --> 00:29:11.980这是我们的非线性，对吗？00:29:11.980 --> 00:29:14.825所以，这就是我们用来做非线性的东西，00:29:14.825 --> 00:29:19.095比如逻辑或t和h，我们用z来计算h，00:29:19.095 --> 00:29:21.690我们可以用z算出h的部分。00:29:21.690 --> 00:29:23.440这就是我们的局部梯度。00:29:23.440 --> 00:29:28.370那么，如果我们同时有上游梯度和局部梯度。00:29:28.370 --> 00:29:32.825我们可以计算出下游的梯度，因为我们知道00:29:32.825 --> 00:29:38.880S乘Z的部分是dsdh乘以，嗯，dhdz。00:29:38.880 --> 00:29:44.995这样，我们就能把下游的梯度传递到下一个节点。00:29:44.995 --> 00:29:47.275可以。所以我们的基本规则，00:29:47.275 --> 00:29:52.320这就是用不同的术语写的链式法则00:29:52.320 --> 00:29:58.010下游梯度等于上游梯度乘以局部梯度。00:29:58.010 --> 00:30:01.480嗯，就这么简单，嗯，好吧。00:30:01.480 --> 00:30:03.435所以，这是嗯，00:30:03.435 --> 00:30:09.510最简单的情况是我们有一个具有一个输入和一个输出的节点。00:30:09.510 --> 00:30:11.230这是一个函数，嗯，00:30:11.230 --> 00:30:13.040像我们的后勤部门。00:30:13.040 --> 00:30:16.780但是，我们也希望对一般的计算图进行处理。00:30:16.780 --> 00:30:18.390那么，我们该怎么做呢？00:30:18.390 --> 00:30:20.780下一个例子是，00:30:20.780 --> 00:30:24.250嗯，如果我们有多个输入呢？00:30:24.250 --> 00:30:29.760所以，如果我们计算z等于w乘以x。00:30:29.760 --> 00:30:36.965呃，实际上，z和x本身就是向量，w，um，00:30:36.965 --> 00:30:42.305是一个矩阵，但我们把x当作输入，把w当作输入，00:30:42.305 --> 00:30:44.735z作为我们的输出，对吗？00:30:44.735 --> 00:30:47.405我们把向量和矩阵组合在一起。00:30:47.405 --> 00:30:51.350如果你有多个输入，00:30:51.350 --> 00:30:54.030然后你会得到多个局部梯度。00:30:54.030 --> 00:30:55.640所以，你可以计算出，嗯，00:30:55.640 --> 00:30:57.860z对x的偏导数，00:30:57.860 --> 00:31:01.645或者z对w的偏导数。00:31:01.645 --> 00:31:05.405你基本上是沿着上游的梯度，00:31:05.405 --> 00:31:09.155乘以每个局部梯度，00:31:09.155 --> 00:31:12.340你沿着各自的路径传递，00:31:12.340 --> 00:31:17.530我们计算这些不同的下游梯度。00:31:17.530 --> 00:31:20.310这有道理吗？00:31:22.260 --> 00:31:25.930是啊。可以。多棒啊。00:31:25.930 --> 00:31:31.930可以。所以，让我们来看一个例子，然后我们将看到另一个例子。00:31:31.930 --> 00:31:34.420下面是小婴儿的例子。00:31:34.420 --> 00:31:37.150这看起来不像神经网络，00:31:37.150 --> 00:31:41.260但是我们有三个输入x，y和z。00:31:41.260 --> 00:31:45.895x和y加在一起，y和z加在一起，得到最大值。00:31:45.895 --> 00:31:50.830然后我们把这两个操作的结果相乘。00:31:50.830 --> 00:31:57.340所以总的来说，我们计算的是x加y乘以y加z的最大值。00:31:57.340 --> 00:32:05.350但是，你知道，我们这里有一个通用的技术，我们可以在任何情况下应用它。00:32:05.350 --> 00:32:09.895好吧，如果我们想要这个图表，我们想把它向前推，00:32:09.895 --> 00:32:13.270我们需要知道x，y和z的值。00:32:13.270 --> 00:32:19.180所以，在我的例子中，x等于1，y等于2，z等于0。00:32:19.180 --> 00:32:23.650那么，我们取这些变量的值，然后00:32:23.650 --> 00:32:28.600将它们推到向前箭头的计算中。00:32:28.600 --> 00:32:32.650然后我们要做的第一件事就是加，结果是三。00:32:32.650 --> 00:32:34.300所以我们可以把它放在箭头上。00:32:34.300 --> 00:32:35.575这是加法的输出。00:32:35.575 --> 00:32:39.730最大值是2，因为加法的输出值是6。00:32:39.730 --> 00:32:42.910所以我们已经对表达式进行了计算。00:32:42.910 --> 00:32:44.710它的值是6。00:32:44.710 --> 00:32:48.700这并不难。可以。那么下一步就是我们00:32:48.700 --> 00:32:54.100然后要运行反向传播来计算梯度。00:32:54.100 --> 00:33:00.205嗯，所以我们有点想知道如何，00:33:00.205 --> 00:33:03.190计算出这些局部梯度。00:33:03.190 --> 00:33:10.570所以A是我们的权利，A是和的结果。00:33:10.570 --> 00:33:12.460这是求和的结果。00:33:12.460 --> 00:33:15.040所以a等于x加y。00:33:15.040 --> 00:33:23.560所以，如果你用的是d dx，那就只有一个，d a，d，y也是有意义的。00:33:23.560 --> 00:33:28.030嗯，最大值有点难，因为在哪里00:33:28.030 --> 00:33:33.610有一些坡度和最大坡度取决于哪个更大。00:33:33.610 --> 00:33:37.930所以，如果y大于z d-δ，00:33:37.930 --> 00:33:40.299b的z部分，00:33:40.299 --> 00:33:49.030加上b除以y是1，否则它是0，反之，b除以z。00:33:49.030 --> 00:33:52.315所以有点依赖。00:33:52.315 --> 00:33:56.410然后我们做乘法，嗯，00:33:56.410 --> 00:33:58.900最后的案子，嗯，00:33:58.900 --> 00:34:04.495并计算出它与A和B的部分。00:34:04.495 --> 00:34:09.520嗯，因为A和B的值是2和3。00:34:09.520 --> 00:34:14.725如果你取f的A部分，它等于2，反之亦然。00:34:14.725 --> 00:34:19.645可以。这意味着我们可以计算出每个节点的局部梯度。00:34:19.645 --> 00:34:22.690所以我们想用这些00:34:22.690 --> 00:34:26.590计算我们向后的梯度和反向传播路径。00:34:26.590 --> 00:34:28.165所以我们从顶部开始。00:34:28.165 --> 00:34:31.030f对f的偏导数是1。00:34:31.030 --> 00:34:37.375因为如果你移动了十分之一，那么你移动了十分之一。00:34:37.375 --> 00:34:39.535所以这是一个取消。00:34:39.535 --> 00:34:42.460可以。所以我们要向后传球。00:34:42.460 --> 00:34:47.230所以，我们首先得到的是这种乘法节点。00:34:47.230 --> 00:34:53.095所以我们研究了-我们知道它的局部梯度，f除以a的部分是2，00:34:53.095 --> 00:34:57.085f的b部分是3。00:34:57.085 --> 00:34:59.350所以我们得到了这些值。00:34:59.350 --> 00:35:03.280所以正式地说，我们采用的是局部梯度00:35:03.280 --> 00:35:07.795把它们乘以上游的梯度，得到我们的3和2。00:35:07.795 --> 00:35:13.045注意到这样一个事实，实际上发生的是两个弧交换的值。00:35:13.045 --> 00:35:15.460嗯，然后我们继续往回走。00:35:15.460 --> 00:35:17.500可以。有一个max节点。00:35:17.500 --> 00:35:23.650所以我们的上游梯度现在是3，然后我们想乘以局部梯度。00:35:23.650 --> 00:35:29.920因为这两个的最大值，在这边有一个斜率。00:35:29.920 --> 00:35:31.300所以你得到三个，00:35:31.300 --> 00:35:34.855这一边没有梯度，我们得到零。00:35:34.855 --> 00:35:37.720然后我们对00:35:37.720 --> 00:35:41.295另一方面，我们有一个局部梯度。00:35:41.295 --> 00:35:47.744所以他们两个都出来了，然后我们要做的另一件事就是注意到，00:35:47.744 --> 00:35:48.975好吧，等一下。00:35:48.975 --> 00:35:52.295有两条弧从y开始00:35:52.295 --> 00:35:56.290这两个我们都支持复杂的梯度。00:35:56.290 --> 00:35:58.870我们该怎么做呢？00:35:58.870 --> 00:36:02.125嗯，我们要做的是求和。00:36:02.125 --> 00:36:07.740所以，f乘x的部分等于f乘z的部分等于00:36:07.740 --> 00:36:13.450f除以y的部分是2和5的和，对吗？00:36:13.450 --> 00:36:15.670所以这不是完全的巫术。00:36:15.670 --> 00:36:21.730从梯度的角度来看，这是有意义的，对吗？00:36:21.730 --> 00:36:23.845所以，我们要说的是，00:36:23.845 --> 00:36:25.615我们正在计算，00:36:25.615 --> 00:36:27.860如果你摆动X一点00:36:27.860 --> 00:36:32.175这对整个事件的结果有多大的影响？00:36:32.175 --> 00:36:34.650所以，你知道，我们应该能够解决这个问题。00:36:34.650 --> 00:36:40.185所以，我们的X开始提供一个，但假设我们稍微改变一下。00:36:40.185 --> 00:36:47.685并使其1.1好，按此输出应变化约0.2，00:36:47.685 --> 00:36:49.485它应该放大两倍。00:36:49.485 --> 00:36:51.330我们应该能解决这个问题，对吧？00:36:51.330 --> 00:36:55.510那么是1.1加2，00:36:55.510 --> 00:36:58.495那就是3.1。00:36:58.495 --> 00:37:03.610然后这里有两个，乘以它，它是6.2。00:37:03.610 --> 00:37:05.890看哪，上升了0.2，对吗？00:37:05.890 --> 00:37:07.510所以这似乎是正确的。00:37:07.510 --> 00:37:09.940如果我们试着做同样的事，00:37:09.940 --> 00:37:11.935好吧，我们来做Z。这很简单。00:37:11.935 --> 00:37:16.510所以如果我们把z的值为0.1的话。00:37:16.510 --> 00:37:18.400这是0.1。00:37:18.400 --> 00:37:21.370当我们最大化的时候，如果这仍然是两个00:37:21.370 --> 00:37:24.655所以计算值不变，它仍然是6。00:37:24.655 --> 00:37:26.830所以这里的梯度是零。00:37:26.830 --> 00:37:28.690晃动这个什么也没用。00:37:28.690 --> 00:37:32.230最后一个是y。00:37:32.230 --> 00:37:35.350所以，它的起始值是2。00:37:35.350 --> 00:37:38.545所以，如果我们稍微摆动一下，让它变成2.1，00:37:38.545 --> 00:37:44.350我们的说法是结果变化了大约0.5。00:37:44.350 --> 00:37:46.735它应该乘以5倍。00:37:46.735 --> 00:37:54.430所以，如果我们把它设为2.1，那么我们有2.1加1和b 3.1。00:37:54.430 --> 00:37:59.245当我们得到最大值时，这里也将是2.1。00:37:59.245 --> 00:38:03.100所以我们有2.1乘以3.1。00:38:03.100 --> 00:38:06.820这对我来说太难了。00:38:06.820 --> 00:38:16.360但如果我们取2.1乘以3.1，结果是6.51。00:38:16.360 --> 00:38:19.210所以，基本上上升了一半。00:38:19.210 --> 00:38:22.390当然，我们不希望答案准确，对吧？00:38:22.390 --> 00:38:24.730因为你知道微积分不是这样的，对吧？00:38:24.730 --> 00:38:29.545[噪音]。这表明我们得到了正确的梯度。00:38:29.545 --> 00:38:32.620可以。所以这确实有效。00:38:32.620 --> 00:38:35.800那么，我们需要知道哪些技术呢？00:38:35.800 --> 00:38:39.310嗯，所以我们已经看到他们了。00:38:39.310 --> 00:38:44.050所以，你知道，我们讨论了当有多个传入弧时，00:38:44.050 --> 00:38:47.905他如何看待锻炼不同的地方衍生品。00:38:47.905 --> 00:38:52.000我们需要知道的另一个主要情况是，00:38:52.000 --> 00:38:54.805在函数计算中有一个分支00:38:54.805 --> 00:38:59.050结果的东西在多个地方被使用。00:38:59.050 --> 00:39:01.060所以这就是这里的情况。00:39:01.060 --> 00:39:03.490这是一个初始变量，00:39:03.490 --> 00:39:06.400但你知道，它可能是由后面的东西计算出来的。00:39:06.400 --> 00:39:10.090所以，如果这个东西在多个地方被使用，00:39:10.090 --> 00:39:14.020你有不同的计算方法。00:39:14.020 --> 00:39:17.680这只是一个简单的规则，当你做反向传播时00:39:17.680 --> 00:39:23.320向后求和从不同输出分支得到的渐变。00:39:23.320 --> 00:39:28.270可以。所以，如果a等于x加y，而这就是我们给你看的那个00:39:28.270 --> 00:39:34.555在这之前，我们做了一个手术，用y算出f的总分量。00:39:34.555 --> 00:39:40.570可以。如果你再多想想，00:39:40.570 --> 00:39:43.480有一些明显的模式，00:39:43.480 --> 00:39:46.855嗯，我们在这个非常简单的例子中看到的。00:39:46.855 --> 00:39:54.310所以，如果你有一个加号，上游的梯度就会00:39:54.310 --> 00:39:56.920有点像是每一个00:39:56.920 --> 00:40:02.080当您有多个分支时，这些授予分支是要求和的。00:40:02.080 --> 00:40:03.950现在，在这种情况下，00:40:03.950 --> 00:40:10.080它的复制没有改变，但这是因为我们的计算是x加y。00:40:10.080 --> 00:40:11.700你知道，可能会更复杂，00:40:11.700 --> 00:40:14.970但我们要把它从每一根树枝上传下来。00:40:14.970 --> 00:40:19.310所以加上上游梯度分布。00:40:19.310 --> 00:40:23.950当你有一个类似于路由操作的最大值时，00:40:23.950 --> 00:40:29.419因为max会把梯度发送到，最大的方向，00:40:29.419 --> 00:40:33.140其他的东西不会有梯度传递给它们。00:40:33.140 --> 00:40:36.280嗯，然后当你有，嗯，00:40:36.280 --> 00:40:39.130这是一个乘法00:40:39.130 --> 00:40:42.610有趣的是，你要做的是改变梯度，对吗？00:40:42.610 --> 00:40:46.355所以这反映了一个事实，当你有u次00:40:46.355 --> 00:40:50.865不管u和v是向量还是只是向量，00:40:50.865 --> 00:40:55.120对结果的导数00:40:55.120 --> 00:41:00.050u是v，这些点结果对v的导数是u。00:41:00.050 --> 00:41:01.550所以，嗯，00:41:01.550 --> 00:41:03.715梯度信号是翻转，00:41:03.715 --> 00:41:07.890嗯，在这两个数字中，两边各有一个。00:41:07.890 --> 00:41:14.070可以。嗯，所以这是我们所拥有的00:41:14.070 --> 00:41:19.730这些计算图，我们可以在它们中向后求出反向传播。00:41:19.730 --> 00:41:23.765还有一部分工作要做，00:41:23.765 --> 00:41:25.780嗯，也就是说g，00:41:25.780 --> 00:41:28.070我们想有效地做到这一点。00:41:28.070 --> 00:41:31.830所以，有一个糟糕的方法可以做到这一点，那就是说，“哦，好吧，00:41:31.830 --> 00:41:37.535我们想用b来计算这个部分，所以我们可以计算这个部分。”00:41:37.535 --> 00:41:41.045这就是我在上一次幻灯片上所做的。00:41:41.045 --> 00:41:48.545我们说，“嗯，s乘以b的部分等于s乘以h的部分，00:41:48.545 --> 00:41:51.040乘以h的部分z，00:41:51.040 --> 00:41:53.590乘以z的b部分，00:41:53.590 --> 00:41:55.120我们有所有这些部分。00:41:55.120 --> 00:41:59.670我们把它们都计算出来，然后把它们相乘，然后有人说，00:41:59.670 --> 00:42:02.590嗯，S和W的部分是什么？00:42:02.590 --> 00:42:05.105我们说，嗯，这又是一条规则，我会再做一遍。00:42:05.105 --> 00:42:08.449是S的一部分00:42:08.449 --> 00:42:11.530嗯，h乘以h的z部分，00:42:11.530 --> 00:42:17.435乘以X和Z的部分，00:42:17.435 --> 00:42:19.750不，不，对，啊，丢了。00:42:19.750 --> 00:42:23.385但是你做了一个长长的清单，然后再计算一遍。00:42:23.385 --> 00:42:25.275这不是我们想做的。00:42:25.275 --> 00:42:26.885相反，我们想说，“哦，00:42:26.885 --> 00:42:29.010看，这是共享的东西。00:42:29.010 --> 00:42:31.985上面有这个错误信号。”00:42:31.985 --> 00:42:37.285我们可以计算出这个节点的上游梯度的误差信号。00:42:37.285 --> 00:42:41.000我们可以用它来计算这个节点的上游梯度。00:42:41.000 --> 00:42:45.860我们可以用这个来计算这个节点的上游梯度，然后，00:42:45.860 --> 00:42:49.360使用有两个计算结果的局部梯度00:42:49.360 --> 00:42:53.880然后我们可以计算这个节点和那个节点。00:42:53.880 --> 00:42:59.885嗯，然后，从这里知道上游梯度，00:42:59.885 --> 00:43:05.035我们可以使用这个节点上的局部梯度来计算这个和那个。00:43:05.035 --> 00:43:10.380所以，我们在做像计算这样高效的计算机科学，00:43:10.380 --> 00:43:14.300嗯，我们不做任何重复的工作。有道理吗？00:43:14.300 --> 00:43:18.880是啊。可以。嗯，如果是这样的话，00:43:18.880 --> 00:43:20.945嗯，整个后俯视图。00:43:20.945 --> 00:43:26.220所以，嗯，这是一个略显粗略的图。00:43:26.220 --> 00:43:29.350这是对这件事的重新妥协。00:43:29.350 --> 00:43:37.175所以，如果你有任何想要执行的计算，嗯，好吧，00:43:37.175 --> 00:43:42.800希望您可以将节点排序为00:43:42.800 --> 00:43:48.215所谓的拓扑分类，是指那些有争议的事物，00:43:48.215 --> 00:43:50.970作为参数的变量在前面排序00:43:50.970 --> 00:43:54.485变量是依赖于该参数的结果。00:43:54.485 --> 00:43:57.980你知道，如果你有东西，有一个循环图，00:43:57.980 --> 00:43:59.465你就能做到。00:43:59.465 --> 00:44:02.435如果你有一个循环图，你就有麻烦了。00:44:02.435 --> 00:44:05.045嗯，好吧，我真的会有技术人员00:44:05.045 --> 00:44:07.890用这些图表，但我现在不想谈。00:44:07.890 --> 00:44:12.160所以，我们已经对节点进行了排序，这些节点在这里是松散表示的。00:44:12.160 --> 00:44:16.615在拓扑排序区域从下到上排序。00:44:16.615 --> 00:44:21.660可以。所以，对于向前的道具，我们可以通过00:44:21.660 --> 00:44:25.445拓扑排序次序和我们00:44:25.445 --> 00:44:30.640如果它是一个变量，我们只需将其值设置为它最喜欢的值-变量值。00:44:30.640 --> 00:44:34.355如果它是从其他变量计算出来的，那么它们的值一定是00:44:34.355 --> 00:44:38.330已经设置了，因为在拓扑排序的早期，嗯，00:44:38.330 --> 00:44:43.815然后我们根据它们的前辈计算这些节点的值，00:44:43.815 --> 00:44:47.380我们把它传下去，算出最终的结果，00:44:47.380 --> 00:44:51.845神经网络的损耗函数，这就是我们的正向传递。00:44:51.845 --> 00:44:55.050可以。然后，在那之后，我们向后传球00:44:55.050 --> 00:45:00.305反向传递我们用一个初始化输出渐变。00:45:00.305 --> 00:45:01.930最重要的总是一个，00:45:01.930 --> 00:45:04.310z相对于z的部分。00:45:04.310 --> 00:45:09.590然后，我们现在用反向拓扑排序的方式来遍历节点。00:45:09.590 --> 00:45:14.645因此，他们每个人都会做好一切准备――一切，00:45:14.645 --> 00:45:18.025啊，任何语言都是复杂的。00:45:18.025 --> 00:45:19.235任何超过这个的。00:45:19.235 --> 00:45:22.680任何我们根据它计算的，啊，00:45:22.680 --> 00:45:28.085向前传球已经计算过了，嗯，00:45:28.085 --> 00:45:32.050它是上游梯度的乘积00:45:32.050 --> 00:45:35.855乘以局部梯度，然后我们可以用它，00:45:35.855 --> 00:45:38.575嗯，把下一件事算下来。00:45:38.575 --> 00:45:43.299嗯，所以基本上，这是一个整体角色00:45:43.299 --> 00:45:47.945对于您计算出其继承者集的任何节点，00:45:47.945 --> 00:45:49.770它上面的东西，00:45:49.770 --> 00:45:52.690这取决于它，然后你说，“好吧，00:45:52.690 --> 00:45:59.080z对x的偏导数就是00:45:59.080 --> 00:46:03.040你的地方梯度的后继者00:46:03.040 --> 00:46:08.105计算节点乘以该节点的上游坡度。”00:46:08.105 --> 00:46:12.565嗯，在我之前给出的例子中，从来没有，00:46:12.565 --> 00:46:14.950决不能有多个上游坡度。00:46:14.950 --> 00:46:18.460但是如果你想象一个，一般的大图表，实际上可能00:46:18.460 --> 00:46:23.885因此，不同的上游梯度正被用于-为不同的继承人。00:46:23.885 --> 00:46:31.480所以，我们把它向后应用，然后在后向传播中计算出来，嗯，00:46:31.480 --> 00:46:33.710每个的梯度，00:46:33.710 --> 00:46:39.110最终结果z相对于图中每个节点的梯度。00:46:39.110 --> 00:46:42.410嗯，要注意的是，00:46:42.410 --> 00:46:45.665如果你做得正确有效，00:46:45.665 --> 00:46:50.550做反向传播的复杂性的更大的O阶正是00:46:50.550 --> 00:46:55.390与正向传播相同，即表达式评估。00:46:55.390 --> 00:46:59.620所以，这不是非常昂贵的复杂手术00:46:59.620 --> 00:47:02.505你可以想象这样做和扩大规模。00:47:02.505 --> 00:47:07.440嗯，实际上你的复杂程度是一样的。00:47:07.440 --> 00:47:11.950可以。嗯，所以当[听不见]在这里输入这个过程，00:47:11.950 --> 00:47:15.635你可以想一想你正在跑步的东西00:47:15.635 --> 00:47:21.875一个任意的图形，计算这个向前传递和向后传递。00:47:21.875 --> 00:47:24.990我的意思是，几乎毫无例外00:47:24.990 --> 00:47:28.660我们实际使用的神经网络有一个规则层00:47:28.660 --> 00:47:31.715就像结构，这就是为什么它00:47:31.715 --> 00:47:35.935根据以下公式计算出这些梯度：00:47:35.935 --> 00:47:40.595向量矩阵和雅可比矩阵，就像我们以前那样。00:47:40.595 --> 00:47:47.120可以。嗯，既然我们现在有了这种非常好的算法，嗯，00:47:47.120 --> 00:47:49.160这意味着，嗯，00:47:49.160 --> 00:47:55.205我们只需要计算就可以做到这一点，所以我们不需要思考或知道如何做数学。00:47:55.205 --> 00:47:59.150嗯，我们可以让电脑用这个来做所有这些。00:47:59.150 --> 00:48:03.020嗯，所以使用这个图表结构，嗯，00:48:03.020 --> 00:48:09.470我们可以自动计算出如何应用，嗯，backprop。00:48:09.470 --> 00:48:12.485有两种情况，对吧？00:48:12.485 --> 00:48:16.455所以，如果每个节点的计算结果，00:48:16.455 --> 00:48:20.165嗯，作为一个符号表达，00:48:20.165 --> 00:48:24.190我们可以让我们的电脑为00:48:24.190 --> 00:48:28.530我们知道这个符号表达式的导数是什么。00:48:28.530 --> 00:48:30.760所以，它可以计算，嗯，00:48:30.760 --> 00:48:36.605这个节点的梯度，通常被称为自动微分。00:48:36.605 --> 00:48:39.810这有点像数学家沃尔夫拉姆阿尔法。00:48:39.810 --> 00:48:41.950你知道怎么做你的数学作业吗？00:48:41.950 --> 00:48:43.235你只要输入你的表情，00:48:43.235 --> 00:48:45.755说什么是导数，它会把它还给你，对吗？00:48:45.755 --> 00:48:51.625嗯，它在做符号计算，为你计算导数。00:48:51.625 --> 00:48:54.660嗯，所以-所以这个方法可以用来00:48:54.660 --> 00:48:57.970计算出局部梯度，然后我们可以使用00:48:57.970 --> 00:49:00.925图结构和现在规则00:49:00.925 --> 00:49:04.844上游梯度乘以局部梯度给出下游梯度，00:49:04.844 --> 00:49:06.500也就是链式法则，嗯，00:49:06.500 --> 00:49:09.070然后通过图表传播它并执行00:49:09.070 --> 00:49:13.340整个后向传球完全自动完成。00:49:13.340 --> 00:49:17.515听起来不错。00:49:17.515 --> 00:49:20.380嗯，有点失望，嗯，00:49:20.380 --> 00:49:23.530目前的深度学习框架并没有给你足够的帮助。00:49:23.530 --> 00:49:27.070嗯，实际上有一个著名的框架试图给你这个。00:49:27.070 --> 00:49:32.020所以蒙特利尔大学开发的Theano框架00:49:32.020 --> 00:49:34.825他们在现代已经抛弃的那些00:49:34.825 --> 00:49:38.065大型科技公司，深度学习框架。00:49:38.065 --> 00:49:40.060西亚诺就是这么做的。00:49:40.060 --> 00:49:43.720它做了全自动微分，嗯，00:49:43.720 --> 00:49:47.545因为我们可以考虑好的或坏的原因，00:49:47.545 --> 00:49:50.260当前的深度学习框架，如TensorFlow或00:49:50.260 --> 00:49:53.500PyTorch实际上做得比这少一点。00:49:53.500 --> 00:49:55.600所以他们所做的就是，说，00:49:55.600 --> 00:49:59.890对于一个独立节点的计算，00:49:59.890 --> 00:50:03.145你得自己做微积分。00:50:03.145 --> 00:50:05.155嗯，对于这个单独的节点，00:50:05.155 --> 00:50:08.860你必须写下正向传播，比如说，00:50:08.860 --> 00:50:13.870返回x加y，你必须写反向传播，00:50:13.870 --> 00:50:16.300说局部梯度，呃，00:50:16.300 --> 00:50:20.290一对二输入x和y，um，00:50:20.290 --> 00:50:23.380但是如果你或其他人00:50:23.380 --> 00:50:28.105写出该节点的前向和后向局部步骤，00:50:28.105 --> 00:50:31.060然后TensorFlow或PyTorch完成所有其他工作00:50:31.060 --> 00:50:34.030并运行反向传播算法。00:50:34.030 --> 00:50:37.420[噪音]嗯，然后，你知道，实际上，00:50:37.420 --> 00:50:42.444这样就省去了你必须有一个大型的符号计算引擎，00:50:42.444 --> 00:50:45.970因为在某种程度上，编码的人00:50:45.970 --> 00:50:49.030正在写入节点计算00:50:49.030 --> 00:50:52.420一点你通常想象的代码00:50:52.420 --> 00:50:54.355你知道，C或帕斯卡，00:50:54.355 --> 00:50:57.294说返回x加y，00:50:57.294 --> 00:51:00.325你知道，局部梯度返回一。00:51:00.325 --> 00:51:05.680正确的？实际上，你不需要有一个完整的符号计算引擎。00:51:05.680 --> 00:51:09.580可以。这意味着整体情况是这样的。00:51:09.580 --> 00:51:12.040正确的？所以，嗯，简而言之，00:51:12.040 --> 00:51:14.905我们有一个计算图，嗯，00:51:14.905 --> 00:51:19.480计算正向计算，嗯，00:51:19.480 --> 00:51:22.810我们，嗯，有点投入00:51:22.810 --> 00:51:26.575我们的计算图中有X和Y变量，00:51:26.575 --> 00:51:32.005然后我们按照拓扑排序的顺序遍历节点，00:51:32.005 --> 00:51:36.910对于每个节点，我们计算其正向和00:51:36.910 --> 00:51:40.000必然的是那些依赖和已经存在的东西00:51:40.000 --> 00:51:43.465通过计算，我们只是向前进行表达式计算。00:51:43.465 --> 00:51:46.345然后我们回来，嗯，00:51:46.345 --> 00:51:48.010图中的最后一个门，00:51:48.010 --> 00:51:51.100这是我们的损失函数，或客观函数。00:51:51.100 --> 00:51:54.430但是，我们还有后传球，00:51:54.430 --> 00:51:55.750对于后传球，00:51:55.750 --> 00:52:00.520我们以逆拓扑的形式进入节点，嗯，重序，00:52:00.520 --> 00:52:02.394对于每个节点，00:52:02.394 --> 00:52:04.990我们已经返回了它们的后向值，00:52:04.990 --> 00:52:06.580对于它们的顶节点，00:52:06.580 --> 00:52:08.560我们返回1的后向值，00:52:08.560 --> 00:52:11.200这就给了我们梯度。00:52:11.200 --> 00:52:14.200这意味着，嗯，00:52:14.200 --> 00:52:19.195对于任何节点，我们执行的任何计算，00:52:19.195 --> 00:52:23.170我们需要写一些代码00:52:23.170 --> 00:52:27.550说他在前传球时做了什么，在后传球时做了什么。00:52:27.550 --> 00:52:30.745所以在向前传球时，嗯，00:52:30.745 --> 00:52:32.440这是我们的乘法，00:52:32.440 --> 00:52:35.935所以我们只是说返回x乘以y。00:52:35.935 --> 00:52:37.030所以这很容易。00:52:37.030 --> 00:52:38.500这就是你习惯做的。00:52:38.500 --> 00:52:42.280但是我们也需要做后传球，00:52:42.280 --> 00:52:45.430局部回归梯度00:52:45.430 --> 00:52:50.395L关于z和x的部分。00:52:50.395 --> 00:52:51.730好吧，要做到这一点，00:52:51.730 --> 00:52:54.085我们得多做点工作。00:52:54.085 --> 00:52:56.425所以我们要做更多的工作，00:52:56.425 --> 00:52:58.555首先，向前传球。00:52:58.555 --> 00:53:00.655所以，在向前传球时，00:53:00.655 --> 00:53:04.870我们必须记住在一些变量中进行排序00:53:04.870 --> 00:53:07.090我们在中计算的值是什么-00:53:07.090 --> 00:53:10.420向前传球给我们的价值是多少，00:53:10.420 --> 00:53:13.480否则我们就无法计算向后传球了。00:53:13.480 --> 00:53:17.620所以我们把x和y的值存储起来，00:53:17.620 --> 00:53:19.030嗯，然后，00:53:19.030 --> 00:53:21.250当我们向后传球时，00:53:21.250 --> 00:53:24.550我们通过上游梯度，00:53:24.550 --> 00:53:29.845误差信号，现在我们计算，嗯，00:53:29.845 --> 00:53:35.064上游梯度乘以局部梯度-上游梯度乘以局部梯度，00:53:35.064 --> 00:53:37.510我们向后返回，00:53:37.510 --> 00:53:40.900嗯，那些下游梯度。00:53:40.900 --> 00:53:45.865如果我们对图中的所有节点都这样做，00:53:45.865 --> 00:53:48.365嗯，我们还有一些东西，嗯，00:53:48.365 --> 00:53:51.940这个系统可以作为一个深入的学习系统来为我们学习。00:53:51.940 --> 00:53:54.580这在实践中意味着什么，00:53:54.580 --> 00:53:56.545嗯，是吗，你知道，00:53:56.545 --> 00:54:01.180这些深度学习框架中的任何一个都附带了一整套工具，00:54:01.180 --> 00:54:04.090这里是一个完全连接的前向层，00:54:04.090 --> 00:54:05.770这是乙状结肠单位，00:54:05.770 --> 00:54:08.560下面是我们以后要做的其他更复杂的事情，00:54:08.560 --> 00:54:10.795像卷积和循环层。00:54:10.795 --> 00:54:13.570如果你使用其中一个，00:54:13.570 --> 00:54:15.955有人为你做了这项工作。00:54:15.955 --> 00:54:19.795正确的？他们定义了，嗯，00:54:19.795 --> 00:54:25.855已经为它们编写了向前和向后的节点或节点层。00:54:25.855 --> 00:54:28.980在某种程度上这是真的，嗯，00:54:28.980 --> 00:54:32.850这意味着制造神经网络是一堆乐趣。就像乐高一样。00:54:32.850 --> 00:54:35.340正确的？你只要把这些层粘在一起说，00:54:35.340 --> 00:54:37.005“天哪，我必须学习一些数据并训练它。”00:54:37.005 --> 00:54:40.560你知道，很容易我的高中生就开始建造这些东西。00:54:40.560 --> 00:54:43.020正确的？嗯，你不必太了解，00:54:43.020 --> 00:54:44.460但是，你知道，00:54:44.460 --> 00:54:47.715如果你真的想做一些原创的研究和思考，00:54:47.715 --> 00:54:50.635“我有一个很酷的想法，那就是如何以不同的方式做事。00:54:50.635 --> 00:54:53.920我将定义我自己的不同计算。”00:54:53.920 --> 00:54:57.580那么，你必须这样做并定义你的类，00:54:57.580 --> 00:54:59.050也可以说，00:54:59.050 --> 00:55:00.760如何计算正向值，00:55:00.760 --> 00:55:02.710你得拿出你的00:55:02.710 --> 00:55:05.665Wolfram Alpha并计算出衍生物是什么，00:55:05.665 --> 00:55:08.005嗯，把球打到后传球。00:55:08.005 --> 00:55:09.955嗯，是的。00:55:09.955 --> 00:55:13.600可以。所以这只是一个小纸条。00:55:13.600 --> 00:55:17.890嗯，你知道，在深入学习的早期，00:55:17.890 --> 00:55:20.995说在2014年之前，00:55:20.995 --> 00:55:24.610我们以前对每个人都非常严厉地说的是，00:55:24.610 --> 00:55:26.590“你应该检查你的梯度，00:55:26.590 --> 00:55:28.660通过进行数值渐变检查。00:55:28.660 --> 00:55:30.459这真的很重要。”00:55:30.459 --> 00:55:34.420嗯，这意味着，嗯，你知道，00:55:34.420 --> 00:55:38.470如果你想知道你是否已经编码了你的后传球权，00:55:38.470 --> 00:55:40.735一个简单的检查方法，嗯，00:55:40.735 --> 00:55:43.015不管你的编码是否正确，00:55:43.015 --> 00:55:46.450是做这个数值梯度00:55:46.450 --> 00:55:50.785在这里，你可以通过稍微摆动来估计坡度，00:55:50.785 --> 00:55:52.915稍微晃动输入，00:55:52.915 --> 00:55:54.640看看它有什么影响。00:55:54.640 --> 00:55:59.080所以我算出函数的值，x加h的f，00:55:59.080 --> 00:56:01.840对于h非常小，比如e到-4，00:56:01.840 --> 00:56:04.150然后x的f减去h，嗯，00:56:04.150 --> 00:56:05.590然后除以2小时，00:56:05.590 --> 00:56:07.945我是说，这里的坡度是多少，00:56:07.945 --> 00:56:11.920我得到一个关于梯度的数值估计，00:56:11.920 --> 00:56:15.265嗯，这里是我的变量x。00:56:15.265 --> 00:56:18.310嗯，这就是你将看到的00:56:18.310 --> 00:56:22.929高中的时候，你做了第一次，嗯，梯度估计，00:56:22.929 --> 00:56:26.710在这里，你计算出x加h除以h的f00:56:26.710 --> 00:56:30.970你做的是上升，然后得到梯度的一个点估计。00:56:30.970 --> 00:56:32.770嗯，完全一样，00:56:32.770 --> 00:56:34.225除此之外，00:56:34.225 --> 00:56:38.410在这种情况下，与其这样单方行动，00:56:38.410 --> 00:56:40.045我们是双面的。00:56:40.045 --> 00:56:42.385如果你真的想这么做，00:56:42.385 --> 00:56:47.035两边渐进地非常好[噪音]00:56:47.035 --> 00:56:48.610所以你总是做得更好00:56:48.610 --> 00:56:52.900双面渐变检查而不是单面渐变检查。00:56:52.900 --> 00:56:56.920嗯，既然你看到了-因为很难实施这个错误，00:56:56.920 --> 00:56:59.470这是一个很好的方法来检查你的梯度00:56:59.470 --> 00:57:02.395如果您自己定义了它们，则是正确的。00:57:02.395 --> 00:57:06.625嗯，作为一种技术，用它[噪音]做任何事情，00:57:06.625 --> 00:57:08.950完全，完全没有希望，00:57:08.950 --> 00:57:12.040因为我们想重来一遍00:57:12.040 --> 00:57:15.520我们的深层次学习模型为一个完全连接的层。00:57:15.520 --> 00:57:17.080这意味着噪音，00:57:17.080 --> 00:57:22.555如果你有这样一个w矩阵，用m表示，你想，嗯，00:57:22.555 --> 00:57:27.474计算你的偏导数，检查它们是否正确，00:57:27.474 --> 00:57:31.360这意味着你必须对矩阵的每一个元素都这样做。00:57:31.360 --> 00:57:34.090所以你必须计算最终的损失，00:57:34.090 --> 00:57:37.390先摇W11，然后摇W12，00:57:37.390 --> 00:57:40.675然后摇动一个-w13，14等等。00:57:40.675 --> 00:57:42.880所以在复杂的网络中，00:57:42.880 --> 00:57:46.030你最终会做数以百万计的功能评估00:57:46.030 --> 00:57:49.565在一个时间点检查梯度。00:57:49.565 --> 00:57:51.520所以，你知道，这是，00:57:51.520 --> 00:57:54.010这不像我登广告要的那样00:57:54.010 --> 00:57:57.220当我说它和计算一样有效时，00:57:57.220 --> 00:57:59.875嗯，正向值。00:57:59.875 --> 00:58:01.840这样做是向前的00:58:01.840 --> 00:58:06.190数值计算时间乘以模型中的参数个数，00:58:06.190 --> 00:58:08.680这对于深度学习网络来说通常是巨大的。00:58:08.680 --> 00:58:10.450所以这是你唯一想要的00:58:10.450 --> 00:58:14.170有可以关闭的内部if语句。00:58:14.170 --> 00:58:18.805所以你可以运行它来检查你的代码是否是bre-um，debuggy。00:58:18.805 --> 00:58:21.640嗯，你知道，老实说，00:58:21.640 --> 00:58:24.220你知道，现在这种需求要少得多，00:58:24.220 --> 00:58:28.120总的来说，你可以把你的组件、层和焊枪插在一起，00:58:28.120 --> 00:58:32.665嗯，其他人写的代码是正确的，它会起作用的。00:58:32.665 --> 00:58:35.515嗯，所以你可能不需要一直这样做。00:58:35.515 --> 00:58:38.050但这仍然是一件有用的事情00:58:38.050 --> 00:58:42.190如果事情出了问题。00:58:42.190 --> 00:58:46.840是啊。好吧，我们现在已经掌握了神经网络的核心技术。00:58:46.840 --> 00:58:51.070我们现在看到了，基本上我们需要知道的关于神经网络的一切，00:58:51.070 --> 00:58:54.280我只是总结了一下。00:58:54.280 --> 00:58:59.140嗯，只是再次强调一下。00:58:59.140 --> 00:59:03.760嗯，你知道，我想有些人认为，00:59:03.760 --> 00:59:07.840为什么我们还要学习，学习所有关于梯度的知识？00:59:07.840 --> 00:59:09.790在某种意义上，它是[听不见的]真的，00:59:09.790 --> 00:59:14.770因为这些现代的深度学习框架将为您计算所有的梯度。00:59:14.770 --> 00:59:17.080你知道，我们让你在家庭作业二上受苦，00:59:17.080 --> 00:59:18.640但在家庭作业三中，00:59:18.640 --> 00:59:21.625你可以计算你的梯度。00:59:21.625 --> 00:59:24.655但是，你知道，我-所以你知道这有点，就像，嗯，00:59:24.655 --> 00:59:27.940你为什么要参加编译器的C-A课程，对吧？00:59:27.940 --> 00:59:33.415在理解引擎盖下面发生的事情时有一些有用的东西，00:59:33.415 --> 00:59:35.080尽管大多数时候，00:59:35.080 --> 00:59:38.815我们非常高兴让C编译器完成它的工作，00:59:38.815 --> 00:59:44.080没有成为X86汇编专家每天的工作周。00:59:44.080 --> 00:59:46.525但是，你知道，还有更多的事情要做。00:59:46.525 --> 00:59:49.840嗯，你知道，因为即使反向传播很好，00:59:49.840 --> 00:59:51.850一旦你建立了复杂的模型，00:59:51.850 --> 00:59:56.320反向传播并不总是如你所期望的那样有效。00:59:56.320 --> 00:59:58.180完美也许是个错误的词，00:59:58.180 --> 01:00:00.565因为你在数学上知道它是完美的。01:00:00.565 --> 01:00:03.595嗯，但它可能无法实现你想要的目标。01:00:03.595 --> 01:00:06.850如果你想调试一个改进的模型，01:00:06.850 --> 01:00:09.775了解发生的事情有点关键。01:00:09.775 --> 01:00:12.880所以，安德烈・卡帕西有一个不错的中号作品，01:00:12.880 --> 01:00:17.890是的，你应该理解教学大纲页上的背面，嗯，01:00:17.890 --> 01:00:21.520说到这里，嗯，嗯，01:00:21.520 --> 01:00:26.410接下来的一周，艾比实际上要去讲讲复发性神经网络，01:00:26.410 --> 01:00:28.300你知道其中一个地方，嗯，01:00:28.300 --> 01:00:30.790你很容易失败，嗯，01:00:30.790 --> 01:00:33.580做反向传播出现在那里，01:00:33.580 --> 01:00:35.485嗯，是个很好的例子。01:00:35.485 --> 01:00:43.250可以。有人对反传播和计算图有什么疑问吗？01:00:45.660 --> 01:00:51.205可以。如果不是，剩下的时间是，嗯，01:00:51.205 --> 01:00:55.165你真正应该知道的事情，01:00:55.165 --> 01:00:57.310如果你想深入学习。01:00:57.310 --> 01:01:00.445所以，是的，这只是一个小问题，01:01:00.445 --> 01:01:01.920但让我说出来。01:01:01.920 --> 01:01:04.335嗯，所以到现在为止，01:01:04.335 --> 01:01:07.080当我们有了um损失函数时，01:01:07.080 --> 01:01:10.560我们一直在最大化数据的可能性，01:01:10.560 --> 01:01:11.760像这样的东西，01:01:11.760 --> 01:01:17.235我们刚刚得到了这部分数据的可能性，01:01:17.235 --> 01:01:19.500我们已经努力将其最大化。01:01:19.500 --> 01:01:27.445嗯，但是，嗯，在实践中通常效果很差，01:01:27.445 --> 01:01:31.510我们需要做一些其他的事情来规范我们的模型。01:01:31.510 --> 01:01:34.645如果你已经完成了机器学习课程，01:01:34.645 --> 01:01:38.065或者类似的事情，你会看到正规化。01:01:38.065 --> 01:01:42.445有很多方法可以实现正则化，但是，嗯，01:01:42.445 --> 01:01:43.930与其他东西相比，01:01:43.930 --> 01:01:46.975正规化更重要，01:01:46.975 --> 01:01:48.820嗯，对于深度学习模式，是吗？01:01:48.820 --> 01:01:54.610所以，嗯，一般的想法是如果你的模型中有很多参数，01:01:54.610 --> 01:02:00.850这些参数基本上可以记住你训练的数据中的内容。01:02:00.850 --> 01:02:04.030所以他们很擅长预测答案。01:02:04.030 --> 01:02:09.205这个模型在预测你训练过的数据的答案方面变得非常好，01:02:09.205 --> 01:02:15.040但该模型在现实世界中的工作能力可能会变差，并且会出现不同的例子。01:02:15.040 --> 01:02:18.250不知怎的，我们想阻止它。01:02:18.250 --> 01:02:22.000这个问题对深度学习模式尤其不利，01:02:22.000 --> 01:02:24.760因为典型的深度学习模式有巨大的01:02:24.760 --> 01:02:26.485大量参数。01:02:26.485 --> 01:02:29.800所以在过去统计学家统治这个节目的好日子里，01:02:29.800 --> 01:02:34.270他们告诉人们说01:02:34.270 --> 01:02:38.650有一些参数接近您的培训示例数量。01:02:38.650 --> 01:02:41.260你知道，你不应该在你的模型中有更多的参数，01:02:41.260 --> 01:02:44.710比你的培训案例数量的十分之一还要多。01:02:44.710 --> 01:02:47.545所以这就是我们告诉你的经验法则，01:02:47.545 --> 01:02:51.865所以你有很多例子来估计每一个参数。01:02:51.865 --> 01:02:54.970嗯，对于深度学习模式来说，这不是真的，01:02:54.970 --> 01:02:57.010只是我们训练的很普通01:02:57.010 --> 01:03:00.550具有10倍多参数的深度学习模型，01:03:00.550 --> 01:03:02.980因为我们有培训的例子。01:03:02.980 --> 01:03:05.485嗯，但神奇的是，它起作用了。01:03:05.485 --> 01:03:06.955事实上，它工作得很出色。01:03:06.955 --> 01:03:10.120那些高度参数化的模型，01:03:10.120 --> 01:03:14.935这就是为什么深度学习如此辉煌的一大秘密来源，01:03:14.935 --> 01:03:18.085但只有当我们规范化模型时，它才起作用。01:03:18.085 --> 01:03:22.630所以，如果你训练的模型没有足够的正规化，01:03:22.630 --> 01:03:29.050你发现你正在训练它，并在训练数据上计算出你的损失，01:03:29.050 --> 01:03:30.910而且模型还在不断改进，01:03:30.910 --> 01:03:32.515更好，更好，更好。01:03:32.515 --> 01:03:38.170嗯，ALG算法必须改善训练数据的损失。01:03:38.170 --> 01:03:39.805所以最糟糕的事情是，01:03:39.805 --> 01:03:43.375这张图会变得绝对平坦。01:03:43.375 --> 01:03:47.199你会发现我们训练的大多数型号，01:03:47.199 --> 01:03:51.565它们的参数太多了，所以会继续下降，01:03:51.565 --> 01:03:56.124直到损失接近零的数值精度，01:03:56.124 --> 01:03:57.940如果你让它训练足够长的时间。01:03:57.940 --> 01:04:01.450它只是学习每个例子的正确答案，01:04:01.450 --> 01:04:04.405因为能有效地记住例子。01:04:04.405 --> 01:04:06.085好吧，但是如果你说，01:04:06.085 --> 01:04:09.640“让我用一些不同的数据来测试这个模型。”01:04:09.640 --> 01:04:11.890你发现的是这条红色的曲线，01:04:11.890 --> 01:04:15.610直到某个时刻，嗯，01:04:15.610 --> 01:04:20.110你也在建立一个模型来更好地预测不同的数据，01:04:20.110 --> 01:04:23.845但过了一段时间，这条曲线又开始向上弯曲。01:04:23.845 --> 01:04:25.930忽略那个看起来又向下弯曲的部分，01:04:25.930 --> 01:04:27.340那是图纸上的错误。01:04:27.340 --> 01:04:31.075嗯，这就是所谓的过度拟合，01:04:31.075 --> 01:04:35.290从这里开始训练模式是01:04:35.290 --> 01:04:39.535只是学习记忆训练数据中的内容，01:04:39.535 --> 01:04:44.590但不是以后来推广到其他例子的方式。01:04:44.590 --> 01:04:46.765所以这不是我们想要的。01:04:46.765 --> 01:04:50.575我们想尽量避免过度合身，01:04:50.575 --> 01:04:54.160我们使用了各种正则化技术。01:04:54.160 --> 01:05:01.060简单的开始是，这里我们通过说，01:05:01.060 --> 01:05:07.389“如果你把参数从零移开，你将受到惩罚。”01:05:07.389 --> 01:05:11.500因此，默认的自然状态是所有参数都是零，01:05:11.500 --> 01:05:13.675所以在计算时它们被忽略了。01:05:13.675 --> 01:05:16.345你可以有大值的参数，01:05:16.345 --> 01:05:18.370但你会尿尿罚四分，01:05:18.370 --> 01:05:21.490这就是所谓的L-2正则化。01:05:21.490 --> 01:05:23.980你知道，这是01:05:23.980 --> 01:05:26.530你可以用正规化做些明智的事，01:05:26.530 --> 01:05:28.600但以后还有很多话要说。01:05:28.600 --> 01:05:32.320在讨论之前，我们将在这类讲座中讨论01:05:32.320 --> 01:05:37.480神经网络中其他智能正则化技术的最终项目。01:05:37.480 --> 01:05:40.840可以。嗯，抓起第二个袋子，01:05:40.840 --> 01:05:44.290矢量化是你这里的术语，01:05:44.290 --> 01:05:46.330但这不仅仅是向量。01:05:46.330 --> 01:05:48.820这也是母系化，01:05:48.820 --> 01:05:52.870更高维的矩阵称为张量，01:05:52.870 --> 01:05:55.135在这个领域，张量化。01:05:55.135 --> 01:05:58.690嗯，让深度学习系统快速运行，01:05:58.690 --> 01:06:05.215只有我们对事物进行矢量化，才有效率。01:06:05.215 --> 01:06:07.210嗯，那是什么意思？01:06:07.210 --> 01:06:09.685这意味着，你知道，01:06:09.685 --> 01:06:13.300写很多代码的简单方法嗯，01:06:13.300 --> 01:06:15.730你在第一节课上看到的，01:06:15.730 --> 01:06:22.120你是说我在范围内，计算随机的randi-1。01:06:22.120 --> 01:06:25.990但是当我们想变得聪明的时候，01:06:25.990 --> 01:06:32.305嗯，人们，嗯，做事情很快，01:06:32.305 --> 01:06:38.620嗯，我们说不是一次算出这个w点一个词的矢量，01:06:38.620 --> 01:06:40.285在四个循环中完成，01:06:40.285 --> 01:06:44.950我们可以把所有的词向量放到一个矩阵中，01:06:44.950 --> 01:06:52.675然后简单地做一个矩阵，用我们的词向量矩阵乘以w。01:06:52.675 --> 01:06:58.735即使你用CPU在笔记本电脑上运行你的代码，01:06:58.735 --> 01:07:02.560你会发现如果你用矢量化的方式来做，01:07:02.560 --> 01:07:04.900事情会变得更快。01:07:04.900 --> 01:07:05.950在这个例子中，01:07:05.950 --> 01:07:08.484它的速度超过了一个数量级，01:07:08.484 --> 01:07:11.785当用矢量化而不是，01:07:11.785 --> 01:07:13.735嗯，一个完整的循环。01:07:13.735 --> 01:07:19.000嗯，只有当我们在GPU上运行代码时，这些收益才会增加，01:07:19.000 --> 01:07:22.600你不会在GPU上获得高的收益和速度，01:07:22.600 --> 01:07:24.190除非你的代码是矢量化的。01:07:24.190 --> 01:07:25.705但是如果它是矢量化的，01:07:25.705 --> 01:07:27.580那么你可以希望有结果，哦，01:07:27.580 --> 01:07:29.650是的，这跑得快40倍，01:07:29.650 --> 01:07:31.735比它在CPU上做的还要多。01:07:31.735 --> 01:07:39.415好的，嗯，是的，所以总是尝试使用向量和矩阵，而不是循环。01:07:39.415 --> 01:07:42.550嗯，当然，它在开发东西给代码计时时很有用，01:07:42.550 --> 01:07:44.065找出什么是慢。01:07:44.065 --> 01:07:45.955嗯，好吧。01:07:45.955 --> 01:07:47.515第三点。01:07:47.515 --> 01:07:53.845嗯，好吧，所以我们讨论了这个想法，嗯，上次，01:07:53.845 --> 01:07:59.575之前的时间，在有了仿射层之后，01:07:59.575 --> 01:08:01.270我们去了哪里，你知道，01:08:01.270 --> 01:08:03.880从X到WX，再加上B。01:08:03.880 --> 01:08:05.500这就是所谓的仿射层，01:08:05.500 --> 01:08:06.880所以我们要这么做，嗯，01:08:06.880 --> 01:08:09.925用一个向量乘以一个矩阵矩阵，01:08:09.925 --> 01:08:12.505加上嗯偏倚。01:08:12.505 --> 01:08:15.925我们必须有力量和深度的网络，嗯，01:08:15.925 --> 01:08:19.915必须有某种形式的非线性。01:08:19.915 --> 01:08:22.495所以，我只想了解一下背景01:08:22.495 --> 01:08:25.644关于非线性是人们使用的，01:08:25.644 --> 01:08:27.085以及使用什么。01:08:27.085 --> 01:08:33.340所以，如果你从我们所知道的逻辑回归开始，嗯，01:08:33.340 --> 01:08:36.549通常被称为乙状曲线，01:08:36.549 --> 01:08:39.670或者更准确地说是物流，01:08:39.670 --> 01:08:42.955嗯，功能就是这张照片。01:08:42.955 --> 01:08:46.060所以有些东西会压扁任何真实的01:08:46.060 --> 01:08:49.660在0到1的范围内为正数或负数。01:08:49.660 --> 01:08:51.730它给你一个概率输出。01:08:51.730 --> 01:08:55.600嗯，这些-这种用法，嗯，01:08:55.600 --> 01:09:00.400逻辑功能在早期神经网络中确实很常见。01:09:00.400 --> 01:09:02.785如果你回到80年代，90年代的神经网络，01:09:02.785 --> 01:09:07.135乙状结肠功能无处不在。01:09:07.135 --> 01:09:10.150嗯，最近几次，01:09:10.150 --> 01:09:13.15090%的时间无人使用01:09:13.150 --> 01:09:16.435事实上，他们的工作相当糟糕。01:09:16.435 --> 01:09:19.870只有当你01:09:19.870 --> 01:09:24.730实际上，需要一个介于0和1之间的值是您的输出。01:09:24.730 --> 01:09:28.270所以我们稍后将讨论您在网络中的门控方式，01:09:28.270 --> 01:09:32.800所以门控是一个你想要在两件事情之间有概率的地方。01:09:32.800 --> 01:09:34.795然后你将使用其中一个，01:09:34.795 --> 01:09:37.240但你在别的地方绝对不用。01:09:37.240 --> 01:09:40.240嗯，这是Tanh曲线。01:09:40.240 --> 01:09:42.880嗯，所以tanh的公式，嗯，01:09:42.880 --> 01:09:46.300看起来像一个可怕的东西，里面有指数的想法，01:09:46.300 --> 01:09:51.145它看起来不像是一个逻辑曲线。01:09:51.145 --> 01:09:56.740但是如果你把你的数学课本挖出来，你就可以说服自己01:09:56.740 --> 01:09:59.920tanh曲线实际上与01:09:59.920 --> 01:10:04.090除了你的逻辑曲线乘以2，01:10:04.090 --> 01:10:06.505所以它的范围是2而不是1，01:10:06.505 --> 01:10:08.095然后把它移到线下。01:10:08.095 --> 01:10:10.735所以，这只是一种重新扩展的物流。01:10:10.735 --> 01:10:13.690现在在一和负一之间是对称的，01:10:13.690 --> 01:10:16.900事实上，输出中的某些度量标准实际上有助于01:10:16.900 --> 01:10:20.545很多都是为了加入神经网络。嗯。01:10:20.545 --> 01:10:25.070因此，Tanh's仍然被合理地广泛使用。01:10:25.070 --> 01:10:29.270在很多地方，你的网络。01:10:29.270 --> 01:10:32.755所以，坦恩应该是你的朋友，你应该知道。01:10:32.755 --> 01:10:36.545但是你知道，使用01:10:36.545 --> 01:10:41.320嗯，像乙状结肠或Tanh这样的超验功能是，01:10:41.320 --> 01:10:48.300你知道，它们涉及到昂贵的数学运算，嗯，这会减慢你的速度。01:10:48.300 --> 01:10:51.050比如说，和蔼可亲有点讨厌01:10:51.050 --> 01:10:53.830在你的电脑里计算指数和tanh's，01:10:53.830 --> 01:10:55.070事情有点慢。01:10:55.070 --> 01:10:58.870所以人们开始玩弄01:10:58.870 --> 01:11:02.940为了加快速度，有人想出了这样的主意，01:11:02.940 --> 01:11:05.360也许我们可以想出一个强硬的坦恩，01:11:05.360 --> 01:11:08.560嗯，这里有点平坦01:11:08.560 --> 01:11:12.000然后它有一个线性的斜坡，然后它在顶部是平的。01:11:12.000 --> 01:11:16.035你知道，看起来有点像晒黑，但我们只是把它平方了。01:11:16.035 --> 01:11:19.580嗯，虽然计算起来很便宜，你说，01:11:19.580 --> 01:11:21.965x小于-1，01:11:21.965 --> 01:11:26.630返回减去一，返回加一或只返回数字。01:11:26.630 --> 01:11:29.135没有复杂的超越。01:11:29.135 --> 01:11:30.700有趣的是，01:11:30.700 --> 01:11:33.475事实证明，这确实很有效。01:11:33.475 --> 01:11:36.580你可能害怕，你也有理由害怕01:11:36.580 --> 01:11:40.340害怕，因为如果你开始考虑梯度，01:11:40.340 --> 01:11:41.645一旦你到了这里，01:11:41.645 --> 01:11:43.190没有梯度，对吗？01:11:43.190 --> 01:11:46.495它在零点完全是平的。01:11:46.495 --> 01:11:51.035所以，事情一旦到了尽头就一去不返了。01:11:51.035 --> 01:11:54.350所以，至少在这个中间区呆一段时间是很重要的01:11:54.350 --> 01:11:58.120一段时间后，它就有了一个斜率，对吗？01:11:58.120 --> 01:11:59.800它是一个不变的斜率。01:11:59.800 --> 01:12:03.920但这足够线性化了01:12:03.920 --> 01:12:08.770在神经网络中工作得很好，你可以训练神经网络。01:12:08.770 --> 01:12:13.995所以，这就把整个领域推向了相反的方向，人们认为，01:12:13.995 --> 01:12:15.880哦，如果能成功的话，01:12:15.880 --> 01:12:18.855也许我们可以让事情变得更简单。01:12:18.855 --> 01:12:24.100这导致了现在著名的被称为[听不见]的雷鲁。01:12:24.100 --> 01:12:27.090所以我在编辑的时候出错了，01:12:27.090 --> 01:12:28.500删除硬褐色。01:12:28.500 --> 01:12:30.830那是错误的幻灯片。01:12:30.830 --> 01:12:32.370[笑声]雷鲁单位，01:12:32.370 --> 01:12:36.795每个人都叫它relu，它代表校正的线性单位。01:12:36.795 --> 01:12:42.250所以，Re-，relu本质上是最简单的非线性。01:12:42.250 --> 01:12:45.950所以relu是零，01:12:45.950 --> 01:12:52.015一旦你处于负的状态，坡度为零，这只是一条直线，坡度为1，01:12:52.015 --> 01:12:53.800当你处于积极的状态时。01:12:53.800 --> 01:12:56.600我是说，当我第一次看到这个的时候，01:12:56.600 --> 01:12:59.835我的意思是，我有点心烦意乱，它可能会起作用。01:12:59.835 --> 01:13:01.770我想是因为01:13:01.770 --> 01:13:07.220我是在这种坦恩和乙状结肠以及这些论点的基础上长大的。01:13:07.220 --> 01:13:13.250关于斜坡，你得到这些梯度，你可以用梯度移动。01:13:13.250 --> 01:13:17.240如果这个函数的一半只表示01:13:17.240 --> 01:13:21.720输出零，没有梯度，另一半就是这条直线。01:13:21.720 --> 01:13:25.365尤其是，当你处于积极的状态时，01:13:25.365 --> 01:13:27.905这只是一个身份函数。01:13:27.905 --> 01:13:35.140你知道，我之前有点争论过，如果你只是组成线性变换，01:13:35.140 --> 01:13:40.560你得不到任何权力，但当这是政权的右手部分时，你就有了权力。01:13:40.560 --> 01:13:42.190因为这是一个身份函数，01:13:42.190 --> 01:13:43.400这正是我们要做的。01:13:43.400 --> 01:13:45.770我们只是组成线性变换。01:13:45.770 --> 01:13:48.280所以你-你有点相信这是不可能的01:13:48.280 --> 01:13:51.755工作，但事实证明，这项工作非常出色。01:13:51.755 --> 01:13:54.860到目前为止01:13:54.860 --> 01:13:59.640当人们正在为深层网络构建提要时的默认选择。01:13:59.640 --> 01:14:05.190人们使用的是非直线性，而且速度很快，01:14:05.190 --> 01:14:09.260他们训练得很快，表现也很好。01:14:09.260 --> 01:14:10.845所以，实际上，你知道，01:14:10.845 --> 01:14:13.630是的，它只是每个u-，01:14:13.630 --> 01:14:15.350取决于输入，01:14:15.350 --> 01:14:20.495每个单元要么是死的，要么是作为一个身份函数传递东西。01:14:20.495 --> 01:14:22.590但这已经够了，丽妮-，01:14:22.590 --> 01:14:24.460你能做到的非线性01:14:24.460 --> 01:14:28.400任意函数近似仍然有一个深度学习网络。01:14:28.400 --> 01:14:32.255现在人们的观点正好相反，01:14:32.255 --> 01:14:41.775因为这个单位在它的非零范围上有一个斜率，这意味着，01:14:41.775 --> 01:14:45.280梯度通过规范非常有效地01:14:45.280 --> 01:14:50.860输入，因此模型训练非常有效，01:14:50.860 --> 01:14:53.655当你有这种曲线的时候，01:14:53.655 --> 01:14:58.850当你在这里的时候，这里的坡度很小，所以你的模特可能训练得很慢。01:14:58.850 --> 01:15:01.760可以。所以，你知道，01:15:01.760 --> 01:15:05.325对于feed-forward网络，请在尝试任何其他操作之前尝试此操作。01:15:05.325 --> 01:15:08.855但当时有一种亚文学说，01:15:08.855 --> 01:15:12.620嗯，也许这太简单了，我们可以做得更好。01:15:12.620 --> 01:15:15.610所以导致泄漏的雷鲁说，01:15:15.610 --> 01:15:19.775“也许我们应该在这里放一点斜坡，这样它就不会完全死了。”01:15:19.775 --> 01:15:22.085所以你可以把它做成一个，01:15:22.085 --> 01:15:25.405这部分的坡度是百分之一。01:15:25.405 --> 01:15:26.690然后人们有，嗯，01:15:26.690 --> 01:15:27.880让我们在这基础上，01:15:27.880 --> 01:15:31.360也许我们可以把另一个参数01:15:31.360 --> 01:15:34.960我们的神经网络和我们可以有一个参数relu。01:15:34.960 --> 01:15:38.980所以，这里有个斜坡，但我们也要01:15:38.980 --> 01:15:45.280反向传播到我们的非线性，有这个额外的α参数，01:15:45.280 --> 01:15:47.645它有多大的坡度。01:15:47.645 --> 01:15:50.835所以，很多人都用过这些，01:15:50.835 --> 01:15:55.150你可以在档案馆找到10份文件，人们说，01:15:55.150 --> 01:15:57.950使用其中一个或另一个可以获得更好的结果。01:15:57.950 --> 01:16:00.770你也可以在报纸上找到人们所说的01:16:00.770 --> 01:16:03.955对他们来说和仅仅使用RELU没有区别。01:16:03.955 --> 01:16:05.334所以，我认为基本上，01:16:05.334 --> 01:16:08.925你可以从那里开始工作。01:16:08.925 --> 01:16:13.495对.所以，参数初始化，01:16:13.495 --> 01:16:18.225当我们在模型中有这些矩阵和参数时，01:16:18.225 --> 01:16:20.910它是至关重要的，至关重要的，至关重要的，01:16:20.910 --> 01:16:27.900必须用小的随机值初始化这些参数权重。01:16:27.900 --> 01:16:30.140这正是我们的教训01:16:30.140 --> 01:16:33.520到了最后的项目时间，有些人还没有发现。01:16:33.520 --> 01:16:36.255所以我要强调它是至关重要的。01:16:36.255 --> 01:16:40.025所以，如果你从重量为零开始，01:16:40.025 --> 01:16:42.699你有点完全对称，01:16:42.699 --> 01:16:45.320对，所有的计算都是一样的，01:16:45.320 --> 01:16:49.350一切都会一成不变，而你实际上并不是在训练01:16:49.350 --> 01:16:54.600这个复杂的网络有很多专门学习不同事物的单元。01:16:54.600 --> 01:16:57.550所以，不知何故，你必须打破对称，我们01:16:57.550 --> 01:17:00.510通过给小的随机权重来做到这一点。01:17:00.510 --> 01:17:02.980所以，你知道，这里有一些要点。01:17:02.980 --> 01:17:04.660当你有偏见的时候，01:17:04.660 --> 01:17:06.400你也可以从零开始，01:17:06.400 --> 01:17:11.640作为中立者，看看系统如何学习你想要的偏见等等。01:17:11.640 --> 01:17:18.965但一般来说，要初始化为小随机值的权重。01:17:18.965 --> 01:17:24.905你可以在Pythorn或其他深入学习的实践包中找到，01:17:24.905 --> 01:17:30.540一个常用的并且经常推荐使用的初始化是Xavier初始化。01:17:30.540 --> 01:17:34.110所以，关键是，01:17:34.110 --> 01:17:37.350对于很多模特和很多地方，01:17:37.350 --> 01:17:40.720想想这些东西，比如这些和这些，01:17:40.720 --> 01:17:46.205你希望网络中的价值观保持微小，01:17:46.205 --> 01:17:48.880在这个中等范围内。01:17:48.880 --> 01:17:53.145好吧，如果你有一个大值的矩阵01:17:53.145 --> 01:17:57.470把向量乘以这个矩阵，01:17:57.470 --> 01:17:58.910你知道，事情可能会变大。01:17:58.910 --> 01:18:00.550然后如果你穿上另一层，01:18:00.550 --> 01:18:03.180它会再变大，然后什么都会变大。01:18:03.180 --> 01:18:05.980会太大，你会有问题的。01:18:05.980 --> 01:18:10.285所以，事实上，Xavier初始化正试图通过说01:18:10.285 --> 01:18:14.590这个节点有多少输入？01:18:14.590 --> 01:18:16.405有多少输出？01:18:16.405 --> 01:18:20.920我们要根据输入对初始化进行临时排序01:18:20.920 --> 01:18:26.395输出，因为有效地，我们会多次使用这个数字。01:18:26.395 --> 01:18:30.185这是一件好事，你可以用它。01:18:30.185 --> 01:18:34.955优化器。到现在为止，01:18:34.955 --> 01:18:37.770我们看到了，刚刚谈到了普通的军士。01:18:37.770 --> 01:18:43.785你知道，普通的SGD实际上工作得很好。01:18:43.785 --> 01:18:46.500但如果你只想用普通的SGD，01:18:46.500 --> 01:18:49.860你必须花时间调整学习速度，01:18:49.860 --> 01:18:54.440乘以梯度的α。01:18:54.440 --> 01:18:58.535对于复杂的网络和情况，或者为了避免担心，01:18:58.535 --> 01:19:03.710现在有了这个大家族和更复杂的自适应优化器。01:19:03.710 --> 01:19:10.055所以，实际上，他们是通过累积梯度来调整参数的，01:19:10.055 --> 01:19:14.370这对他们学习每个参数的学习率有影响。01:19:14.370 --> 01:19:18.120以便他们可以看到哪些参数对移动有用01:19:18.120 --> 01:19:22.580取决于这些参数的敏感度，哪一个更重要。01:19:22.580 --> 01:19:24.040所以，如果东西是平的，01:19:24.040 --> 01:19:25.960你可以试着快速移动。01:19:25.960 --> 01:19:27.450在那里很多东西都在跳动，01:19:27.450 --> 01:19:30.550你将试图移动一点，以免超调。01:19:30.550 --> 01:19:32.850所以，这其中有一个大家庭：Adagrad，01:19:32.850 --> 01:19:35.200亚当，还有其他的。01:19:35.200 --> 01:19:37.675有亚当・麦克斯和他们很多人。01:19:37.675 --> 01:19:43.395我的意思是，亚当是一个相当可靠的人，许多人使用，这并不坏。01:19:43.395 --> 01:19:45.685再滑一张，我就完了。01:19:45.685 --> 01:19:47.555是的，所以学习率。01:19:47.555 --> 01:19:51.420所以，通常你必须选择一个学习率。01:19:51.420 --> 01:19:54.515所以，一个选择就是保持恒定的学习速度。01:19:54.515 --> 01:19:59.500你选一个数字，可能是10减去3，然后说这是我的学习率。01:19:59.500 --> 01:20:04.165你希望你的学习速度是数量级的，对吧。01:20:04.165 --> 01:20:07.670如果你的学习率太高，01:20:07.670 --> 01:20:12.990你的模型可能会发散或不收敛，因为它只是让你跳跃01:20:12.990 --> 01:20:19.915巨大的垃圾运动，你完全错过了你的功能空间的好部分。01:20:19.915 --> 01:20:22.925如果你的模型，如果你的学习率太低，01:20:22.925 --> 01:20:28.600你的模特可能不会在任务截止日期前训练，然后你会不高兴的。01:20:28.600 --> 01:20:30.675所以，你看到了，你知道，01:20:30.675 --> 01:20:35.730一般来说，人们尝试10的力量，并看到它的样子，对吧。01:20:35.730 --> 01:20:39.445他们可能会尝试，你知道，0.01，0.001，01:20:39.445 --> 01:20:45.6800.0001看看，看看损失是如何下降的，看看有什么效果。01:20:45.680 --> 01:20:46.690一般来说，您要使用01:20:46.690 --> 01:20:50.960最快的学习速度不会使事情变得不稳定。01:20:50.960 --> 01:20:58.000通常，你可以通过在训练时降低学习率来获得更好的效果。01:20:58.000 --> 01:21:00.660所以，有时候人们只是用手来做。01:21:00.660 --> 01:21:03.190所以，我们用epoch这个词来表示一个完整的过程01:21:03.190 --> 01:21:05.895通过你的培训数据，人们可能会说，01:21:05.895 --> 01:21:08.520每三个时期学习率的一半01:21:08.520 --> 01:21:11.215当你训练的时候，这可以很好地工作。01:21:11.215 --> 01:21:16.385你可以用公式来计算每个时代的学习率。01:21:16.385 --> 01:21:18.550甚至还有更高级的方法。01:21:18.550 --> 01:21:22.075如果你愿意，你可以在网上查询循环学习率。01:21:22.075 --> 01:21:24.350哪种方式能真正提高学习率？01:21:24.350 --> 01:21:26.770有时大，有时小，01:21:26.770 --> 01:21:29.460人们发现这对你的出狱很有用01:21:29.460 --> 01:21:32.440以有趣的方式描述坏地区。01:21:32.440 --> 01:21:35.820另一件事是，01:21:35.820 --> 01:21:38.775如果你使用的是一个更高级的优化器，01:21:38.775 --> 01:21:43.160他们仍然要求你的学习率，但学习率是01:21:43.160 --> 01:21:49.790初始学习率，通常优化器在训练时会收缩。01:21:49.790 --> 01:21:53.690所以，通常如果你使用像亚当这样的东西，01:21:53.690 --> 01:21:58.740你可能从说学习率是0.1开始，01:21:58.740 --> 01:22:03.945因此，随着训练的进行，这个数字会越来越大。01:22:03.945 --> 01:22:08.480
Okay, all done. See you next week.

