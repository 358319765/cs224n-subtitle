WEBVTT
Kind: captions
Language: en

00:00:04.850 --> 00:00:07.410
好。大家好！00:00:07.410 --> 00:00:11.430
让我们开始吧[噪音]好。00:00:11.430 --> 00:00:14.280
在今天的课上，00:00:14.280 --> 00:00:20.025
我们要做的是研究树递归神经网络。00:00:20.025 --> 00:00:22.110
我的意思是，这实际上是，00:00:22.110 --> 00:00:26.660
这是一个我特别喜欢和依恋的话题，00:00:26.660 --> 00:00:32.975
因为实际上当我们2010年开始在斯坦福为NLP做深度学习时，00:00:32.975 --> 00:00:37.940
从2010年到2015年，00:00:37.940 --> 00:00:42.320
我们研究的主要观点是你如何00:00:42.320 --> 00:00:46.945
将递归树结构构造成神经网络。00:00:46.945 --> 00:00:50.645
所以在某种程度上，有趣的是我现在才开始。00:00:50.645 --> 00:00:53.420
我的意思是，这是有原因的，00:00:53.420 --> 00:00:56.720
但我认为有很多有趣的想法00:00:56.720 --> 00:01:00.365
这与语言结构密切相关，00:01:00.365 --> 00:01:02.720
所以这是很好的东西。00:01:02.720 --> 00:01:04.625
但实际上，00:01:04.625 --> 00:01:08.105
事实证明，这些想法很难推广00:01:08.105 --> 00:01:11.870
不一定要在实践中做得更好00:01:11.870 --> 00:01:15.140
我们花了更多时间在这些事情上00:01:15.140 --> 00:01:19.100
意思是像看LSTMs和变形金刚，00:01:19.100 --> 00:01:20.570
诸如此类。00:01:20.570 --> 00:01:25.860
这就是为什么我们在课程快结束的时候把他们分流了。00:01:25.860 --> 00:01:29.090
但我想谈谈动机，00:01:29.090 --> 00:01:31.190
构建树状结构的方法，00:01:31.190 --> 00:01:34.220
以及神经网络，看看其中的一些可能性，00:01:34.220 --> 00:01:36.065
我们探索了00:01:36.065 --> 00:01:38.170
在这堂课上。00:01:38.170 --> 00:01:44.690
关于这门课的另一个事实是这是我要讲的最后一节课。00:01:44.690 --> 00:01:47.060
下周还有两节课。00:01:47.060 --> 00:01:49.320
别忘了下周的事00:01:49.320 --> 00:01:52.275
CS224N类,嗯,00:01:52.275 --> 00:01:54.220
但是周二，00:01:54.220 --> 00:01:57.280
我们邀请到了最后一位演讲者00:01:57.280 --> 00:02:00.410
谁是一个伟大的演讲者，有很多有趣的东西00:02:00.410 --> 00:02:04.865
说到NLP和AI中的公平和伦理。00:02:04.865 --> 00:02:06.425
最后一节课，00:02:06.425 --> 00:02:09.650
我的另一个博士生00:02:09.650 --> 00:02:13.270
给大家讲讲最近发生的一些事情，00:02:13.270 --> 00:02:16.965
2018年19年深度学习领域发生了什么，00:02:16.965 --> 00:02:20.975
一些最近在NLP和深度学习方面的发展。嗯,00:02:20.975 --> 00:02:24.875
那么，嗯，让我们-我将在这篇文章的最后说再见。00:02:24.875 --> 00:02:28.730
希望每个人都提交了00:02:28.730 --> 00:02:33.380
他们期末项目的里程碑。00:02:33.380 --> 00:02:37.310
如果你还没有，你应该开始你的里程碑00:02:37.310 --> 00:02:40.640
你知道，在这附近的某个地方，00:02:40.640 --> 00:02:46.290
开始出现问题人们的情况是什么都不管用，00:02:46.290 --> 00:02:48.855
一切都太慢了，你会感到恐慌。00:02:48.855 --> 00:02:52.095
嗯，嗯，这发生了。00:02:52.095 --> 00:02:54.030
我当然祝你好运。00:02:54.030 --> 00:02:55.900
我的意思是，你能做什么?00:02:55.900 --> 00:03:00.785
我的意思是，当你遇到一些不顺利的事情，00:03:00.785 --> 00:03:02.255
为什么它们不工作，00:03:02.255 --> 00:03:03.800
以及如何解决这些问题。00:03:03.800 --> 00:03:08.660
我的意思是，我经常认为最好的办法就是回去做一些事情00:03:08.660 --> 00:03:14.405
很简单，你可以开始工作，然后继续向前。00:03:14.405 --> 00:03:18.980
拥有非常小的数据集也非常有帮助。00:03:18.980 --> 00:03:23.540
我真的建议你有一个10个项目的策略，00:03:23.540 --> 00:03:28.510
或20项数据集，并检查你的模型是否完美，00:03:28.510 --> 00:03:31.320
对这类数据的训练达到100%的准确性00:03:31.320 --> 00:03:34.610
set为你节省了大量的时间，00:03:34.610 --> 00:03:39.170
在你对少量数据做了一些简单的处理之后，00:03:39.170 --> 00:03:41.780
这是正确的时间，00:03:41.780 --> 00:03:44.600
再向前扩展。00:03:44.600 --> 00:03:47.330
你一定要确保00:03:47.330 --> 00:03:49.970
您可以完全过度适应您的培训数据集。00:03:49.970 --> 00:03:51.180
这有点，嗯，00:03:51.180 --> 00:03:52.370
不是很好证明，00:03:52.370 --> 00:03:56.780
但这至少是正确实现模型的第一个良好需求。00:03:56.780 --> 00:04:01.130
你，你知道存在的部分诀窍00:04:01.130 --> 00:04:03.920
一个成功的深度学习研究者实际上是00:04:03.920 --> 00:04:07.565
设法把事情做完，而不是浪费大量的时间。00:04:07.565 --> 00:04:10.300
所以，你知道，00:04:10.300 --> 00:04:12.975
在你训练的过程中策划00:04:12.975 --> 00:04:16.160
开发错误，这样你就能知道东西是否在工作，00:04:16.160 --> 00:04:17.900
或者如果事情不顺利，00:04:17.900 --> 00:04:20.930
你应该放弃，重新开始一个新的实验，00:04:20.930 --> 00:04:25.505
这样可以节省你的时间，让你做得更多。00:04:25.505 --> 00:04:27.350
一旦一切正常，00:04:27.350 --> 00:04:30.080
有很多方法可以让它更好地工作。00:04:30.080 --> 00:04:33.410
有L2和Dropout的正则化，00:04:33.410 --> 00:04:36.969
还有时间做超参数搜索，00:04:36.969 --> 00:04:38.530
你知道，00:04:38.530 --> 00:04:42.620
经常做这些事情，对什么有很大的影响00:04:42.620 --> 00:04:46.820
你的最终结果是，所以有时间做这些事情是很好的。00:04:46.820 --> 00:04:48.680
但很明显，你想要得到一些东西，00:04:48.680 --> 00:04:51.590
在你开始之前先工作00:04:51.590 --> 00:04:54.380
鼓励人们坚持下去00:04:54.380 --> 00:04:57.470
如果你有什么问题，可以在办公时间来找我，00:04:57.470 --> 00:05:00.530
我们会尽我们最大的努力来帮助你们00:05:00.530 --> 00:05:05.205
我们所能做的有限之处仅仅是被问题击倒。00:05:05.205 --> 00:05:08.240
好的,嗯,是的。00:05:08.240 --> 00:05:13.595
所以，我想说一些关于，00:05:13.595 --> 00:05:16.680
语言与语言理论，00:05:16.680 --> 00:05:22.790
在激发这些树递归网络的上下文中。00:05:22.790 --> 00:05:27.485
这是卡内基梅隆大学的一个艺术装置。00:05:27.485 --> 00:05:29.150
作为一个NLP人，00:05:29.150 --> 00:05:31.700
我真的很喜欢这个艺术装置。00:05:31.700 --> 00:05:36.805
所以我们需要斯坦福工程学院周围更好的艺术装置。00:05:36.805 --> 00:05:40.510
这是一个文字包艺术装置。00:05:40.510 --> 00:05:42.710
有一个袋子，里面有很多字。00:05:42.710 --> 00:05:44.120
你看下面这里，00:05:44.120 --> 00:05:47.015
有“停”字，“the”和“the us”，00:05:47.015 --> 00:05:49.135
从袋子里掉出来的，00:05:49.135 --> 00:05:52.175
并在地面上表示为停止字。00:05:52.175 --> 00:05:55.730
美丽的艺术作品,对吧?所以,嗯,00:05:55.730 --> 00:06:02.690
关于语言的NLP模型有一个有趣的发现，00:06:02.690 --> 00:06:04.670
我认为这是更真实的00:06:04.670 --> 00:06:08.600
深度学习的世界比以前更加，00:06:08.600 --> 00:06:11.510
伙计，你可以用文字袋模型做很多事，对吧?00:06:11.510 --> 00:06:15.920
你可以通过说，00:06:15.920 --> 00:06:18.400
让我们得到神经词向量，00:06:18.400 --> 00:06:20.670
我们要把它们平均起来或者最大限度地集中起来，00:06:20.670 --> 00:06:21.975
或者像这样，00:06:21.975 --> 00:06:23.580
什么也不做，00:06:23.580 --> 00:06:26.300
这给了我一个很好的句子表示00:06:26.300 --> 00:06:30.035
我可以在分类器中使用的文档表示。00:06:30.035 --> 00:06:33.830
有时候，你只能做到这一点，然后变得更好。00:06:33.830 --> 00:06:37.910
所以人们做了像深度平均网络这样的事情00:06:37.910 --> 00:06:40.400
一个袋装文字模型的输出00:06:40.400 --> 00:06:43.475
通过更多的层来喂养它并改进它。00:06:43.475 --> 00:06:47.300
这是完全不同的00:06:47.300 --> 00:06:51.560
在语言学中占主导地位的语言结构。00:06:51.560 --> 00:06:58.340
在语言学中，重点是识别00:06:58.340 --> 00:07:05.480
大量的语言结构通过非常复杂的形式主义。00:07:05.480 --> 00:07:10.455
我想这有点像乔姆斯基的极简主义语法树，00:07:10.455 --> 00:07:15.260
上面这张图是头部驱动的短语结构语法。00:07:15.260 --> 00:07:18.035
这个理论主要是，00:07:18.035 --> 00:07:21.935
90年代在斯坦福大学开发的。00:07:21.935 --> 00:07:25.070
但是非常复杂的数据结构00:07:25.070 --> 00:07:28.895
用于描述语言学的铰接结构。00:07:28.895 --> 00:07:32.645
这两者之间有很大的差距。00:07:32.645 --> 00:07:36.260
你可能会想，00:07:36.260 --> 00:07:41.180
中间有一些好的点我们有一定的结构，00:07:41.180 --> 00:07:43.460
这将帮助我们做我们想做的。00:07:43.460 --> 00:07:46.525
特别是，00:07:46.525 --> 00:07:49.745
如果我们想从语义上解释语言，00:07:49.745 --> 00:07:53.300
似乎我们不只是想要单词向量，00:07:53.300 --> 00:07:56.120
我们想要更大的短语的意思。00:07:56.120 --> 00:07:59.705
这是滑雪板运动员跳过一个大人物，00:07:59.705 --> 00:08:03.170
一个人在滑雪板上跳到空中。00:08:03.170 --> 00:08:06.635
我们想说的是滑雪者00:08:06.635 --> 00:08:10.790
意思基本上和一个人在滑雪板上是一样的。00:08:10.790 --> 00:08:13.220
所以我们想要这些块00:08:13.220 --> 00:08:17.255
语言在语言学中是组成短语，00:08:17.255 --> 00:08:19.430
说它们有意义，00:08:19.430 --> 00:08:21.830
我们希望能够比较它们的意思。00:08:21.830 --> 00:08:26.870
现在，我们已经看到了至少一个工具可以让我们拥有语言块，对吧?00:08:26.870 --> 00:08:30.320
因为我们研究了卷积神经网络00:08:30.320 --> 00:08:34.805
用三个词表示卷积神经网络，00:08:34.805 --> 00:08:38.090
但根本的区别在于00:08:38.090 --> 00:08:41.254
人类语言有这些有意义的块，00:08:41.254 --> 00:08:43.055
大小不同。00:08:43.055 --> 00:08:45.935
所以我们想说滑雪板运动员00:08:45.935 --> 00:08:50.120
在语义上几乎等同于一个人在滑雪板上，00:08:50.120 --> 00:08:52.790
但最上面的是两个单词，00:08:52.790 --> 00:08:55.295
最下面的是五个单词。00:08:55.295 --> 00:08:59.200
如果我们能做到这一点00:08:59.200 --> 00:09:03.230
我们想要这些组成块00:09:03.230 --> 00:09:07.070
能够使用并在神经网络中表示它们。00:09:07.070 --> 00:09:08.630
这有点，嗯，00:09:08.630 --> 00:09:11.870
中心思想是什么00:09:11.870 --> 00:09:16.235
激发了我要给你们看的树形结构神经网络。00:09:16.235 --> 00:09:21.380
还有一件相关的事情你可能想要考虑一下，00:09:21.380 --> 00:09:23.240
一个人在滑雪板上，00:09:23.240 --> 00:09:27.950
人类如何理解这意味着什么?00:09:27.950 --> 00:09:31.055
然后一个人在滑雪板上跳到空中，00:09:31.055 --> 00:09:35.655
人们如何理解这意味着什么?00:09:35.655 --> 00:09:41.030
这似乎是唯一可能的答案00:09:41.030 --> 00:09:46.595
这就是通常所说的构成原则。00:09:46.595 --> 00:09:49.040
人们知道person这个词，00:09:49.040 --> 00:09:50.240
他们知道这个词，00:09:50.240 --> 00:09:52.760
因此，他们知道单板滑雪这个词，00:09:52.760 --> 00:09:55.880
他们可以算出在滑雪板上是什么意思00:09:55.880 --> 00:10:00.170
他们还能通过了解滑雪板上的人是什么意思00:10:00.170 --> 00:10:05.900
组件的意义，并将它们组合成更大的块。00:10:05.900 --> 00:10:08.295
有一个f-有一个著名的，00:10:08.295 --> 00:10:12.500
应用数学家统计学家，00:10:12.500 --> 00:10:17.235
在布朗大学，Stu Geman，我想他总结的方式是，00:10:17.235 --> 00:10:21.680
要么构成原则是正确的，要么上帝存在。00:10:21.680 --> 00:10:24.285
因为他曾经是00:10:24.285 --> 00:10:27.200
你可以想怎么拿就怎么拿，00:10:27.200 --> 00:10:30.480
我想他的意思是00:10:30.480 --> 00:10:32.880
你可以把这些数变成无穷大00:10:32.880 --> 00:10:36.085
无限长的句子，人类能够理解，00:10:36.085 --> 00:10:39.365
它必须是人们可以知道的00:10:39.365 --> 00:10:43.650
单词和组合意义的方法，并使意义更大，因为，00:10:43.650 --> 00:10:47.945
你知道，如果不是这样，人们怎么会理解句子呢?00:10:47.945 --> 00:10:50.375
我们希望能够做到这一点。00:10:50.375 --> 00:10:54.664
我们希望能够计算出更小元素的语义组合，00:10:54.664 --> 00:10:57.415
计算出更大的部分的意思。00:10:57.415 --> 00:11:01.200
这显然不仅仅是语言上的问题，00:11:01.200 --> 00:11:05.050
合成，嗯，也出现在其他地方。00:11:05.050 --> 00:11:10.620
所以，如果你想了解一些机器是如何工作的，00:11:10.620 --> 00:11:14.195
你想知道的是它有不同的子组件。00:11:14.195 --> 00:11:16.140
如果你能理解00:11:16.140 --> 00:11:19.370
不同的子组件工作，以及它们是如何装配在一起的，00:11:19.370 --> 00:11:24.605
这样你就能对整个场景有一些了解了。00:11:24.605 --> 00:11:31.170
嗯，还有，嗯，构图似乎在视觉上也起作用。00:11:31.170 --> 00:11:35.950
这是一个场景，看起来这个场景有很多部分。00:11:35.950 --> 00:11:38.650
所以有一些小的部分在一起。00:11:38.650 --> 00:11:41.725
所以有些人聚在一起，00:11:41.725 --> 00:11:44.860
有一个屋顶，二层和另一个屋顶。00:11:44.860 --> 00:11:48.850
一层和教堂的图片连在一起。00:11:48.850 --> 00:11:54.275
所以这也是一种组合的场景在其中，各个部分是相互联系的。00:11:54.275 --> 00:11:58.695
所以对于语言理解来说，00:11:58.695 --> 00:12:02.810
然后对于我们用于智力的其他很多东西，00:12:02.810 --> 00:12:05.495
我们需要能够理解00:12:05.495 --> 00:12:09.335
更大的东西来自于对更小部分的了解。00:12:09.335 --> 00:12:14.870
我之前提到过，00:12:14.870 --> 00:12:16.850
有时候最有名的，00:12:16.850 --> 00:12:20.480
语言学家是麻省理工学院的Noam Chomsky，00:12:20.480 --> 00:12:24.480
你知道，真正的计算语言学家，00:12:24.480 --> 00:12:28.355
很多时候我们都不太友好00:12:28.355 --> 00:12:32.590
语言学家，尤其是诺姆・乔姆斯基，00:12:32.590 --> 00:12:36.250
语言理论，因为他从来没有00:12:36.250 --> 00:12:39.970
赞同机器学习的理念。00:12:39.970 --> 00:12:44.075
或者说从数据中学习的经验能力。00:12:44.075 --> 00:12:46.020
他一直都是00:12:46.020 --> 00:12:48.575
[噪音]想要拒绝这种存在。00:12:48.575 --> 00:12:51.470
但是，如果我们继续寻找，00:12:51.470 --> 00:12:53.565
嗯，这方面的见解。00:12:53.565 --> 00:12:58.115
你知道，这是乔姆斯基最近的一篇论文，作者是00:12:58.115 --> 00:13:03.120
试图给出人类语言的独特之处。00:13:03.120 --> 00:13:05.280
本质上，他们，嗯，00:13:05.280 --> 00:13:07.985
零是这样的，00:13:07.985 --> 00:13:09.910
如果你在看，00:13:09.910 --> 00:13:13.730
人类和其他相当聪明的生物。00:13:13.730 --> 00:13:17.790
他们认为人类的决定性差异，00:13:17.790 --> 00:13:22.095
他们有能力对递归进行建模。00:13:22.095 --> 00:13:27.450
因此，这篇论文认为，这一独特的区别00:13:27.450 --> 00:13:30.390
人类语言的发展就是我们00:13:30.390 --> 00:13:33.555
可以把更小的部分组装成更大的东西，00:13:33.555 --> 00:13:38.495
在递归过程中这就是定义新能力的方法。00:13:38.495 --> 00:13:41.135
不确定我不确定我信不信00:13:41.135 --> 00:13:43.920
嗯，[笑声]你可以决定你的想法。00:13:43.920 --> 00:13:46.325
但我认为，00:13:46.325 --> 00:13:51.390
这是毫无疑问的吗00:13:51.390 --> 00:13:56.990
人类语言句子的结构有这些片段，00:13:56.990 --> 00:14:01.260
然后形成层次结构的成分00:14:01.260 --> 00:14:05.555
递归地变成更大的块。00:14:05.555 --> 00:14:11.190
特别是这个递归式你会得到一个名词短语meat，00:14:11.190 --> 00:14:15.375
然后出现在一个更大的名词短语中，比如带肉的意大利面。00:14:15.375 --> 00:14:17.640
你可以重复几次，00:14:17.640 --> 00:14:19.530
给出一个递归结构。00:14:19.530 --> 00:14:22.710
上面有一个蓝色的例子。00:14:22.710 --> 00:14:25.180
所以站在他旁边的人就从00:14:25.180 --> 00:14:28.035
收购你曾经工作过的公司00:14:28.035 --> 00:14:32.675
嗯，这整件事是一个很大的名词短语。00:14:32.675 --> 00:14:36.280
但是里面有一个名词短语，00:14:36.280 --> 00:14:39.780
那个买下你以前工作的公司的人，00:14:39.780 --> 00:14:41.880
这是另一个名词短语。00:14:41.880 --> 00:14:43.820
在里面，嗯，00:14:43.820 --> 00:14:47.260
还有一些较小的名词短语，比如，00:14:47.260 --> 00:14:49.890
收购你曾经工作过的公司的那家公司。00:14:49.890 --> 00:14:53.190
但是，你知道，它仍然存在于名词短语中，比如，00:14:53.190 --> 00:14:55.055
你以前工作的那家公司。00:14:55.055 --> 00:14:57.660
实际上，即使它在里面，00:14:57.660 --> 00:14:59.365
较小的名词短语，00:14:59.365 --> 00:15:02.035
这就是你这个词。00:15:02.035 --> 00:15:06.945
所以一个单独的代词也是一个名词短语。00:15:06.945 --> 00:15:11.000
嗯，只是一种结构00:15:11.000 --> 00:15:13.565
语言，你从哪里得到这种00:15:13.565 --> 00:15:16.895
层级结构，以及它们内部相同的东西。00:15:16.895 --> 00:15:20.465
我认为这是完全正确的。00:15:20.465 --> 00:15:23.130
嗯，那个――那个声明，00:15:23.130 --> 00:15:26.210
我们的语言是递归的，00:15:26.210 --> 00:15:29.615
在正式意义上并不十分清楚，00:15:29.615 --> 00:15:33.365
很明显。00:15:33.365 --> 00:15:36.850
这就是为什么――说某个东西是递归的，00:15:36.850 --> 00:15:39.500
它必须重复到无穷，对吧。00:15:39.500 --> 00:15:43.180
所以一旦你对某个东西进行了限制，00:15:43.180 --> 00:15:48.819
你说，“看，这是一个名词短语，你刚刚给了我五个层次的嵌套。”00:15:48.819 --> 00:15:52.615
真不敢相信有人会这么说。00:15:52.615 --> 00:15:54.630
所以一旦你，00:15:54.630 --> 00:15:56.570
我想说的是，00:15:56.570 --> 00:15:57.960
即使他们这么说，00:15:57.960 --> 00:16:01.110
没有人会说一个有10层嵌套的名词短语。00:16:01.110 --> 00:16:04.340
如果你对它施加一些严格的限制，00:16:04.340 --> 00:16:08.970
从某种意义上说，它并不是真正的递归因为它没有趋于无穷。00:16:08.970 --> 00:16:10.290
但是，你知道，00:16:10.290 --> 00:16:12.280
不管你怎么想，00:16:12.280 --> 00:16:16.090
这并没有否定你得到这个层次结构的基本论点00:16:16.090 --> 00:16:19.900
和名词短语一样的结构，00:16:19.900 --> 00:16:26.780
句子，动词短语，以一种没有明确界限的方式彼此出现。00:16:26.780 --> 00:16:30.200
就像我给你们看一个复杂的句子，00:16:30.200 --> 00:16:35.430
你可以说，我可以把它放进一个更大，更复杂的句子里，00:16:35.430 --> 00:16:38.315
你对我说过，然后又说，00:16:38.315 --> 00:16:39.940
我的判决，没错。00:16:39.940 --> 00:16:44.880
这就是它看起来确实是一个递归生成过程的意义，00:16:44.880 --> 00:16:50.510
尽管实际上人们说复杂句子的方式是有限制的。00:16:50.510 --> 00:16:53.625
这就是得到的结构00:16:53.625 --> 00:16:57.605
在这些选区里捕获的，嗯，结构树。00:16:57.605 --> 00:17:02.780
在我们讲解析之前你们也做了一些，00:17:02.780 --> 00:17:05.365
我强调了依赖解析。00:17:05.365 --> 00:17:08.220
但另一种解析实际上是00:17:08.220 --> 00:17:12.035
我今天要讲的模型，00:17:12.035 --> 00:17:15.605
这就是所谓的选民吗00:17:15.605 --> 00:17:19.790
解析或语言学家通常称之为短语结构语法，00:17:19.790 --> 00:17:24.635
或者在计算机科学的形式语言理论中。00:17:24.635 --> 00:17:27.185
这些是上下文无关的语法，00:17:27.185 --> 00:17:29.525
我们有，嗯，这些，00:17:29.525 --> 00:17:32.129
像名词短语这样的非终结词，00:17:32.129 --> 00:17:35.105
动词短语，这是在另一个名词短语里面，00:17:35.105 --> 00:17:36.715
它在另一个动词短语里面，00:17:36.715 --> 00:17:38.879
也就是更多的动词短语，00:17:38.879 --> 00:17:40.775
在句子的开头。00:17:40.775 --> 00:17:43.895
这就是我们的选区语法。00:17:43.895 --> 00:17:47.585
当我们偶尔提到佩恩河岸树时，00:17:47.585 --> 00:17:52.495
这是一棵原始的Penn tree bank树基本上，00:17:52.495 --> 00:17:54.005
短语结构语法，比如，00:17:54.005 --> 00:17:56.705
加上各种额外的注解，00:17:56.705 --> 00:17:58.850
嗯，在节点上。00:17:58.850 --> 00:18:04.775
好吧，那么你是怎么捕捉到这些特性的呢，00:18:04.775 --> 00:18:07.925
看起来我们想要一个神经模型00:18:07.925 --> 00:18:11.405
可以利用一些相同的树结构。00:18:11.405 --> 00:18:17.720
所以我们想要做的是计算出成分的语义相似性，00:18:17.720 --> 00:18:20.255
我们想要的不仅仅是拥有00:18:20.255 --> 00:18:25.185
一个矢量空间，就像我们在这节课开始时用的，00:18:25.185 --> 00:18:30.065
但是我们希望能够使用更大的成分，比如名词短语，00:18:30.065 --> 00:18:31.565
我出生的国家，00:18:31.565 --> 00:18:33.475
我出生的地方，00:18:33.475 --> 00:18:35.535
并赋予它们一个意义。00:18:35.535 --> 00:18:39.370
所以看起来我们想要做的是有一个方法00:18:39.370 --> 00:18:44.140
用合成的方式计算短语的意思，00:18:44.140 --> 00:18:47.320
这样，最终的结果也是那样00:18:47.320 --> 00:18:52.410
这些短语可以放在向量空间模型中。00:18:52.410 --> 00:18:56.300
所以我们还是用短语的向量空间语义，00:18:56.300 --> 00:18:59.625
我们要计算短语的意思。00:18:59.625 --> 00:19:01.530
问题是，00:19:01.530 --> 00:19:04.715
我们该怎么做呢?00:19:04.715 --> 00:19:07.865
答案一是我们要用00:19:07.865 --> 00:19:11.375
既然我们确信它是正确的，00:19:11.375 --> 00:19:15.740
所以，构成原则本质上是说，00:19:15.740 --> 00:19:20.170
如果你想知道它的意思，或者这里它说的是一个句子。00:19:20.170 --> 00:19:24.340
但是任何短语，任何成分的意思都是00:19:24.340 --> 00:19:29.050
通过了解单词的含义来构建它，00:19:29.050 --> 00:19:31.565
然后是结合这些意义的规则。00:19:31.565 --> 00:19:34.280
从我出生的国家开始，00:19:34.280 --> 00:19:37.310
我应该能够计算出我出生的意义，00:19:37.310 --> 00:19:39.105
以及这个国家的意义，00:19:39.105 --> 00:19:43.540
以及我出生的国家的意义。00:19:43.540 --> 00:19:47.680
所以我们有意义组合规则，让我们计算00:19:47.680 --> 00:19:52.520
较大的成分或句子的意义向上。00:19:52.520 --> 00:19:57.290
这似乎是正确的做法。00:19:57.290 --> 00:20:00.140
问题是，我们能，00:20:00.140 --> 00:20:04.470
然后建立一个模型，如何做到这一点?00:20:04.470 --> 00:20:08.630
这是一种简单的方法。00:20:08.630 --> 00:20:16.105
我们有我们计算过的单词的单词向量。00:20:16.105 --> 00:20:20.405
我们要做的是算出00:20:20.405 --> 00:20:23.625
然后是这个句子的意思表示。00:20:23.625 --> 00:20:26.680
现在我们有两件事要做。00:20:26.680 --> 00:20:31.590
我们需要分析出句子的正确结构，00:20:31.590 --> 00:20:35.250
然后是意义计算00:20:35.250 --> 00:20:39.515
计算出这个句子的意思表示。00:20:39.515 --> 00:20:42.900
对于解析，我们要做的是，00:20:42.900 --> 00:20:45.280
名词短语，介词短语，00:20:45.280 --> 00:20:48.120
动词短语，句子之类的单位，00:20:48.120 --> 00:20:49.895
为了让“猫坐在垫子上”，00:20:49.895 --> 00:20:51.380
然后威尔，00:20:51.380 --> 00:20:52.935
如果我们有的话，00:20:52.935 --> 00:20:57.220
然后我们可以运行某种意义计算程序，00:20:57.220 --> 00:20:59.195
给我们一个向量空间，00:20:59.195 --> 00:21:01.310
嗯，这些句子的意思。00:21:01.310 --> 00:21:02.970
这就是我们想要的，00:21:02.970 --> 00:21:04.270
两者都要做，00:21:04.270 --> 00:21:07.355
一会儿我会给你们看一个这样的例子00:21:07.355 --> 00:21:10.660
一种接近它的方法。00:21:10.660 --> 00:21:13.085
但在这之前，先退一步00:21:13.085 --> 00:21:15.935
这里有什么不同?00:21:15.935 --> 00:21:18.395
这里我们有00:21:18.395 --> 00:21:21.630
递归神经网络00:21:21.630 --> 00:21:25.125
我们这门课的主要工具，00:21:25.125 --> 00:21:26.465
它给你，00:21:26.465 --> 00:21:30.605
它让你看到了我出生的国家的意义，00:21:30.605 --> 00:21:32.920
你可以这样说，00:21:32.920 --> 00:21:34.740
我出生的国家00:21:34.740 --> 00:21:36.980
或者我们讨论了其他的技巧，00:21:36.980 --> 00:21:39.979
在所有这些地方做最大汇聚，00:21:39.979 --> 00:21:42.820
或者你可以在这里有一个单独的节点，00:21:42.820 --> 00:21:44.620
对这些的关注也是如此。00:21:44.620 --> 00:21:49.240
所以它确实给了你一种表达，00:21:49.240 --> 00:21:50.950
这句话的意思是，00:21:50.950 --> 00:21:54.785
也包括单词的子序列。00:21:54.785 --> 00:21:57.410
但是他们有点不同，对吧?00:21:57.410 --> 00:21:59.290
上面这个，00:21:59.290 --> 00:22:01.435
树递归神经网络，00:22:01.435 --> 00:22:07.775
它需要一个句子或任何一种短语具有树形结构。00:22:07.775 --> 00:22:10.390
所以我们知道它的组成部分是什么，00:22:10.390 --> 00:22:14.935
然后我们要做的是意义表征00:22:14.935 --> 00:22:20.800
对于对句法结构敏感的短语，00:22:20.800 --> 00:22:24.215
这些词是如何组合成短语的。00:22:24.215 --> 00:22:27.875
而对于递归神经网络00:22:27.875 --> 00:22:31.549
以一种不经意的方式运行一个序列模型，00:22:31.549 --> 00:22:33.500
然后计算，00:22:33.500 --> 00:22:35.120
很明显，00:22:35.120 --> 00:22:38.390
它并没有以任何明显的方式给出一个意义表示，00:22:38.390 --> 00:22:41.970
我的出生，或者包含在里面的出生。00:22:41.970 --> 00:22:46.240
我们对整个序列只有一个意义表示，00:22:46.240 --> 00:22:48.680
但是如果我们这样做，00:22:48.680 --> 00:22:54.705
我们确实对句子中不同的有意义的部分有意义表示。00:22:54.705 --> 00:22:58.730
好。这就明白我们要做什么了吗?00:22:59.120 --> 00:23:01.625
好。我们怎么做呢，00:23:01.625 --> 00:23:03.240
开始做那件事?00:23:03.240 --> 00:23:08.385
我们要做的是，00:23:08.385 --> 00:23:09.859
如果我们自下而上地工作，00:23:09.859 --> 00:23:14.245
在最底部我们有字向量，00:23:14.245 --> 00:23:19.890
所以我们想递归地计算更大组分的意义。00:23:19.890 --> 00:23:25.280
所以如果我们想计算"on the mat"的意思我们可以这样说，00:23:25.280 --> 00:23:30.135
我们已经有了on和mat的意思表示。00:23:30.135 --> 00:23:33.910
如果我们把这些输入神经网络，因为这是我们的00:23:33.910 --> 00:23:37.820
一个工具，我们可以得到两个东西。00:23:37.820 --> 00:23:42.055
我们可以得到一个很好的分数。00:23:42.055 --> 00:23:44.420
这就是我们要用到的解析。00:23:44.420 --> 00:23:49.450
我们会说，你相信-你相信你能把on和00:23:49.450 --> 00:23:55.055
“mat”来形成一个好的组成部分，它是解析树的一部分?00:23:55.055 --> 00:23:58.055
如果答案为真，这将是一个很大的正数，00:23:58.055 --> 00:23:59.865
如果这不是真的，00:23:59.865 --> 00:24:03.305
然后我们有了一个有意义的合成装置，00:24:03.305 --> 00:24:05.270
“好的，嗯，00:24:05.270 --> 00:24:07.480
如果你把这两件事放在一起，00:24:07.480 --> 00:24:11.965
我们放在一起的东西有什么意义呢?”00:24:11.965 --> 00:24:16.200
这是我们探索的第一个模型00:24:16.200 --> 00:24:19.660
是用一种非常简单的方法做的，对吧?00:24:19.660 --> 00:24:22.750
这就是我们的意思组合，00:24:22.750 --> 00:24:27.665
我们把这两个分量的向量连接起来，00:24:27.665 --> 00:24:30.230
我们把它们乘以一个矩阵，加上a00:24:30.230 --> 00:24:31.840
偏见和往常一样,00:24:31.840 --> 00:24:33.955
用tan h表示，00:24:33.955 --> 00:24:35.345
这件作品已经很老了，00:24:35.345 --> 00:24:36.815
这有点像之前的事情，比如，00:24:36.815 --> 00:24:38.110
ReLUs成为受欢迎的,00:24:38.110 --> 00:24:41.735
但也许棕褐色的h更合适，00:24:41.735 --> 00:24:43.235
一个递归神经网络，00:24:43.235 --> 00:24:47.580
这就是我们的意义组合赋予了父结点的意义。00:24:47.580 --> 00:24:52.305
另一方面，这句话是否合适，00:24:52.305 --> 00:24:55.370
我们取父向量表示，00:24:55.370 --> 00:24:58.949
再乘以另一个向量，00:24:58.949 --> 00:25:01.990
这给了我们一个数字。00:25:02.100 --> 00:25:06.180
如果你在做这个的时候想一下，00:25:06.180 --> 00:25:10.699
你可能认为这不是一个完美的意义组合模型，00:25:10.699 --> 00:25:14.800
在以后的课程中，我会讲到一些更复杂的模型，00:25:14.800 --> 00:25:17.880
然后我们开始探索。00:25:17.880 --> 00:25:21.815
但这已经足够让我们继续了00:25:21.815 --> 00:25:24.520
这给了我们一种建造的方法00:25:24.520 --> 00:25:29.805
一个递归神经网络解析器，00:25:29.805 --> 00:25:33.530
并为它们设计了一个意义表示。00:25:33.530 --> 00:25:37.565
所以我们用最简单的方法，00:25:37.565 --> 00:25:39.580
那就是一个贪婪的解析器。00:25:39.580 --> 00:25:42.620
所以如果我们从“猫坐在垫子上”开始，00:25:42.620 --> 00:25:43.960
我们能做的是，00:25:43.960 --> 00:25:47.040
也许你应该把"the"和"cat"连在一起。00:25:47.040 --> 00:25:48.220
让我们试试。00:25:48.220 --> 00:25:49.865
通过我们的神经网络，00:25:49.865 --> 00:25:53.315
它会得到一个分数和一个意义表示，00:25:53.315 --> 00:25:55.565
我们可以试着为cat做这个00:25:55.565 --> 00:25:58.635
我们可以试着用"sat"和"on"来表示。00:25:58.635 --> 00:26:03.170
我们可以试着用on和the我们可以试着用the和mat。00:26:03.170 --> 00:26:06.665
这时我们会说，好的，00:26:06.665 --> 00:26:12.725
我们能把这些向量组合起来的最好的短语就是“猫”。00:26:12.725 --> 00:26:14.770
所以让我们专注于这一点，00:26:14.770 --> 00:26:17.360
它有这样的语义表示，00:26:17.360 --> 00:26:20.825
在这一点上我们可以重复。00:26:20.825 --> 00:26:25.190
现在，我们在那里做的所有工作都可以重复使用，因为没有任何变化，00:26:25.190 --> 00:26:28.850
但我们也可以考虑加入"cat"00:26:28.850 --> 00:26:32.965
作为"sat"的一名成员，并得到一个分数。00:26:32.965 --> 00:26:34.930
在这一点上，我们决定，00:26:34.930 --> 00:26:37.140
垫子是最好的组成部分，00:26:37.140 --> 00:26:41.455
为此，计算“on the mat”的含义表示。00:26:41.455 --> 00:26:44.025
看起来不错，承诺吧，00:26:44.025 --> 00:26:46.080
然后继续前进，00:26:46.080 --> 00:26:51.200
所以我们有一个机制来选择a中的句子的解析，00:26:51.200 --> 00:26:52.655
以贪婪的方式。00:26:52.655 --> 00:26:55.300
但是，当我们看依赖分析时，00:26:55.300 --> 00:26:57.230
我们也在贪婪地这样做，对吧?00:26:57.230 --> 00:27:00.955
然后提出一个意义表示。00:27:00.955 --> 00:27:06.105
好。这是我们第一个有树递归神经网络的模型，00:27:06.105 --> 00:27:07.630
并将其用于解析。00:27:07.630 --> 00:27:12.630
这里还有一些细节，00:27:12.630 --> 00:27:16.200
有些可能不是超级的，00:27:16.200 --> 00:27:18.335
这一点很重要，对吧?00:27:18.335 --> 00:27:22.790
所以我们可以通过对每个节点的得分求和来给树打分，00:27:22.790 --> 00:27:25.290
为了锻炼，00:27:25.290 --> 00:27:27.730
对于我们正在计算的优化，00:27:27.730 --> 00:27:33.545
我们用的是我们在其他地方看到的这种最大保证金损失。00:27:33.545 --> 00:27:37.780
最简单的方法就是贪婪。00:27:37.780 --> 00:27:41.825
你只要在每一点上找到最好的当地决策，00:27:41.825 --> 00:27:42.845
构造这个结构，00:27:42.845 --> 00:27:43.985
继续。00:27:43.985 --> 00:27:45.610
但如果你想做得更好，00:27:45.610 --> 00:27:47.100
我们对此进行了探索，00:27:47.100 --> 00:27:48.750
你可以说，00:27:48.750 --> 00:27:50.910
我们可以做光束搜索。00:27:50.910 --> 00:27:54.020
我们可以探索几种合并的好方法，00:27:54.020 --> 00:27:59.480
然后在树的高处决定哪条路是最好的，嗯，合并。00:27:59.480 --> 00:28:03.155
我们还没有在这门课上讲过，00:28:03.155 --> 00:28:05.240
但是我想提一下，00:28:05.240 --> 00:28:08.490
以防有人看到00:28:08.490 --> 00:28:12.545
传统的选区解析，这里有符号，00:28:12.545 --> 00:28:14.405
比如NP或者VP。00:28:14.405 --> 00:28:19.240
有一些高效的动态规划算法00:28:19.240 --> 00:28:24.660
求一个句子在多项式时间内的最优解析。00:28:24.660 --> 00:28:26.190
所以在立方时间内。00:28:26.190 --> 00:28:29.375
如果你有一个常规的上下文无关语法，00:28:29.375 --> 00:28:32.615
规则的概率上下文无关语法，00:28:32.615 --> 00:28:35.010
如果你想知道什么是最好的解析00:28:35.010 --> 00:28:38.385
根据概率上下文无关文法，00:28:38.385 --> 00:28:43.265
你可以写一个三次动态规划算法你可以找到它。00:28:43.265 --> 00:28:47.560
这很好。在CS224N的旧时代，00:28:47.560 --> 00:28:51.605
在神经网络出现之前我们让每个人都这样做。00:28:51.605 --> 00:28:57.240
这是旧CS224N中最令人伤脑筋的任务00:28:57.240 --> 00:29:02.380
编写这个动态程序是为了对一个句子进行上下文无关的语法分析。00:29:02.380 --> 00:29:05.620
有点可悲的是，00:29:05.620 --> 00:29:08.810
一旦你进入这种神经网络表示，00:29:08.810 --> 00:29:12.975
你不能再写聪明的动态规划算法了，00:29:12.975 --> 00:29:18.094
因为聪明的动态规划算法只在你有符号的时候才有效00:29:18.094 --> 00:29:23.580
从一个相对较小的非终端集开始因为如果是这样的话，00:29:23.580 --> 00:29:26.110
可以有碰撞，对吧?00:29:26.110 --> 00:29:29.075
你有很多方法来解析下面的东西，00:29:29.075 --> 00:29:30.760
哪种，呃，00:29:30.760 --> 00:29:33.485
事实证明，造名词短语的方法是不同的，00:29:33.485 --> 00:29:35.850
或者不同的介词短语，00:29:35.850 --> 00:29:38.825
因此你可以用动态规划来节省工作。00:29:38.825 --> 00:29:40.645
如果你有这样一个模型，00:29:40.645 --> 00:29:44.490
因为你构建的所有东西都要经过多层神经网络，00:29:44.490 --> 00:29:47.745
你得到了一个意义表示，某个高维向量，00:29:47.745 --> 00:29:49.760
物体永远不会碰撞，00:29:49.760 --> 00:29:53.070
所以你永远不能通过动态编程来节省工作。00:29:53.070 --> 00:29:58.520
所以，你要么做指数函数来探索所有的东西，00:29:58.520 --> 00:30:03.950
或者你用某种光束来探索一些可能的东西。00:30:04.090 --> 00:30:09.260
是的。实际上我们也应用了这个，00:30:09.260 --> 00:30:11.915
同时也是为了视觉。00:30:11.915 --> 00:30:14.330
所以这并不完全是00:30:14.330 --> 00:30:17.300
一个模糊的动机，嗯，00:30:17.300 --> 00:30:23.255
视觉场景中有我们已经开始探索的部分你可以，00:30:23.255 --> 00:30:27.590
这些场景，然后算出来，00:30:27.590 --> 00:30:32.885
使用类似组合形式的场景表示。00:30:32.885 --> 00:30:35.405
特别地，00:30:35.405 --> 00:30:40.715
有这样一个数据集，00:30:40.715 --> 00:30:43.894
视觉中的多类分割，00:30:43.894 --> 00:30:48.830
你从很小的补丁开始然后你想把它们结合起来00:30:48.830 --> 00:30:51.050
进入一个场景的一部分00:30:51.050 --> 00:30:54.170
认出了这幅画的哪一部分是这栋建筑，00:30:54.170 --> 00:30:58.025
天空，道路，各种各样的其他类。00:30:58.025 --> 00:31:02.435
我们当时做得非常好，00:31:02.435 --> 00:31:06.140
使用其中一种树递归结构神经网络更好00:31:06.140 --> 00:31:11.435
比之前在21世纪头十年做的工作还要多。00:31:11.435 --> 00:31:17.225
好。那么我们如何――我们如何构建神经网络，00:31:17.225 --> 00:31:19.550
嗯，会做这种事吗?00:31:19.550 --> 00:31:25.730
当我们开始研究这些树状结构的神经网络时，00:31:25.730 --> 00:31:29.480
我们认为这是一个很酷的原创想法，没有人00:31:29.480 --> 00:31:33.305
曾成功地研究过树状神经网络。00:31:33.305 --> 00:31:39.560
但事实证明我们错了，上世纪90年代中期有几个德国人，00:31:39.560 --> 00:31:44.765
已经开始研究树状结构神经网络，00:31:44.765 --> 00:31:45.950
他们的数学。00:31:45.950 --> 00:31:49.535
对应于时间反向传播算法，00:31:49.535 --> 00:31:52.865
艾比在做递归神经网络的时候提到过。00:31:52.865 --> 00:31:55.310
他们计算出了这个树状结构的案例00:31:55.310 --> 00:31:58.310
叫做反向传播，通过结构。00:31:58.310 --> 00:32:02.810
这里有几张幻灯片00:32:02.810 --> 00:32:07.505
幻灯片，但我想跳过它们。00:32:07.505 --> 00:32:08.765
如果有人想看，00:32:08.765 --> 00:32:10.610
他们在网上，你可以看到他们。00:32:10.610 --> 00:32:14.720
我的意思是，实际上没有什么新东西。00:32:14.720 --> 00:32:17.420
如果你还记得with- with00:32:17.420 --> 00:32:21.725
严重的疤痕或者是这门课程的早期课程，00:32:21.725 --> 00:32:26.450
神经网络的导数以及它与递归神经网络的关系。00:32:26.450 --> 00:32:28.130
有点像，对吧。00:32:28.130 --> 00:32:32.585
在树结构的不同层次上有这个递归矩阵。00:32:32.585 --> 00:32:37.220
把所有出现的导数加起来。00:32:37.220 --> 00:32:40.370
唯一的区别是我们现在有了树状结构，00:32:40.370 --> 00:32:43.040
你在往下分解东西。00:32:43.040 --> 00:32:45.410
嗯,是的。00:32:45.410 --> 00:32:48.560
所以我们向前推进计算。00:32:48.560 --> 00:32:51.245
当我们做后台支撑时，00:32:51.245 --> 00:32:55.910
当我们进行反向传播时，误差信号来自上面。00:32:55.910 --> 00:32:58.025
然后我们，嗯，合并它，00:32:58.025 --> 00:33:00.530
用这个节点的计算。00:33:00.530 --> 00:33:03.350
然后我们把它送回树状结构中00:33:03.350 --> 00:33:06.845
直到我们脚下的每一根树枝。00:33:06.845 --> 00:33:11.960
这是我们的第一个版本，我们得到了一些不错的结果。00:33:11.960 --> 00:33:16.985
我们得到了我给你们看的很好的视力结果，00:33:16.985 --> 00:33:19.490
嗯，一些好的，嗯，00:33:19.490 --> 00:33:23.030
语言的解析和做-我们有00:33:23.030 --> 00:33:27.410
有些结果我还没有在这里做解释，00:33:27.410 --> 00:33:33.545
句子之间的判断，它很好地模拟了事物。00:33:33.545 --> 00:33:37.760
但一旦我们开始更多地思考这个问题00:33:37.760 --> 00:33:41.780
非常简单的神经网络功能不可能00:33:41.780 --> 00:33:46.955
计算我们想要计算的句子意义。00:33:46.955 --> 00:33:49.760
所以我们就开始想办法00:33:49.760 --> 00:33:52.970
一些更复杂的方法00:33:52.970 --> 00:33:56.180
的意义，组成函数和节点00:33:56.180 --> 00:33:59.630
然后可以用来建立一个更好的神经网络。00:33:59.630 --> 00:34:04.520
这张幻灯片上有一些。00:34:04.520 --> 00:34:07.280
但是，你知道，对于第一个版本，我们只是00:34:07.280 --> 00:34:10.130
没有足够的神经网络复杂度?00:34:10.130 --> 00:34:13.730
所以当我们有两种成分时，我们把它们连接起来00:34:13.730 --> 00:34:19.040
然后乘以一个权重矩阵。00:34:19.040 --> 00:34:21.590
这就是我们所拥有的一切。00:34:21.590 --> 00:34:27.740
我希望你们对这门课有了更多的了解。00:34:27.740 --> 00:34:31.610
如果你只是连接并乘以一个权重矩阵，00:34:31.610 --> 00:34:36.200
实际上你并没有在建模这两个向量之间的相互作用。00:34:36.200 --> 00:34:39.500
因为你可以把这个权重矩阵看做00:34:39.500 --> 00:34:43.490
除以2。5乘以这个向量，00:34:43.490 --> 00:34:45.605
它的一半乘以这个向量。00:34:45.605 --> 00:34:49.685
所以这两个词的意思并不相互作用。00:34:49.685 --> 00:34:52.280
所以你必须建立你的神经网络，00:34:52.280 --> 00:34:54.620
比这更复杂。00:34:54.620 --> 00:34:59.944
但另一种似乎过于简单的方式是在第一个模型中，00:34:59.944 --> 00:35:04.505
我们只有一个权值矩阵我们用它来表示所有东西。00:35:04.505 --> 00:35:08.120
至少如果你是个语言学家00:35:08.120 --> 00:35:12.050
想想语言的结构你可能会想，00:35:12.050 --> 00:35:14.630
等等，有时候你会00:35:14.630 --> 00:35:17.645
把动词和宾语名词短语放在一起。00:35:17.645 --> 00:35:19.490
嗯，击球。00:35:19.490 --> 00:35:24.125
有时候你会把一篇文章和一个名词，呃，球放在一起。00:35:24.125 --> 00:35:29.150
有时候你要做形容词修饰blue ball。00:35:29.150 --> 00:35:32.705
这些东西在语义上是非常不同的。00:35:32.705 --> 00:35:36.980
你真的可以只有一个权重矩阵吗00:35:36.980 --> 00:35:41.645
把短语的意思组合在一起的通用组合函数?00:35:41.645 --> 00:35:43.280
这可能行得通吗?00:35:43.280 --> 00:35:45.020
你可能会怀疑，00:35:45.020 --> 00:35:46.820
嗯，它不工作。00:35:46.820 --> 00:35:49.355
所以我要继续，00:35:49.355 --> 00:35:53.150
展示一些不同的东西。00:35:53.150 --> 00:35:57.965
但在我展示不同的东西之前，00:35:57.965 --> 00:36:02.810
我要再展示一个版本和第一件事有关，00:36:02.810 --> 00:36:07.355
这实际上提供了一个非常成功和好的解析器，00:36:07.355 --> 00:36:10.250
感谢你所做的00:36:10.250 --> 00:36:14.195
上下文无关样式的选区解析。00:36:14.195 --> 00:36:21.950
这是另一种避免解析完全贪婪的方法。00:36:21.950 --> 00:36:26.900
实际上是把g的两部分分开00:36:26.900 --> 00:36:31.880
必须为我们的句子想出一个树形结构，00:36:31.880 --> 00:36:34.805
让我们来计算一下这个句子的意思。00:36:34.805 --> 00:36:37.280
所以他们的想法是，00:36:37.280 --> 00:36:42.965
在决定一个句子的树状结构时，00:36:42.965 --> 00:36:47.015
实际上，你可以用符号语法做得很好。00:36:47.015 --> 00:36:49.940
但符号语法的问题并不是这样00:36:49.940 --> 00:36:53.270
他们不能把树结构放在句子上。00:36:53.270 --> 00:36:55.790
这些语法的问题是，00:36:55.790 --> 00:36:59.390
它们不能计算意义表示，它们不能00:36:59.390 --> 00:37:03.710
非常擅长在备选树结构之间进行选择。00:37:03.710 --> 00:37:07.175
但是我们可以把这两部分分开。00:37:07.175 --> 00:37:08.750
所以我们可以说，00:37:08.750 --> 00:37:13.175
让我们使用常规的无上下文概率语法00:37:13.175 --> 00:37:16.580
为句子生成可能的树结构。00:37:16.580 --> 00:37:19.205
我们可以生成一个k个最佳列表，00:37:19.205 --> 00:37:21.110
50个最好的是什么，00:37:21.110 --> 00:37:24.230
这个句子的上下文无关语法结构?00:37:24.230 --> 00:37:28.775
这是我们可以用动态规划算法非常有效地做到的。00:37:28.775 --> 00:37:33.125
然后我们可以算出一个神经网络，00:37:33.125 --> 00:37:38.105
这样就能算出句子的意思了。00:37:38.105 --> 00:37:41.675
这就导致了，00:37:41.675 --> 00:37:46.670
这就是所谓的句法松散递归神经网络。00:37:46.670 --> 00:37:51.260
本质上，这意味着我们00:37:51.260 --> 00:37:55.925
ha-每个节点和句子都有一个类别，00:37:55.925 --> 00:37:58.940
符号上下文无关语法。00:37:58.940 --> 00:38:02.630
它们是A类，B类和c类00:38:02.630 --> 00:38:06.500
我们把所有的东西放在一起，我们就可以说，好的。00:38:06.500 --> 00:38:10.430
我们有一个规则，嗯，00:38:10.430 --> 00:38:12.620
X到BC，00:38:12.620 --> 00:38:15.365
这就许可了这个结点。00:38:15.365 --> 00:38:18.800
这部分解析是符号的。00:38:18.800 --> 00:38:21.050
然后我们想00:38:21.050 --> 00:38:24.110
找出这个短语的意思。00:38:24.110 --> 00:38:27.770
我讲的第二个问题00:38:27.770 --> 00:38:32.075
是不是只有一种作图的方法00:38:32.075 --> 00:38:35.990
期望太多而不能拥有吗00:38:35.990 --> 00:38:40.535
动词和宾语与形容词和名词以同样的方式构成。00:38:40.535 --> 00:38:42.980
所以我们有这样的想法，00:38:42.980 --> 00:38:46.190
因为我们现在知道的句法范畴00:38:46.190 --> 00:38:51.215
孩子们可能知道这是一个形容词，这是一个名词。00:38:51.215 --> 00:38:55.010
我们可以用不同的权重矩阵00:38:55.010 --> 00:38:58.895
根据类别的不同进行组合。00:38:58.895 --> 00:39:01.790
而不是以前00:39:01.790 --> 00:39:07.325
这是一个通用的权重矩阵它的意思是做所有有意义的组合。00:39:07.325 --> 00:39:08.810
这里，00:39:08.810 --> 00:39:11.840
这是组合的权重矩阵00:39:11.840 --> 00:39:15.439
一个形容词和一个名词的意思，它会计算，00:39:15.439 --> 00:39:17.810
这个成分的意思。00:39:17.810 --> 00:39:21.590
然后我们会有一个不同的权重矩阵来组合00:39:21.590 --> 00:39:27.870
把限定词的意思和名词短语或类似的词放在一起。00:39:30.090 --> 00:39:34.450
好。嗯,是的。00:39:34.450 --> 00:39:37.360
所以我总是说这个，00:39:37.360 --> 00:39:40.825
嗯，我们想要能够快速地做事情。00:39:40.825 --> 00:39:44.830
所以我们的解决方案是00:39:44.830 --> 00:39:49.525
使用概率上下文无关语法查找可能的语法，00:39:49.525 --> 00:39:55.120
然后计算出我们的意思对于那些，很有可能的。00:39:55.120 --> 00:39:59.050
我们把这个结果叫做合成向量语法00:39:59.050 --> 00:40:03.745
PCFG和树递归神经网络的组合。00:40:03.745 --> 00:40:07.440
嗯,是的。00:40:07.440 --> 00:40:11.010
所以，基本上在那个时候，00:40:11.010 --> 00:40:14.235
这实际上提供了一个非常好的选区解析器。00:40:14.235 --> 00:40:16.845
这里有很多结果。00:40:16.845 --> 00:40:20.040
最上面的是我们的经典老歌，00:40:20.040 --> 00:40:25.285
斯坦福解析器是PCFG，是人们建立的那种解析器。00:40:25.285 --> 00:40:32.080
这是我们2013年完成的合成向量语法，00:40:32.080 --> 00:40:34.690
它不是可用的最佳解析器。00:40:34.690 --> 00:40:38.155
尤金・查尼亚克在布朗大学有一些更好的作品。00:40:38.155 --> 00:40:41.980
但实际上我们有一个很好的解析器。00:40:41.980 --> 00:40:46.495
但更有趣的是，00:40:46.495 --> 00:40:50.845
我们不仅有一个解析器，它的目的是提供正确的解析树。00:40:50.845 --> 00:40:54.850
我们还计算节点的意义表示。00:40:54.850 --> 00:40:58.210
因此，00:40:58.210 --> 00:41:02.140
您不仅可以查看节点的表示形式。00:41:02.140 --> 00:41:06.355
你可以学习这些模型正在学习的权重矩阵，00:41:06.355 --> 00:41:08.980
当它们结合在一起的时候。00:41:08.980 --> 00:41:13.600
记住我们有这些特定于分类的W矩阵，00:41:13.600 --> 00:41:17.455
我们和孩子们一起去猜意思。00:41:17.455 --> 00:41:20.875
这些有点难理解。00:41:20.875 --> 00:41:24.130
但问题是，当我们加载这些矩阵时，00:41:24.130 --> 00:41:27.310
我们将它们初始化为一对对角矩阵。00:41:27.310 --> 00:41:32.125
这些是2×1的矩形矩阵因为有两个子矩阵。00:41:32.125 --> 00:41:35.335
一半是，00:41:35.335 --> 00:41:37.090
乘以左子结点，00:41:37.090 --> 00:41:39.535
另一半乘以正确的子结点。00:41:39.535 --> 00:41:45.520
我们初始化它们就像一个compi-两个单位矩阵00:41:45.520 --> 00:41:48.895
另一个会给我们默认语义00:41:48.895 --> 00:41:52.840
只是取平均值，直到学到一些不同的东西，00:41:52.840 --> 00:41:55.690
在权重向量中。00:41:55.690 --> 00:42:02.070
在某种程度上，这个模型没有学到任何有趣的东西，00:42:02.070 --> 00:42:08.325
对角线上是黄色，其余部分是天蓝色。00:42:08.325 --> 00:42:11.100
在某种程度上它学到了一些东西00:42:11.100 --> 00:42:14.355
有趣的是，从孩子的语义学中，00:42:14.355 --> 00:42:17.770
然后你会在对角线上看到红色和橙色，00:42:17.770 --> 00:42:22.465
还有深蓝，绿色等等。00:42:22.465 --> 00:42:26.200
你会发现如果你训练这个模型，00:42:26.200 --> 00:42:34.225
它是学习哪个短语的孩子才是真正重要的。00:42:34.225 --> 00:42:36.370
这些是说，如果你00:42:36.370 --> 00:42:39.310
把名词短语和配位结合起来，00:42:39.310 --> 00:42:41.830
所以像"猫和"00:42:41.830 --> 00:42:45.055
大部分的语义学都可以在"猫"中找到00:42:45.055 --> 00:42:48.760
而且在“and”中找不到太多的语义。00:42:48.760 --> 00:42:52.825
但是如果你把所有格代词组合在一起，00:42:52.825 --> 00:42:55.045
像她或他，00:42:55.045 --> 00:42:58.615
里面有个名词短语，00:42:58.615 --> 00:43:02.425
她的虎斑猫之类的。00:43:02.425 --> 00:43:06.595
然后大部分的意思是可以在虎斑猫的成分中找到。00:43:06.595 --> 00:43:10.555
它实际上是在学习句子的重要语义。00:43:10.555 --> 00:43:18.460
有很多这样的例子。嗯,是的。00:43:18.460 --> 00:43:21.850
这张图显示了00:43:21.850 --> 00:43:26.575
形容词或副词的修饰结构，00:43:26.575 --> 00:43:31.330
修改名词短语或形容词短语00:43:31.330 --> 00:43:35.995
仅仅一个形容词就可以乘以一个名词短语。00:43:35.995 --> 00:43:40.120
你注意到的是有一些特定的维度00:43:40.120 --> 00:43:44.200
某种意义上的修改。00:43:44.200 --> 00:43:50.395
所以第6维和第11维出现在这些不同的地方，00:43:50.395 --> 00:43:53.830
这里的组合，就像某种意义上的组件。00:43:53.830 --> 00:43:55.645
这很简洁。00:43:55.645 --> 00:44:00.430
这个稍微复杂一点的模型运行得很好00:44:00.430 --> 00:44:05.035
善于捕捉短语和句子的意思。00:44:05.035 --> 00:44:07.105
在这个测试中，00:44:07.105 --> 00:44:11.920
我们给这个系统一个测试句，然后说，00:44:11.920 --> 00:44:17.785
还有哪些句子在意思上最相似，00:44:17.785 --> 00:44:22.120
最接近我们语料库中这个句子的释义?00:44:22.120 --> 00:44:25.990
因此，如果所有的数据都根据季节变化进行调整，00:44:25.990 --> 00:44:29.650
语料库中最相似的两个句子是，00:44:29.650 --> 00:44:32.995
所有的数字都是根据季节性兽医波动调整的。00:44:32.995 --> 00:44:34.270
这很简单。00:44:34.270 --> 00:44:37.960
或者所有的数据都经过调整以去除通常的季节模式。00:44:37.960 --> 00:44:40.240
这看起来很有效。00:44:40.240 --> 00:44:43.000
骑士骑士不会评论作者，00:44:43.000 --> 00:44:46.360
哈尔斯科拒绝透露是哪个国家下的订单。”00:44:46.360 --> 00:44:48.640
这里的语义有点不同00:44:48.640 --> 00:44:51.490
看起来它捕捉到了类似的东西。00:44:51.490 --> 00:44:53.860
“嗯，海岸航空不愿透露相关条款。”00:44:53.860 --> 00:44:55.630
这是一个非常有趣的问题，00:44:55.630 --> 00:45:00.010
因为这个词的意思很相似，但是它是用00:45:00.010 --> 00:45:05.215
在用词和句法结构上有很大的不同。00:45:05.215 --> 00:45:09.820
这就是进步，因为现在00:45:09.820 --> 00:45:14.815
我们可以用不同的矩阵来表示不同的组成类型。00:45:14.815 --> 00:45:22.015
但还是有理由认为我们没有足够的力量00:45:22.015 --> 00:45:28.225
我们仍然在使用这个非常简单的组成结构00:45:28.225 --> 00:45:34.960
我们只是把这两个子向量连接起来然后乘以一个矩阵。00:45:34.960 --> 00:45:37.885
这意味着这两个词，00:45:37.885 --> 00:45:41.470
它们之间没有相互作用。00:45:41.470 --> 00:45:49.450
但是，我们似乎想让它们按照它们的意思相互作用，对吧?00:45:49.450 --> 00:45:54.100
如果你仔细想想00:45:54.100 --> 00:45:59.305
人类语言和人们在语言语义上所看到的东西，00:45:59.305 --> 00:46:04.465
你得到的词似乎是修饰语或运算符。00:46:04.465 --> 00:46:06.970
所以这个词非常，00:46:06.970 --> 00:46:09.580
这本身没什么意义。00:46:09.580 --> 00:46:14.980
我的意思是，它的意思是加强，或更多，或类似的东西，00:46:14.980 --> 00:46:18.160
但是你知道，它并没有真正的意义，对吧?00:46:18.160 --> 00:46:20.050
它没有任何意义。00:46:20.050 --> 00:46:22.360
你不能给我看很多东西，对吧?00:46:22.360 --> 00:46:25.135
你可以给我看看椅子和笔00:46:25.135 --> 00:46:27.910
孩子们，但你不能给我看太多东西，00:46:27.910 --> 00:46:32.695
非常的意思似乎是后面有东西，很好。00:46:32.695 --> 00:46:39.580
这有一种算子的意思是在这个东西的规模上增加，00:46:39.580 --> 00:46:42.490
它可以在任何方向上增加。00:46:42.490 --> 00:46:45.115
你可以有很好的或很坏的。00:46:45.115 --> 00:46:49.990
所以如果我们想要捕捉这种语义，00:46:49.990 --> 00:46:53.425
似乎我们不能仅仅通过00:46:53.425 --> 00:46:58.330
将两个向量连接起来并将它们乘以一个矩阵。00:46:58.330 --> 00:47:04.300
看起来我们真正想说的是非常抓住00:47:04.300 --> 00:47:06.880
把握好和的意义00:47:06.880 --> 00:47:10.915
以某种方式修改它，产生一个新的意义，非常好。00:47:10.915 --> 00:47:15.130
事实上，这是一种典型的方法，00:47:15.130 --> 00:47:17.980
语言语义学。00:47:17.980 --> 00:47:20.530
所以在语义的语言学理论中，00:47:20.530 --> 00:47:22.120
你通常会说，00:47:22.120 --> 00:47:23.620
好是有意义的，00:47:23.620 --> 00:47:29.530
very是一个函数，它接受good的含义并返回very good的含义。00:47:29.530 --> 00:47:31.825
所以我们想要，00:47:31.825 --> 00:47:35.050
一种将其放入神经网络的方法。00:47:35.050 --> 00:47:40.750
因此，我们要尝试提出一个新的复合函数来解决这个问题。00:47:40.750 --> 00:47:44.530
有很多方法可以考虑00:47:44.530 --> 00:47:48.340
这样做，其他人也有过几次不同的尝试。00:47:48.340 --> 00:47:52.915
但本质上，我们脑子里想的是，00:47:52.915 --> 00:47:55.120
我们有字向量，00:47:55.120 --> 00:48:01.990
如果我们想说，非常接受好的意思，并返回一个新的意思，00:48:01.990 --> 00:48:05.410
最明显的做法就是说“非常”00:48:05.410 --> 00:48:08.830
加上一个矩阵因为我们可以用，00:48:08.830 --> 00:48:13.990
把这个矩阵和好的向量相乘得到一个新的，00:48:13.990 --> 00:48:16.360
向量出来了。00:48:16.360 --> 00:48:19.045
然后，00:48:19.045 --> 00:48:21.310
问题是00:48:21.310 --> 00:48:25.810
哪些单词有向量哪些单词有矩阵?00:48:25.810 --> 00:48:27.700
这有点，嗯，00:48:27.700 --> 00:48:30.190
很难知道答案。00:48:30.190 --> 00:48:32.485
我的意思是，特别是，00:48:32.485 --> 00:48:35.875
作为运算符的单词，00:48:35.875 --> 00:48:39.550
他们自己经常被修改。00:48:39.550 --> 00:48:44.305
所以你知道00:48:44.305 --> 00:48:49.680
good也可以是一个算子，对吧?00:48:49.680 --> 00:48:52.740
所以从一个人的角度来说，00:48:52.740 --> 00:48:56.115
你可以找一个好人，他也是一个操作员，00:48:56.115 --> 00:48:58.290
非常好。00:48:58.290 --> 00:49:03.460
所以我们的想法是，我们不要试图预先确定所有这些。00:49:03.460 --> 00:49:07.630
为什么我们不说每个词每个短语都有00:49:07.630 --> 00:49:12.385
同时与矩阵和向量相连。00:49:12.385 --> 00:49:15.175
这是我们的好电影。00:49:15.175 --> 00:49:16.645
对于每个单词，00:49:16.645 --> 00:49:19.945
我们有一个向量的意思，它也有一个矩阵的意思，00:49:19.945 --> 00:49:23.530
然后当我们开始建立一些短语，比如“非常好”00:49:23.530 --> 00:49:28.795
它们也有向量和矩阵的意义。00:49:28.795 --> 00:49:32.635
所以我们的建议是，00:49:32.635 --> 00:49:34.390
首先，00:49:34.390 --> 00:49:37.015
我们希望能够00:49:37.015 --> 00:49:40.765
来计算向量的意义。00:49:40.765 --> 00:49:47.005
所以算出一个短语的向量的意思很好。00:49:47.005 --> 00:49:49.540
每个单词都有一个矩阵的意思。00:49:49.540 --> 00:49:53.680
所以我们要把它们对立的矩阵和向量的意义结合起来。00:49:53.680 --> 00:49:56.860
我们取矩阵的意义00:49:56.860 --> 00:50:00.610
很好，乘以向量的意义。00:50:00.610 --> 00:50:03.910
我们要取非常和的矩阵意义00:50:03.910 --> 00:50:07.525
把它乘以向量“好”的意思。00:50:07.525 --> 00:50:11.185
所以我们会得到这两个东西。00:50:11.185 --> 00:50:17.620
然后我们会有一个像之前那样的神经网络层把它们结合起来。00:50:17.620 --> 00:50:19.540
这就是那个红色的盒子。00:50:19.540 --> 00:50:22.045
然后这两件事被连接起来，00:50:22.045 --> 00:50:25.840
通过我们之前讲过的神经网络层00:50:25.840 --> 00:50:30.235
最后一个向量，意思是这个短语。00:50:30.235 --> 00:50:34.675
然后我们还需要一个矩阵来表示这个短语的意思。00:50:34.675 --> 00:50:38.395
矩阵的意思是。00:50:38.395 --> 00:50:44.185
我们做了这种简单的模型实际上可能不是很好，00:50:44.185 --> 00:50:50.260
我们把两个矩阵连接起来，00:50:50.260 --> 00:50:53.560
组分，乘以它们00:50:53.560 --> 00:50:57.280
另一个矩阵，然后得到一个矩阵，00:50:57.280 --> 00:50:59.920
父节点的版本。00:50:59.920 --> 00:51:05.605
这就给了我们新的更复杂更强大的合成程序。00:51:05.605 --> 00:51:11.620
这看起来确实能做一些好事，00:51:11.620 --> 00:51:17.980
是一种运算符语义，一个词改变了另一个词的意思。00:51:17.980 --> 00:51:25.610
这是我们能用它做的一件很巧妙的事情。00:51:25.620 --> 00:51:30.040
我们希望能够解决这个问题00:51:30.040 --> 00:51:34.330
操作符修改另一个单词的语义。00:51:34.330 --> 00:51:40.450
令人难以置信地讨厌，令人难以置信地令人敬畏，令人难以置信地悲伤。00:51:40.450 --> 00:51:44.260
不烦人，不可怕，不悲伤。00:51:44.260 --> 00:51:48.340
(噪音)所以这是一个对比，00:51:48.340 --> 00:51:54.205
我们的旧模型和新模型。00:51:54.205 --> 00:51:58.345
这个量表是从正到负的。00:51:58.345 --> 00:52:03.210
所以这是完全负的到完全正的，对吧?00:52:03.210 --> 00:52:06.750
所以你会得到一种对比，00:52:06.750 --> 00:52:09.910
那是为了00:52:09.910 --> 00:52:15.290
简单的模型并不认为这是非常消极的，00:52:15.290 --> 00:52:19.070
尽管新模型认为这在意义上是相当中立的，00:52:19.070 --> 00:52:22.615
这似乎是正确的。00:52:22.615 --> 00:52:25.210
但不悲伤，00:52:25.210 --> 00:52:31.180
这意味着它有一点积极的意义，两个模型都试图捕捉到这一点，00:52:31.180 --> 00:52:34.775
你知道，这里的结果有点矛盾，00:52:34.775 --> 00:52:36.970
但是，看起来他们有点00:52:36.970 --> 00:52:40.105
朝着我们想要的方向。是的。00:52:40.105 --> 00:52:42.640
“不悲伤”这个例子的基本事实是什么?00:52:42.640 --> 00:52:45.745
哦,是的。所以这个基本事实00:52:45.745 --> 00:52:49.510
我们实际上让很多人说，00:52:49.510 --> 00:52:53.215
嗯，评价一下[笑声]不悲伤的意思，00:52:53.215 --> 00:52:55.210
从1到10。00:52:55.210 --> 00:52:58.390
也许这不是一个很清晰的任务因为你可以看到，00:52:58.390 --> 00:53:01.630
跳来跳去[笑声]00:53:01.630 --> 00:53:05.350
我们得到了什么样的评价。00:53:05.350 --> 00:53:08.230
但是，是的，这实际上是一种人类的判断。00:53:08.230 --> 00:53:13.464
我们也用这个，00:53:13.464 --> 00:53:15.220
模型会说，00:53:15.220 --> 00:53:18.910
我们能做语义分类吗?”00:53:18.910 --> 00:53:24.965
所以如果我们想了解不同名词短语之间的关系，00:53:24.965 --> 00:53:28.255
这是一个数据集，00:53:28.255 --> 00:53:32.695
两个名词短语之间有关系。00:53:32.695 --> 00:53:37.390
我的公寓有一个相当大的厨房，这被视为一个例子00:53:37.390 --> 00:53:43.840
一个成分-整体，两个名词短语之间关系的一部分，00:53:43.840 --> 00:53:49.210
不同种类的名词短语之间还有其他的关系。00:53:49.210 --> 00:53:52.240
如果是关于战争的电影，00:53:52.240 --> 00:53:54.535
那是一个信息主题，00:53:54.535 --> 00:53:59.455
所以有一些传播媒介包含一些主题关系。00:53:59.455 --> 00:54:01.930
所以我们用了这种00:54:01.930 --> 00:54:05.860
神经网络来构建我们的意义表征00:54:05.860 --> 00:54:06.940
然后把它们放进去00:54:06.940 --> 00:54:12.395
另一个神经网络层作为分类器，看看我们做得如何。00:54:12.395 --> 00:54:15.580
所以我们得到了一些相当好的结果。00:54:15.580 --> 00:54:18.970
这是一个人们研究过的数据集00:54:18.970 --> 00:54:24.070
传统的NLP系统采用不同的机器学习方法。00:54:24.070 --> 00:54:26.170
但在某种意义上，00:54:26.170 --> 00:54:29.710
我们感兴趣的是我们似乎正在取得进步00:54:29.710 --> 00:54:32.440
一个更好的语义组合系统00:54:32.440 --> 00:54:37.180
旧的递归神经网络得到了75%00:54:37.180 --> 00:54:40.255
而我们的新数据是79%00:54:40.255 --> 00:54:45.280
我们可以通过在我们的系统中添加更多的功能来进一步提高。00:54:45.280 --> 00:54:47.770
这就是进步，00:54:47.770 --> 00:54:50.815
但我们没有就此打住。00:54:50.815 --> 00:54:55.375
我们一直在努力想出更好的做事方法。00:54:55.375 --> 00:54:59.660
所以，即使这里的情况很好，00:54:59.660 --> 00:55:07.500
看起来这种做矩阵的方法并不一定很好。00:55:07.500 --> 00:55:09.675
它有两个问题。00:55:09.675 --> 00:55:15.910
一个问题是它引入了大量的参数，00:55:15.910 --> 00:55:19.630
你知道，对于我们所做的一切，00:55:19.630 --> 00:55:22.360
单词有一个向量，00:55:22.360 --> 00:55:28.090
也许有时候我们用高维向量，比如1024，00:55:28.090 --> 00:55:31.900
嗯，[噪音]但是，你知道，这是一个相对有限的参数。00:55:31.900 --> 00:55:35.305
然而一旦我们引入这个矩阵，00:55:35.305 --> 00:55:40.540
我们得到了每个单词的平方参数。00:55:40.540 --> 00:55:43.300
本质上是因为这个数00:55:43.300 --> 00:55:46.690
参数能够计算出这个模型，00:55:46.690 --> 00:55:49.180
我们让向量变小。00:55:49.180 --> 00:55:51.250
我们实际上用的是00:55:51.250 --> 00:55:55.570
只有25维向量所以25的平方，00:55:55.570 --> 00:56:01.495
仍然是安全的，在我们可以计算的范围内。00:56:01.495 --> 00:56:03.775
这是第一个问题。00:56:03.775 --> 00:56:05.485
第二个问题是，00:56:05.485 --> 00:56:08.620
我们没有很好的方法00:56:08.620 --> 00:56:13.210
建立更大短语的矩阵意义。00:56:13.210 --> 00:56:14.350
我是说，00:56:14.350 --> 00:56:17.830
这似乎是我们可以做的简单的事情，但它没有，00:56:17.830 --> 00:56:21.970
你知道，感觉这是一个很好的方式来获得一个短语的矩阵意义。00:56:21.970 --> 00:56:24.760
所以我们想用另一种方法来做00:56:24.760 --> 00:56:28.375
可以解决这两个问题。00:56:28.375 --> 00:56:33.940
然后，这就引出了递归神经张量网络的研究。00:56:33.940 --> 00:56:39.415
这里有一个关于神经张量的很好的想法，00:56:39.415 --> 00:56:44.590
这个想法在其他地方也得到了应用，00:56:44.590 --> 00:56:49.210
致力于知识图的向量嵌入等等，00:56:49.210 --> 00:56:51.550
这是个不错的主意。00:56:51.550 --> 00:56:55.570
所以我想展示一下这个模型是如何工作的。00:56:55.570 --> 00:56:58.930
但我想说的是，首先，00:56:58.930 --> 00:57:03.670
我们应用这个模型的一个地方是情绪分析问题。00:57:03.670 --> 00:57:08.650
现在，我想情绪分析这个词已经出现过几次了00:57:08.650 --> 00:57:13.720
实际上我在上节课提到过。00:57:13.720 --> 00:57:17.320
但我想我们从未真正交谈过五分钟00:57:17.320 --> 00:57:19.450
在情感分析这门课上，00:57:19.450 --> 00:57:22.645
我给你们举个例子。00:57:22.645 --> 00:57:24.910
情感分析实际上00:57:24.910 --> 00:57:30.940
在自然语言处理中非常常见和重要的应用。00:57:30.940 --> 00:57:34.705
你看着一段文字，你会说，00:57:34.705 --> 00:57:37.810
“是积极的还是消极的?”00:57:37.810 --> 00:57:42.160
这对很多人都很有用00:57:42.160 --> 00:57:46.955
看产品评论或做品牌的商业应用，00:57:46.955 --> 00:57:51.530
意识和诸如此类的东西，观察与事物相关的情感。00:57:51.530 --> 00:57:55.540
在某种程度上做情绪分析很简单，对吧?00:57:55.540 --> 00:57:57.220
你可以说，00:57:57.220 --> 00:57:58.840
“嗯，看一段文字。00:57:58.840 --> 00:58:00.700
如果你看到像“爱”这样的话，00:58:00.700 --> 00:58:03.265
很好，令人印象深刻，不可思议，然后是积极的。00:58:03.265 --> 00:58:04.570
这是一个积极的评价。00:58:04.570 --> 00:58:06.880
如果它说，糟糕透顶，00:58:06.880 --> 00:58:08.440
然后就是负面评论。”00:58:08.440 --> 00:58:13.420
在某种程度上，这是你可以使用的情绪分析的基线00:58:13.420 --> 00:58:18.805
只需要选择单词特性，或者将所有单词放在一个单词包中。00:58:18.805 --> 00:58:20.035
如果你这样做了，00:58:20.035 --> 00:58:22.780
你其实做得并不差，00:58:22.780 --> 00:58:25.150
在情绪分析方面。00:58:25.150 --> 00:58:26.650
如果你有更长的文档，00:58:26.650 --> 00:58:31.135
在情绪分析中，仅仅看一堆词就能给你90%的答案。00:58:31.135 --> 00:58:32.335
但另一方面，00:58:32.335 --> 00:58:35.140
事情往往会变得更棘手，对吧?00:58:35.140 --> 00:58:38.020
这个来自烂番茄。00:58:38.020 --> 00:58:40.450
有了这个演员阵容和主题，00:58:40.450 --> 00:58:43.480
这部电影应该更有趣，更有趣。00:58:43.480 --> 00:58:47.004
如果你假装自己是一个文字袋模型，00:58:47.004 --> 00:58:52.840
这里面唯一明显带有感情色彩的词，00:58:52.840 --> 00:58:57.969
这两个词都很积极，00:58:57.969 --> 00:59:04.615
但是很明显，这实际上是对这部电影的一个不好的评价。00:59:04.615 --> 00:59:07.360
那么，我们怎么知道呢?00:59:07.360 --> 00:59:11.320
看起来我们要做的就是定义合成。00:59:11.320 --> 00:59:15.310
我们需要一些类似“本该如此”的短语00:59:15.310 --> 00:59:21.070
然后意识到这实际上是一个短语的否定意义。00:59:21.070 --> 00:59:25.690
所以我们想探索如何看待这些意义00:59:25.690 --> 00:59:33.310
短语和探索建立这些意义作为在树的意义组成。00:59:33.310 --> 00:59:36.400
我们做的第一件事，00:59:36.400 --> 00:59:42.490
我们建立了一个情感树库让人们给情感打分。00:59:42.490 --> 00:59:45.910
这就引出了斯坦福情感树银行，00:59:45.910 --> 00:59:49.675
这仍然是一个你经常看到的数据集，00:59:49.675 --> 00:59:54.280
用一大堆数据集做各种各样的计算。的确,00:59:54.280 --> 00:59:57.175
它出现在上周的《decaNLP》杂志上。00:59:57.175 --> 01:00:00.545
我们要做的是，01:00:00.545 --> 01:00:06.265
嗯，电影里烂番茄的句子。01:00:06.265 --> 01:00:13.450
我们对它们进行解析，给出树的结构，然后我们要求机械土耳其人01:00:13.450 --> 01:00:16.745
评价不同的phra-不同的单词和01:00:16.745 --> 01:00:21.460
从非常积极到非常消极的情绪量表。01:00:21.460 --> 01:00:25.660
所以很多东西都是白色的，因为它没有感情色彩，对吧?01:00:25.660 --> 01:00:27.575
有些词是，01:00:27.575 --> 01:00:29.710
还有像电影和01:00:29.710 --> 01:00:33.325
这部电影没有任何感情，01:00:33.325 --> 01:00:37.180
但是你有一些非常积极的部分01:00:37.180 --> 01:00:42.025
然后用蓝色和红色表示树的负部分。01:00:42.025 --> 01:00:45.265
在情感数据集中，01:00:45.265 --> 01:00:49.720
人们只是给整句话贴上标签说，01:00:49.720 --> 01:00:53.140
“这是一个积极的句子或非常积极的句子。01:00:53.140 --> 01:00:55.840
这是一个否定句，或者是一个非常否定的句子。”01:00:55.840 --> 01:01:01.810
重要的是，我们在这里所做的不同之处在于句子中的每个短语01:01:01.810 --> 01:01:08.170
根据我们的树结构我们给它的积极或消极的标签。01:01:08.170 --> 01:01:11.080
也许并不奇怪，01:01:11.080 --> 01:01:14.990
事实上你有很多这样的注解，01:01:14.990 --> 01:01:20.400
只是改进了分类器的行为，因为你可以01:01:20.400 --> 01:01:26.735
更好地辨别句子中哪些词是肯定的，哪些词是否定的。嗯。01:01:26.735 --> 01:01:32.810
这些是之前模型的结果。01:01:32.810 --> 01:01:39.650
所以绿色是一个朴素的贝叶斯模型除了它不仅使用单独的单词，01:01:39.650 --> 01:01:41.630
但是它使用成对的单词。01:01:41.630 --> 01:01:45.590
如果你在构建一个传统的分类器01:01:45.590 --> 01:01:49.940
想要做情感分析而不是像主题分类，01:01:49.940 --> 01:01:54.500
如果您还使用单词对功能，您会得到更好的结果。01:01:54.500 --> 01:01:58.850
那是因为它为你做了一点点的。01:01:58.850 --> 01:02:01.659
你不仅有不有趣的功能，01:02:01.659 --> 01:02:03.610
但是你可以有一个非的特性01:02:03.610 --> 01:02:07.030
有趣的是，这让你可以建模一些东西。01:02:07.030 --> 01:02:10.630
这是我们上一代的神经网络，01:02:10.630 --> 01:02:14.935
我们的原始树结构神经网络和我们的矩阵向量1。01:02:14.935 --> 01:02:19.070
对于这些固定的模型，01:02:19.070 --> 01:02:23.810
只要我们有来自新树银行更丰富的监管，01:02:23.810 --> 01:02:26.330
它提高了每个模型的性能。01:02:26.330 --> 01:02:29.450
所以即使只是，01:02:29.450 --> 01:02:33.695
朴素贝叶斯模型的性能提高了4%01:02:33.695 --> 01:02:35.940
因为事实是，01:02:35.940 --> 01:02:38.405
现在它知道了更多01:02:38.405 --> 01:02:42.005
特定的词在句子中是肯定的或否定的。01:02:42.005 --> 01:02:47.075
但是这些表演都不是很好。01:02:47.075 --> 01:02:53.120
所以我们仍然认为我们可以建立更好的模型来做这件事。01:02:53.120 --> 01:02:56.390
尤其是，如果你用01:02:56.390 --> 01:02:59.450
各种各样的否定，01:02:59.450 --> 01:03:01.505
比如应该更有趣，01:03:01.505 --> 01:03:06.170
一般来说，这些模型仍然不能为它们捕获正确的含义。01:03:06.170 --> 01:03:11.600
这就引出了我们的第四个模型，01:03:11.600 --> 01:03:15.965
这就是递归神经张量网络的概念。01:03:15.965 --> 01:03:22.550
所以我们想要做的就是回到，01:03:22.550 --> 01:03:26.660
单词的意义是向量，01:03:26.660 --> 01:03:30.335
但是尽管如此，还是能够01:03:30.335 --> 01:03:34.355
有一个有意义的短语，其中两个向量，01:03:34.355 --> 01:03:36.140
相互作用。01:03:36.140 --> 01:03:39.245
你知道，这种，01:03:39.245 --> 01:03:41.810
这是我们当时的情况01:03:41.810 --> 01:03:44.615
以一种双线的方式来处理注意力，对吧?01:03:44.615 --> 01:03:46.625
我们有两个单词的向量。01:03:46.625 --> 01:03:50.330
我们在中间插入一个矩阵，然后用01:03:50.330 --> 01:03:54.775
然后给予关注，得到了关注分数。01:03:54.775 --> 01:03:59.245
让这两个向量相互作用，01:03:59.245 --> 01:04:02.635
但它只产生一个数字作为输出。01:04:02.635 --> 01:04:04.630
但有一种方法可以解决这个问题，01:04:04.630 --> 01:04:10.295
也就是说，这里不是一个矩阵，01:04:10.295 --> 01:04:14.555
我们可以在这里画一个三维立方体，01:04:14.555 --> 01:04:19.220
现在物理学家和深度学习的人称之为张量，对吧?01:04:19.220 --> 01:04:22.550
张量就是更高的多维数组，01:04:22.550 --> 01:04:24.560
在计算机科学术语中。01:04:24.560 --> 01:04:28.595
如果我们把它变成一个张量，01:04:28.595 --> 01:04:33.035
你知道，就像我们这里有多层矩阵。01:04:33.035 --> 01:04:38.795
所以最终的结果是我们在这里得到一个数，在这里也得到一个数。01:04:38.795 --> 01:04:42.979
总的来说，我们得到一个大小为2的向量，01:04:42.979 --> 01:04:46.355
在我的婴儿例子中我们需要什么呢01:04:46.355 --> 01:04:50.300
婴儿的例子，我们只有这两个组成向量的单词。01:04:50.300 --> 01:04:52.250
但是一般来说，我们有一个张量01:04:52.250 --> 01:04:55.835
额外提到的向量大小的维数。01:04:55.835 --> 01:04:58.985
因此，我们会得到一个向量w帽，01:04:58.985 --> 01:05:03.800
我们会从合成中得到一个短语向量它的大小是一样的01:05:03.800 --> 01:05:06.980
输入向量01:05:06.980 --> 01:05:12.450
在理解整个事物的意义时相互作用。01:05:12.910 --> 01:05:17.390
好。嗯,好吧。01:05:17.390 --> 01:05:19.220
所以在那个时候，01:05:19.220 --> 01:05:23.250
我们用得到的向量，01:05:24.610 --> 01:05:28.265
我们有了神经张量网络。01:05:28.265 --> 01:05:33.695
我们把它和之前的图层结合起来，01:05:33.695 --> 01:05:37.310
我们的第一个RNN，也许你不需要这么做，01:05:37.310 --> 01:05:39.485
但是我们决定把它也加进去，01:05:39.485 --> 01:05:42.470
把东西放到非线性中，然后01:05:42.470 --> 01:05:45.770
给我们新的短语表示。01:05:45.770 --> 01:05:49.190
我们把它搭在树上，最后，01:05:49.190 --> 01:05:53.120
我们可以对任何短语的意思进行分类，01:05:53.120 --> 01:05:56.900
用同样的方法用softmax回归，我们可以01:05:56.900 --> 01:06:00.585
用梯度下降训练这些权重来预测情绪。01:06:00.585 --> 01:06:03.910
所以这实际上运行得很好。01:06:03.910 --> 01:06:05.245
我的意思是，01:06:05.245 --> 01:06:09.820
仅仅使用句子标签并没有什么效果。01:06:09.820 --> 01:06:13.194
但是如果我们用树桩训练模型，01:06:13.194 --> 01:06:15.820
然后我们可以得到一种01:06:15.820 --> 01:06:18.700
这是另外几个百分点的表现，01:06:18.700 --> 01:06:20.575
这看起来不错。01:06:20.575 --> 01:06:21.880
特别地，01:06:21.880 --> 01:06:26.920
它似乎在理解意义构成方面做得更好。01:06:26.920 --> 01:06:32.095
这是一种句子，有一些缓慢重复的部分，01:06:32.095 --> 01:06:35.245
但是它有足够的香料来保持它的趣味性。01:06:35.245 --> 01:06:38.470
这里看到的模型非常善于理解。01:06:38.470 --> 01:06:41.400
好的，这部分句子是否定的，01:06:41.400 --> 01:06:43.880
这部分句子是肯定的，01:06:43.880 --> 01:06:46.310
实际上当你把这两部分粘在一起，01:06:46.310 --> 01:06:50.450
最终的结果是一个意义积极的句子。01:06:50.450 --> 01:06:54.170
但要多关注一些表面现象01:06:54.170 --> 01:06:58.610
这是第一次特别好01:06:58.610 --> 01:07:02.270
看起来它能做得更好吗01:07:02.270 --> 01:07:07.220
计算出当你做否定的时候会发生什么。01:07:07.220 --> 01:07:11.975
所以我们现在看到的是非常枯燥的而且绝对不是枯燥的。01:07:11.975 --> 01:07:14.000
所以如果它绝对不沉闷，01:07:14.000 --> 01:07:16.130
这实际上意味着它很好，对吧?01:07:16.130 --> 01:07:20.480
我们能理解它的意思吗?01:07:20.480 --> 01:07:25.610
这些，这是01:07:25.610 --> 01:07:31.505
显示当你有一个负号时，会发生什么，01:07:31.505 --> 01:07:34.790
一个否定句被进一步否定。01:07:34.790 --> 01:07:39.890
如果你从，01:07:39.890 --> 01:07:42.305
如果你这样做了01:07:42.305 --> 01:07:48.710
附件-否定一个否定的东西应该变成适度的积极，对吧?01:07:48.710 --> 01:07:54.065
所以如果你有沉闷是消极的如果你说不沉闷，01:07:54.065 --> 01:07:56.105
这并不意味着它很棒，01:07:56.105 --> 01:07:58.400
但这意味着它是适度正的。01:07:58.400 --> 01:08:04.070
对于朴素贝叶斯模型或之前的模型，01:08:04.070 --> 01:08:09.755
他们无法捕捉到那种从沉闷到不沉闷的变化，01:08:09.755 --> 01:08:14.135
你的意义计算结果不再是正的了。01:08:14.135 --> 01:08:17.000
而这种神经张量网络01:08:17.000 --> 01:08:22.470
抓住不枯燥这一事实意味着它相当不错。01:08:22.750 --> 01:08:26.960
这就是进步。嗯,是的。01:08:26.960 --> 01:08:31.460
我想这就是我要向你们展示的关于申请的全部内容01:08:31.460 --> 01:08:37.590
这些树状结构的神经网络，自然语言。01:08:37.810 --> 01:08:43.190
我想我在开头说过的总结是01:08:43.190 --> 01:08:48.275
你知道，它们是一些有趣的想法和语言联系。01:08:48.275 --> 01:08:52.325
我的意思是，由于种种原因，01:08:52.325 --> 01:08:55.100
这些想法并没有，01:08:55.100 --> 01:08:59.570
近年来进行了大量的自然语言处理。01:08:59.570 --> 01:09:04.610
一个是人们发现，01:09:04.610 --> 01:09:08.090
一旦有了高维向量01:09:08.090 --> 01:09:11.480
在我们研究过的序列模型中，01:09:11.480 --> 01:09:15.980
不管是LSTM模型之类的01:09:15.980 --> 01:09:21.200
最近的语境语言模型，效果非常好，01:09:21.200 --> 01:09:24.890
目前还不清楚这些模型的整体效果是否更好。01:09:24.890 --> 01:09:28.399
第二个原因是计算上的原因，01:09:28.399 --> 01:09:35.495
也就是说，当你在做统一计算的时候gpu工作得很好。01:09:35.495 --> 01:09:40.130
序列模型的美妙之处在于，01:09:40.130 --> 01:09:43.595
你只需要做一个行列式的计算01:09:43.595 --> 01:09:47.180
在卷积神经网络中，01:09:47.180 --> 01:09:49.010
有一个行列式，01:09:49.010 --> 01:09:51.530
你正在做的计算，01:09:51.530 --> 01:09:54.170
通过卷积层，01:09:54.170 --> 01:09:58.805
事物可以在GPU上有效地表示和计算。01:09:58.805 --> 01:10:03.020
这类模型的最大问题是你的计算是什么01:10:03.020 --> 01:10:07.475
这取决于你给句子分配的结构，01:10:07.475 --> 01:10:12.110
每个句子都有不同的结构，因此，01:10:12.110 --> 01:10:15.200
无法将计算批处理到一组01:10:15.200 --> 01:10:18.860
语句，并进行相同的计算01:10:18.860 --> 01:10:22.310
不同的句子，会削弱这种能力01:10:22.310 --> 01:10:26.365
在大范围内高效地建立这些模型。01:10:26.365 --> 01:10:31.145
我想我应该在最后说一下。01:10:31.145 --> 01:10:36.195
有趣的是，虽然这些并没有被广泛使用，01:10:36.195 --> 01:10:39.250
在过去的几年里01:10:39.250 --> 01:10:45.650
他们在不同的地方发现了不同的应用，01:10:45.650 --> 01:10:48.215
嗯，这看起来有点可爱。01:10:48.215 --> 01:10:52.850
这实际上是物理学的一个应用。01:10:52.850 --> 01:10:56.020
我想我要读一下这个01:10:56.020 --> 01:10:58.890
所以我不知道一半的单词是什么意思。01:10:58.890 --> 01:11:04.295
但是，到目前为止，它说的是在碰撞中最常见的结构01:11:04.295 --> 01:11:10.295
大型强子对撞机是高能强子的准直喷射，称为喷流。01:11:10.295 --> 01:11:14.135
这些射流是由氢离子的碎片化和强化氢生成的01:11:14.135 --> 01:11:19.200
量子色动力学中描述的夸克和胶子。01:11:19.200 --> 01:11:21.420
有人知道这是什么意思吗?01:11:21.420 --> 01:11:23.550
我希望你能跟上。01:11:23.550 --> 01:11:26.970
一个引人注目的物理挑战是寻找01:11:26.970 --> 01:11:32.000
高强度的标准模型粒子强子衰变。01:11:32.000 --> 01:11:36.935
不幸的是，有一个很大的背景来自于更平凡的飞机，01:11:36.935 --> 01:11:41.090
QCD，这是量子色动力学过程。01:11:41.090 --> 01:11:44.610
在这项工作中，我们提出了一个解决方案01:11:44.610 --> 01:11:48.215
基于射流分类之间的类比01:11:48.215 --> 01:11:52.470
量子色动力学和自然语言的启发01:11:52.470 --> 01:11:56.775
自然语言的一些作品，嗯，加工。01:11:56.775 --> 01:11:59.520
就像句子是由单词组成的一样01:11:59.520 --> 01:12:02.865
按照语法结构组织成一个解析树，01:12:02.865 --> 01:12:08.050
射流也由4动量组成，其结构由01:12:08.050 --> 01:12:09.760
QCD并通过01:12:09.760 --> 01:12:14.100
序列共组合射流算法的聚类历史。01:12:14.100 --> 01:12:18.005
嗯，不管怎样，嗯，是的，你可以看到这些喷射流01:12:18.005 --> 01:12:23.794
上面有一个树形结构他们使用树形递归神经网络，01:12:23.794 --> 01:12:25.100
嗯，建模。01:12:25.100 --> 01:12:31.435
这有点离题太远了，但再举一个例子，01:12:31.435 --> 01:12:35.320
另一个地方，这些模型实际上是相当01:12:35.320 --> 01:12:39.840
有用的是用编程语言做事情。01:12:39.840 --> 01:12:42.020
我认为在某种程度上，01:12:42.020 --> 01:12:46.545
这是因为应用程序在编程语言中更容易。01:12:46.545 --> 01:12:51.150
所以不像在自然语言中我们对什么是有不确定性01:12:51.150 --> 01:12:55.775
正确的解析树因为在自然语言中有很多歧义，01:12:55.775 --> 01:12:58.295
在编程语言中，01:12:58.295 --> 01:13:01.175
解析树实际上是很好的行列式。01:13:01.175 --> 01:13:07.560
伯克利的一群人，唐恩・宋和她的学生一直在做01:13:07.560 --> 01:13:10.870
编译程序语言01:13:10.870 --> 01:13:14.490
树递归神经网络编解码器。01:13:14.490 --> 01:13:17.375
这样就建立了一个树形结构01:13:17.375 --> 01:13:22.120
用一种语言表示程序的神经网络。01:13:22.120 --> 01:13:26.345
这是一个CoffeeScript程序，然后你想要构建一个树01:13:26.345 --> 01:13:31.760
树模型，然后将其转换成另一种语言的程序。01:13:31.760 --> 01:13:35.150
他们已经做到了，并且取得了很好的效果。01:13:35.150 --> 01:13:38.760
我懒得重新打这张桌子。01:13:38.760 --> 01:13:40.610
所以这可能有点，01:13:40.610 --> 01:13:42.010
有点难读。01:13:42.010 --> 01:13:46.320
但与之形成对比的是对于一些项目来说这是01:13:46.320 --> 01:13:51.650
从CoffeeScript到JavaScript，呃，呃，翻译。01:13:51.650 --> 01:13:54.980
他们正在比较使用树和树模型。01:13:54.980 --> 01:13:57.130
然后用序列对序列01:13:57.130 --> 01:14:00.120
然后他们尝试了其他两种组合，01:14:00.120 --> 01:14:03.345
顺序到树，树到顺序。01:14:03.345 --> 01:14:06.215
他们发现你可以得到最好的01:14:06.215 --> 01:14:10.175
结果建立了树对树的神经网络模型。01:14:10.175 --> 01:14:13.665
特别是这些树对树的模型01:14:13.665 --> 01:14:17.345
随着注意力的增强，他们就像我们说过的那样拥有注意力01:14:17.345 --> 01:14:21.490
序列到序列模型，然后你就可以把注意力回到01:14:21.490 --> 01:14:26.990
树结构中的节点这是一种很自然的转换方式。01:14:26.990 --> 01:14:31.320
这些结果表明如果你没有这些结果01:14:31.320 --> 01:14:36.035
如果你没有注意力操作，它就会完全不起作用。01:14:36.035 --> 01:14:37.680
太难了，01:14:37.680 --> 01:14:39.060
去拿东西01:14:39.060 --> 01:14:41.050
如果你只是想创造01:14:41.050 --> 01:14:45.810
一个单一的树表示，然后生成转换-从那个翻译。01:14:45.810 --> 01:14:48.245
但如果你能把注意力放在这上面01:14:48.245 --> 01:14:51.735
进入不同的模式，嗯，很好。01:14:51.735 --> 01:14:56.385
如果你知道什么是CoffeeScript01:14:56.385 --> 01:14:58.740
有点像作弊，因为01:14:58.740 --> 01:15:02.125
CoffeeScript有点像JavaScript。01:15:02.125 --> 01:15:03.725
但是他们也01:15:03.725 --> 01:15:05.310
用其他语言写。01:15:05.310 --> 01:15:11.090
这是介于Java和c#之间的01:15:11.090 --> 01:15:14.490
手写的Java到c#转换器，你可以01:15:14.490 --> 01:15:18.820
如果你想的话，可以从GitHub上下载，但它实际上并没有那么好用。01:15:18.820 --> 01:15:20.570
他们能够证明，01:15:20.570 --> 01:15:23.120
他们能够建立一个更好的，01:15:23.120 --> 01:15:25.580
Java到c#转换器，01:15:25.580 --> 01:15:28.080
嗯,这样做。01:15:28.080 --> 01:15:30.390
这其实挺酷的。01:15:30.390 --> 01:15:33.110
知道树状递归神经网络是件好事01:15:33.110 --> 01:15:34.905
对某些东西有好处。01:15:34.905 --> 01:15:37.820
我很高兴看到这样的工作。01:15:37.820 --> 01:15:41.135
好。我快做完了，但我想，01:15:41.135 --> 01:15:43.515
在结束之前01:15:43.515 --> 01:15:47.135
我只想提到另一件(噪音)事情，那就是什么都不做01:15:47.135 --> 01:15:50.850
用自然语言精确地处理，但它是关于人工智能的。01:15:50.850 --> 01:15:54.570
但是我想做一些广告。01:15:54.570 --> 01:15:57.335
我们很多人都有过这样的经历01:15:57.335 --> 01:16:00.855
努力工作了一年左右，01:16:00.855 --> 01:16:06.710
正在开发一个新的斯坦福以人为中心的人工智能研究所。01:16:06.710 --> 01:16:11.880
实际上，这个研究所的成立将在考试周的周一，01:16:11.880 --> 01:16:16.365
当你最大限度地专注于这样的事情时。01:16:16.365 --> 01:16:19.680
但是我们希望我们可以有很多01:16:19.680 --> 01:16:23.495
围绕人工智能的新活动，01:16:23.495 --> 01:16:27.510
从更广阔的角度来看人工智能，01:16:27.510 --> 01:16:34.770
也就是从人类的角度来审视它，01:16:34.770 --> 01:16:38.080
我会探讨更广泛的问题01:16:38.080 --> 01:16:41.490
是否抱着大学其他很多人的兴趣01:16:41.490 --> 01:16:45.100
它是社会科学和人文学科，或者多种多样01:16:45.100 --> 01:16:48.945
在像法学院和商学院这样的专业学校。01:16:48.945 --> 01:16:52.760
让我们快速地说一下。01:16:52.760 --> 01:16:58.980
在我生命的大部分时间里，我的想法都是激励人心的01:16:58.980 --> 01:17:01.270
人工智能似乎是一种01:17:01.270 --> 01:17:03.670
一个有趣的智力探索01:17:03.670 --> 01:17:06.420
你是否可以编写一些软件来做任何事情，01:17:06.420 --> 01:17:10.245
有一半的智商但很明显不是这样的01:17:10.245 --> 01:17:12.820
接下来25年将会发生什么。01:17:12.820 --> 01:17:14.830
我们现在在01:17:14.830 --> 01:17:19.680
人工智能系统正在向社会释放。01:17:19.680 --> 01:17:23.730
希望他们能做些好事，但就像我们一样01:17:23.730 --> 01:17:26.120
越来越多的人看到了01:17:26.120 --> 01:17:29.070
也有很多机会让他们做坏事。01:17:29.070 --> 01:17:32.510
即使我们没有想象终结者的场景，01:17:32.510 --> 01:17:35.530
有很多地方人们都在使用01:17:35.530 --> 01:17:39.545
机器学习和用于决策的人工智能算法。01:17:39.545 --> 01:17:42.570
其中最糟糕的是量刑指南01:17:42.570 --> 01:17:46.950
法庭上有非常有偏见的算法做出错误的决定01:17:46.950 --> 01:17:51.425
人们开始越来越意识到这些问题01:17:51.425 --> 01:17:54.075
实际上，我们想要这个研究所01:17:54.075 --> 01:17:56.940
包括很多社会科学家的工作，01:17:56.940 --> 01:18:01.420
伦理学家和其他人实际上是在探索如何拥有人工智能01:18:01.420 --> 01:18:06.030
这确实在改善人类生活，而不是产生相反的效果。01:18:06.030 --> 01:18:07.860
所以这三个主题，01:18:07.860 --> 01:18:09.390
嗯，我们，嗯，01:18:09.390 --> 01:18:15.870
这个研究所主要强调的是左上角的第一个01:18:15.870 --> 01:18:19.329
开发人工智能技术，但我们尤其如此01:18:19.329 --> 01:18:22.770
对与人类智能的联系很感兴趣。01:18:22.770 --> 01:18:25.575
认知科学和神经科学01:18:25.575 --> 01:18:28.925
那时候很多早期的人工智能成型工作01:18:28.925 --> 01:18:31.410
包括所有01:18:31.410 --> 01:18:35.330
神经网络的早期工作，比如反向传播的发展，01:18:35.330 --> 01:18:38.400
这实际上很大程度上是在认知科学的背景下完成的。01:18:38.400 --> 01:18:42.090
对吧?这是一种容易迷失的联系01:18:42.090 --> 01:18:47.140
90年代和2000年代的统计机器学习重点。01:18:47.140 --> 01:18:49.030
我认为更新它会很好。01:18:49.030 --> 01:18:51.240
右上角，01:18:51.240 --> 01:18:53.910
人们更加关注01:18:53.910 --> 01:18:59.310
人工智能对人类和社会的影响这是一个法律问题，01:18:59.310 --> 01:19:01.920
经济问题，劳动力，01:19:01.920 --> 01:19:05.905
道德，绿色能源，政治，不管你是什么。01:19:05.905 --> 01:19:09.520
但在底部是它看起来的样子01:19:09.520 --> 01:19:13.725
有很多机会可以做得更多，01:19:13.725 --> 01:19:18.825
我们怎样才能创造出真正能提高人类生活质量的技术。01:19:18.825 --> 01:19:25.860
就像某种程度上的科技――我们有人工智能技术来增加人类的生活。01:19:25.860 --> 01:19:29.200
现在你所有的手机都有语音识别功能。01:19:29.200 --> 01:19:31.235
这就是AI，01:19:31.235 --> 01:19:34.055
这可以增加，嗯，你的人类生活。01:19:34.055 --> 01:19:37.205
但从某种意义上说，并没有太多01:19:37.205 --> 01:19:43.055
人工智能实际上已经被用于增加人类的生活。01:19:43.055 --> 01:19:45.840
就像手机上的大部分东西一样01:19:45.840 --> 01:19:48.160
有点聪明可爱的东西01:19:48.160 --> 01:19:51.270
HCI的员工和设计师都非常好01:19:51.270 --> 01:19:55.470
你使用地图程序的时间，但我们没有01:19:55.470 --> 01:20:00.455
这些设备中的许多人工智能有助于改善人们的生活。01:20:00.455 --> 01:20:05.880
所以我们希望不仅仅是个人，01:20:05.880 --> 01:20:08.240
做更多的推杆01:20:08.240 --> 01:20:12.420
将人工智能应用到以人为中心的应用中。01:20:12.420 --> 01:20:14.755
总之，这就是我的简短广告。01:20:14.755 --> 01:20:17.840
嗯，在你不准备考试的时候要注意这个。01:20:17.840 --> 01:20:20.700
我认为会有很多机会，01:20:20.700 --> 01:20:24.315
让学生和其他人在接下来的几个月里更多地参与进来。01:20:24.315 --> 01:20:26.290
好。非常感谢。01:20:26.290 --> 01:20:37.290
我们待会儿见，在海报环节。01:20:37.290 --> 01:20:37.400
[APPLAUSE].

