WEBVTT
Kind: captions
Language: en

00:00:04.160 --> 00:00:07.470可以。大家好。00:00:07.470 --> 00:00:10.410可以。我们开始吧。00:00:10.410 --> 00:00:12.480嗯-很高兴在这里见到你们。00:00:12.480 --> 00:00:17.385欢迎回到CS224N的第二周。00:00:17.385 --> 00:00:21.345嗯-所以-这是一个小预览00:00:21.345 --> 00:00:25.800本周和下周的课程内容。00:00:25.800 --> 00:00:29.790嗯-你知道，这周可能是这个班最糟糕的一周。00:00:29.790 --> 00:00:36.435[笑声]。所以在第二周的课上，我们希望00:00:36.435 --> 00:00:43.360了解一些神经网络的基本情况以及它们的训练方式，00:00:43.360 --> 00:00:48.795以及如何通过反向传播来学习好的神经网络，00:00:48.795 --> 00:00:52.820这意味着我们要特别谈谈训练00:00:52.820 --> 00:00:58.100算法和做微积分，从证明中算出梯度。00:00:58.100 --> 00:01:01.580嗯，所以我们要找一个比-一点，00:01:01.580 --> 00:01:07.130在-um-um，单词窗口分类命名为实体识别。00:01:07.130 --> 00:01:12.290所以这里有一些自然语言处理，但基本上，00:01:12.290 --> 00:01:15.290有点像第二周，00:01:15.290 --> 00:01:18.440嗯-深度学习的数学和00:01:18.440 --> 00:01:23.045神经网络模型和一些真正的神经网络基础。00:01:23.045 --> 00:01:26.130嗯，但希望这能给你带来好感00:01:26.130 --> 00:01:29.740很好地理解这些东西是如何工作的，00:01:29.740 --> 00:01:32.570我们会给你所有你需要做的信息，00:01:32.570 --> 00:01:35.240嗯-接下来的家庭作业，然后，00:01:35.240 --> 00:01:38.280在第三周，我们有点轻率。00:01:38.280 --> 00:01:42.110所以，第三周主要是00:01:42.110 --> 00:01:45.020自然语言处理，然后我们将讨论00:01:45.020 --> 00:01:48.230把句法结构放在句子上，00:01:48.230 --> 00:01:51.215用于构建句子的依赖性分析00:01:51.215 --> 00:01:54.260这就是作业三中用到的。00:01:54.260 --> 00:01:56.060所以我们很快就走了。00:01:56.060 --> 00:01:59.210然后我们来讨论一下00:01:59.210 --> 00:02:02.870引入神经语言模型的句子。00:02:02.870 --> 00:02:05.640嗯-家庭作业。00:02:05.640 --> 00:02:09.665作业一大约两分钟前就要交了，00:02:09.665 --> 00:02:15.190嗯-所以我希望每个人都提交了作业，我的意思是，00:02:15.190 --> 00:02:18.570嗯-只是一种警告，00:02:18.570 --> 00:02:21.950嗯-一般来说，你知道家庭作业，我们00:02:21.950 --> 00:02:25.550希望你找到了一个很好的热身，不要太用力。00:02:25.550 --> 00:02:29.059所以最好尽快拿到作业00:02:29.059 --> 00:02:32.690而不是浪费很多时间做作业。00:02:32.690 --> 00:02:35.419嗯，现在在网站上，00:02:35.419 --> 00:02:37.160嗯，有家庭作业二。00:02:37.160 --> 00:02:39.895嗯，所以，我们一直在努力。00:02:39.895 --> 00:02:44.025所以家庭作业两种相当于本周的讲座。00:02:44.025 --> 00:02:47.430所以在第一部分，我们希望你00:02:47.430 --> 00:02:52.335钻研一些计算梯度导数的数学问题。00:02:52.335 --> 00:02:55.970嗯-然后第二部分就是实现00:02:55.970 --> 00:02:59.480你自己版本的word2vec使用了numpy。00:02:59.480 --> 00:03:02.480所以这次是编写一个python程序。00:03:02.480 --> 00:03:04.555它不再是ipython笔记本。00:03:04.555 --> 00:03:07.575嗯，我鼓励你早点，00:03:07.575 --> 00:03:10.545嗯-看看材料，00:03:10.545 --> 00:03:11.880嗯-在网上。00:03:11.880 --> 00:03:15.635我的意思是，特别是今天的讲座，00:03:15.635 --> 00:03:18.980嗯-一些很好的教程材料00:03:18.980 --> 00:03:22.250在网站上，所以也鼓励你去看看那些。00:03:22.250 --> 00:03:23.520[噪音]。00:03:23.520 --> 00:03:24.650嗯-一般来说，00:03:24.650 --> 00:03:27.950只是想对事情多做些评论。00:03:27.950 --> 00:03:32.990我是说，我想斯坦福的很多课程都是这样的，但是，00:03:32.990 --> 00:03:37.460你知道，当我们得到这门课的课程评论时，我们总是00:03:37.460 --> 00:03:42.710全谱的人都说这个班很糟糕，而且工作太多，00:03:42.710 --> 00:03:45.050嗯-对于那些说这是一个非常棒的班级的人来说，00:03:45.050 --> 00:03:47.180他们在斯坦福最喜欢的课程之一，00:03:47.180 --> 00:03:49.520很明显教员很关心，等等。00:03:49.520 --> 00:03:52.910我的意思是，这部分反映了我们得到了，00:03:52.910 --> 00:03:57.750嗯-很多人一方面来上课，00:03:57.750 --> 00:04:01.550在右边，也许我们有物理博士学位，00:04:01.550 --> 00:04:06.140在左边的空白处，我们有一些新的人认为无论如何这样做会很有趣。00:04:06.140 --> 00:04:09.125嗯，我们欢迎E-我们欢迎所有人，00:04:09.125 --> 00:04:13.205但原则上这是研究生级别的课程。00:04:13.205 --> 00:04:15.470你知道，这并不意味着我们想让人们失望，00:04:15.470 --> 00:04:20.295我们希望每个人都能成功，但也希望毕业班能成功。00:04:20.295 --> 00:04:22.000嗯-我们希望你-你知道，00:04:22.000 --> 00:04:24.575为你的成功采取一些主动。00:04:24.575 --> 00:04:26.900意思是，如果你需要知道一些事情00:04:26.900 --> 00:04:29.375做作业，但你不知道，00:04:29.375 --> 00:04:33.755嗯-那你应该主动找一些教程，00:04:33.755 --> 00:04:37.100上班时间来和别人聊天00:04:37.100 --> 00:04:41.615你需要的任何帮助，学习如何解决你知识上的任何漏洞。00:04:41.615 --> 00:04:44.420可以。这是今天的计划。00:04:44.420 --> 00:04:47.280嗯-这就是课程信息更新。00:04:47.280 --> 00:04:49.675所以你知道，是-这是某种，00:04:49.675 --> 00:04:53.030从某种意义上说，你知道机器学习神经网络只是为了00:04:53.030 --> 00:04:56.400试着确保其他人都能掌握所有这些东西。00:04:56.400 --> 00:04:59.510我来谈谈分类，嗯，00:04:59.510 --> 00:05:02.000介绍神经网络，嗯，00:05:02.000 --> 00:05:04.505绕道进入命名实体识别，00:05:04.505 --> 00:05:08.580然后展示一个做的模型，嗯00:05:08.580 --> 00:05:13.330窗口-单词窗口分类，然后是结尾部分，00:05:13.330 --> 00:05:17.030然后我们深入研究我们的工具00:05:17.030 --> 00:05:21.920需要学习神经网络所以今天我们要去00:05:21.920 --> 00:05:29.060在矩阵微积分复习和入门之间的某个地方，然后会00:05:29.060 --> 00:05:32.720引导下一次的讲座00:05:32.720 --> 00:05:37.040关于反传播和计算图的更多信息。00:05:37.040 --> 00:05:42.755所以，是的。所以这个材料，尤其是最后的部分。00:05:42.755 --> 00:05:45.740你知道对某些人来说00:05:45.740 --> 00:05:49.430幼稚的如果-这是你每周做的事情，嗯，00:05:49.430 --> 00:05:52.490对其他人来说，可能看起来不可能00:05:52.490 --> 00:05:56.300很难，但希望你们中间有很大一部分人00:05:56.300 --> 00:06:00.290这将是一个非常有用的回顾00:06:00.290 --> 00:06:06.125矩阵微积分和我们希望你能在家庭作业二上做的事情。00:06:06.125 --> 00:06:09.840嗯，好吧。嗯，是的。00:06:09.840 --> 00:06:12.000很抱歉，如果我让一些人感到无聊。00:06:12.000 --> 00:06:15.300如果你上个季度坐到229号，00:06:15.300 --> 00:06:19.510看到了，嗯，分类器是什么样的，希望这样00:06:19.510 --> 00:06:23.200看起来很熟悉，但我只是希望00:06:23.200 --> 00:06:27.160在第二周，每个人都会加快速度，大致在同一个页面上。00:06:27.160 --> 00:06:29.380这是我们的分类设置。00:06:29.380 --> 00:06:34.800所以我们假设我们有一个训练数据集，其中我们有这些um向量00:06:34.800 --> 00:06:41.245我们的x分之和，然后每一个我们都有一个班。00:06:41.245 --> 00:06:46.344所以输入可能是单词、句子、文档或其他东西，00:06:46.344 --> 00:06:48.415这里有d个向量，嗯，00:06:48.415 --> 00:06:52.490我们想要的Yi、标签或类00:06:52.490 --> 00:06:57.155分类到，我们有一组C类，我们正试图预测它们。00:06:57.155 --> 00:07:00.770所以这些可能是文件的主题，00:07:00.770 --> 00:07:03.860积极或消极的情绪00:07:03.860 --> 00:07:08.125文档或稍后，我们将更多地查看命名实体。00:07:08.125 --> 00:07:12.645可以。如果我们有这个嗯-00:07:12.645 --> 00:07:18.570对于这种直觉，我们得到了向量空间00:07:18.570 --> 00:07:22.730一张二维图片，我们在向量空间中有点，00:07:22.730 --> 00:07:27.620与RX项目相对应，我们要做的是00:07:27.620 --> 00:07:31.370看看我们培训样本中的那些，看看哪些是00:07:31.370 --> 00:07:35.810绿色和红色是我们在这里上的两门课，然后我们想学点东西。00:07:35.810 --> 00:07:39.680一条可以在果岭和00:07:39.680 --> 00:07:45.670红色的是尽可能最好的，学习的是我们的分类器。00:07:45.670 --> 00:07:50.285因此，在传统的机器学习或统计中，我们有00:07:50.285 --> 00:07:54.860XI向量是纯固定的数据项，但00:07:54.860 --> 00:08:00.245然后我们将这些XI乘以00:08:00.245 --> 00:08:03.440一些估计的权重向量和00:08:03.440 --> 00:08:08.215然后，估计的权重向量将进入分类决策。00:08:08.215 --> 00:08:10.880我在这里展示的分类器是00:08:10.880 --> 00:08:15.050一种SoftMax分类器，几乎相同但不完全相同00:08:15.050 --> 00:08:20.090逻辑回归分类器，您应该在CS 109或统计中看到它00:08:20.090 --> 00:08:26.110类或类似的东西，它给出了不同类的概率。00:08:26.110 --> 00:08:29.780可以。尤其是如果你00:08:29.780 --> 00:08:35.195一个SoftMax分类器或一个逻辑-逻辑回归分类器，00:08:35.195 --> 00:08:37.805这就是所谓的线性分类器。00:08:37.805 --> 00:08:41.630所以这两个类之间的决策边界00:08:41.630 --> 00:08:45.860是某个合适的高维空间中的一条线。00:08:45.860 --> 00:08:50.710所以一旦你有了更大的期望值，它就是一个平面或者超平面。00:08:50.710 --> 00:08:55.140可以。这是我们的SoftMax分类器。00:08:55.140 --> 00:08:57.815嗯，这有两部分。00:08:57.815 --> 00:09:02.660所以在重量矩阵中，双u00:09:02.660 --> 00:09:07.600我们有一行对应于每个类，然后00:09:07.600 --> 00:09:11.350那一排，我们有点像点生产它00:09:11.350 --> 00:09:17.170我们的数据点向量XI，这给了我们一种分数00:09:17.170 --> 00:09:22.030示例属于该类的可能性有多大，然后00:09:22.030 --> 00:09:27.190通过一个SoftMax函数运行它，正如我们在第一周看到的，00:09:27.190 --> 00:09:32.825SoftMax取一组数字并将其转化为概率分布。00:09:32.825 --> 00:09:34.605这对人们有意义吗？00:09:34.605 --> 00:09:38.265人们还记得上周的事吗？到目前为止还不错？00:09:38.265 --> 00:09:43.885可以。嗯，我不想详细讨论这个，但我的意思是，00:09:43.885 --> 00:09:50.285啊-本质上这也是逻辑回归的作用。00:09:50.285 --> 00:09:58.760不同的是，在这个设置中，我们有一个权重向量。00:09:58.760 --> 00:10:02.590每节课，而什么00:10:02.590 --> 00:10:07.460做逻辑回归的统计学家说体重，00:10:07.460 --> 00:10:12.425这给了我们一个比实际需要更多的权重向量。00:10:12.425 --> 00:10:15.650我们可以离开去上C课，00:10:15.650 --> 00:10:18.560我们可以用c减去一个权重向量。00:10:18.560 --> 00:10:20.270所以特别是如果你在做二进制00:10:20.270 --> 00:10:23.780逻辑回归只需要一个权重向量，而00:10:23.780 --> 00:10:26.330这个SoftMax回归公式00:10:26.330 --> 00:10:29.115实际上有两个权重向量，每个类一个。00:10:29.115 --> 00:10:31.610嗯，我们之间有点不同00:10:31.610 --> 00:10:34.020可以进入，但基本上是一样的。00:10:34.020 --> 00:10:38.520只是说我们要么在做SoftMax要么在做逻辑回归，没关系。00:10:38.520 --> 00:10:43.130嗯，所以当我们训练我们想要的00:10:43.130 --> 00:10:48.865做的是我们想能够预测，嗯，正确的班级。00:10:48.865 --> 00:10:52.180所以我们要做的就是00:10:52.180 --> 00:10:55.730训练我们的模型，使其给我们最大的概率00:10:55.730 --> 00:10:59.750可能到正确的班级，因此他们会给我们00:10:59.750 --> 00:11:06.170低概率的po-尽可能的um到um错误的类。00:11:06.170 --> 00:11:10.850所以我们的标准是00:11:10.850 --> 00:11:16.160我们的作业的负对数概率，然后00:11:16.160 --> 00:11:21.770我们要最小化负对数概率，对应于最大化。00:11:21.770 --> 00:11:28.155对应于概率最大化的对数概率。嗯。00:11:28.155 --> 00:11:32.665但是，嗯，有点，00:11:32.665 --> 00:11:37.465很快，我们将开始用深度学习框架做更多的事情，00:11:37.465 --> 00:11:41.275尤其是Pythorch，你会发现，00:11:41.275 --> 00:11:43.540实际上有一种叫NLL的东西00:11:43.540 --> 00:11:47.515表示负对数似然损失的损失。00:11:47.515 --> 00:11:52.150基本上，没有人使用它，因为更方便使用的是所谓的00:11:52.150 --> 00:11:54.820交叉熵损失，所以你会00:11:54.820 --> 00:11:58.000到处都能听到我们在训练交叉熵损失。00:11:58.000 --> 00:12:02.995所以，我只想简单地提一下，并解释一下那里发生了什么。00:12:02.995 --> 00:12:06.430所以交叉熵的概念来自00:12:06.430 --> 00:12:10.855婴儿信息论，它是关于我所知道的信息量的理论。00:12:10.855 --> 00:12:17.800嗯，我们假设有一个真实的概率分布p和我们的模型，00:12:17.800 --> 00:12:20.890我们建立了一些概率分布。00:12:20.890 --> 00:12:25.270这就是我们用软最大回归建立的，我们希望00:12:25.270 --> 00:12:30.685衡量我们估计的概率分布是否是一个好的度量。00:12:30.685 --> 00:12:33.595我们用交叉熵的方法是，00:12:33.595 --> 00:12:36.370我们在课堂上说，00:12:36.370 --> 00:12:40.045“根据真实模型，班级的概率是多少？”00:12:40.045 --> 00:12:44.320利用等待，我们计算出，嗯，00:12:44.320 --> 00:12:50.215根据我们的估计模型的概率，我们求和并求反，00:12:50.215 --> 00:12:53.305这就是我们的交叉熵度量。00:12:53.305 --> 00:12:59.635可以。嗯，但是-所以总的来说这给了你00:12:59.635 --> 00:13:06.520分布之间的某种信息的度量。00:13:06.520 --> 00:13:09.355但在我们的特殊情况下，00:13:09.355 --> 00:13:11.725记住，对于每个例子，00:13:11.725 --> 00:13:14.260我们假设这是00:13:14.260 --> 00:13:18.025为培训数据贴上标签，因此我们就举个例子来说，00:13:18.025 --> 00:13:20.470正确答案是七班。00:13:20.470 --> 00:13:23.665因此，我们的真实分布，00:13:23.665 --> 00:13:26.740我们的p是-对于这个例子，00:13:26.740 --> 00:13:31.030是七级概率一级00:13:31.030 --> 00:13:34.000其他概率为零的。00:13:34.000 --> 00:13:37.929所以如果你想想这个公式会发生什么，00:13:37.929 --> 00:13:40.630你得到了所有课程的总和。00:13:40.630 --> 00:13:45.055PFC要么是1，要么是0，它将是1。00:13:45.055 --> 00:13:49.750只为真正的班级，所以你剩下的是，00:13:49.750 --> 00:13:54.055这等于减去qc的对数，嗯，00:13:54.055 --> 00:14:01.465对于我们在上一张幻灯片中计算的真正类。00:14:01.465 --> 00:14:04.930可以。那就是-嗯，是的。00:14:04.930 --> 00:14:08.425这就是交叉熵损失的基本情况。00:14:08.425 --> 00:14:12.085嗯，但还有一个概念要提。00:14:12.085 --> 00:14:16.300所以当你有一个完整的数据集，包括一大堆例子，00:14:16.300 --> 00:14:21.010交叉熵损失则取每个例子的平均值。00:14:21.010 --> 00:14:25.270所以，我想这就是信息论中人们有时所说的交叉熵率。00:14:25.270 --> 00:14:27.280因此，除此之外，还考虑了其中的因素。00:14:27.280 --> 00:14:33.025如果你在训练它，任何例子都是，向量中的那个例子。00:14:33.025 --> 00:14:36.805可以。嗯，好吧。00:14:36.805 --> 00:14:39.865这就是交叉熵损失。00:14:39.865 --> 00:14:40.990可以吗？是啊。00:14:40.990 --> 00:14:45.775[噪音]地面上有一些实际标签的混合物？00:14:45.775 --> 00:14:47.980当然。好问题。00:14:47.980 --> 00:14:52.615正确的。所以，最简单的情况是你的黄金数据，00:14:52.615 --> 00:14:55.225有人用手给它贴上标签，00:14:55.225 --> 00:14:58.060嗯，他们标了一个，剩下的是零。00:14:58.060 --> 00:15:02.380嗯，是的-你可以想想那些不是这样的情况。00:15:02.380 --> 00:15:05.800我的意思是，有一种情况是你可以相信人类00:15:05.800 --> 00:15:09.520有时候不知道正确的答案，所以如果人类说，00:15:09.520 --> 00:15:13.870“我不确定是三班还是四班，”你可以想象我们00:15:13.870 --> 00:15:18.685我们把概率的一半放在训练数据上，00:15:18.685 --> 00:15:21.550嗯，那不是一件疯狂的事，00:15:21.550 --> 00:15:26.680这样你就可以得到一个真正的交叉熵损失，使用更多的分布。00:15:26.680 --> 00:15:34.270嗯，在实际操作中更常用的情况是，00:15:34.270 --> 00:15:39.490在很多情况下，人们想做半监督学习。00:15:39.490 --> 00:15:41.230所以，我想这是一个00:15:41.230 --> 00:15:44.740我的团队和Chris Re的团队都做了很多工作，00:15:44.740 --> 00:15:47.935我们实际上没有完全标记的数据，00:15:47.935 --> 00:15:51.340但是我们有一些猜测标签是什么的方法00:15:51.340 --> 00:15:54.790如果我们尝试猜测数据的标签，00:15:54.790 --> 00:15:56.845我们经常会说，00:15:56.845 --> 00:15:58.465“这是数据。00:15:58.465 --> 00:16:00.760这是这个标签的三分之二的机会，00:16:00.760 --> 00:16:05.125但也可以是其他四个标签，“我们用概率分布，00:16:05.125 --> 00:16:07.825是的，这是更普遍的交叉熵损失。00:16:07.825 --> 00:16:12.560可以？嗯，对。所以，嗯，00:16:12.560 --> 00:16:16.335这就是交叉熵损失，很好。00:16:16.335 --> 00:16:19.425嗯，这个底部有点不同，嗯，00:16:19.425 --> 00:16:20.895也就是说，“现在我们，00:16:20.895 --> 00:16:23.130这是完整的数据集。”00:16:23.130 --> 00:16:25.290另一件值得注意的事，嗯，00:16:25.290 --> 00:16:33.285当我们有一个完整的数据-我们可以有一个完整的X的数据集，00:16:33.285 --> 00:16:37.665嗯，然后我们有一整套的重量。00:16:37.665 --> 00:16:40.770嗯，我们在哪工作？00:16:40.770 --> 00:16:43.020一个类权重的行向量，00:16:43.020 --> 00:16:45.420但我们要为所有的课程做准备。00:16:45.420 --> 00:16:48.180所以，我们可以简化我们正在写的东西00:16:48.180 --> 00:16:51.015这里我们可以开始使用矩阵表示法00:16:51.015 --> 00:16:56.980直接用矩阵w来计算，好的。00:16:56.980 --> 00:17:01.075因此，对于传统的ML优化，00:17:01.075 --> 00:17:05.200我们的参数是这些权重集，00:17:05.200 --> 00:17:07.240嗯，对于不同的班级。00:17:07.240 --> 00:17:09.280所以对于每一个班级，00:17:09.280 --> 00:17:12.085我们有一个三维的，嗯，00:17:12.085 --> 00:17:14.620行向量的权重，因为我们要00:17:14.620 --> 00:17:19.225点积的排序wi-与rd，维，输入向量。00:17:19.225 --> 00:17:28.975所以我们有c乘以d项和w矩阵，这些是我们模型的参数。00:17:28.975 --> 00:17:35.305所以如果我们想用梯度下降的思想来学习这个模型，00:17:35.305 --> 00:17:37.960随机梯度下降，我们要做的是00:17:37.960 --> 00:17:40.585有点像我们上次开始谈论的。00:17:40.585 --> 00:17:43.255我们有这些参数集。00:17:43.255 --> 00:17:45.295我们解决了，嗯，00:17:45.295 --> 00:17:50.845梯度，所有这些的偏导数，嗯，00:17:50.845 --> 00:17:54.790关于所有这些参数的损失，我们00:17:54.790 --> 00:17:58.840用它来获取损失函数的梯度更新，00:17:58.840 --> 00:18:01.240我们绕着W区移动，00:18:01.240 --> 00:18:04.930绕W移动相当于00:18:04.930 --> 00:18:09.700移动这条在类和我们之间分开的线00:18:09.700 --> 00:18:15.490为了尽量减少我们的损失，我们选择了一条00:18:15.490 --> 00:18:22.045在某种意义上，最好在类的项之间进行分隔。00:18:22.045 --> 00:18:25.135可以。这是一个基本的分类器。00:18:25.135 --> 00:18:28.945所以第一个问题是，00:18:28.945 --> 00:18:35.840神经网络分类器有什么不同？00:18:36.120 --> 00:18:40.765嗯，所以中心观察是这样的00:18:40.765 --> 00:18:46.405大多数经典的分类器，人们经常使用，00:18:46.405 --> 00:18:50.470包括一些像朴素的贝叶斯模型，嗯，00:18:50.470 --> 00:18:52.810基本支持向量机，00:18:52.810 --> 00:18:55.495SoftMax或Logistic回归。00:18:55.495 --> 00:19:00.955它们是相当简单的分类器。00:19:00.955 --> 00:19:06.100特别是那些都是线性分类器，将通过绘图进行分类。00:19:06.100 --> 00:19:08.680一条线或在高维空间中00:19:08.680 --> 00:19:12.175画出一种将例子分开的平面。00:19:12.175 --> 00:19:18.235在某些情况下，拥有这样一个简单的分类器是有用的。00:19:18.235 --> 00:19:22.090我的意思是，这给了你一个机器学习的高偏差分类器，00:19:22.090 --> 00:19:25.465在CS229里有很多00:19:25.465 --> 00:19:27.430但是如果你有数据集，嗯，00:19:27.430 --> 00:19:32.050就像这样，你不能很好地分类00:19:32.050 --> 00:19:34.390如果你有，所有的点都是正确的。00:19:34.390 --> 00:19:38.080一个高偏差分类器，因为你只需要画一条线。00:19:38.080 --> 00:19:41.995所以你想要一个更强大的分类器。00:19:41.995 --> 00:19:46.390从本质上讲，是什么推动了00:19:46.390 --> 00:19:51.580深度学习是指在很多情况下，当你有自然信号时，00:19:51.580 --> 00:19:53.950所以这些都是，嗯，演讲，00:19:53.950 --> 00:19:56.620语言、图像和类似的东西，00:19:56.620 --> 00:20:02.440您有大量的数据，因此可以学习相当复杂的分类器。00:20:02.440 --> 00:20:10.180嗯，但是用输入数据表示类是非常复杂的。00:20:10.180 --> 00:20:14.365你不可能只在两个类之间画一条线就可以做到。00:20:14.365 --> 00:20:19.150所以，您需要使用一些更复杂的分类器。00:20:19.150 --> 00:20:22.060所以神经网络，多层00:20:22.060 --> 00:20:25.015神经网络，我们将要开始进入，00:20:25.015 --> 00:20:31.480他们所做的正是为你提供一种学习非常复杂的方法，00:20:31.480 --> 00:20:35.560你知道，几乎无限复杂的分类器。00:20:35.560 --> 00:20:41.364所以如果你看看他们在原始空间做出的决定，00:20:41.364 --> 00:20:44.110他们可以像这样学习案例。00:20:44.110 --> 00:20:47.050嗯，我放了这个-我放了，00:20:47.050 --> 00:20:50.920嗯，指针在几张幻灯片上。00:20:50.920 --> 00:20:55.330嗯，这个-这是安德烈・卡帕西的一个形象化。00:20:55.330 --> 00:20:58.525几年前他还是这里的博士生。00:20:58.525 --> 00:21:00.535这是一个小小的javascript，嗯，00:21:00.535 --> 00:21:03.430你可以在他的网站上找到的应用程序00:21:03.430 --> 00:21:06.385其实玩起来很有趣，看看是什么样的，00:21:06.385 --> 00:21:11.035嗯，决策边界你可以得到一个神经网络。00:21:11.035 --> 00:21:21.415可以。嗯，为了得到-为了得到更高级的分类，00:21:21.415 --> 00:21:24.760嗯，一个用于自然语言的神经网络，00:21:24.760 --> 00:21:29.260有两件事你可以做，00:21:29.260 --> 00:21:31.810我想说的是00:21:31.810 --> 00:21:35.110有些人在归根结底时也会有同样的感觉。00:21:35.110 --> 00:21:41.890但一开始我会单独提到其中一个是我们00:21:41.890 --> 00:21:45.370有这些词向量，然后00:21:45.370 --> 00:21:49.915第二个问题是我们要建立更深层的多层网络。00:21:49.915 --> 00:21:53.065可以。所以，一开始重要的区别是，00:21:53.065 --> 00:21:56.290嗯，我们已经开始看到了，嗯，00:21:56.290 --> 00:21:59.770我们上周所做的不是00:21:59.770 --> 00:22:03.595有点像“房子”这个词，00:22:03.595 --> 00:22:10.720相反，我们说房子是实数的向量，我们能做的是00:22:10.720 --> 00:22:14.620更改与House In对应的向量00:22:14.620 --> 00:22:19.030这样我们就能建立更好的分类器，00:22:19.030 --> 00:22:23.080这意味着我们将成为一个移动房屋的代表00:22:23.080 --> 00:22:27.220捕捉我们感兴趣的事物的空间，比如单词相似性，00:22:27.220 --> 00:22:29.605类比之类的。00:22:29.605 --> 00:22:31.150所以这实际上是，你知道，00:22:31.150 --> 00:22:35.830与常规步骤或ML相比，这是一个奇怪的想法。00:22:35.830 --> 00:22:40.075与其说我们只有参数w，00:22:40.075 --> 00:22:47.290我们还说，所有这些单词表示也是我们模型的参数。00:22:47.290 --> 00:22:49.840所以，我们实际上要改变00:22:49.840 --> 00:22:54.685单词的表示使我们的分类器做得更好。00:22:54.685 --> 00:22:57.320所以，我们同时改变重量00:22:57.320 --> 00:23:00.290我们正在改变词语的表达方式，00:23:00.290 --> 00:23:04.685我们正在同时优化这两个模型，使我们的模型成为，00:23:04.685 --> 00:23:06.410嗯，尽可能好。00:23:06.410 --> 00:23:10.250所以，这就是人们经常谈论的意义00:23:10.250 --> 00:23:15.210我们正在进行代表性学习的深层次学习模型。00:23:15.230 --> 00:23:18.375我说有两种方法，00:23:18.375 --> 00:23:19.830我想说两件事。00:23:19.830 --> 00:23:21.540一种是这种，嗯，00:23:21.540 --> 00:23:25.050字向量表示学习，然后是第二个00:23:25.050 --> 00:23:29.160我们将开始研究更深层的多层神经网络。00:23:29.160 --> 00:23:32.835嗯，有点隐藏在幻灯片上00:23:32.835 --> 00:23:37.035你真的能想到单词的观察，00:23:37.035 --> 00:23:40.770文字矢量嵌入就像把你的，00:23:40.770 --> 00:23:44.280具有多个神经网络层的模型。00:23:44.280 --> 00:23:50.280所以，如果你想象每个词都是一个热向量，00:23:50.280 --> 00:23:53.730嗯，对于你的模型中不同的单词类型。00:23:53.730 --> 00:23:55.095所以，你有，呃，你知道，00:23:55.095 --> 00:24:00.735150000维矢量，一个热编码的不同单词。00:24:00.735 --> 00:24:03.855嗯，那么你可以说你有一个妈妈，嗯，00:24:03.855 --> 00:24:11.790矩阵L是你的字典矩阵，你将传递你的一个热向量00:24:11.790 --> 00:24:15.870通过神经网络层的一个词00:24:15.870 --> 00:24:20.925乘以一个热向量或l1，即一个热向量。00:24:20.925 --> 00:24:22.905因为这是一个单一的热向量，00:24:22.905 --> 00:24:30.525这样做的效果是去掉一列L.so，00:24:30.525 --> 00:24:34.590实际上，我们有一个额外的矩阵层，嗯，00:24:34.590 --> 00:24:37.215在我们的神经网络中，我们正在学习00:24:37.215 --> 00:24:41.595矩阵的参数和我们学习的一样，00:24:41.595 --> 00:24:44.910嗯，一个用于其他目的的深层神经网络。00:24:44.910 --> 00:24:48.030所以，从数学上来说，这完全有意义，而且00:24:48.030 --> 00:24:51.375这是一种明智的思考方式，00:24:51.375 --> 00:24:53.385嗯，你在做什么，嗯，00:24:53.385 --> 00:24:56.250在神经网络中嵌入单词。00:24:56.250 --> 00:24:58.890嗯，从实现的角度来看，这使得00:24:58.890 --> 00:25:02.100完全没有意义，也没有人这样做，因为它只是没有00:25:02.100 --> 00:25:08.100当矩阵相乘的结果是，有意义的做一个矩阵相乘。00:25:08.100 --> 00:25:12.405这是ID 17，嗯，有点，00:25:12.405 --> 00:25:16.380然后用00:25:16.380 --> 00:25:20.325一个在位置17，然后做一个矩阵相乘，没有意义。00:25:20.325 --> 00:25:22.065你只要拿起，嗯，00:25:22.065 --> 00:25:24.090列或，或，行，00:25:24.090 --> 00:25:29.230正如我们所讨论的，你的矩阵中有17个，这就是每个人实际做的。00:25:29.230 --> 00:25:35.940可以。这是我给全班学生的一张关于神经元的必选照片。00:25:35.940 --> 00:25:38.475所以，别错过了，我不会再给大家看了。00:25:38.475 --> 00:25:42.585可以。所以，神经网络的起源，嗯，00:25:42.585 --> 00:25:47.130从某种意义上说是尝试和构建00:25:47.130 --> 00:25:50.520一种人工神经元00:25:50.520 --> 00:25:54.540某种意义上的计算，00:25:54.540 --> 00:25:57.765嗯，那是在人脑中发生的。00:25:57.765 --> 00:26:02.205这是一个很松散的类比，但是，你知道，00:26:02.205 --> 00:26:04.200我们的模型就是这些是我们的，00:26:04.200 --> 00:26:07.500这是我们大脑中的肺结核部分。00:26:07.500 --> 00:26:08.940所以，这里是神经元，00:26:08.940 --> 00:26:12.075这是一个神经元细胞，00:26:12.075 --> 00:26:14.565神经元是由什么组成的？00:26:14.565 --> 00:26:16.470嗯，那么，在后面，00:26:16.470 --> 00:26:19.455它有这些树突，很多树突。00:26:19.455 --> 00:26:25.200然后是一个细胞体，如果树突上有东西，嗯，00:26:25.200 --> 00:26:28.800细胞体将变得活跃，然后一切开始。00:26:28.800 --> 00:26:32.880把这个叫做轴突的长东西刺下去。00:26:32.880 --> 00:26:35.580所以，这些轴突会导致00:26:35.580 --> 00:26:39.300树突不同细胞或许多不同细胞的树突，对吗？00:26:39.300 --> 00:26:41.640这张，嗯，我不确定是否有，但是00:26:41.640 --> 00:26:44.625其中一些会进入不同的细胞。00:26:44.625 --> 00:26:48.105嗯，那么，你就有了这种，嗯，00:26:48.105 --> 00:26:51.120轴突上的终端按钮有点关闭00:26:51.120 --> 00:26:54.270到树突，但它们之间有一点空隙，还有一些微小的-00:26:54.270 --> 00:26:56.955生物化学的奇迹就发生在那里。00:26:56.955 --> 00:26:58.920当然，这就是突触，00:26:58.920 --> 00:27:03.870然后你会有一种进入下一个神经元的激活流动。00:27:03.870 --> 00:27:06.870所以，这就是开始，嗯，00:27:06.870 --> 00:27:10.710人们想在计算中尝试和模拟的模型。00:27:10.710 --> 00:27:15.060所以，人们提出了一个人工神经元的模型。00:27:15.060 --> 00:27:22.050所以，我们有来自其他神经元的东西在某种程度上被激活。00:27:22.050 --> 00:27:25.710所以，这是一个X0，x1，x2的数字。00:27:25.710 --> 00:27:30.690嗯，那么突触的变化取决于兴奋程度00:27:30.690 --> 00:27:35.535他们知道让信号穿过突触有多容易。00:27:35.535 --> 00:27:42.675所以，这个模型是用它们乘以一个重量w0，w1，w2。00:27:42.675 --> 00:27:46.365然后是细胞体，有点正确，00:27:46.365 --> 00:27:50.190有点像是加上了这么多的激励00:27:50.190 --> 00:27:54.510从不同的树突中获取，嗯，00:27:54.510 --> 00:27:58.320然后它会有自己的偏见，即它有多大可能开火，00:27:58.320 --> 00:28:00.210那是B。嗯，所以，00:28:00.210 --> 00:28:06.240我们得到了这个结果，然后它就有了一个整体的阈值，或者说是射击倾向。00:28:06.240 --> 00:28:09.405所以，我们把它通过一个激活函数，00:28:09.405 --> 00:28:11.280嗯，这有点，00:28:11.280 --> 00:28:13.920将确定一个发射率，这将是，00:28:13.920 --> 00:28:17.205输出轴突上的信号。00:28:17.205 --> 00:28:21.150所以，这是一个开始，但是，00:28:21.150 --> 00:28:22.575你知道，真的，嗯，00:28:22.575 --> 00:28:24.600为了我们最终的计算。00:28:24.600 --> 00:28:28.890我们这里有一点婴儿数学，实际上，00:28:28.890 --> 00:28:33.090你看到的那种婴儿数学看起来很熟悉00:28:33.090 --> 00:28:37.920在线性代数和数理统计中，这其实没有什么不同。00:28:37.920 --> 00:28:40.140所以，特别是，嗯，00:28:40.140 --> 00:28:46.380神经元很容易成为二元逻辑回归单元。00:28:46.380 --> 00:28:49.305嗯，所以，这有点，00:28:49.305 --> 00:28:52.800对于输入x的逻辑回归，00:28:52.800 --> 00:28:54.855把它乘以一个权重向量。00:28:54.855 --> 00:28:58.065你在加，嗯，你的，嗯，00:28:58.065 --> 00:29:01.920偏倚项，然后你把它通过，00:29:01.920 --> 00:29:03.990嗯，非线性，00:29:03.990 --> 00:29:06.315比如后勤功能。00:29:06.315 --> 00:29:10.665嗯，然后，你要计算一个逻辑回归，00:29:10.665 --> 00:29:14.625嗯，在这种神经元模型里。00:29:14.625 --> 00:29:17.535嗯，这就是，00:29:17.535 --> 00:29:20.970这是软最大逻辑回归之间的差异，00:29:20.970 --> 00:29:26.655我是说两个类有两组参数的软最大值。00:29:26.655 --> 00:29:30.420这种类型只有一组参数z和您的建模00:29:30.420 --> 00:29:35.790这两个类通过给出一个类从0到1的概率，00:29:35.790 --> 00:29:37.500取决于输入00:29:37.500 --> 00:29:41.490逻辑回归是高度负性或高度正性的。00:29:41.490 --> 00:29:47.625可以。所以，真的，我们可以说这些人工神经元有点像00:29:47.625 --> 00:29:51.780二元逻辑回归单位，或者我们可以使变量00:29:51.780 --> 00:29:56.730二元逻辑回归单位使用不同的F函数。00:29:56.730 --> 00:29:59.925我们会很快再次回到这一点上。00:29:59.925 --> 00:30:04.560可以。嗯，好吧，那就给我们一个神经元。00:30:04.560 --> 00:30:09.495因此，一个神经元是一个逻辑回归单元。00:30:09.495 --> 00:30:13.410所以，最关键的是，我们想用神经网络做的是，00:30:13.410 --> 00:30:16.500为什么只进行一次逻辑回归？00:30:16.500 --> 00:30:17.865我们为什么不，嗯，00:30:17.865 --> 00:30:21.960同时进行一系列的逻辑回归？00:30:21.960 --> 00:30:27.465所以，你知道，这是我们的输入，这是我们的小逻辑回归单位，嗯，00:30:27.465 --> 00:30:30.420但是我们可以在00:30:30.420 --> 00:30:33.855同时也可以运行任意数量的。00:30:33.855 --> 00:30:39.690嗯，好吧，这很好，但有点像传统的训练00:30:39.690 --> 00:30:42.810一种统计模型00:30:42.810 --> 00:30:47.865确定逻辑回归的橙色输出。00:30:47.865 --> 00:30:51.540你知道，我们在训练他们去尝试和捕捉什么。00:30:51.540 --> 00:30:56.235我们必须有数据来预测他们要尝试捕获什么。00:30:56.235 --> 00:31:02.865所以，建立更大的神经网络的秘诀是，00:31:02.865 --> 00:31:06.720我们其实不想提前决定什么00:31:06.720 --> 00:31:11.595这些橙色的逻辑回归正试图捕捉。00:31:11.595 --> 00:31:15.450我们希望神经网络能自我组织，00:31:15.450 --> 00:31:18.450所以那些橙色的逻辑回归，00:31:18.450 --> 00:31:22.500单位学习一些有用的东西。00:31:22.500 --> 00:31:25.020那么，什么是有用的呢？00:31:25.020 --> 00:31:27.270好吧，我们的想法是说，00:31:27.270 --> 00:31:31.095我们确实有一些我们想要做的任务。00:31:31.095 --> 00:31:34.650所以，我们-我们有一些我们想做的任务。00:31:34.650 --> 00:31:39.450所以也许我们想决定电影评论是正面的还是负面的，00:31:39.450 --> 00:31:41.925像情绪分析之类的。00:31:41.925 --> 00:31:44.670有件事我们想在一天结束时做。00:31:44.670 --> 00:31:47.625嗯，我们要，呃，00:31:47.625 --> 00:31:52.275那里的逻辑回归分类器告诉我们是正的还是负的。00:31:52.275 --> 00:31:55.440嗯，但是这些投入不会00:31:55.440 --> 00:31:58.785直接成为文档中的单词。00:31:58.785 --> 00:32:03.480它们将成为逻辑回归单元的中间层。00:32:03.480 --> 00:32:09.180我们要训练这整件事，使交叉熵损失最小化。00:32:09.180 --> 00:32:12.120基本上，我们想要的00:32:12.120 --> 00:32:15.015在反向传播算法中发生会对我们有帮助，00:32:15.015 --> 00:32:18.015就是说，你的事情在中间，00:32:18.015 --> 00:32:23.280你的工作是找到一些有用的方法来计算00:32:23.280 --> 00:32:29.715这样的基础数据将帮助我们的最终分类器做出一个好的决策。00:32:29.715 --> 00:32:31.695我的意思是，你知道，00:32:31.695 --> 00:32:34.605回到这张照片，你知道。00:32:34.605 --> 00:32:38.520最后一个分类器，它只是一个线性分类器，00:32:38.520 --> 00:32:40.770软最大值或逻辑回归。00:32:40.770 --> 00:32:42.915会有这样一条线。00:32:42.915 --> 00:32:45.900但是如果中间分类器，00:32:45.900 --> 00:32:48.000它们就像一个单词嵌入，00:32:48.000 --> 00:32:52.260它们可以某种程度上重新表示空间，改变周围的事物。00:32:52.260 --> 00:32:57.180因此，他们可以学会以这样一种方式改变事情00:32:57.180 --> 00:33:03.190您正在学习原始输入空间的高度非线性函数。00:33:06.810 --> 00:33:10.930可以。嗯，在那一点上，00:33:10.930 --> 00:33:12.430只是说说而已，00:33:12.430 --> 00:33:13.960好吧，为什么停在那里？00:33:13.960 --> 00:33:18.085如果我们多加一层也许会更好。00:33:18.085 --> 00:33:24.400这让我们进入了深度学习的领域，00:33:24.400 --> 00:33:26.590嗯，这是，嗯，00:33:26.590 --> 00:33:31.180这就是神经网络的三种形式。00:33:31.180 --> 00:33:33.940所以50年代的第一部作品是00:33:33.940 --> 00:33:38.320当人们有一个单一神经元的模型时00:33:38.320 --> 00:33:41.710像这样，然后才逐渐了解00:33:41.710 --> 00:33:45.965与更传统的统计数据相关。00:33:45.965 --> 00:33:53.220嗯，神经网络的第二个版本，我们看到了80年代和90年代早期，嗯，00:33:53.220 --> 00:33:56.640在那里人们，嗯，建立了这样的神经网络00:33:56.640 --> 00:34:01.865这个隐藏层可以在中间学习表示。00:34:01.865 --> 00:34:06.220但在那个时候，它真的没有效果。00:34:06.220 --> 00:34:12.850在所有人中，没有人能够建立更深层的网络，让他们做任何有用的事情。00:34:12.850 --> 00:34:18.040所以你有一些神经网络有一个隐藏层00:34:18.040 --> 00:34:24.715从深入学习开始的研究00:34:24.715 --> 00:34:29.710嗯，我们相信我们能做得更复杂，00:34:29.710 --> 00:34:32.950嗯，更复杂任务的分类。00:34:32.950 --> 00:34:36.760比如语音识别和图像识别00:34:36.760 --> 00:34:40.960拥有更深层的网络00:34:40.960 --> 00:34:45.100有效地学习更复杂的输入函数00:34:45.100 --> 00:34:49.405允许我们做一些事情，比如识别语言的声音。00:34:49.405 --> 00:34:52.060我们怎么可能训练这样一个，00:34:52.060 --> 00:34:55.210嗯，网络，他们能有效地工作吗？00:34:55.210 --> 00:34:56.875就这样，00:34:56.875 --> 00:34:58.465嗯，接下来，00:34:58.465 --> 00:35:03.055嗯，更重要的是，在下一节课中开始这节课。00:35:03.055 --> 00:35:05.859但在我们到达那里之前，00:35:05.859 --> 00:35:08.050嗯，只是再次强调一下。00:35:08.050 --> 00:35:11.440所以一旦我们有了这样的东西，00:35:11.440 --> 00:35:13.765嗯，神经网络层。00:35:13.765 --> 00:35:16.330我们有一个输入向量，00:35:16.330 --> 00:35:21.040我们有一个输出向量，一切都是00:35:21.040 --> 00:35:28.135连接起来，这样我们就可以在每一条黑线上都有这样的权重。00:35:28.135 --> 00:35:32.145所以我们可以说A1是你要的00:35:32.145 --> 00:35:36.715权重乘以x1的每个分量，并添加一个偏差项，00:35:36.715 --> 00:35:41.305嗯，然后你就要跑步了00:35:41.305 --> 00:35:47.575这部分，然后通过我们的非线性运行，这将给我们一个输出。00:35:47.575 --> 00:35:51.265我们要为A1，A2和A3中的每一个做这个。00:35:51.265 --> 00:35:56.710嗯，再一次，我们可以认为A是一个向量，我们可以00:35:56.710 --> 00:36:02.260有点像是将它折叠成这个矩阵符号来计算层的效果。00:36:02.260 --> 00:36:07.780全连接层是有效的权重矩阵，嗯，00:36:07.780 --> 00:36:13.720通常这样重写，我们有一个偏倚项作为偏倚项的向量。00:36:13.720 --> 00:36:15.445这里有一种选择。00:36:15.445 --> 00:36:18.670您可以随时导入，然后00:36:18.670 --> 00:36:24.190偏倚项成为一个稍大矩阵的权重的一部分，再加上一个，00:36:24.190 --> 00:36:28.670嗯，多加一列或一行。00:36:30.810 --> 00:36:36.835再加一个，一排，对吗？00:36:36.835 --> 00:36:41.530或者你也可以把它们单独放在那些BS中。00:36:41.530 --> 00:36:45.580可以。嗯，然后是最后一个音符-对吗？00:36:45.580 --> 00:36:49.600所以一旦我们计算了这部分，00:36:49.600 --> 00:36:54.760我们总是将事物置于非线性状态，即00:36:54.760 --> 00:36:58.000激活功能等等00:36:58.000 --> 00:37:02.395我之前展示的逻辑转换是一个激活函数。00:37:02.395 --> 00:37:07.660这就是端口中的向量，嗯，00:37:07.660 --> 00:37:11.319提供矢量输出的激活函数，00:37:11.319 --> 00:37:16.060这通常意味着，我们对这个函数元素的应用是明智的。00:37:16.060 --> 00:37:19.720所以我们要应用逻辑函数，它是00:37:19.720 --> 00:37:25.705很自然，一个输入一个输出函数，就像我之前展示的小图一样。00:37:25.705 --> 00:37:28.180所以当我们把它应用到向量上时，00:37:28.180 --> 00:37:32.845我们把它应用到向量元素的每个元素上。00:37:32.845 --> 00:37:39.820可以。我们很快就会回来说00:37:39.820 --> 00:37:46.945关于非线性和人们实际使用的非线性的更多信息。00:37:46.945 --> 00:37:48.705但是，你知道，00:37:48.705 --> 00:37:51.585你可能想知道的是，00:37:51.585 --> 00:37:53.850为什么他总是有这些非线性00:37:53.850 --> 00:37:56.265假设这里有一个F函数？00:37:56.265 --> 00:37:58.140我们为什么不，嗯，00:37:58.140 --> 00:38:02.190计算一层中z等于wx加上b，然后00:38:02.190 --> 00:38:07.250在另一层上，z2等于w2，00:38:07.250 --> 00:38:12.040Z1加上B继续这样的图层？00:38:12.040 --> 00:38:16.360有一个非常确切的原因，如果你想的话00:38:16.360 --> 00:38:21.540让神经网络学习任何有趣的东西，00:38:21.540 --> 00:38:24.960你必须坚持某种功能，即00:38:24.960 --> 00:38:30.365一个非线性函数，如我之前展示的logistic曲线。00:38:30.365 --> 00:38:36.955原因是如果你在做00:38:36.955 --> 00:38:43.915线性变换，比如wx加b，然后w2 z1加b，00:38:43.915 --> 00:38:49.360W3Z2加上B，你在做一系列的线性变换。00:38:49.360 --> 00:38:54.790好吧，多个线性变换只是组成一个线性变换，对吗？00:38:54.790 --> 00:39:01.135所以一个线性变换是旋转和拉伸空间，你可以00:39:01.135 --> 00:39:04.510再次旋转和拉伸空间，但结果是00:39:04.510 --> 00:39:07.915这只是一个更大的旋转和空间的延伸。00:39:07.915 --> 00:39:11.230所以你不会为分类器获得额外的能量00:39:11.230 --> 00:39:14.620通过简单的多重线性变换。00:39:14.620 --> 00:39:20.980但只要你坚持几乎任何形式的非线性，00:39:20.980 --> 00:39:23.830然后你得到额外的能量。00:39:23.830 --> 00:39:26.965所以总的来说，00:39:26.965 --> 00:39:30.760当我们在做深层网络时，我们在做什么，嗯，00:39:30.760 --> 00:39:34.210在他们中间，我们不会想，“啊，00:39:34.210 --> 00:39:37.225这真的很重要00:39:37.225 --> 00:39:41.845关于概率的非线性思考。00:39:41.845 --> 00:39:44.500我们的总体情况很好，00:39:44.500 --> 00:39:50.095我们希望能够进行有效的函数逼近或曲线拟合。00:39:50.095 --> 00:39:55.750我们想学一个这样的空间，只有当我们加入00:39:55.750 --> 00:40:03.085一些允许我们学习这些曲线决策的非线性，嗯，模式。00:40:03.085 --> 00:40:07.390而so-so f被有效地用来做精确的00:40:07.390 --> 00:40:14.630[噪声]Fu-函数近似或模式匹配。00:40:14.880 --> 00:40:20.365可以。你已经落后了。嗯，好吧。00:40:20.365 --> 00:40:25.120这就是婴儿神经网络的介绍。00:40:25.120 --> 00:40:28.100一切都好吗？有问题吗？00:40:28.200 --> 00:40:31.150对？00:40:31.150 --> 00:40:34.780是的，就像急诊室，特写一和特写00:40:34.780 --> 00:40:39.475四个如果-如果你把它乘在一起，它就非常像标签Y，00:40:39.475 --> 00:40:41.845你能谈一下那个产品关系吗00:40:41.845 --> 00:40:43.960只是说[噪音]两层是线性的？00:40:43.960 --> 00:40:46.765嗯，是的。好问题。00:40:46.765 --> 00:40:49.780所以，在传统的统计数据中，00:40:49.780 --> 00:40:53.470您具有基本的输入功能，并且00:40:53.470 --> 00:40:58.210人们正在手工建立一个逻辑回归模型，00:40:58.210 --> 00:41:00.070人们常说，00:41:00.070 --> 00:41:03.910对分类真正重要的是00:41:03.910 --> 00:41:08.260看一对功能4和功能7。00:41:08.260 --> 00:41:09.625嗯，你知道的，00:41:09.625 --> 00:41:13.210如果这两个都是真的，同时我很重要00:41:13.210 --> 00:41:17.545发生了，所以在统计中通常称之为交互项，00:41:17.545 --> 00:41:22.470你可以通过手工将交互术语添加到你的模型中。00:41:22.470 --> 00:41:29.490所以，基本上，这里的秘密很大一部分是有这些中间层。00:41:29.490 --> 00:41:33.890他们可以自己学习、构建交互术语。00:41:33.890 --> 00:41:36.700是的，所以有点，嗯，00:41:36.700 --> 00:41:42.560自动搜索您想要放入模型中的高阶术语。00:41:46.800 --> 00:41:50.635可以。我接着说，还有其他问题吗？00:41:50.635 --> 00:41:55.360可以。嗯，那么，嗯，是的。00:41:55.360 --> 00:41:59.230所以这里有一个简短的小插曲00:41:59.230 --> 00:42:03.430NLP，这是一个我们要看一会儿的问题。00:42:03.430 --> 00:42:08.860所以这是我上一次提到的命名实体识别的任务。00:42:08.860 --> 00:42:12.820所以，如果我们有文本，00:42:12.820 --> 00:42:15.595等等，这里没有出现。00:42:15.595 --> 00:42:17.710可以。嗯，好吧。00:42:17.710 --> 00:42:19.000如果我们有文本，00:42:19.000 --> 00:42:22.525人们想做的事情00:42:22.525 --> 00:42:28.800我想找出上面提到的东西的名字。00:42:28.800 --> 00:42:32.230嗯，通常情况下，还有，00:42:32.230 --> 00:42:35.920找到你真正想要分类的东西的名字，00:42:35.920 --> 00:42:38.800比如说他们中的一些人是组织，00:42:38.800 --> 00:42:41.065其中一些是人，00:42:41.065 --> 00:42:43.765嗯，有些是地方。00:42:43.765 --> 00:42:46.855所以你知道这有很多用途，00:42:46.855 --> 00:42:49.120人们喜欢跟踪公司和00:42:49.120 --> 00:42:51.850人、报纸和类似的东西。00:42:51.850 --> 00:42:56.320嗯，当人们回答问题的时候，很多时候答案00:42:56.320 --> 00:43:00.520问题是我们称之为实体的人的名字，00:43:00.520 --> 00:43:03.430地点、组织、流行歌曲，00:43:03.430 --> 00:43:07.645电影名称所有这些东西都被命名为实体。00:43:07.645 --> 00:43:10.570嗯，如果你想开始的话00:43:10.570 --> 00:43:13.690从大量文本中自动建立知识库，00:43:13.690 --> 00:43:15.730你平时想做的就是出去00:43:15.730 --> 00:43:19.600命名实体，并消除它们之间的关系。00:43:19.600 --> 00:43:21.250所以这是一个常见的任务。00:43:21.250 --> 00:43:24.430那么，我们该怎么做呢？00:43:24.430 --> 00:43:28.420通常的做法是说，00:43:28.420 --> 00:43:32.905我们要一个一个地念一遍这些词00:43:32.905 --> 00:43:38.035它们将是一个上下文中的单词，就像它们是逐字逐句，00:43:38.035 --> 00:43:43.630我们要做的是运行一个分类器，然后给它们分配一个类。00:43:43.630 --> 00:43:46.420所以我们要说的第一个词是组织，00:43:46.420 --> 00:43:48.085第二个词是组织，00:43:48.085 --> 00:43:50.440第三个词不是命名实体，00:43:50.440 --> 00:43:51.700第四个词是人，00:43:51.700 --> 00:43:54.235第五个字是一个人，继续往下。00:43:54.235 --> 00:43:58.300所以在运行一个词的分类时00:43:58.300 --> 00:44:03.085文本中的一个位置，它周围有单词。00:44:03.085 --> 00:44:07.075嗯，所以说实体是什么00:44:07.075 --> 00:44:11.650许多实体都是多词术语，因此最简单的方法就是00:44:11.650 --> 00:44:15.880想象一下，这样做只是说我们要把所有分类的序列00:44:15.880 --> 00:44:20.935同样的，称之为电子实体沈国芳或类似的东西。00:44:20.935 --> 00:44:23.755这是有点瑕疵的原因，所以00:44:23.755 --> 00:44:26.635人们经常使用的是生物编码，00:44:26.635 --> 00:44:31.150嗯，我在右边展示的，但我现在就跑过去不做了。00:44:31.150 --> 00:44:37.240嗯，那么，一开始命名实体识别似乎是微不足道的，因为你知道，00:44:37.240 --> 00:44:41.230你有公司名称谷歌和Facebook是公司名称。00:44:41.230 --> 00:44:47.125每当你看到谷歌或Facebook，你都会说“公司”，你怎么会错呢？00:44:47.125 --> 00:44:49.225但实际上，有很多微妙之处，00:44:49.225 --> 00:44:51.310命名实体识别容易出错。00:44:51.310 --> 00:44:54.370所以这只是一些困难的情况。00:44:54.370 --> 00:44:58.810因此，通常很难确定实体的边界。00:44:58.810 --> 00:44:59.994所以在这句话中，00:44:59.994 --> 00:45:04.840第一国民银行唐向史密斯堡未来的学校捐赠了两辆面包车。00:45:04.840 --> 00:45:10.960所以，这里大概有一家银行的名字，但它是国家银行，第一家是00:45:10.960 --> 00:45:14.320只是句子的第一个大写的单词00:45:14.320 --> 00:45:18.160像第一次一样，她点了些食物之类的。00:45:18.160 --> 00:45:20.560所以有点不清楚它是什么。00:45:20.560 --> 00:45:24.310有时很难知道某个事物是否是一个实体。00:45:24.310 --> 00:45:29.455所以在这句话的结尾是未来学校的名字00:45:29.455 --> 00:45:33.040某种令人兴奋的21世纪学校00:45:33.040 --> 00:45:36.970这意味着这是一所未来的学校，将建在这个城市，对吗？00:45:36.970 --> 00:45:39.535它是一个实体还是根本不是？00:45:39.535 --> 00:45:44.530计算一个实体的类通常很困难，因此要找出更多00:45:44.530 --> 00:45:49.480关于Zig Ziglar和阅读功能，Zig Ziglar是什么班级的？00:45:49.480 --> 00:45:51.370很难说你是否不知道。00:45:51.370 --> 00:45:54.700嗯，实际上是一个人的名字，嗯，00:45:54.700 --> 00:45:58.870有很多不明确的实体，对吗？00:45:58.870 --> 00:46:02.230所以查尔斯・施瓦布在文中是00:46:02.230 --> 00:46:08.42590%的时间是一个组织的名字，因为有查尔斯・施瓦布经纪人。00:46:08.425 --> 00:46:10.945嗯，但在这句话里，00:46:10.945 --> 00:46:13.180在伍德塞德，拉里・埃里森和00:46:13.180 --> 00:46:16.524查尔斯・施瓦布可以谨慎地住在树林中，00:46:16.524 --> 00:46:19.450这就是查尔斯・施瓦布的名字。00:46:19.450 --> 00:46:25.330所以需要有一点不同的理解，才能使它正确。00:46:25.330 --> 00:46:30.250可以。嗯，那我们该怎么办？00:46:30.250 --> 00:46:32.875所以这意味着，嗯，00:46:32.875 --> 00:46:40.270我们要做的是为在上下文中工作的语言构建分类器。00:46:40.270 --> 00:46:42.760嗯，你知道，总的来说，00:46:42.760 --> 00:46:45.460对一个词进行分类不是很有趣00:46:45.460 --> 00:46:49.000在一个环境之外，我们实际上在NLP中做的并不多。00:46:49.000 --> 00:46:51.685但是一旦你在一个环境中，嗯，00:46:51.685 --> 00:46:55.360那么做和命名实体识别就很有趣了00:46:55.360 --> 00:46:58.300一种情况是有很多其他地方出现。00:46:58.300 --> 00:47:00.085我是说，这里有一个稍微酷一点的，00:47:00.085 --> 00:47:02.500有一些词可以表示00:47:02.500 --> 00:47:06.190他们自己和他们的对手同时在一起，对吗？00:47:06.190 --> 00:47:09.040因此，制裁某件事既可以意味着允许00:47:09.040 --> 00:47:12.460或者它可能意味着惩罚那些做过的人00:47:12.460 --> 00:47:17.770种东西或者种东西，或者意味着种种子00:47:17.770 --> 00:47:20.440你在播种的东西00:47:20.440 --> 00:47:23.275把西瓜之类的东西的种子拿出来，对吧？00:47:23.275 --> 00:47:27.060你只需要知道它的上下文。00:47:27.060 --> 00:47:32.195可以。所以，这意味着我们可以对一个词进行分类的任务00:47:32.195 --> 00:47:36.995在其相邻词的上下文中，任何一个都有这样一个例子。00:47:36.995 --> 00:47:39.530问题是我们该怎么做？00:47:39.530 --> 00:47:42.770一个非常简单的方法就是说，“好吧，00:47:42.770 --> 00:47:46.145我们有一排字00:47:46.145 --> 00:47:50.105每个词都有一个从单词到vec的向量。00:47:50.105 --> 00:47:52.490嗯，也许我们可以平分一下00:47:52.490 --> 00:47:56.540这些词向量，然后对结果向量进行分类。00:47:56.540 --> 00:48:02.075问题是，这不太好，因为你失去了职位信息。00:48:02.075 --> 00:48:04.550你真的不知道00:48:04.550 --> 00:48:08.270这些词向量就是你要分类的词。00:48:08.270 --> 00:48:12.065所以，一个比这更好的简单方法就是，00:48:12.065 --> 00:48:16.190“那么，我们为什么不制作一个大矢量的单词窗口呢？”00:48:16.190 --> 00:48:20.510这是单词，每个单词都有一个词向量，00:48:20.510 --> 00:48:26.960所以要在这里对中间词进行分类，加上或减去两个词，00:48:26.960 --> 00:48:31.970我们只需要将这五个向量连接在一起，然后说00:48:31.970 --> 00:48:37.280一个更大的向量，让我们在这个向量上建立一个分类器。00:48:37.280 --> 00:48:41.480所以，我们对x窗口进行分类，它是00:48:41.480 --> 00:48:47.150啊，5d，如果我们用的是三维字向量。00:48:47.150 --> 00:48:56.525我们可以像以前那样做，嗯，00:48:56.525 --> 00:48:59.570我们可以说，“好吧，00:48:59.570 --> 00:49:03.110对于那个大向量，我们要学习w权00:49:03.110 --> 00:49:06.965我们要通过一个SoftMax分类器，00:49:06.965 --> 00:49:09.485然后我们要做决定。”00:49:09.485 --> 00:49:13.445嗯，这是个很好的方法，00:49:13.445 --> 00:49:16.640嗯，为了这个目的。00:49:16.640 --> 00:49:19.580在最后一部分我想说的是00:49:19.580 --> 00:49:23.240开始看我的，嗯，矩阵微积分。00:49:23.240 --> 00:49:26.885你知道我们可以用这个模型00:49:26.885 --> 00:49:30.740一个分类器，学习它的权重，实际上，00:49:30.740 --> 00:49:35.750我们建议你看看网站上的讲义00:49:35.750 --> 00:49:41.345用这种类型的SoftMax分类器来实现。00:49:41.345 --> 00:49:46.940嗯，但是举个我在课堂上做的例子，我试着让它简单一点。00:49:46.940 --> 00:49:51.410嗯，我想这么做，我想得很快，因为我快没时间了。00:49:51.410 --> 00:49:56.150所以，早期著名的神经性NLP论文之一，嗯，00:49:56.150 --> 00:49:59.090这篇论文是科洛伯特和韦斯顿写的吗？00:49:59.090 --> 00:50:03.755一份2008年的ICML论文，实际上就在几周前，00:50:03.755 --> 00:50:08.270嗯，赢得了ICML 2018时间测试奖。00:50:08.270 --> 00:50:13.405嗯，还有一个更新的IT 2011杂志版本。00:50:13.405 --> 00:50:16.780嗯，他们用这个想法00:50:16.780 --> 00:50:21.880窗口分类，用于分配类，如命名实体，00:50:21.880 --> 00:50:25.015上下文中的单词，嗯，00:50:25.015 --> 00:50:28.255但他们的做法略有不同。00:50:28.255 --> 00:50:30.835所以，他们说，“嗯，00:50:30.835 --> 00:50:35.750我们有这些窗户，这是一个有，嗯，00:50:35.750 --> 00:50:38.480中间名为实体的位置和00:50:38.480 --> 00:50:41.660这是中间没有位置实体的一个。00:50:41.660 --> 00:50:47.825所以，我们要做的是建立一个返回分数的系统，00:50:47.825 --> 00:50:52.280在这种情况下，它应该像实数一样返回一个高分，并且00:50:52.280 --> 00:50:56.960如果没有的话，它应该会得到一个很低的分数。00:50:56.960 --> 00:51:00.605啊，本例中窗口中间的位置名。00:51:00.605 --> 00:51:05.105所以，明确地说，模型只是返回分数。00:51:05.105 --> 00:51:09.875所以，如果你的神经网络是一流的，00:51:09.875 --> 00:51:13.430然后用向量u做点积，00:51:13.430 --> 00:51:16.250然后你就可以得到最终的点积了，00:51:16.250 --> 00:51:19.295你只要返回一个实数。00:51:19.295 --> 00:51:22.595他们将其作为分类器的基础。00:51:22.595 --> 00:51:24.290所以在荣耀中，00:51:24.290 --> 00:51:27.950你所拥有的就是你拥有这扇语言的窗户，00:51:27.950 --> 00:51:32.810你查了每个单词的矢量，然后，嗯，00:51:32.810 --> 00:51:35.690乘以那个，好吧，你00:51:35.690 --> 00:51:38.810连接窗口的字向量。00:51:38.810 --> 00:51:42.680你把它们乘以一个矩阵，然后编辑一个偏差，得到00:51:42.680 --> 00:51:47.300第二个隐藏层是，然后乘以00:51:47.300 --> 00:51:51.710最后一个向量，给了你一个窗口分数，然后你00:51:51.710 --> 00:51:56.525希望分数大，如果是位置小，00:51:56.525 --> 00:51:58.860如果不是地点的话。00:51:58.860 --> 00:52:04.900所以，在这个假设的例子中，我们有四维的词向量，00:52:04.900 --> 00:52:07.720嗯，这意味着你知道窗户，00:52:07.720 --> 00:52:11.245这是一个20 x 1的向量。00:52:11.245 --> 00:52:14.440为了计算下一个隐藏层00:52:14.440 --> 00:52:17.515得到一个8乘20的矩阵加上偏压向量。00:52:17.515 --> 00:52:20.770然后，我们得到了这种8维的第二隐藏层00:52:20.770 --> 00:52:24.205然后我们计算最后一个实数。00:52:24.205 --> 00:52:31.250可以。嗯，至关重要的是，这是一个问题的例子。00:52:31.250 --> 00:52:33.800嗯，我们在这里加了一层，对吧？00:52:33.800 --> 00:52:36.620我们可以说这是一个词向量，00:52:36.620 --> 00:52:39.035上下文的一个大的词向量。00:52:39.035 --> 00:52:41.240让我们用一个SoftMax或者00:52:41.240 --> 00:52:45.455后勤分类在顶部，表示位置是或否。00:52:45.455 --> 00:52:47.900但是通过加入额外的隐藏层00:52:47.900 --> 00:52:51.860精确地说，这个额外的隐藏层可以计算00:52:51.860 --> 00:52:56.090输入字向量之间的非线性相互作用。00:52:56.090 --> 00:52:58.745所以，它可以计算出00:52:58.745 --> 00:53:03.410第一个词是“博物馆”一词，第二个词和第二个词00:53:03.410 --> 00:53:06.710是一个像介词一样的词在还是00:53:06.710 --> 00:53:11.300在那时这是一个很好的信号00:53:11.300 --> 00:53:14.900啊，在窗户中间的位置。00:53:14.900 --> 00:53:18.560所以，神经网络的额外层让我们计算00:53:18.560 --> 00:53:22.850这些基本特征之间的交互术语。00:53:22.850 --> 00:53:25.640可以。嗯，所以有00:53:25.640 --> 00:53:29.735这里还有几张幻灯片，介绍了他们模型的细节，00:53:29.735 --> 00:53:34.100但我暂时不去看，因为我有点落后了。00:53:34.100 --> 00:53:38.120最后我们得到了这个分数。00:53:38.120 --> 00:53:42.200这是我们的模型，我刚刚概述了我们的位置00:53:42.200 --> 00:53:47.810计算分数，我们想要一个大分数，嗯，位置。00:53:47.810 --> 00:53:52.625所以，我们要做的是考虑，嗯，00:53:52.625 --> 00:53:56.405我们如何使用这个模型，00:53:56.405 --> 00:53:59.000嗯，学习，嗯，00:53:59.000 --> 00:54:01.580我们在神经网络中的参数。00:54:01.580 --> 00:54:03.095嗯，特别是，00:54:03.095 --> 00:54:06.155记住这和我们以前的经历是一样的。00:54:06.155 --> 00:54:08.975我们有一个损失函数j，00:54:08.975 --> 00:54:11.480我们想解决问题，嗯，00:54:11.480 --> 00:54:17.180关于损耗函数当前θ参数的梯度。00:54:17.180 --> 00:54:22.490然后，我们想把它减去一点点，嗯，00:54:22.490 --> 00:54:27.320根据我们当前参数的学习率得到更新后的参数，00:54:27.320 --> 00:54:29.570如果我们重复这样做，那么随机的00:54:29.570 --> 00:54:32.870梯度下降我们会有更好的参数00:54:32.870 --> 00:54:35.900这给了事情更大的可能性00:54:35.900 --> 00:54:39.305我们在训练数据中观察到的。00:54:39.305 --> 00:54:42.215所以，我们想知道的是，00:54:42.215 --> 00:54:45.350一般来说，我们该怎么做，嗯，00:54:45.350 --> 00:54:50.585微分并求出损失函数的梯度？00:54:50.585 --> 00:54:55.370所以，我有点想在这节课上剩下的时间里解决这个问题，00:54:55.370 --> 00:54:59.270嗯，看看我们怎么能用手做到，嗯，00:54:59.270 --> 00:55:02.090使用数学，然后会导致00:55:02.090 --> 00:55:06.275更广泛地讨论了反向传播算法，00:55:06.275 --> 00:55:08.060嗯，下一个。00:55:08.060 --> 00:55:10.490可以。所以，如果我们这样做，嗯，00:55:10.490 --> 00:55:16.700用手计算梯度，我们在做多变量微积分，多变量导数。00:55:16.700 --> 00:55:23.090但特别是通常最有用的思考方式是00:55:23.090 --> 00:55:26.780矩阵演算，这意味着我们直接与00:55:26.780 --> 00:55:30.935计算梯度的向量和矩阵，00:55:30.935 --> 00:55:36.230这通常会更快更方便00:55:36.230 --> 00:55:41.660总结我们的神经网络层，而不是试图做一个非矢量化的方式。00:55:41.660 --> 00:55:44.555但这并不意味着这是唯一的方法。00:55:44.555 --> 00:55:47.285如果你对发生的事情有点困惑，00:55:47.285 --> 00:55:49.340有时会想清楚00:55:49.340 --> 00:55:53.780非矢量化方法可以更好地理解正在发生的事情，00:55:53.780 --> 00:55:55.040嗯，取得更多进展。00:55:55.040 --> 00:55:56.795所以，就像当，嗯，00:55:56.795 --> 00:55:59.570上次我用2vec这个词00:55:59.570 --> 00:56:03.260当我在黑板上写得太小的时候，00:56:03.260 --> 00:56:08.750抱歉，嗯，那是用非矢量化的方法计算重量，00:56:08.750 --> 00:56:10.700单独谈论它们。00:56:10.700 --> 00:56:12.875嗯，但我们要做的是，00:56:12.875 --> 00:56:14.945嗯，向量和矩阵。00:56:14.945 --> 00:56:19.175再次，寻找讲义来更详细地介绍这些材料。00:56:19.175 --> 00:56:22.025尤其是，这样就不会有人错过。00:56:22.025 --> 00:56:25.130嗯，让我澄清一下讲稿的意思。00:56:25.130 --> 00:56:29.645所以，如果你看左边的课程大纲，嗯，00:56:29.645 --> 00:56:32.840你可以下载幻灯片，00:56:32.840 --> 00:56:34.400在幻灯片的正下方，00:56:34.400 --> 00:56:35.915上面写着讲稿。00:56:35.915 --> 00:56:38.105这就是课堂讲稿的意思。00:56:38.105 --> 00:56:42.530在中间的一列中，它有一些读数，00:56:42.530 --> 00:56:47.150实际上，这里有一些不同的东西，包括类似的材料。00:56:47.150 --> 00:56:48.725嗯，所以有，嗯，00:56:48.725 --> 00:56:51.110所以它们可能也有帮助。00:56:51.110 --> 00:56:54.830但首先最接近我要呈现的东西，00:56:54.830 --> 00:56:58.730这是立即出现在幻灯片链接下的课堂讲稿。00:56:58.730 --> 00:57:03.815可以。嗯，所以我希望在这里，嗯，00:57:03.815 --> 00:57:06.520我的希望是：嗯，00:57:06.520 --> 00:57:10.730如果你不记得如何做单变量微积分，00:57:10.730 --> 00:57:13.565对不起，你基本上已经沉下去了，最好现在就走。00:57:13.565 --> 00:57:15.560嗯，[笑声]我假设你知道怎么做00:57:15.560 --> 00:57:21.245单变量微积分，我假设你知道向量和矩阵是什么。00:57:21.245 --> 00:57:23.960但是你知道，嗯，00:57:23.960 --> 00:57:27.680我有点希望即使你从来没有00:57:27.680 --> 00:57:31.820是多变量微积分还是你记不起来了？00:57:31.820 --> 00:57:34.220这有点像我们在这里必须做的，00:57:34.220 --> 00:57:37.445不难，你可以做到。00:57:37.445 --> 00:57:40.430所以，这就是你所做的。00:57:40.430 --> 00:57:42.230嗯，好吧。00:57:42.230 --> 00:57:46.820所以，如果我们有一个简单的函数，f等于x的立方，对吧。00:57:46.820 --> 00:57:50.765它的梯度，嗯，那么梯度就是斜率，对吗？00:57:50.765 --> 00:57:53.915说什么东西的坡度有多陡或有多浅，00:57:53.915 --> 00:57:59.075然后当我们看到斜坡的方向，当我们进入多个维度。00:57:59.075 --> 00:58:01.610它的梯度和导数一样。00:58:01.610 --> 00:58:04.430所以，它的导数是3x平方。00:58:04.430 --> 00:58:07.865如果你在点X等于3，你知道，00:58:07.865 --> 00:58:10.415这27种草料，00:58:10.415 --> 00:58:12.305嗯，非常陡峭。00:58:12.305 --> 00:58:20.060可以。那么，如果我们有一个函数有一个输出，但是现在它有许多输入呢？00:58:20.060 --> 00:58:24.170嗯，所以我们就是这样做的，嗯，00:58:24.170 --> 00:58:31.295就像我们做UTV或WTX的点产品一样，00:58:31.295 --> 00:58:33.170嗯，计算一个值。00:58:33.170 --> 00:58:36.410那么我们要计算的是00:58:36.410 --> 00:58:42.815相对于每个输入的偏导数向量的梯度。00:58:42.815 --> 00:58:45.065所以，你拿，嗯，00:58:45.065 --> 00:58:49.505函数的斜率，当你改变x1时，00:58:49.505 --> 00:58:55.040函数的斜率，当你通过，啊，的斜率改变x2时，00:58:55.040 --> 00:59:00.710当你改变xn的时候，每一个函数都可以像你做的那样计算出来。00:59:00.710 --> 00:59:04.910单变量微积分，你只需要把它们全部放到一个向量中，然后00:59:04.910 --> 00:59:09.920然后给你梯度，然后梯度和多维的，00:59:09.920 --> 00:59:14.570空间给你一种方向和坡度00:59:14.570 --> 00:59:19.730一个接触到你的多维，嗯，f函数的表面。00:59:19.730 --> 00:59:22.670可以。所以越来越可怕了，00:59:22.670 --> 00:59:24.500但比那更可怕一点00:59:24.500 --> 00:59:27.635因为如果我们有一个中性网络层，00:59:27.635 --> 00:59:32.000然后我们有一个函数，它将有n个输入，00:59:32.000 --> 00:59:33.770它们是输入神经元，00:59:33.770 --> 00:59:36.305它将有m个输出。00:59:36.305 --> 00:59:39.260如果是这样的话，嗯，00:59:39.260 --> 00:59:44.765然后有一个偏导数矩阵，称为雅可比矩阵。00:59:44.765 --> 00:59:47.435所以在雅各布中，嗯，00:59:47.435 --> 00:59:51.695你可以用这些偏导数，嗯，00:59:51.695 --> 00:59:54.125关于每个，嗯，00:59:54.125 --> 01:00:00.455沿行输出，并相对于每个输入向下输出列。01:00:00.455 --> 01:00:04.235所以你得到这些m乘以n的偏导数，01:00:04.235 --> 01:00:09.620考虑到输出和输入的每一个组合。01:00:09.620 --> 01:00:13.700嗯，但同样，你可以填充这个矩阵的每个单元格01:00:13.700 --> 01:00:18.545只要做单变量微积分，你就不会感到困惑。01:00:18.545 --> 01:00:24.665可以。嗯，当我们做word2vec的时候我们已经看到了，01:00:24.665 --> 01:00:29.435我们必须用这种中心工具来解决问题，01:00:29.435 --> 01:00:32.480嗯，为了锻炼，嗯，01:00:32.480 --> 01:00:35.150我们的衍生产品01:00:35.150 --> 01:00:37.490我们有一个神经网络模型01:00:37.490 --> 01:00:40.820我们一个接一个运行的函数序列。01:00:40.820 --> 01:00:43.280所以，嗯，在神经网络中，你01:00:43.280 --> 01:00:45.875一个接一个地运行一系列函数。01:00:45.875 --> 01:00:47.660所以我们必须使用，嗯，01:00:47.660 --> 01:00:52.880组成函数时求导数的链式法则。01:00:52.880 --> 01:00:56.240如果我们有一个变量函数，那么我们有，01:00:56.240 --> 01:01:00.665嗯，c等于3y，y等于x的平方。01:01:00.665 --> 01:01:03.725如果我们想锻炼，嗯，01:01:03.725 --> 01:01:07.430z对x的导数，01:01:07.430 --> 01:01:11.180我们说，啊哈，这是两个函数的组合。01:01:11.180 --> 01:01:13.190所以我用链式法则。01:01:13.190 --> 01:01:18.350这意味着我要做的是，乘以导数。01:01:18.350 --> 01:01:21.620所以我要，嗯，dz/dy。01:01:21.620 --> 01:01:25.220那是2倍，嗯，01:01:25.220 --> 01:01:29.030等等，对不起，我说错了，对吗？01:01:29.030 --> 01:01:30.530我的例子错了吗？01:01:30.530 --> 01:01:32.290是的，没错，DZ/DY。01:01:32.290 --> 01:01:34.180所以是的，DZ/DY才三岁。01:01:34.180 --> 01:01:36.790这是，对，这是顶线的导数，01:01:36.790 --> 01:01:40.120然后dy/dx是2x。01:01:40.120 --> 01:01:44.005我把它们相乘得到答案，嗯，01:01:44.005 --> 01:01:48.490z对x的导数是6x。01:01:48.490 --> 01:01:53.990可以。嗯，这一点会变得更奇怪，但这是真的。01:01:53.990 --> 01:01:57.560如果你同时有很多变量，01:01:57.560 --> 01:02:02.465你只需乘以雅各比就能得到正确的答案。01:02:02.465 --> 01:02:05.390所以如果我们现在想象我们的神经网络，01:02:05.390 --> 01:02:07.910这是我们典型的神经网络，对吧？01:02:07.910 --> 01:02:11.480所以我们做的是神经网络层01:02:11.480 --> 01:02:15.290我们的权重矩阵乘以它们的输入向量加上，01:02:15.290 --> 01:02:19.520嗯，偏差，然后我们把它放进一个非线性。01:02:19.520 --> 01:02:24.875如果我们想知道h和x的分式是什么，01:02:24.875 --> 01:02:27.320我们只是说，嗯，这是一个函数组合。01:02:27.320 --> 01:02:28.820所以这很容易做到。01:02:28.820 --> 01:02:31.340我们设计出我们的第一个雅各比人，01:02:31.340 --> 01:02:34.025这是h相对于z的分式，01:02:34.025 --> 01:02:38.225然后我们把它乘以z对x的粒子，01:02:38.225 --> 01:02:39.965我们得到了正确的答案。01:02:39.965 --> 01:02:44.135嗯，简单点。01:02:44.135 --> 01:02:47.840嗯，所以这里有点嗯01:02:47.840 --> 01:02:53.660一个雅各比的例子，这是一个特殊的情况，出现了很多。01:02:53.660 --> 01:02:58.850嗯，很高兴认识到这一点，我们将用我们的神经网络看到。01:02:58.850 --> 01:03:03.560所以我们所拥有的东西之一就是这些元素的激活函数。01:03:03.560 --> 01:03:06.035所以我们有h等于z的f。01:03:06.035 --> 01:03:09.200那么，嗯，什么是，嗯，01:03:09.200 --> 01:03:14.330h对z.um的偏导数，01:03:14.330 --> 01:03:18.710好吧，事情是-记住我们有点明智地应用这个元素。01:03:18.710 --> 01:03:22.475所以我们实际上是说，hi等于zi的f。01:03:22.475 --> 01:03:28.460所以，你知道，正式来说，这个函数有n个输入和n个输出，01:03:28.460 --> 01:03:33.035所以它的偏导数将是n乘n雅可比。01:03:33.035 --> 01:03:36.529但是如果我们想想那里发生了什么，01:03:36.529 --> 01:03:40.615嗯，实际上我们要找到的是，01:03:40.615 --> 01:03:45.010当我们制定条款的时候，我们正在制定，01:03:45.010 --> 01:03:50.985当你改变ZJ时，Zi的f是如何改变的？01:03:50.985 --> 01:03:55.280如果j不等于i，01:03:55.280 --> 01:03:57.410这不会有什么区别的，对吧？01:03:57.410 --> 01:04:00.020所以如果我的f函数是通过01:04:00.020 --> 01:04:04.160逻辑函数或任何其他绝对值，01:04:04.160 --> 01:04:07.880这对计算zi的f没有任何影响。01:04:07.880 --> 01:04:12.005如果我链zj，因为它不在方程中。01:04:12.005 --> 01:04:16.925因此，唯一将要发生的术语01:04:16.925 --> 01:04:22.235非零是我等于j的条件。01:04:22.235 --> 01:04:28.300所以求出这些偏导数，如果我不等于，它是零。01:04:28.300 --> 01:04:30.564如果我等于j，01:04:30.564 --> 01:04:34.390然后我们要计算出一个单变量微积分。01:04:34.390 --> 01:04:37.090什么是导数，嗯，01:04:37.090 --> 01:04:41.290激活功能的，嗯，01:04:41.290 --> 01:04:44.670这就是为什么，01:04:44.670 --> 01:04:49.445一个嗯，雅可比函数看起来像一个激活函数。01:04:49.445 --> 01:04:51.605这是一个对角线矩阵。01:04:51.605 --> 01:04:53.510其他一切都是零，01:04:53.510 --> 01:04:55.640我们认为这个激活功能，01:04:55.640 --> 01:04:57.455我们算出它的导数，01:04:57.455 --> 01:05:00.095然后我们计算这个差，嗯，01:05:00.095 --> 01:05:05.630我们有不同的，嗯，子价值观。01:05:05.630 --> 01:05:10.970可以。嗯，那是一个，01:05:10.970 --> 01:05:14.165嗯，激活函数的雅可比函数。01:05:14.165 --> 01:05:15.950其他主要案例是什么？01:05:15.950 --> 01:05:18.410呃，我们需要一个神经网络吗？01:05:18.410 --> 01:05:23.690在同一个讲稿中，我会慢慢地讲这些。01:05:23.690 --> 01:05:27.965但它们有点像我们在第一节课上看到的。01:05:27.965 --> 01:05:34.490所以如果我们想算出wx加b对x的偏导数，01:05:34.490 --> 01:05:39.000嗯，我们得到的是，01:05:39.580 --> 01:05:47.225如果我们想算出wx加b对b的偏导数，01:05:47.225 --> 01:05:54.080这意味着我们得到了一个单位矩阵，因为b有点像1b，对吗？01:05:54.080 --> 01:05:56.270这几乎总是在向量上，01:05:56.270 --> 01:06:00.455所以你只是让他们出来保护B.um，01:06:00.455 --> 01:06:02.585就是这样，嗯，01:06:02.585 --> 01:06:05.030我们看到的，嗯，01:06:05.030 --> 01:06:07.640当我们做向量这个词的时候。01:06:07.640 --> 01:06:13.415如果你有一个u和h的向量点积，你说，01:06:13.415 --> 01:06:17.870关于u的偏导数是多少？01:06:17.870 --> 01:06:21.530然后你出来换位。01:06:21.530 --> 01:06:25.340嗯，如果你以前没见过这些，01:06:25.340 --> 01:06:29.420嗯，看看讲义讲义，嗯，01:06:29.420 --> 01:06:34.010看看你能不能计算出它们，它们在家里是有意义的，嗯，01:06:34.010 --> 01:06:38.540但现在我们要相信这些并用它们01:06:38.540 --> 01:06:44.030看看我们如何计算出神经网络中的导数。01:06:44.030 --> 01:06:48.695可以。这就是我们以前看到的神经网络。01:06:48.695 --> 01:06:51.365所以我们有了一扇窗，01:06:51.365 --> 01:06:53.150我们在研究矢量词，01:06:53.150 --> 01:06:55.040我们要穿过一个隐藏层，01:06:55.040 --> 01:06:57.590然后我们做一个向量模态，嗯，01:06:57.590 --> 01:07:00.935矢量点积，你得到最后的分数。01:07:00.935 --> 01:07:05.840所以，我们[噪音]想要做什么来训练我们的神经网络，01:07:05.840 --> 01:07:15.425我们想知道S是如何根据模型的所有参数变化的。01:07:15.425 --> 01:07:18.950x，w，b，u.um，01:07:18.950 --> 01:07:24.320所以我们要求S的偏导数01:07:24.320 --> 01:07:30.110因为我们可以计算出来，如果你把B向上移动，01:07:30.110 --> 01:07:32.465嗯，分数越来越好，01:07:32.465 --> 01:07:36.350如果中间有个加号，那就好了，01:07:36.350 --> 01:07:38.435因此，我们要加快速度，01:07:38.435 --> 01:07:41.270嗯，B元素合适。01:07:41.270 --> 01:07:45.050好吧，嗯，我只是在做梯度01:07:45.050 --> 01:07:48.725从这里的得分来看，我跳过了那几张幻灯片。01:07:48.725 --> 01:07:50.930嗯，如果你只是，有点，01:07:50.930 --> 01:07:52.970盯着这张照片说，01:07:52.970 --> 01:07:58.400如何计算s对b的偏导数？01:07:58.400 --> 01:08:00.950嗯，可能看起来不明显。01:08:00.950 --> 01:08:04.520所以你要做的第一件事就是分手01:08:04.520 --> 01:08:09.080把方程分成简单的几部分组合在一起，对吗？01:08:09.080 --> 01:08:12.170所以你有输入x，01:08:12.170 --> 01:08:16.475然后进入z等于wx加b，01:08:16.475 --> 01:08:19.580然后你把它和下一件事结合起来。01:08:19.580 --> 01:08:23.240所以h等于z的f，我们的激活函数，01:08:23.240 --> 01:08:27.500然后这个h进入下一个，s等于uth。01:08:27.500 --> 01:08:29.960我们得到了这些函数序列。01:08:29.960 --> 01:08:35.480你很想尽可能地把事情搞得一团糟。01:08:35.480 --> 01:08:37.880我的意思是，我本可以把这件事说得更进一步的。01:08:37.880 --> 01:08:41.000我可以说z1等于wx，01:08:41.000 --> 01:08:44.165z等于z1加上b.um，01:08:44.165 --> 01:08:45.620结果是，嗯，01:08:45.620 --> 01:08:48.200但如果你只是加减乘除，01:08:48.200 --> 01:08:52.790你可以一步一步地完成这一点，因为这样的路径将01:08:52.790 --> 01:08:54.170做衍生产品时，01:08:54.170 --> 01:08:59.090但其他任何组合在一起的东西，你都想把它拼出来。01:08:59.090 --> 01:09:05.605可以。所以现在我们的神经网络正在做一系列的功能组合。01:09:05.605 --> 01:09:07.000当我们说，好的，01:09:07.000 --> 01:09:09.295我们知道怎么做，链式法则。01:09:09.295 --> 01:09:13.975所以如果你想算出s对b的分式，01:09:13.975 --> 01:09:19.960它只是一路上每一步衍生产品的产物。01:09:19.960 --> 01:09:25.130所以它是，关于h乘以h的s的一部分。01:09:25.130 --> 01:09:30.530关于z乘以z，关于b，这会给我们正确的答案。01:09:30.530 --> 01:09:35.225所以我们要做的就是计算它。01:09:35.225 --> 01:09:38.580嗯，所以，我认为这只是一种01:09:38.580 --> 01:09:42.040好的，我们来看看这篇文章的每一步。01:09:42.040 --> 01:09:44.975可以。现在我们要计算它。01:09:44.975 --> 01:09:49.800所以这就是我要使用雅各比人的地方01:09:49.800 --> 01:09:54.415在前一张幻灯片上没有太多证据的断言。01:09:54.415 --> 01:10:00.280可以。所以首先，我们有ds/dh。01:10:00.280 --> 01:10:03.900这就是两个向量的点积。01:10:03.900 --> 01:10:11.070所以，呃，雅可比式的，就是h转置。01:10:11.070 --> 01:10:12.640好吧，这是个开始。01:10:12.640 --> 01:10:15.935然后我们得到，h等于z的f。01:10:15.935 --> 01:10:18.505这就是激活功能。01:10:18.505 --> 01:10:22.270所以，雅各比式的是01:10:22.270 --> 01:10:27.615这个对角矩阵是由函数的元素阶导数构成的。01:10:27.615 --> 01:10:32.190然后我们得到z的一部分。01:10:32.190 --> 01:10:37.080关于b，这就是作为单位矩阵出现的比特。01:10:37.080 --> 01:10:45.950所以这就给我们计算，s对b的偏导数。01:10:46.460 --> 01:10:51.890所以我们可以看到-单位矩阵01:10:51.890 --> 01:10:57.905所以我们最后得到的是，z的素数的ht乘以f的组合。01:10:57.905 --> 01:11:05.960好吧，假设我们想继续计算，s对w的偏导数？01:11:05.960 --> 01:11:08.410嗯，因为起点是01:11:08.410 --> 01:11:13.640与我们计算每个阶段的链式法则完全相同。01:11:13.640 --> 01:11:19.120所以，首先你要做的就是01:11:19.120 --> 01:11:24.500从wx部分中取出z，然后通过非线性，01:11:24.500 --> 01:11:27.860然后做向量的点积。01:11:27.860 --> 01:11:29.700所以这部分是相同的。01:11:29.700 --> 01:11:33.500你应该注意的是如果你01:11:33.500 --> 01:11:39.620比较s相对于w的部分与s相对于b的部分，01:11:39.620 --> 01:11:45.510大多数都是一样的，只是最后的部分不同。01:11:45.510 --> 01:11:49.260就我们的神经网络来说，这是有意义的，对吗？01:11:49.260 --> 01:11:57.370当我们有了神经网络，W和B就来了。01:11:57.370 --> 01:12:01.740一旦你对他们做了一些事情，你就要把事情解决了。01:12:01.740 --> 01:12:07.230同样的激活功能和做同样的点产品来创建分数。01:12:07.230 --> 01:12:11.085所以，你做的计算方法和你现在做的是一样的。01:12:11.085 --> 01:12:13.675所以你应该得到01:12:13.675 --> 01:12:16.100相同的衍生产品01:12:16.100 --> 01:12:20.270发生-与发生在那一点的偏导数相同。01:12:20.270 --> 01:12:26.410哎呀。实际上你知道01:12:26.410 --> 01:12:29.930这些偏导数对应于01:12:29.930 --> 01:12:35.650神经网络中的计算，在W和B所在的位置上。01:12:35.650 --> 01:12:40.585所以这些通常被称为delta，01:12:40.585 --> 01:12:44.880注意delta，它不同于偏导数d。01:12:44.880 --> 01:12:49.295delta被称为误差信号和神经网络对话。01:12:49.295 --> 01:12:52.520所以，这就是你所计算的01:12:52.520 --> 01:12:55.070上面的偏导数01:12:55.070 --> 01:12:59.885计算偏导数的参数。01:12:59.885 --> 01:13:04.270所以，我们下次会看到很多秘密，01:13:04.270 --> 01:13:11.240反向传播的很多秘密是01:13:11.240 --> 01:13:15.160只是我们想在01:13:15.160 --> 01:13:19.525这就是计算机科学人们喜欢做有效计算的方式。01:13:19.525 --> 01:13:23.170所以我们要注意的是01:13:23.170 --> 01:13:28.510一个来自上面的错误信号，我们想计算一次。01:13:28.510 --> 01:13:31.145然后在计算时重用它01:13:31.145 --> 01:13:36.075关于w和b的偏导数。01:13:36.075 --> 01:13:43.100可以。所以还有两件事要做。01:13:43.100 --> 01:13:46.920所以一个是好的，01:13:46.920 --> 01:13:49.990知道偏导数是什么是有用的。01:13:49.990 --> 01:13:53.150关于w的s实际上看起来像。01:13:53.150 --> 01:13:55.065我的意思是，这是一个数字，一个向量，01:13:55.065 --> 01:13:58.185矩阵，三维张量？01:13:58.185 --> 01:14:02.000然后我们真的想算出它的价值01:14:02.000 --> 01:14:05.970为了实现它的价值，我们仍然需要努力01:14:05.970 --> 01:14:08.640关于z的偏导数01:14:08.640 --> 01:14:13.755但是，如果我们首先试着弄清楚它的形状，01:14:13.755 --> 01:14:17.160它有什么样的形状？01:14:17.160 --> 01:14:20.900实际上这有点棘手01:14:20.900 --> 01:14:25.240做这种矩阵演算有点下流。01:14:25.240 --> 01:14:31.060既然我们的权向量是一个n乘m的矩阵，01:14:31.060 --> 01:14:37.870s关于w的部分的最终结果是我们有一个函数01:14:37.870 --> 01:14:45.995n乘以m输入w的所有元素，只输入一个输出，即我们的分数。01:14:45.995 --> 01:14:49.920所以，听起来就像我之前说的那样01:14:49.920 --> 01:14:54.380应该有一个乘以N的M雅各比。01:14:54.380 --> 01:14:57.010但事实证明这不是我们真正想要的，对吧？01:14:57.010 --> 01:15:01.450因为我们想做的是使用我们计算的01:15:01.450 --> 01:15:07.380在这个随机梯度下降更新算法中。01:15:07.380 --> 01:15:12.470如果我们这样做有点像01:15:12.470 --> 01:15:19.130旧的权重矩阵，我们想减去一个位格式，得到一个新的权重矩阵。01:15:19.130 --> 01:15:29.465所以，如果雅各比的形状和W的形状相同，那就做个好人吧。-01:15:29.465 --> 01:15:32.520我们，总的来说，你总是想做什么01:15:32.520 --> 01:15:37.500神经网络遵循我们所说的形状约定01:15:37.500 --> 01:15:45.875我们将要表示雅可比，所以它和输入的形状是一样的。01:15:45.875 --> 01:15:50.775这整件事有点-坏的部分01:15:50.775 --> 01:15:56.105做矩阵演算的坏处。01:15:56.105 --> 01:16:00.615就像人们如何表示矩阵微积分有很多不一致。01:16:00.615 --> 01:16:03.030一般来说，如果你去不同的领域01:16:03.030 --> 01:16:05.890经济学和物理学有些人使用分子约定。01:16:05.890 --> 01:16:08.085有些人使用分母约定。01:16:08.085 --> 01:16:09.350我们两个都没用。01:16:09.350 --> 01:16:12.580我们将使用这个形状约定，以便匹配01:16:12.580 --> 01:16:16.655这样就可以很容易地更新体重。01:16:16.655 --> 01:16:23.125可以。所以。正确的。所以这就是我们想要的答案。01:16:23.125 --> 01:16:26.690所以，我们要做的最后一件事就是01:16:26.690 --> 01:16:30.320s相对于w的一部分是误差信号增量01:16:30.320 --> 01:16:34.105这将是答案的一部分，然后我们要计算出部分01:16:34.105 --> 01:16:39.415关于W.01:16:39.415 --> 01:16:44.360嗯，那会是什么？01:16:44.360 --> 01:16:47.690好吧，事实证明我即将01:16:47.690 --> 01:16:51.195我只剩两分钟了，就被钟救了。01:16:51.195 --> 01:16:57.070嗯，事实证明我们最终的目的是01:16:57.070 --> 01:17:03.250部分的积-delta乘以x的积。01:17:03.250 --> 01:17:08.020有效地，我们得到了W以上的局部误差信号，然后我们01:17:08.020 --> 01:17:13.715有了输入x，我们就得到了它们的一个外积。01:17:13.715 --> 01:17:19.565思考这个问题的方法，对于W。01:17:19.565 --> 01:17:23.190你知道，我们得到了w矩阵的元素，01:17:23.190 --> 01:17:26.780这些神经元之间的不同连接。01:17:26.780 --> 01:17:32.170所以每一个都是连接一个输出和一个输入。01:17:32.170 --> 01:17:35.990所以我们要做的是01:17:35.990 --> 01:17:40.700我们的偏导数的矩阵，将是01:17:40.700 --> 01:17:44.880适当输出的错误信号01:17:44.880 --> 01:17:50.505乘以输入，得到偏导数。01:17:50.505 --> 01:17:55.980我在最后一分钟很快就跳到前面去了。01:17:55.980 --> 01:17:59.850可以。所以，呃，是的。01:17:59.850 --> 01:18:01.430所以这就是我说的01:18:01.430 --> 01:18:04.675形状约定。我要跳过这个。01:18:04.675 --> 01:18:10.770可以。所以，嗯，我-我在结尾的时候有点没时间了，但是我的意思是，01:18:10.770 --> 01:18:13.860我想希望大部分01:18:13.860 --> 01:18:18.630如何使用链规则的想法，以及01:18:18.630 --> 01:18:22.080计算出衍生产品，然后计算出01:18:22.080 --> 01:18:26.400这些向量和矩阵导数的项。01:18:26.400 --> 01:18:31.170[噪音]本质上，我们要做的是要说，我们如何才能01:18:31.170 --> 01:18:37.205做啊，让电脑自动为我们做这件事，并有效地做。01:18:37.205 --> 01:18:39.820这就是那种深度学习框架01:18:39.820 --> 01:18:43.110TensorFlow和PyTorch可以，以及如何做到这一点。01:18:43.110 --> 01:18:45.600
We'll look at more next time.

