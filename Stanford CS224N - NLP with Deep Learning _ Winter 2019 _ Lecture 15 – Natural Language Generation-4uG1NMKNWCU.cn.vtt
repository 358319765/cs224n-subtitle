WEBVTT
Kind: captions
Language: en

00:00:04.760 --> 00:00:09.990
今天我们要学习自然语言生成。00:00:09.990 --> 00:00:12.460
这可能会有点不同00:00:12.460 --> 00:00:16.380
我之前的讲座因为这更像是一种调查，00:00:16.380 --> 00:00:17.745
有很多尖端技术00:00:17.745 --> 00:00:21.060
NLG目前正在进行的研究课题。00:00:21.060 --> 00:00:22.800
在这之前，00:00:22.800 --> 00:00:24.315
我们有几个通知。00:00:24.315 --> 00:00:26.370
所以我想主要宣布的是，00:00:26.370 --> 00:00:28.290
非常感谢大家的辛勤工作。00:00:28.290 --> 00:00:31.020
我知道，嗯，最近一两个星期很艰难。00:00:31.020 --> 00:00:33.450
作业五真的很难，00:00:33.450 --> 00:00:35.300
我认为，在8天内完成它是一个挑战。00:00:35.300 --> 00:00:38.000
所以我们非常感谢你们所付出的努力。00:00:38.000 --> 00:00:40.910
我们也知道项目提案是，00:00:40.910 --> 00:00:45.590
有时候有些人的期望很难理解。00:00:45.590 --> 00:00:47.990
嗯，是的，这两个都是新的组成部分00:00:47.990 --> 00:00:50.465
今年的班级去年没有出现。00:00:50.465 --> 00:00:52.115
所以你知道，00:00:52.115 --> 00:00:55.265
我们必须通过一些学习曲线以及教师。00:00:55.265 --> 00:00:57.710
所以我们真的想说非常感谢你00:00:57.710 --> 00:01:00.365
感谢你把所有的东西都放到这门课上。00:01:00.365 --> 00:01:03.230
请继续给我们正确的反馈00:01:03.230 --> 00:01:06.990
现在和季度末进行反馈调查。00:01:07.080 --> 00:01:10.685
好的，这是我们今天要做的概述。00:01:10.685 --> 00:01:13.955
所以今天我们要学习世界上正在发生的事情00:01:13.955 --> 00:01:16.960
自然语言生成的神经方法。00:01:16.960 --> 00:01:18.690
那是超级杯00:01:18.690 --> 00:01:22.065
超宽标题，自然生成语言。00:01:22.065 --> 00:01:25.685
NLG包含了很多研究领域00:01:25.685 --> 00:01:27.800
几乎每一个都可以00:01:27.800 --> 00:01:29.825
他们自己的讲座，我们可以教一个整体，00:01:29.825 --> 00:01:34.035
整整一个季度的课程，啊，NLG。00:01:34.035 --> 00:01:37.475
但是我们今天要讲的内容有很多。00:01:37.475 --> 00:01:41.520
而且，呃，呃，主要是，呃，00:01:41.520 --> 00:01:42.895
受这些东西的指引00:01:42.895 --> 00:01:46.040
我见过一些我认为很酷、有趣或令人兴奋的东西。00:01:46.040 --> 00:01:48.140
所以它绝不是全面的，但是00:01:48.140 --> 00:01:51.300
我希望你们会喜欢我们将要学习的一些东西。00:01:52.070 --> 00:01:56.660
好的，首先我们要回顾一下00:01:56.660 --> 00:02:01.070
已经知道自然语言生成，以确保我们在同一页上。00:02:01.070 --> 00:02:04.235
我们还会学到一些关于解码算法的知识。00:02:04.235 --> 00:02:05.870
我们之前学过一点，00:02:05.870 --> 00:02:08.329
贪婪解码和波束搜索解码，00:02:08.329 --> 00:02:10.280
但是今天我们要学习一些额外的信息00:02:10.280 --> 00:02:13.085
以及其他类型的解码算法。00:02:13.085 --> 00:02:15.710
在那之后我们会继续，00:02:15.710 --> 00:02:17.570
非常快的参观了很多00:02:17.570 --> 00:02:21.860
不同的NLG任务和神经方法的选择。00:02:21.860 --> 00:02:25.580
之后我们会讨论NLG研究中最大的问题，00:02:25.580 --> 00:02:29.945
这就是NLG的评估以及为什么它是如此棘手的情况。00:02:29.945 --> 00:02:33.665
最后，对NLG的研究进行总结。00:02:33.665 --> 00:02:37.440
当前的趋势是什么?未来的发展方向是什么?00:02:38.260 --> 00:02:47.875
好。第一部分，我们来回顾一下。00:02:47.875 --> 00:02:51.440
自然语言生成来定义它00:02:51.440 --> 00:02:55.090
指生成某种文本的任何设置。00:02:55.090 --> 00:02:58.385
例如，NLG是一个重要的子组件00:02:58.385 --> 00:03:01.400
很多不同的任务，比如，机器翻译，00:03:01.400 --> 00:03:04.370
我们已经见过，呃，抽象的总结，00:03:04.370 --> 00:03:06.305
我们以后会学到更多，00:03:06.305 --> 00:03:09.760
对话既聊天又基于任务。00:03:09.760 --> 00:03:15.155
还有一些创造性的写作任务，比如写故事，甚至写诗。00:03:15.155 --> 00:03:17.420
NLG也是，00:03:17.420 --> 00:03:19.610
自由形式的问题回答。00:03:19.610 --> 00:03:22.280
我知道你们很多人现在都在做小组项目00:03:22.280 --> 00:03:26.030
这不是NLG任务因为你只是从，00:03:26.030 --> 00:03:27.245
原始文件。00:03:27.245 --> 00:03:29.510
但还有其他回答问题的任务00:03:29.510 --> 00:03:31.775
有一个自然语言生成组件。00:03:31.775 --> 00:03:35.870
图像字幕是另一个例子，00:03:35.870 --> 00:03:38.610
一个有NLG子组件的任务。00:03:38.810 --> 00:03:43.625
NLG是很多NLP任务的一个很酷的组件。00:03:43.625 --> 00:03:45.440
好的，让我们回顾一下。00:03:45.440 --> 00:03:47.450
首先我想回顾一下，00:03:47.450 --> 00:03:48.725
什么是语言建模?00:03:48.725 --> 00:03:53.300
我注意到有些人对此有点困惑，我想，00:03:53.300 --> 00:03:56.300
可能是因为名称语言建模听起来像它的意思00:03:56.300 --> 00:04:00.635
只是简单地编码语言，比如使用嵌入式或其他东西来表示语言。00:04:00.635 --> 00:04:02.930
作为一种提醒，00:04:02.930 --> 00:04:04.135
有更确切的意思。00:04:04.135 --> 00:04:09.245
语言建模的任务是预测到目前为止给出的下一个单词。00:04:09.245 --> 00:04:11.300
所以任何产生的系统00:04:11.300 --> 00:04:16.420
完成这项任务的条件概率分布称为语言模型。00:04:16.420 --> 00:04:18.645
如果这个语言模型，00:04:18.645 --> 00:04:20.025
系统是一个RNN，00:04:20.025 --> 00:04:24.190
然后我们通常将其缩写为rnn语言模型。00:04:24.350 --> 00:04:27.390
好吧，我希望你能记住。00:04:27.390 --> 00:04:29.060
我们要重述的下一件事是00:04:29.060 --> 00:04:31.060
还记得什么是条件语言模型吗?00:04:31.060 --> 00:04:35.040
条件语言建模的任务是当你预测，00:04:35.040 --> 00:04:37.910
下一个词是什么，但你也在适应00:04:37.910 --> 00:04:41.585
一些其他的输入x以及你目前所有的单词。00:04:41.585 --> 00:04:45.640
我们来回顾一下条件语言建模的一些例子包括，00:04:45.640 --> 00:04:49.145
机器翻译是指你对源句x进行条件设置，00:04:49.145 --> 00:04:52.940
你对你想要总结的输入文本进行了调整。00:04:52.940 --> 00:04:57.510
对话，你在限制你的对话历史等等。00:04:57.680 --> 00:05:03.455
好了，接下来我们快速回顾一下如何训练一个rn语言模型?00:05:03.455 --> 00:05:07.760
我想，它也可以是基于转换的语言模型或者基于cnn的语言模型，00:05:07.760 --> 00:05:11.090
现在你知道这些了，它可以是有条件的，也可以不是。00:05:11.090 --> 00:05:15.620
所以我想提醒你们的主要事情是当你训练这个系统的时候，00:05:15.620 --> 00:05:18.410
然后你输入你想要的目标序列00:05:18.410 --> 00:05:21.620
所以当它说目标句子来自语料库时，00:05:21.620 --> 00:05:24.290
也就是说，你有一个你想要的序列00:05:24.290 --> 00:05:27.785
生成并将其输入解码器，即rn语言模型。00:05:27.785 --> 00:05:31.385
然后预测接下来会出现什么词。00:05:31.385 --> 00:05:35.010
所以最重要的是在训练中，00:05:35.010 --> 00:05:39.430
我们把黄金，也就是参考目标句输入解码器，00:05:39.430 --> 00:05:41.980
不管解码器预测什么。00:05:41.980 --> 00:05:46.750
所以即使我们说这是一个非常糟糕的解码器它不能预测正确的单词，00:05:46.750 --> 00:05:48.550
你知道，这根本不是预测它们会很高，00:05:48.550 --> 00:05:51.790
没关系，我们还是，00:05:51.790 --> 00:05:55.595
输入目标-黄金目标序列到解码器。00:05:55.595 --> 00:05:58.660
我强调这一点是因为后面会讲到，00:05:58.660 --> 00:06:00.895
这种训练方法叫做教师强迫。00:06:00.895 --> 00:06:03.170
这可能是你在其他地方遇到过的一个短语。00:06:03.170 --> 00:06:05.975
是的，它指的是老师，00:06:05.975 --> 00:06:09.160
这有点像黄金输入是强迫的，00:06:09.160 --> 00:06:11.260
在每个步骤中使用它的语言模型00:06:11.260 --> 00:06:14.495
而不是对每一步都使用自己的预测。00:06:14.495 --> 00:06:18.630
这就是如何训练一个可能是有条件的rn语言模型。00:06:18.630 --> 00:06:20.610
呃,好吧。00:06:20.610 --> 00:06:22.815
现在我们来回顾一下解码算法。00:06:22.815 --> 00:06:26.945
你已经有了训练有素的语言模型可能是有条件的。00:06:26.945 --> 00:06:29.345
问题是如何使用它来生成文本?00:06:29.345 --> 00:06:32.000
所以答案是你需要一个解码算法。00:06:32.000 --> 00:06:34.640
解码算法是你用来00:06:34.640 --> 00:06:37.585
从训练好的语言模型生成文本。00:06:37.585 --> 00:06:39.680
所以，在NMT讲座中00:06:39.680 --> 00:06:42.470
几周前，我们学习了两种不同的解码算法。00:06:42.470 --> 00:06:45.620
我们学习了贪婪解码和波束搜索。00:06:45.620 --> 00:06:47.795
让我们快速回顾一下。00:06:47.795 --> 00:06:51.215
贪心解码是一个很简单的算法。00:06:51.215 --> 00:06:53.030
每走一步，你只需要做你想做的00:06:53.030 --> 00:06:55.895
根据语言模型选出最可能的单词。00:06:55.895 --> 00:06:59.245
你可以用argmax作为下一个单词，00:06:59.245 --> 00:07:01.340
您将其作为下一步的输入输入。00:07:01.340 --> 00:07:03.860
一直这样做，直到得到某种结果00:07:03.860 --> 00:07:06.620
令牌或当您达到某个最大长度时。00:07:06.620 --> 00:07:09.805
我想你们都很熟悉这个因为你们在作业五中做过。00:07:09.805 --> 00:07:15.220
是的，这张图显示了贪婪解码是如何产生句子的。00:07:15.220 --> 00:07:17.415
我们之前学过，00:07:17.415 --> 00:07:19.610
由于缺少一种回溯和00:07:19.610 --> 00:07:22.130
如果你选错了00:07:22.130 --> 00:07:25.050
贪婪解码的输出通常是，00:07:25.050 --> 00:07:30.280
非常差，可能是不合语法的，也可能是不自然的，有点荒谬。00:07:30.280 --> 00:07:33.275
好了，让我们回顾一下波束搜索解码。00:07:33.275 --> 00:07:38.660
因此，波束搜索是一种寻找高概率序列的搜索算法。00:07:38.660 --> 00:07:43.610
所以如果我们在做翻译这个序列就是翻译单词的序列，00:07:43.610 --> 00:07:48.200
一次追踪多个可能的序列。00:07:48.200 --> 00:07:51.980
核心思想是在解码器的每一步，00:07:51.980 --> 00:07:53.060
你要跟踪00:07:53.060 --> 00:07:57.515
K个最可能的部分序列我们称之为假设。00:07:57.515 --> 00:08:01.000
这里K是一个超超参数叫做光束大小。00:08:01.000 --> 00:08:02.570
我的想法是，00:08:02.570 --> 00:08:05.870
考虑到很多不同的假设我们将尝试有效地搜索00:08:05.870 --> 00:08:07.790
这是一个高概率序列00:08:07.790 --> 00:08:10.285
不能保证这是最优的，00:08:10.285 --> 00:08:12.870
最高概率序列。00:08:12.870 --> 00:08:15.930
在光束搜索的最后，00:08:15.930 --> 00:08:17.930
你达到了我们所说的停止标准00:08:17.930 --> 00:08:20.300
以前讲过，但我不会再详细讲了。00:08:20.300 --> 00:08:22.610
一旦你达到了停车标准00:08:22.610 --> 00:08:25.175
你选择概率最大的序列，00:08:25.175 --> 00:08:29.495
考虑到长度的调整，这就是输出。00:08:29.495 --> 00:08:31.400
再做一次。00:08:31.400 --> 00:08:35.435
这是我们在NMT课程中看到的关于波束搜索解码的图表，00:08:35.435 --> 00:08:40.025
一旦它完成，在这个场景中，我们有两个光束大小。00:08:40.025 --> 00:08:43.180
这是我们完成探索问题后的样子，00:08:43.180 --> 00:08:45.420
这是我们探索过的完整的树，00:08:45.420 --> 00:08:49.145
然后我们得到了某种停止准则我们确定了顶部，00:08:49.145 --> 00:08:50.480
假设和，00:08:50.480 --> 00:08:52.510
用绿色标出。00:08:52.510 --> 00:08:55.815
所以在波束搜索解码这个课题上，00:08:55.815 --> 00:08:57.810
前几天我在看电视，00:08:57.810 --> 00:09:00.710
我在《西部世界》里发现了一些东西。00:09:00.710 --> 00:09:06.020
我想主持人――[笑声]《西部世界》的人工智能主持人可能使用了波束搜索。00:09:06.020 --> 00:09:08.840
这是我没想到会在电视上看到的。00:09:08.840 --> 00:09:10.945
[笑声]这是一个场景，00:09:10.945 --> 00:09:12.390
顺便说一下，《西部世界》00:09:12.390 --> 00:09:14.240
一部科幻系列，00:09:14.240 --> 00:09:16.580
非常令人信服的类人AI系统。00:09:16.580 --> 00:09:19.040
有一个场景是00:09:19.040 --> 00:09:22.055
人工智能系统面临的现实是，00:09:22.055 --> 00:09:23.975
我想她是00:09:23.975 --> 00:09:29.850
不是人类，因为她在说话的时候看到了单词的生成系统，00:09:29.850 --> 00:09:31.680
我看着电视想，00:09:31.680 --> 00:09:32.775
那是光束搜索吗?00:09:32.775 --> 00:09:35.840
因为这个图看起来很像这个图，00:09:35.840 --> 00:09:38.580
嗯，但也许用更大的光束尺寸。00:09:38.580 --> 00:09:40.605
所以，我想，这很酷，因为，你知道，00:09:40.605 --> 00:09:43.490
当你在电视上看到光束搜索时，人工智能已经成为主流。00:09:43.490 --> 00:09:45.200
如果你放大的很厉害，你就能看到00:09:45.200 --> 00:09:49.470
截图中还有一些令人兴奋的词，比如知识库，00:09:49.470 --> 00:09:51.170
前向链和后向链，00:09:51.170 --> 00:09:54.200
将其等同于前进支柱和后退支柱，00:09:54.200 --> 00:09:57.185
还有模糊逻辑算法和神经网络。00:09:57.185 --> 00:09:59.390
嗯，是的，波束搜索，00:09:59.390 --> 00:10:00.875
现在已经成为主流，00:10:00.875 --> 00:10:04.100
对《西部世界》来说已经足够好了00:10:04.100 --> 00:10:05.060
也许这对我们来说已经足够好了。00:10:05.060 --> 00:10:08.500
用光束搜索，对吧?00:10:08.500 --> 00:10:12.055
我们已经讨论过如何得到超参数k或者光束大小。00:10:12.055 --> 00:10:14.110
有一件事我们上节课没有讲，00:10:14.110 --> 00:10:16.660
现在我们要离开重述部分了，00:10:16.660 --> 00:10:20.980
是改变光束大小k的效果，00:10:20.980 --> 00:10:22.480
如果k很小，00:10:22.480 --> 00:10:26.065
然后你就会遇到类似贪婪解码的问题。00:10:26.065 --> 00:10:27.370
事实上，如果k等于1，00:10:27.370 --> 00:10:29.890
那么你实际上只是在做贪婪解码。00:10:29.890 --> 00:10:32.410
所以同样的问题，你知道，不合语法，00:10:32.410 --> 00:10:36.805
也许是不自然的，荒谬的，只是不正确的输出。00:10:36.805 --> 00:10:39.415
一旦k变大，00:10:39.415 --> 00:10:41.305
如果你有一个更大的光束尺寸，00:10:41.305 --> 00:10:46.750
然后你在做你的搜索算法但是考虑了更多的假设，对吧?00:10:46.750 --> 00:10:48.610
你的搜索空间变大了00:10:48.610 --> 00:10:51.100
你在考虑更多不同的可能性。00:10:51.100 --> 00:10:55.495
如果你这样做，我们通常会发现这样可以减少上面的一些问题。00:10:55.495 --> 00:10:58.540
所以你不太可能有这种不符合语法的情况，00:10:58.540 --> 00:11:01.015
你知道的，不连贯的输出。00:11:01.015 --> 00:11:04.930
但是提高k值也有一些缺点，00:11:04.930 --> 00:11:06.969
更大的k在计算上更昂贵00:11:06.969 --> 00:11:09.250
如果你想00:11:09.250 --> 00:11:11.530
例如，生成你的，00:11:11.530 --> 00:11:12.850
输出很大，00:11:12.850 --> 00:11:15.475
测试集的NMT例子。00:11:15.475 --> 00:11:17.680
但更严重的是00:11:17.680 --> 00:11:19.870
增加k会带来其他一些问题。00:11:19.870 --> 00:11:23.245
例如，在NMT中，00:11:23.245 --> 00:11:28.030
增加过多的光束尺寸实际上会降低蓝光分数。00:11:28.030 --> 00:11:30.625
这有点违反直觉，对吧?00:11:30.625 --> 00:11:32.875
因为我们在考虑光束搜索00:11:32.875 --> 00:11:35.410
这个算法试图找到最优解。00:11:35.410 --> 00:11:37.015
当然，如果你增加k，00:11:37.015 --> 00:11:39.970
那么你只会找到一个更好的解，对吧?00:11:39.970 --> 00:11:44.440
我想这里的关键是最优性的区别00:11:44.440 --> 00:11:46.300
就搜索问题而言00:11:46.300 --> 00:11:48.895
一个高概率序列和蓝色分数，00:11:48.895 --> 00:11:50.080
这是两个不同的东西，00:11:50.080 --> 00:11:54.310
也不能保证它们实际上是对应的，对吧?00:11:54.310 --> 00:11:57.850
我的意思是，在BLEU评分和实际翻译之间还是有区别的，00:11:57.850 --> 00:11:59.440
我们都知道质量。00:11:59.440 --> 00:12:01.720
如果你看看我链接的两篇论文00:12:01.720 --> 00:12:04.390
这里是显示，00:12:04.390 --> 00:12:07.330
增大光束的尺寸会降低蓝光的分数。00:12:07.330 --> 00:12:10.690
他们解释说这是主要原因00:12:10.690 --> 00:12:14.365
因为当你把光束的尺寸增大太多，00:12:14.365 --> 00:12:18.370
然后就会产生太短的翻译。00:12:18.370 --> 00:12:22.720
所以我的意思是，这在某种程度上解释了翻译太短，00:12:22.720 --> 00:12:24.130
所以它们的蓝色很低00:12:24.130 --> 00:12:26.230
可能漏掉了应该包含的单词。00:12:26.230 --> 00:12:29.860
但问题是，为什么较大的光束尺寸会产生较短的平移?00:12:29.860 --> 00:12:31.210
我觉得这很难回答。00:12:31.210 --> 00:12:34.975
无论在哪里，在这两篇论文中，我都没有看到一个明确的解释。00:12:34.975 --> 00:12:37.920
我想可能是更大范围的传球00:12:37.920 --> 00:12:41.565
我们看到有时用光束搜索当你真正增加你的，00:12:41.565 --> 00:12:43.440
搜索空间，使搜索更多00:12:43.440 --> 00:12:46.620
功能强大，可以考虑很多不同的选择。00:12:46.620 --> 00:12:49.620
它最终会发现这些高概率，00:12:49.620 --> 00:12:53.205
序列并不是你想要的。00:12:53.205 --> 00:12:55.260
当然，它们的概率很高00:12:55.260 --> 00:12:57.350
但它们并不是你想要的。00:12:57.350 --> 00:13:00.550
另一个例子是00:13:00.550 --> 00:13:03.625
在开放式任务中，比如聊天对话00:13:03.625 --> 00:13:04.825
你想说的是00:13:04.825 --> 00:13:07.330
跟你的谈话对象说一些有趣的事情，00:13:07.330 --> 00:13:10.300
如果我们使用大尺寸的光束搜索，00:13:10.300 --> 00:13:13.495
我们发现这可以得到一些非常通用的输出。00:13:13.495 --> 00:13:16.405
我举个例子来说明我的意思。00:13:16.405 --> 00:13:20.545
这些是闲聊的例子，00:13:20.545 --> 00:13:22.825
我正在做的对话项目。00:13:22.825 --> 00:13:24.190
这是，呃，00:13:24.190 --> 00:13:28.330
你的人类聊天伙伴说，我基本上吃新鲜的和生的食物，00:13:28.330 --> 00:13:29.785
所以我节省了杂货。00:13:29.785 --> 00:13:34.030
这是聊天机器人根据光束大小回复的信息。00:13:34.030 --> 00:13:37.790
我会让你们读的。00:13:43.590 --> 00:13:47.350
所以我想说这是你所看到的相当有特色的00:13:47.350 --> 00:13:50.500
当你提高或降低光束大小[噪声]时发生。00:13:50.500 --> 00:13:51.955
当光束尺寸较小时，00:13:51.955 --> 00:13:54.700
嗯，这可能更像是正题。00:13:54.700 --> 00:13:57.580
就像这里，我们可以看到吃得健康，吃得健康，00:13:57.580 --> 00:13:59.710
我是一名护士，所以我不吃生的食物，00:13:59.710 --> 00:14:02.335
这与用户所说的有关，00:14:02.335 --> 00:14:04.150
嗯，但这是一种糟糕的英语，对吗?00:14:04.150 --> 00:14:06.100
有一些重复，00:14:06.100 --> 00:14:08.020
它并不总是那么有意义，对吧?00:14:08.020 --> 00:14:09.580
嗯[噪音]但是，00:14:09.580 --> 00:14:10.885
当你提高光束的尺寸时，00:14:10.885 --> 00:14:12.250
然后收敛到00:14:12.250 --> 00:14:17.185
一个安全的，所谓正确的回答，但它是通用的，不太相关，对吧?00:14:17.185 --> 00:14:19.600
它适用于所有的情况，你以什么为生。00:14:19.600 --> 00:14:21.970
所以，00:14:21.970 --> 00:14:24.160
我在这里使用的数据集是，00:14:24.160 --> 00:14:25.240
一个叫Persona-Chat,00:14:25.240 --> 00:14:26.440
我稍后会告诉你们更多。00:14:26.440 --> 00:14:28.240
但它是，00:14:28.240 --> 00:14:31.315
它是一个聊天对话数据集，00:14:31.315 --> 00:14:35.575
conv-会话伙伴的角色是一系列特征的集合。00:14:35.575 --> 00:14:37.600
所以它一直说要当护士的原因是00:14:37.600 --> 00:14:39.160
我想是因为它在人物角色中。00:14:39.160 --> 00:14:42.340
[噪音]但这里的重点是，00:14:42.340 --> 00:14:45.685
我们有一个不幸的权衡，00:14:45.685 --> 00:14:48.805
没有金发姑娘区，这是很明显的。00:14:48.805 --> 00:14:50.410
我的意思是，有，有一个，是的，00:14:50.410 --> 00:14:53.290
这是一种不幸的权衡，00:14:53.290 --> 00:14:56.680
糟糕的输出，糟糕的英语，还有无聊的事情。00:14:56.680 --> 00:15:00.800
所以这是我们在波束搜索中遇到的问题之一。00:15:01.320 --> 00:15:03.790
好。我们已经讨论过，00:15:03.790 --> 00:15:06.445
贪婪解码和波束搜索。是的。00:15:06.445 --> 00:15:13.000
所以光束大小取决于[听不清]00:15:13.000 --> 00:15:14.050
问题是，我们能00:15:14.050 --> 00:15:17.755
一个自适应光束大小取决于你所处的位置?00:15:17.755 --> 00:15:19.060
你是说在序列里?00:15:19.060 --> 00:15:26.040
是的。那是在[听不清]。00:15:26.040 --> 00:15:29.220
是的。我的意思是，我想我可能听过这样的研究论文?00:15:29.220 --> 00:15:34.885
这就提高了假设空间的容量。00:15:34.885 --> 00:15:37.135
我的意思是，这听起来很尴尬，00:15:37.135 --> 00:15:40.990
因为，你知道，事情适合在你的GPU一个固定的空间。00:15:40.990 --> 00:15:42.580
但我觉得这是有可能的00:15:42.580 --> 00:15:46.225
我想你们必须学习增加光束的标准，00:15:46.225 --> 00:15:49.300
梁尺寸,是的。看起来似乎是可能的。00:15:49.300 --> 00:15:51.625
好。我们已经讨论过，00:15:51.625 --> 00:15:53.365
波束搜索和贪婪解码。00:15:53.365 --> 00:15:55.990
这是一个新的解码家族00:15:55.990 --> 00:15:59.095
算法很简单，基于采样的解码。00:15:59.095 --> 00:16:03.235
我称之为纯抽样是因为我不知道还能叫什么。00:16:03.235 --> 00:16:04.855
这只是，00:16:04.855 --> 00:16:07.360
简单的抽样方法是，00:16:07.360 --> 00:16:08.890
解码器t的时间步长，00:16:08.890 --> 00:16:12.040
你只需要从概率分布中随机抽取样本，00:16:12.040 --> 00:16:13.780
为了得到你的下一个词。00:16:13.780 --> 00:16:15.490
这很简单。00:16:15.490 --> 00:16:17.335
这就像贪婪解码。00:16:17.335 --> 00:16:19.285
但是我们没有选择最重要的词，00:16:19.285 --> 00:16:22.220
而是从分布中抽取样本。00:16:22.350 --> 00:16:28.600
我之所以称其为纯抽样是为了与n次以上抽样区分开。00:16:28.600 --> 00:16:30.640
这通常被称为top-k00:16:30.640 --> 00:16:33.400
采样，但是我已经把k叫做光束大小，00:16:33.400 --> 00:16:36.340
我不想把它搞混，所以我暂时把它叫做top-n抽样。00:16:36.340 --> 00:16:38.935
这里的想法也很简单。00:16:38.935 --> 00:16:40.585
每一步t，00:16:40.585 --> 00:16:44.035
你想从概率分布中随机抽取样本00:16:44.035 --> 00:16:48.265
你要限制到最可能的n个单词。00:16:48.265 --> 00:16:50.185
这就是说，00:16:50.185 --> 00:16:51.430
这很简单，00:16:51.430 --> 00:16:56.515
纯抽样方法但你想截断概率分布，00:16:56.515 --> 00:16:59.020
你知道，最可能的单词。00:16:59.020 --> 00:17:03.145
这里的想法有点像光束搜索，00:17:03.145 --> 00:17:06.610
给你一个超参数是介于贪婪解码和，00:17:06.610 --> 00:17:08.935
你知道，呃，非常详尽的搜索。00:17:08.935 --> 00:17:12.025
同理，我们有一个超参数n00:17:12.025 --> 00:17:15.340
它可以让你在贪婪搜索和纯粹抽样之间徘徊。00:17:15.340 --> 00:17:16.630
如果你想一下，00:17:16.630 --> 00:17:19.150
如果n是1，你就把上面的截去。00:17:19.150 --> 00:17:21.085
你只是取了贪心的arg max。00:17:21.085 --> 00:17:22.660
如果n是vocab大小，00:17:22.660 --> 00:17:24.085
然后你就不会把它截短了。00:17:24.085 --> 00:17:25.510
你从所有东西中取样，00:17:25.510 --> 00:17:27.790
这是纯抽样法。00:17:27.790 --> 00:17:31.000
所以，我希望这里应该很清楚，00:17:31.000 --> 00:17:33.715
如果你想一下如果你增加n，00:17:33.715 --> 00:17:36.910
然后你会得到更多样化和更有风险的产出，对吧?00:17:36.910 --> 00:17:39.235
因为你付出的更多00:17:39.235 --> 00:17:42.760
选择越多，概率分布就越低，00:17:42.760 --> 00:17:44.770
降低到不太可能的地方。00:17:44.770 --> 00:17:46.270
然后，如果你减少n，00:17:46.270 --> 00:17:48.580
然后你会得到更一般的安全输出因为你00:17:48.580 --> 00:17:51.890
更多地限制在高概率选项上。00:17:53.460 --> 00:17:56.440
所以这两个都比00:17:56.440 --> 00:17:58.630
光束搜索，我认为这一点很重要，00:17:58.630 --> 00:18:02.425
因为没有多种假设可以追踪，对吧?00:18:02.425 --> 00:18:04.735
因为在波束搜索中，解码器的每一步t，00:18:04.735 --> 00:18:06.115
你有k个不同的，00:18:06.115 --> 00:18:08.770
光束大小，需要跟踪的假设很多。00:18:08.770 --> 00:18:11.560
而这里，如果你只生成一个样本，00:18:11.560 --> 00:18:12.760
只有一件事需要追踪。00:18:12.760 --> 00:18:14.440
这是一个非常简单的算法。00:18:14.440 --> 00:18:19.310
这就是这些基于采样的算法优于波束搜索的一个优点。00:18:21.200 --> 00:18:25.560
好。所以，我想告诉你们的最后一件与解码有关的事情是，00:18:25.560 --> 00:18:27.165
呃，软最大值[噪音]温度。00:18:27.165 --> 00:18:30.930
如果你回忆一下解码器的时间步长t，00:18:30.930 --> 00:18:34.590
你的语言模型计算了某种概率分布P_t，00:18:34.590 --> 00:18:39.030
通过将softmax函数应用于某个地方得到的分数向量。00:18:39.030 --> 00:18:42.735
比如你的变压器或者RNN。00:18:42.735 --> 00:18:44.670
这又是softmax函数。00:18:44.670 --> 00:18:47.580
它说一个单词W的概率是这个softmax函数，00:18:47.580 --> 00:18:50.115
考虑到分数。00:18:50.115 --> 00:18:55.080
所以，这里软最大值上的温度是00:18:55.080 --> 00:19:01.200
温度超参数你要把它应用到这个，呃，softmax。00:19:01.200 --> 00:19:04.920
我们要做的就是用div-除以所有的分数，00:19:04.920 --> 00:19:06.375
或者你可以叫他们logits，00:19:06.375 --> 00:19:08.565
由温度超参数决定。00:19:08.565 --> 00:19:10.545
如果你仔细想想，00:19:10.545 --> 00:19:12.570
你会看到温度升高，00:19:12.570 --> 00:19:13.800
它在增加，00:19:13.800 --> 00:19:19.935
超参数，这会让你的概率分布更均匀。00:19:19.935 --> 00:19:23.415
这就涉及到一个问题，00:19:23.415 --> 00:19:25.830
当你把所有的分数乘以一个常数，00:19:25.830 --> 00:19:28.980
这对softmax有什么影响?00:19:28.980 --> 00:19:33.885
那么，当你取指数函数时，它们之间的距离是更大还是更小呢?00:19:33.885 --> 00:19:36.690
所以，你可以自己在纸上写出来，00:19:36.690 --> 00:19:38.520
但是作为一个00:19:38.520 --> 00:19:40.125
一种记忆捷径，00:19:40.125 --> 00:19:43.500
一个很好的思考方法是，如果你提高温度，00:19:43.500 --> 00:19:47.490
然后分布会融化，变得柔软，糊状，均匀。00:19:47.490 --> 00:19:48.810
如果你00:19:48.810 --> 00:19:51.150
降低温度，让它变冷，00:19:51.150 --> 00:19:54.690
概率分布变得更尖了，对吧?00:19:54.690 --> 00:19:59.115
所以，就像那些被评为高概率的东西变得更像，00:19:59.115 --> 00:20:02.670
和其他东西相比，概率高得不成比例。00:20:02.670 --> 00:20:05.535
我想这是一个很容易记住的方法。00:20:05.535 --> 00:20:07.935
今天我得把它写在纸上然后00:20:07.935 --> 00:20:09.135
我意识到，00:20:09.135 --> 00:20:12.375
温度可视化通常能让我更快到达那里。00:20:12.375 --> 00:20:18.480
所以，嗯，我想指出的一件事是softmax温度不是一个解码算法。00:20:18.480 --> 00:20:21.120
我知道我把它放在解码算法部分，00:20:21.120 --> 00:20:23.715
那只是因为这是一件事，a00:20:23.715 --> 00:20:29.880
你可以在测试时做一件简单的事情来改变解码的发生，对吧?00:20:29.880 --> 00:20:31.320
你不需要训练00:20:31.320 --> 00:20:33.765
温度是softmax。00:20:33.765 --> 00:20:36.225
所以，它本身不是解码算法。00:20:36.225 --> 00:20:38.415
这是一个你可以在考试时应用的技巧00:20:38.415 --> 00:20:41.040
结合解码算法。00:20:41.040 --> 00:20:44.385
例如，如果你在做光束搜索或者抽样，00:20:44.385 --> 00:20:48.615
然后你也可以用一个软最大值温度来改变，00:20:48.615 --> 00:20:54.160
你知道，这是一种风险与安全的权衡。00:20:55.220 --> 00:21:03.060
有什么问题吗?好。这是00:21:03.060 --> 00:21:06.270
总结一下我们刚刚学的解码算法。00:21:06.270 --> 00:21:09.375
贪心译码是一种简单的方法。00:21:09.375 --> 00:21:14.265
与其他算法相比，它的输出质量较低，至少是波束搜索。00:21:14.265 --> 00:21:17.160
光束搜索，特别是当你有一个高光束尺寸的时候，00:21:17.160 --> 00:21:20.955
它通过许多不同的假设来搜索高概率输出。00:21:20.955 --> 00:21:24.120
这通常会比贪婪搜索提供更好的质量，00:21:24.120 --> 00:21:26.730
但是如果光束太大，00:21:26.730 --> 00:21:29.385
我们之前讨论过的一些反直觉的问题。00:21:29.385 --> 00:21:32.865
你得到了一些高概率但不合适的输出。00:21:32.865 --> 00:21:35.415
比如说，有些东西太普通或者太短。00:21:35.415 --> 00:21:37.290
我们稍后会详细讨论。00:21:37.290 --> 00:21:41.220
抽样方法是一种获得更多多样性的方法，00:21:41.220 --> 00:21:43.095
呃，通过，通过随机性。00:21:43.095 --> 00:21:46.380
嗯，你的目标本身就是随机。00:21:46.380 --> 00:21:49.485
如果你想要，比如说，00:21:49.485 --> 00:21:51.930
开放式或创造性的世代环境，比如，00:21:51.930 --> 00:21:53.910
创作诗歌或故事，00:21:53.910 --> 00:21:56.370
那么抽样可能比00:21:56.370 --> 00:21:59.700
光束搜索因为你想要有一种随机性的来源，00:21:59.700 --> 00:22:02.160
创造性地写不同的东西。00:22:02.160 --> 00:22:07.170
而top-n抽样可以通过，00:22:07.170 --> 00:22:09.330
改变n，最后，00:22:09.330 --> 00:22:11.610
softmax温度是控制多样性的另一种方法。00:22:11.610 --> 00:22:14.520
这里有很多不同的旋钮。00:22:14.520 --> 00:22:16.260
这不是解码算法，00:22:16.260 --> 00:22:20.190
它只是一种技术，你可以应用在任何解码算法。00:22:20.190 --> 00:22:22.830
虽然用它没有意义00:22:22.830 --> 00:22:26.370
贪心解码，因为即使你让它更尖或更平，00:22:26.370 --> 00:22:29.950
argmax仍然是argmax，所以没有意义。00:22:31.400 --> 00:22:34.350
好。酷。我要讲第二部分。00:22:34.350 --> 00:22:39.135
第二部分是NLG任务和神经方法。00:22:39.135 --> 00:22:42.330
如前所述，这不是对NLG的概述。00:22:42.330 --> 00:22:43.620
那是完全不可能的。00:22:43.620 --> 00:22:45.195
这是一些选定的亮点。00:22:45.195 --> 00:22:47.490
特别地，我要从00:22:47.490 --> 00:22:51.270
深入研究一个我比较熟悉的NLG任务，00:22:51.270 --> 00:22:53.250
这就是总结。00:22:53.250 --> 00:22:57.660
因此，让我们从一个用于摘要的任务定义开始。00:22:57.660 --> 00:23:02.325
一个合理的定义是:给定某种输入文本x，00:23:02.325 --> 00:23:04.890
你想写一个比y更短的摘要00:23:04.890 --> 00:23:07.825
包含了x的主要信息。00:23:07.825 --> 00:23:11.360
因此，摘要可以是单文档的，也可以是多文档的。00:23:11.360 --> 00:23:16.510
单文档的意思是只有一个文档x的摘要y。00:23:16.510 --> 00:23:20.040
在多文档摘要中，您是说您想要编写00:23:20.040 --> 00:23:24.390
多个文档x_1到x_n的单个摘要y。00:23:24.390 --> 00:23:28.980
通常x_1到x_n之间会有一些重叠的内容。00:23:28.980 --> 00:23:32.040
例如，它们可能都是不同的新闻文章00:23:32.040 --> 00:23:35.220
来自不同的报纸关于同一事件的报道，对吧?00:23:35.220 --> 00:23:39.030
因为写一个总结总结所有这些是有意义的。00:23:39.030 --> 00:23:45.010
总结不同主题的东西就没那么有意义了。00:23:45.920 --> 00:23:48.015
还有，呃，00:23:48.015 --> 00:23:51.270
在总结中，任务定义的细分。00:23:51.270 --> 00:23:53.835
我将通过一些数据集来描述它。00:23:53.835 --> 00:23:58.455
这里有一些不同的非常常见的数据集尤其是在，00:23:58.455 --> 00:24:01.800
神经总结，嗯，它们对应不同的，00:24:01.800 --> 00:24:04.035
比如，文本的长度和不同的风格。00:24:04.035 --> 00:24:05.430
一个常见的例子是，00:24:05.430 --> 00:24:07.050
Gigaword数据集。00:24:07.050 --> 00:24:09.360
这里的任务是你想要映射的00:24:09.360 --> 00:24:13.710
标题新闻文章标题的前一两句话00:24:13.710 --> 00:24:16.290
[噪音]你可以把它想象成句子压缩，00:24:16.290 --> 00:24:19.140
尤其是如果是一个句子作为标题，因为你从00:24:19.140 --> 00:24:22.710
较长的句子到较短的标题式句子。00:24:22.710 --> 00:24:26.955
下一个是我00:24:26.955 --> 00:24:29.130
我想告诉你的是00:24:29.130 --> 00:24:31.320
这是一个中文摘要数据集，00:24:31.320 --> 00:24:33.690
我看到人们经常使用它。00:24:33.690 --> 00:24:36.480
这是，呃，来自一个微博，00:24:36.480 --> 00:24:39.945
人们写文章摘要的网站。00:24:39.945 --> 00:24:42.270
实际的总结任务是00:24:42.270 --> 00:24:44.790
你有一段文字，然后你想，00:24:44.790 --> 00:24:46.230
总结一下，00:24:46.230 --> 00:24:48.180
我想，一个简单的句子总结。00:24:48.180 --> 00:24:51.120
再来一个，实际上是两个，00:24:51.120 --> 00:24:55.650
是纽约时报和CNN/每日邮报的数据集00:24:55.650 --> 00:24:57.180
这两个都是形式，00:24:57.180 --> 00:24:59.940
你有一篇很长的新闻报道00:24:59.940 --> 00:25:03.690
几百个单词，然后你想把它总结成，00:25:03.690 --> 00:25:06.840
比如，一个单句或多句的总结。00:25:06.840 --> 00:25:10.560
《纽约时报》是由我想00:25:10.560 --> 00:25:13.125
图书管理员，00:25:13.125 --> 00:25:16.440
为图书馆写摘要。00:25:16.440 --> 00:25:18.885
然后，00:25:18.885 --> 00:25:22.365
今天我在写这个清单的时候发现了一个新的，00:25:22.365 --> 00:25:25.845
相当新的，就像过去六个月维基百科的数据集。00:25:25.845 --> 00:25:27.840
所以，就我所知，00:25:27.840 --> 00:25:31.950
您已经从wikiHow获得了完整的how to文章，然后您想将其归结为00:25:31.950 --> 00:25:34.200
总结句很巧妙00:25:34.200 --> 00:25:37.185
摘自维基百科全文。00:25:37.185 --> 00:25:38.790
它们有点像标题。00:25:38.790 --> 00:25:42.390
我看了这篇论文，00:25:42.390 --> 00:25:45.840
这很有趣，因为这是一种不同类型的文本。00:25:45.840 --> 00:25:48.990
你们可能已经注意到其他的大部分都是基于新闻的，00:25:48.990 --> 00:25:51.825
不，所以这就带来了不同的挑战。00:25:51.825 --> 00:25:57.360
另一种总结方法是句子简化。00:25:57.360 --> 00:26:00.690
所以，这是一个相关但实际上不同的任务。00:26:00.690 --> 00:26:04.410
概括地说，你想要写一些更短的内容00:26:04.410 --> 00:26:08.220
主要信息，但可能仍然是用复杂的语言写的，00:26:08.220 --> 00:26:13.785
而在句子简化中，你想用更简单的语言重写源文本，00:26:13.785 --> 00:26:15.420
语言更简单，对吧?00:26:15.420 --> 00:26:18.765
比如更简单的单词选择和句子结构。00:26:18.765 --> 00:26:21.240
这可能意味着它更短，但不一定。00:26:21.240 --> 00:26:22.890
举个例子，00:26:22.890 --> 00:26:25.950
简单的Wiki- Wikipedia是一个标准的数据集。00:26:25.950 --> 00:26:28.470
你的想法是，嗯，你知道，00:26:28.470 --> 00:26:31.440
标准的维基百科，你有一个简单的维基百科版本。00:26:31.440 --> 00:26:32.550
它们大部分是对齐的，00:26:32.550 --> 00:26:33.960
你想要从00:26:33.960 --> 00:26:37.365
有些句子在一个句子里相当于在另一个句子里[噪音]。00:26:37.365 --> 00:26:41.880
另一个数据来源是Newsela，这是一个网站，00:26:41.880 --> 00:26:44.085
为孩子们改写新闻00:26:44.085 --> 00:26:46.320
事实上，我认为在不同的学习水平。00:26:46.320 --> 00:26:49.900
所以，你有多种选择来判断它简化了多少。00:26:50.180 --> 00:26:54.690
好。所以,嗯,所以00:26:54.690 --> 00:26:59.085
这就是对不同任务的总结的定义。00:26:59.085 --> 00:27:00.810
现在我要给大家介绍一下，00:27:00.810 --> 00:27:02.190
主要是什么，00:27:02.190 --> 00:27:04.095
做总结的技巧。00:27:04.095 --> 00:27:06.390
总结有两种主要策略。00:27:06.390 --> 00:27:10.560
你可以称它们为提取摘要和抽象摘要。00:27:10.560 --> 00:27:12.735
正如我之前提到的，00:27:12.735 --> 00:27:15.720
这是你在提取摘要中选择的吗00:27:15.720 --> 00:27:19.050
对部分原文形成总结。00:27:19.050 --> 00:27:22.770
通常这是完整的句子但可能会更细;00:27:22.770 --> 00:27:24.825
也许，呃，短语或单词。00:27:24.825 --> 00:27:27.360
而抽象概括，你会00:27:27.360 --> 00:27:31.275
使用NLG技术生成一些新的文本。00:27:31.275 --> 00:27:33.840
所以这个想法是，你知道，从零开始。00:27:33.840 --> 00:27:37.590
我用视觉上的比喻就是高亮之间的区别00:27:37.590 --> 00:27:42.370
用高光笔或自己用笔写摘要。00:27:43.100 --> 00:27:47.160
我认为这两种技术的高级知识是00:27:47.160 --> 00:27:50.610
提取摘要基本上比较简单，00:27:50.610 --> 00:27:52.725
至少要建立一个像样的系统，00:27:52.725 --> 00:27:57.120
因为选择内容可能比从头开始编写文本更容易。00:27:57.120 --> 00:28:00.945
但是提取总结是有限制的，对吧?00:28:00.945 --> 00:28:02.760
因为你不能解释任何东西，00:28:02.760 --> 00:28:05.430
你不能做任何强大的句子压缩00:28:05.430 --> 00:28:08.475
如果你只能选择句子。00:28:08.475 --> 00:28:12.195
当然，还有作为范例的抽象概括00:28:12.195 --> 00:28:15.645
更灵活，更像人类的总结，00:28:15.645 --> 00:28:18.150
但正如我说的，这很难。00:28:18.150 --> 00:28:23.835
所以，我要给你们一个快速的视图，看看前神经总结是什么样子的。00:28:23.835 --> 00:28:24.945
这里我们有，呃，00:28:24.945 --> 00:28:26.700
这是来自，呃，00:28:26.700 --> 00:28:28.800
语言处理书。00:28:28.800 --> 00:28:33.120
所以，神经前总结系统主要是提取的。00:28:33.120 --> 00:28:35.370
就像神经前NMT，00:28:35.370 --> 00:28:37.125
我们在NMT课上学过，00:28:37.125 --> 00:28:40.395
它通常有一个管道，如图所示。00:28:40.395 --> 00:28:43.065
因此，一个典型的管道可能有三个部分。00:28:43.065 --> 00:28:46.170
首先，你有内容选择，00:28:46.170 --> 00:28:49.785
从源文档中选择要包含的一些句子。00:28:49.785 --> 00:28:52.155
然后，你要做一些信息00:28:52.155 --> 00:28:56.050
排序的意思是选择我应该把这些句子放在什么顺序。00:28:56.050 --> 00:28:59.750
这是一个非常重要的问题00:28:59.750 --> 00:29:01.580
做多个文档摘要00:29:01.580 --> 00:29:03.560
因为你的句子可能来自不同的文档。00:29:03.560 --> 00:29:05.060
最后，00:29:05.060 --> 00:29:08.255
你要做的句子实现实际上是，00:29:08.255 --> 00:29:12.135
把你选择的句子变成你真正的总结。00:29:12.135 --> 00:29:13.680
虽然我们没有这么做，00:29:13.680 --> 00:29:15.825
自由形式的文本生成，00:29:15.825 --> 00:29:19.290
可能会有一些编辑，比如，简化，编辑，00:29:19.290 --> 00:29:21.885
或者去掉多余的部分，00:29:21.885 --> 00:29:23.865
或者解决连续性问题。00:29:23.865 --> 00:29:26.220
例如，你不能引用00:29:26.220 --> 00:29:28.920
像她这样的人，如果你从一开始就没有介绍过他们。00:29:28.920 --> 00:29:32.380
也许你需要把她改成那个人的名字。00:29:33.180 --> 00:29:35.890
所以特别的[噪音]00:29:35.890 --> 00:29:37.945
这些神经前总结系统，00:29:37.945 --> 00:29:41.230
有一些非常复杂的内容选择算法。00:29:41.230 --> 00:29:43.450
举个例子，00:29:43.450 --> 00:29:46.240
你会有一些句子评分功能。00:29:46.240 --> 00:29:48.145
这是最简单的方法00:29:48.145 --> 00:29:50.770
你可能会给所有的句子单独打分吗00:29:50.770 --> 00:29:53.620
你可以根据特征给它们打分，比如，00:29:53.620 --> 00:29:56.650
你知道，句子里有主题关键词吗?00:29:56.650 --> 00:29:59.380
如果是这样，也许这是一个重要的句子，我们应该包括。00:29:59.380 --> 00:30:02.725
你可以计算这些，00:30:02.725 --> 00:30:06.760
关键字使用，呃，统计数据，例如tf-idf。00:30:06.760 --> 00:30:10.960
[噪音]你也可以使用非常基本但功能强大的功能，比如，00:30:10.960 --> 00:30:12.925
嗯，这个句子在文件中出现在哪里?00:30:12.925 --> 00:30:14.260
如果它在文档顶部附近，00:30:14.260 --> 00:30:16.510
那么它更有可能是重要的。00:30:16.510 --> 00:30:18.100
嗯，还有00:30:18.100 --> 00:30:21.910
一些更复杂的内容选择算法，比如，00:30:21.910 --> 00:30:25.420
有一些基于图形的算法可以把文档看作00:30:25.420 --> 00:30:29.005
一组句子，这些句子是图表的节点，00:30:29.005 --> 00:30:30.760
你想象所有的句子，00:30:30.760 --> 00:30:33.190
句子对之间有一条边，00:30:33.190 --> 00:30:36.760
边缘的权重就是句子的相似程度。00:30:36.760 --> 00:30:39.925
那么，如果你从这个意义上考虑这个图，00:30:39.925 --> 00:30:43.600
现在你可以试着找出哪些是句子00:30:43.600 --> 00:30:47.500
重要的是找出图表中的中心句子。00:30:47.500 --> 00:30:49.540
所以你可以应用一些通用的东西00:30:49.540 --> 00:30:52.930
gla- graph算法，计算出哪些[噪声]节点是中心，00:30:52.930 --> 00:30:56.180
这是找到中心句的一种方法。00:30:56.340 --> 00:31:03.355
好。所以，[噪音]回到总结任务上来。00:31:03.355 --> 00:31:06.940
我不记得我们是否已经讨论过胭脂。00:31:06.940 --> 00:31:08.140
我们已经讨论过蓝色。00:31:08.140 --> 00:31:09.820
但我现在要告诉你们胭脂是什么00:31:09.820 --> 00:31:12.400
用于摘要的主要自动度量。00:31:12.400 --> 00:31:17.695
因此，胭脂代表代表以recall为导向的评价指标。00:31:17.695 --> 00:31:19.480
我不确定这是不是他们想到的第一件事00:31:19.480 --> 00:31:21.790
或者他们做的和蓝色相配。00:31:21.790 --> 00:31:24.610
这是，00:31:24.610 --> 00:31:26.050
方程是这样的，00:31:26.050 --> 00:31:28.855
因为，我想胭脂指标之一。00:31:28.855 --> 00:31:31.210
稍后我会告诉你们更多这意味着什么，你们可以00:31:31.210 --> 00:31:34.105
阅读更多的原始文件，这是链接在底部。00:31:34.105 --> 00:31:38.095
总的来说胭脂和蓝色非常相似。00:31:38.095 --> 00:31:40.015
它基于n克重叠。00:31:40.015 --> 00:31:45.655
所以，与蓝色的主要区别是胭脂没有简洁的惩罚。00:31:45.655 --> 00:31:47.230
我待会再详细讲。00:31:47.230 --> 00:31:52.195
另一个重要的是胭脂是基于回忆而蓝色是基于精确。00:31:52.195 --> 00:31:53.440
你可以在标题中看到。00:31:53.440 --> 00:31:57.115
[噪音]嗯，如果你仔细想想，00:31:57.115 --> 00:32:02.245
我想你可以说精确度对机器翻译来说更重要。00:32:02.245 --> 00:32:09.130
也就是说，你只想生成出现在你的参考文献中的文本，00:32:09.130 --> 00:32:12.520
翻译，然后避免采取00:32:12.520 --> 00:32:14.770
一个非常保守的策略，你只生成00:32:14.770 --> 00:32:17.545
非常安全的东西在一个非常短的翻译。00:32:17.545 --> 00:32:20.035
这就是为什么要加上简洁的代价来确保00:32:20.035 --> 00:32:23.035
它试图写出足够长的东西。00:32:23.035 --> 00:32:24.640
相比之下，00:32:24.640 --> 00:32:26.290
回忆更重要00:32:26.290 --> 00:32:30.265
因为你想包含所有的信息，00:32:30.265 --> 00:32:33.190
你总结中的重要信息，对吧?00:32:33.190 --> 00:32:36.490
参考文献摘要中的信息是，00:32:36.490 --> 00:32:38.080
被认为是重要信息00:32:38.080 --> 00:32:40.240
回忆意味着你抓住了所有这些。00:32:40.240 --> 00:32:42.460
如果你假设你有00:32:42.460 --> 00:32:45.040
摘要系统的最大长度约束，00:32:45.040 --> 00:32:47.950
那么这两种情况就需要权衡，对吧?00:32:47.950 --> 00:32:52.720
你想要包括所有的信息，但不能写太长的摘要。00:32:52.720 --> 00:32:55.435
所以我认为这就是为什么你有这样的理由00:32:55.435 --> 00:32:58.150
这两个不同任务的查全率和查准率。00:32:58.150 --> 00:33:01.480
然而，令人困惑的是，通常是F1，00:33:01.480 --> 00:33:03.910
即精度与召回版本的结合00:33:03.910 --> 00:33:06.940
无论如何，胭脂在综述文献中都有报道。00:33:06.940 --> 00:33:09.490
说实话，我不太清楚为什么会这样00:33:09.490 --> 00:33:11.140
也许是因为缺乏，00:33:11.140 --> 00:33:13.495
显式最大长度限制。00:33:13.495 --> 00:33:17.815
不管怎么说，我试着查了一下，但是没有找到答案。00:33:17.815 --> 00:33:21.100
这是关于胭脂的更多信息。00:33:21.100 --> 00:33:22.840
如果你还记得，00:33:22.840 --> 00:33:24.940
蓝色是一个单独的数字，对吧?00:33:24.940 --> 00:33:26.980
蓝色只是一个数字，它确实是00:33:26.980 --> 00:33:30.640
不同n克精度的组合00:33:30.640 --> 00:33:32.950
通常是1-4，然而00:33:32.950 --> 00:33:36.910
胭脂评分通常为每n克单独报告。00:33:36.910 --> 00:33:42.250
因此，最常见的胭脂评分是胭脂-1、胭脂-2和胭脂- l。00:33:42.250 --> 00:33:47.365
所以，《胭脂一号》，不要和《侠盗一号》混淆了:《星球大战》的故事。00:33:47.365 --> 00:33:49.060
我觉得自从那部电影上映后00:33:49.060 --> 00:33:51.610
我看到很多人打错了，我想这是有关联的。00:33:51.610 --> 00:33:54.730
ROUGE-1是，00:33:54.730 --> 00:33:57.295
基于单图重叠，00:33:57.295 --> 00:34:01.015
[噪声]和基于双图重叠的ROUGE-2。00:34:01.015 --> 00:34:03.310
这有点像蓝色，00:34:03.310 --> 00:34:05.245
基于回忆，而不是精确。00:34:05.245 --> 00:34:10.450
更有趣的是ROUGE-L，它是最长公共子序列重叠。00:34:10.450 --> 00:34:14.590
这里的意思是你感兴趣的不仅仅是，00:34:14.590 --> 00:34:16.855
特定的n克匹配，00:34:16.855 --> 00:34:18.310
你知道有多少人00:34:18.310 --> 00:34:23.240
你能找到在这两种语言中出现的单词序列有多长?00:34:23.520 --> 00:34:26.635
所以你可以，呃，阅读更多关于这些指标的资料00:34:26.635 --> 00:34:29.065
在论文的前一页被链接。00:34:29.065 --> 00:34:31.495
另一件需要注意的重要事情是现在有噪音00:34:31.495 --> 00:34:35.200
一个方便的Python实现胭脂，00:34:35.200 --> 00:34:38.155
也许这并不明显，00:34:38.155 --> 00:34:40.420
但实际上这很令人兴奋，因为在很长一段时间里，00:34:40.420 --> 00:34:42.480
只有这个Perl脚本，00:34:42.480 --> 00:34:46.365
这是相当困难的运行，相当困难的设置和理解。00:34:46.365 --> 00:34:49.440
所以外面有人一直是个英雄00:34:49.440 --> 00:34:52.290
实现了一个纯Python版本的ROUGE并检查了它00:34:52.290 --> 00:34:55.320
确实与人们以前使用的Perl脚本匹配。00:34:55.320 --> 00:34:58.890
所以如果你们中有人在使用胭脂或为你们的项目做总结，00:34:58.890 --> 00:35:00.075
确保你00:35:00.075 --> 00:35:02.530
去用它吧，因为它可能会节省你一些时间。00:35:02.530 --> 00:35:06.085
(噪音)。00:35:06.085 --> 00:35:08.020
我们稍后再回到胭脂。00:35:08.020 --> 00:35:10.210
我知道你们在作业4中想过00:35:10.210 --> 00:35:12.790
蓝色作为度量单位的缺点是，00:35:12.790 --> 00:35:16.555
当然胭脂有一些短的缺点，以及一个衡量总结。00:35:16.555 --> 00:35:18.920
我们待会再谈这个。00:35:19.080 --> 00:35:23.230
好。所以，我们将继续学习神经方法来进行总结。00:35:23.230 --> 00:35:27.969
[噪音]回到2015年，00:35:27.969 --> 00:35:30.310
恐怕我没有另一个戏剧性的重演了。00:35:30.310 --> 00:35:32.710
[噪音]拉什等人。00:35:32.710 --> 00:35:35.590
发表第一篇seq2seq综述论文。00:35:35.590 --> 00:35:39.070
[噪音]所以他们把这个看成，00:35:39.070 --> 00:35:41.395
NMT最近非常成功，00:35:41.395 --> 00:35:44.500
为什么我们不把抽象摘要看作翻译任务呢00:35:44.500 --> 00:35:48.565
因此，应用标准的翻译seq2seq方法。00:35:48.565 --> 00:35:51.910
这就是他们所做的，他们申请了，00:35:51.910 --> 00:35:53.500
一个标准的注意力模型，00:35:53.500 --> 00:35:58.000
然后他们做了一个很好的工作，呃，十亿字总结。00:35:58.000 --> 00:35:59.620
就是你现在所在的地方00:35:59.620 --> 00:36:03.130
把新闻文章的第一句话变成标题。00:36:03.130 --> 00:36:05.575
这有点像句子压缩。00:36:05.575 --> 00:36:10.570
所以关键是，这个长度的数量级和NMT是一样的，对吧?00:36:10.570 --> 00:36:13.810
因为NMT是句子对句子，这是句子对句子，00:36:13.810 --> 00:36:15.805
也许最多两句两句。00:36:15.805 --> 00:36:18.310
所以这个效果很好，你可以做得很好，00:36:18.310 --> 00:36:20.920
使用这种方法生成标题或压缩句子。00:36:20.920 --> 00:36:23.515
(噪音)。00:36:23.515 --> 00:36:25.510
从2015年开始，00:36:25.510 --> 00:36:29.380
神经抽象概括的研究有了很大的发展。00:36:29.380 --> 00:36:31.435
你可以，00:36:31.435 --> 00:36:33.865
把这些发展放在一起，00:36:33.865 --> 00:36:35.440
一系列主题。00:36:35.440 --> 00:36:38.020
所以一个主题是让它更容易复制。00:36:38.020 --> 00:36:41.050
这看起来很明显，因为总的来说，00:36:41.050 --> 00:36:44.035
你会想要复制每一个，相当多的单词甚至短语，00:36:44.035 --> 00:36:46.615
但是不要复制太多。00:36:46.615 --> 00:36:48.130
还有一件事，如果你能做到的话00:36:48.130 --> 00:36:49.630
太容易复制，那么你就复制了太多。00:36:49.630 --> 00:36:52.600
所以，还有其他研究表明如何防止过多的复制。00:36:52.600 --> 00:36:58.135
[噪音]下一件事是某种层次或多层次的注意力。00:36:58.135 --> 00:36:59.470
正如我刚才展示的，00:36:59.470 --> 00:37:01.690
注意力是关键00:37:01.690 --> 00:37:04.000
到目前为止的抽象神经总结。00:37:04.000 --> 00:37:05.605
所以我们做了一些研究，00:37:05.605 --> 00:37:08.890
我们能否让这种注意力在更高层次上发挥作用，00:37:08.890 --> 00:37:12.100
低成本精版如此00:37:12.100 --> 00:37:16.030
我们可以在高层和低层进行选择。00:37:16.030 --> 00:37:18.985
另一个相关的东西是拥有00:37:18.985 --> 00:37:21.700
一些更全局的内容选择。00:37:21.700 --> 00:37:23.515
如果你还记得我们讨论的时候，00:37:23.515 --> 00:37:26.020
管道前神经总结，00:37:26.020 --> 00:37:28.435
他们有不同的内容选择算法。00:37:28.435 --> 00:37:30.250
我想你可以说，00:37:30.250 --> 00:37:32.110
有点天真的注意，00:37:32.110 --> 00:37:34.630
注意seq2seq不一定00:37:34.630 --> 00:37:37.495
对摘要进行内容选择的最佳方法是，00:37:37.495 --> 00:37:40.885
也许你想要一个更全球化的战略，在那里你选择什么是重要的。00:37:40.885 --> 00:37:44.049
当你在做这个小范围的总结时，就不那么明显了，00:37:44.049 --> 00:37:45.430
但是如果你想象你在总结00:37:45.430 --> 00:37:48.294
一篇完整的新闻文章，你要选择哪些信息，00:37:48.294 --> 00:37:50.455
决定每一个解码器步骤，00:37:50.455 --> 00:37:53.170
选择什么似乎不是最全球化的策略。00:37:53.170 --> 00:37:56.005
呃，我们还有什么?00:37:56.005 --> 00:37:59.410
我们可以用强化学习来直接最大化00:37:59.410 --> 00:38:01.300
胭脂或其他分散的目标00:38:01.300 --> 00:38:03.820
比如关心总结的长度。00:38:03.820 --> 00:38:07.495
我说离散是因为胭脂是不可微的，00:38:07.495 --> 00:38:09.640
生成输出的函数。00:38:09.640 --> 00:38:12.160
没有简单的方法来求导00:38:12.160 --> 00:38:14.200
用平常的方法在训练中学习。00:38:14.200 --> 00:38:20.170
我想说的最后一点是00:38:20.170 --> 00:38:24.040
恢复前神经思想，比如我提到的那些图算法00:38:24.040 --> 00:38:25.960
更早地投入工作00:38:25.960 --> 00:38:32.005
这些新的seq2seq抽象神经系统我相信还有更多。00:38:32.005 --> 00:38:34.150
我要给你们看一些，00:38:34.150 --> 00:38:37.660
尤其因为即使你对总结不是特别感兴趣，00:38:37.660 --> 00:38:40.930
我们在这里要探讨的很多想法实际上都是适用的00:38:40.930 --> 00:38:45.260
到NLG的其他领域或者只是NLP的其他领域的深度学习。00:38:45.300 --> 00:38:48.700
所以，列表上的第一件事是让它更容易复制，00:38:48.700 --> 00:38:50.875
这似乎是你要解决的第一件事，00:38:50.875 --> 00:38:53.335
如果你已经注意到了基本的seq2seq。00:38:53.335 --> 00:38:55.795
所以，嗯，复制机制，00:38:55.795 --> 00:38:58.900
它可以存在于总结之外。00:38:58.900 --> 00:39:03.160
你想要这个的原因是基本的seq2seq，00:39:03.160 --> 00:39:05.590
他们擅长写流畅的输出，正如我们所知，00:39:05.590 --> 00:39:09.835
但他们不擅长正确地复制罕见单词等细节。00:39:09.835 --> 00:39:13.210
复制机制是一种明智的想法，00:39:13.210 --> 00:39:17.950
让我们有一个明确的机制来复制单词。00:39:17.950 --> 00:39:20.140
例如，你可以用00:39:20.140 --> 00:39:25.375
注意分配-，选择你要复制的东西。00:39:25.375 --> 00:39:28.890
如果你允许两者都复制00:39:28.890 --> 00:39:32.235
通过你的语言模型以通常的方式生成单词，00:39:32.235 --> 00:39:37.220
现在你有了一种混合提取/抽象的方法来进行总结。00:39:37.220 --> 00:39:40.360
所以，有几篇论文提出了00:39:40.360 --> 00:39:43.330
某种复制机制的变体，我想，00:39:43.330 --> 00:39:45.040
之所以有多重是因为有00:39:45.040 --> 00:39:48.730
你可以做一些不同的选择来实现它，00:39:48.730 --> 00:39:53.380
这意味着有几个不同的版本来实现复制机制。00:39:53.380 --> 00:39:56.155
这里有几篇论文你可以看看。00:39:56.155 --> 00:39:58.690
我要给你们看一张纸上的图表，00:39:58.690 --> 00:40:01.150
几年前我和克里斯一起做过。00:40:01.150 --> 00:40:04.915
这只是复制机制的一个例子。00:40:04.915 --> 00:40:06.505
所以，我们的方法是，00:40:06.505 --> 00:40:08.485
我们说在每个解码器步骤上，00:40:08.485 --> 00:40:11.590
你要计算这个概率Pgen00:40:11.590 --> 00:40:15.370
生成下一个单词而不是复制它的概率，00:40:15.370 --> 00:40:19.090
这个想法是基于你当前的环境来计算的，00:40:19.090 --> 00:40:20.935
您当前的解码器隐藏状态。00:40:20.935 --> 00:40:22.585
一旦你做到了这一点，00:40:22.585 --> 00:40:24.790
然后你的注意力分布是00:40:24.790 --> 00:40:27.280
你得到了你想要的输出，00:40:27.280 --> 00:40:31.360
你知道，世代分布是正常的你要用这个Pgen，00:40:31.360 --> 00:40:32.545
这只是一个标量。00:40:32.545 --> 00:40:35.049
你可以用它来组合，00:40:35.049 --> 00:40:38.005
把这两个概率分布混合起来。00:40:38.005 --> 00:40:40.120
这个方程告诉你的是，00:40:40.120 --> 00:40:41.410
这是说00:40:41.410 --> 00:40:44.230
最终的输出分布，00:40:44.230 --> 00:40:45.595
下一个词是什么00:40:45.595 --> 00:40:47.080
这就像是在说，00:40:47.080 --> 00:40:48.685
它是生成的概率00:40:48.685 --> 00:40:51.895
乘以生成的概率分布00:40:51.895 --> 00:40:54.279
还有复制的概率00:40:54.279 --> 00:40:57.220
还有你当时在做什么。00:40:57.220 --> 00:41:01.555
所以，最重要的是，你在用注意力作为你的复制机制。00:41:01.555 --> 00:41:03.610
所以，注意力在这里起着双重作用。00:41:03.610 --> 00:41:07.885
这对发电机来说都很有用00:41:07.885 --> 00:41:10.000
你知道，呃，也许可以换种说法00:41:10.000 --> 00:41:12.460
但作为一种复制机制，它也很有用。00:41:12.460 --> 00:41:15.430
我认为这是这些不同的论文所做的不同的事情之一。00:41:15.430 --> 00:41:18.940
我想，我看过一篇论文，可能有两个分开的，00:41:18.940 --> 00:41:21.685
注意分配，一个用于复印，一个用于出席。00:41:21.685 --> 00:41:24.460
你可以做出其他不同的选择，比如，00:41:24.460 --> 00:41:27.430
D1 Pgen是介于0和00:41:27.430 --> 00:41:30.730
1或者你想让它是一个很难的东西要么是0要么是1。00:41:30.730 --> 00:41:33.970
你也可以做决定00:41:33.970 --> 00:41:37.000
您希望Pgen在培训期间接受监督吗?00:41:37.000 --> 00:41:40.165
你想给你的数据集加上注释说明这些东西是复制的吗，00:41:40.165 --> 00:41:43.540
这些东西不是，或者你只是想端到端的学习?00:41:43.540 --> 00:41:46.075
有很多方法可以做到这一点，00:41:46.075 --> 00:41:48.980
现在这已经变得非常非常标准了。00:41:50.100 --> 00:41:52.990
复制机制是这样的，00:41:52.990 --> 00:41:55.960
这似乎是个明智的想法，但有一个大问题，00:41:55.960 --> 00:41:58.330
这就是我之前提到的问题，00:41:58.330 --> 00:41:59.665
他们复制得太多了。00:41:59.665 --> 00:42:03.309
所以，当你――当你在总结的时候运行这些系统的时候，00:42:03.309 --> 00:42:05.530
你会发现他们最终会大量复制00:42:05.530 --> 00:42:08.860
很长的短语，有时甚至是整句话，00:42:08.860 --> 00:42:11.860
不幸的是，你梦想拥有一个抽象的摘要系统，00:42:11.860 --> 00:42:13.795
不会成功的，因为你的00:42:13.795 --> 00:42:16.510
复制增广的seq2seq系统00:42:16.510 --> 00:42:20.035
不幸的是，它最终变成了一个以采掘为主的体系。00:42:20.035 --> 00:42:22.060
还有一个问题，00:42:22.060 --> 00:42:25.165
复制机制模型是它们所不擅长的00:42:25.165 --> 00:42:28.600
整体内容选择，特别是输入文档很长时，00:42:28.600 --> 00:42:30.250
这就是我之前暗示的。00:42:30.250 --> 00:42:33.580
让我们假设，你总结了一些非常00:42:33.580 --> 00:42:37.090
就像一篇长达数百字的新闻文章，00:42:37.090 --> 00:42:38.995
你想写几句话的摘要。00:42:38.995 --> 00:42:41.575
这似乎不是最明智的选择00:42:41.575 --> 00:42:44.350
在写几句话总结的每一步，00:42:44.350 --> 00:42:46.390
但你又在选择关注什么00:42:46.390 --> 00:42:48.325
选择什么，总结什么。00:42:48.325 --> 00:42:52.795
似乎最好在开始时做一个全局决策，然后总结一下。00:42:52.795 --> 00:42:56.560
所以，是的，问题是，没有选择内容的总体策略。00:42:56.560 --> 00:43:03.825
这是我找到的一篇论文。不,还没有。00:43:03.825 --> 00:43:08.450
好。那么，如何才能更好地为神经摘要选择内容呢?00:43:08.450 --> 00:43:12.010
所以，如果你还记得我们在神经学之前的总结，00:43:12.010 --> 00:43:14.890
整个过程中有完全不同的阶段，对吧?00:43:14.890 --> 00:43:16.870
有了内容选择阶段00:43:16.870 --> 00:43:20.260
一种表面实现，即文本生成阶段。00:43:20.260 --> 00:43:22.750
但在我们的seq2seq注意力系统中，00:43:22.750 --> 00:43:25.240
这两个阶段完全混合在一起，对吧?00:43:25.240 --> 00:43:28.780
你在一步一步地实现表面实现也就是文本生成，00:43:28.780 --> 00:43:31.735
然后在每一个上面，你也在做内容选择。00:43:31.735 --> 00:43:35.305
所以，这说不通。00:43:35.305 --> 00:43:37.510
我找到了一篇论文，00:43:37.510 --> 00:43:39.745
我想是去年出版的00:43:39.745 --> 00:43:42.160
这是一种很好的00:43:42.160 --> 00:43:47.050
这个问题的简单解决方案叫做自底向上总结。00:43:47.050 --> 00:43:51.715
在这篇论文中，如果你看这个图，00:43:51.715 --> 00:43:53.260
主要的想法很简单。00:43:53.260 --> 00:43:57.370
它说，首先你要有一个内容选择阶段，这是00:43:57.370 --> 00:44:01.990
就像一个神经序列标记模型问题，对吧?00:44:01.990 --> 00:44:04.660
您将遍历源文档并00:44:04.660 --> 00:44:07.615
你把每个单词都标记为包含或不包含。00:44:07.615 --> 00:44:09.790
所以，你只是在决定什么是重要的，00:44:09.790 --> 00:44:11.680
好像应该把它做成什么总结什么的00:44:11.680 --> 00:44:15.625
没有，然后自下而上的注意力阶段说，00:44:15.625 --> 00:44:18.010
现在你要用注意力系统，00:44:18.010 --> 00:44:19.945
它将生成摘要。00:44:19.945 --> 00:44:21.610
你要用面膜吗?00:44:21.610 --> 00:44:23.125
应用一个硬约束，00:44:23.125 --> 00:44:26.905
你不能注意到标签上没有包括的单词。00:44:26.905 --> 00:44:30.595
这其实很简单但很有效，00:44:30.595 --> 00:44:34.090
因为这是一个更好的整体内容选择策略00:44:34.090 --> 00:44:38.800
第一个内容选择阶段是序列标签，00:44:38.800 --> 00:44:42.730
只做选择的同时不做生成，00:44:42.730 --> 00:44:44.800
我认为这是一个更好的方法00:44:44.800 --> 00:44:47.815
更好的决定包括什么，然后分开，00:44:47.815 --> 00:44:49.900
这也意味着作为一个很好的副作用，00:44:49.900 --> 00:44:53.500
生成模型中对长序列的复制更少。00:44:53.500 --> 00:44:56.830
因为如果你不能专心做事00:44:56.830 --> 00:44:58.225
你不应该包括，00:44:58.225 --> 00:45:01.960
这样就很难复制一个很长的序列，对吧?00:45:01.960 --> 00:45:05.320
比如你想复制一个完整的句子，但是这个句子有00:45:05.320 --> 00:45:08.980
很多都不包括文字，00:45:08.980 --> 00:45:11.635
你不能复制一个很长的序列，你必须把它分解。00:45:11.635 --> 00:45:12.970
那么，模型最终会做什么，00:45:12.970 --> 00:45:14.320
是不是要跳过，00:45:14.320 --> 00:45:17.110
跳过本应包含的部分，然后强制执行00:45:17.110 --> 00:45:20.650
更抽象地把各个部分放在一起。是的。00:45:20.650 --> 00:45:25.510
他们是如何反向传播掩蔽决定的，因为它看起来像00:45:25.510 --> 00:45:28.720
因为在培训中[听不清]掩盖决策。00:45:28.720 --> 00:45:32.035
是的，我想可能是分开训练的。00:45:32.035 --> 00:45:33.610
我是说，你可以去看看报纸。00:45:33.610 --> 00:45:35.890
最近几天我看了很多报纸，我不太记得了。00:45:35.890 --> 00:45:37.990
我认为，这可能是单独训练的，但他们可能00:45:37.990 --> 00:45:40.660
我试过一起训练它，但效果不太好。00:45:40.660 --> 00:45:42.860
我不确定。你可以去看看。00:45:43.200 --> 00:45:48.745
好。我想告诉你们的另一篇论文是，00:45:48.745 --> 00:45:53.965
使用强化学习直接最大化胭脂用于神经总结。00:45:53.965 --> 00:45:56.275
这是两年前的一篇论文。00:45:56.275 --> 00:45:58.360
主要的想法是他们可以用RL00:45:58.360 --> 00:46:01.870
直接优化在这种情况下，ROUGE-L，度规。00:46:01.870 --> 00:46:06.010
相比之下，训练的标准最大可能性是00:46:06.010 --> 00:46:07.840
我们一直在谈论的培训目标00:46:07.840 --> 00:46:10.390
到目前为止，关于语言模型的课程，00:46:10.390 --> 00:46:13.840
这不能直接优化ROUGE-L因为它是一个不可微函数。00:46:13.840 --> 00:46:16.870
所以他们使用了RL技术00:46:16.870 --> 00:46:21.820
在训练中计算胭脂值，00:46:21.820 --> 00:46:26.110
使用强化学习来支持模型。00:46:26.110 --> 00:46:33.220
因此，这篇论文有趣的发现是，如果他们只使用RL目标，00:46:33.220 --> 00:46:36.040
然后他们确实会得到更高的胭脂分数。00:46:36.040 --> 00:46:38.470
这样他们就能成功地优化00:46:38.470 --> 00:46:40.240
这是他们的目标00:46:40.240 --> 00:46:42.760
但问题是，当你这样做的时候，00:46:42.760 --> 00:46:44.725
你的人类判断力得分较低。00:46:44.725 --> 00:46:47.050
在右边我们看到只有RL模型00:46:47.050 --> 00:46:51.775
实际上可读性非常非常差人类的判断得分。00:46:51.775 --> 00:46:57.235
它比最大似然监督训练系统更糟糕。00:46:57.235 --> 00:47:00.685
这是他们博客上的一段话，00:47:00.685 --> 00:47:02.950
“我们观察到，我们的模特胭脂得分最高00:47:02.950 --> 00:47:05.335
还生成了几乎无法阅读的摘要。”00:47:05.335 --> 00:47:06.760
这是-这是，00:47:06.760 --> 00:47:08.140
我想这是个问题，对吧?00:47:08.140 --> 00:47:11.170
如果你想直接优化度规，00:47:11.170 --> 00:47:13.450
然后你可能会发现你在玩游戏00:47:13.450 --> 00:47:16.680
而不是对真正的任务进行优化，00:47:16.680 --> 00:47:20.550
因为我们知道，正如我们知道蓝色并不是一个完美的类比00:47:20.550 --> 00:47:22.530
实际翻译质量也是如此00:47:22.530 --> 00:47:26.255
和总结质量不是一个完美的类比。00:47:26.255 --> 00:47:28.660
但是他们确实做了一些很酷的事情，他们发现如果00:47:28.660 --> 00:47:31.419
你把这两个目标结合起来，00:47:31.419 --> 00:47:33.025
所以他们有点，呃，你知道，00:47:33.025 --> 00:47:36.910
预测语言模型的序列目标，然后他们也喜欢生产00:47:36.910 --> 00:47:41.305
一个整体的总结，得到一个高胭脂评分的目标，你把它们结合起来，00:47:41.305 --> 00:47:45.370
然后你就能得到一个更好的人的评价分数00:47:45.370 --> 00:47:48.220
这是我们最接近，00:47:48.220 --> 00:47:49.930
对实际摘要质量的度量。00:47:49.930 --> 00:47:54.340
(噪音)。00:47:54.340 --> 00:47:57.280
接下来我要讲的是对话，00:47:57.280 --> 00:48:01.750
这是另一种NLG，一类任务。00:48:01.750 --> 00:48:05.590
所以，真正的对话包含了各种各样的场景。00:48:05.590 --> 00:48:06.700
我们不会涵盖所有的，00:48:06.700 --> 00:48:08.800
这里是所有不同种类的概述00:48:08.800 --> 00:48:11.185
当人们说到对话的时候00:48:11.185 --> 00:48:15.490
有任务导向的对话这种对话指的是任何场景，00:48:15.490 --> 00:48:18.205
你想要在谈话中做点什么。00:48:18.205 --> 00:48:19.690
举个例子，如果你有00:48:19.690 --> 00:48:23.590
辅助性任务假设你有00:48:23.590 --> 00:48:27.040
对话代理正试图帮助人类用户进行此操作00:48:27.040 --> 00:48:30.700
比如提供客户服务或推荐，00:48:30.700 --> 00:48:32.890
回答问题，帮助用户，00:48:32.890 --> 00:48:35.950
你知道，完成一项任务，比如买东西或订东西。00:48:35.950 --> 00:48:38.350
这些就是虚拟系统所执行的任务00:48:38.350 --> 00:48:41.740
你的手机可以或者可以。00:48:41.740 --> 00:48:46.585
另一类以任务为导向的对话任务是合作任务。00:48:46.585 --> 00:48:49.150
这就是你有两个代理人的情况00:48:49.150 --> 00:48:52.120
试着通过对话一起解决一项任务。00:48:52.120 --> 00:48:54.715
与之相反的就是对抗性。00:48:54.715 --> 00:48:58.600
所以当你有两个代理人试图在一个任务中竞争时，00:48:58.600 --> 00:49:01.400
竞争是通过对话进行的。00:49:02.340 --> 00:49:08.950
[噪音]与任务导向对话相反的是社会对话。00:49:08.950 --> 00:49:13.600
所以，我想，除了社交，没有什么明确的任务。00:49:13.600 --> 00:49:16.105
所以闲聊对话，00:49:16.105 --> 00:49:20.200
只是对话，你只是为了社交或陪伴而做。00:49:20.200 --> 00:49:24.910
我也看到了一些关于治疗或心理健康对话的研究，00:49:24.910 --> 00:49:26.740
我不确定这是任务还是社交，00:49:26.740 --> 00:49:28.105
这是一种混合，00:49:28.105 --> 00:49:30.580
但我想这些就是我们的目标00:49:30.580 --> 00:49:34.285
为人类用户提供情感支持。00:49:34.285 --> 00:49:40.030
作为一个非常简短的概述，00:49:40.030 --> 00:49:42.070
深度学习00:49:42.070 --> 00:49:45.070
文艺复兴改变了对话研究，00:49:45.070 --> 00:49:48.595
我想你可以说在深度学习之前，00:49:48.595 --> 00:49:50.620
开放式的困难00:49:50.620 --> 00:49:53.830
自由形式的自然语言生成，意味着，00:49:53.830 --> 00:49:55.405
对话系统，00:49:55.405 --> 00:49:58.360
不是自由形式的NLG。00:49:58.360 --> 00:50:00.730
它们可能使用预定义的模板，也就是您所拥有的模板00:50:00.730 --> 00:50:03.775
一个模板，你只需要用内容填充一些槽，00:50:03.775 --> 00:50:06.700
或者你可以从00:50:06.700 --> 00:50:09.625
为了找到，00:50:09.625 --> 00:50:11.380
你知道，一个适合用户的回应。00:50:11.380 --> 00:50:13.330
这些绝不是简单的系统，00:50:13.330 --> 00:50:16.180
他们做了一些非常复杂的事情，比如决定，00:50:16.180 --> 00:50:19.570
他们的对话状态是什么，你应该使用什么模板，等等00:50:19.570 --> 00:50:23.905
到目前为止，所有的自然语言理解都是理解上下文的组成部分。00:50:23.905 --> 00:50:26.455
但有一个影响是，00:50:26.455 --> 00:50:28.825
深度学习的经验是00:50:28.825 --> 00:50:31.915
从2015年开始，00:50:31.915 --> 00:50:34.615
成为了标准00:50:34.615 --> 00:50:38.440
就像总结一样，很多论文将seq2seq方法应用到对话中。00:50:38.440 --> 00:50:43.435
这在某种程度上导致了对开放式、自由形式的对话系统的重新兴趣。00:50:43.435 --> 00:50:45.760
如果你想知道是怎么回事00:50:45.760 --> 00:50:48.130
早期的seq2seq对话文件是这样的，00:50:48.130 --> 00:50:53.090
这里有两种早期的可能是第一个应用seq2seq的。00:50:55.530 --> 00:51:00.400
好。所以人们很快就申请了seq2seq，00:51:00.400 --> 00:51:03.160
NMT方法的对话，但它很快成为00:51:03.160 --> 00:51:06.130
很明显，这种幼稚的应用00:51:06.130 --> 00:51:08.560
标准的NMT方法有00:51:08.560 --> 00:51:13.915
一些严重的普遍缺陷，当应用于像聊天对话这样的任务。00:51:13.915 --> 00:51:16.960
这甚至比总结更真实。00:51:16.960 --> 00:51:21.145
那么，这些严重的pervas普遍缺陷有哪些例子呢?00:51:21.145 --> 00:51:24.430
一个是泛泛之论或者无聊的回答，00:51:24.430 --> 00:51:26.710
我待会再详细讲。00:51:26.710 --> 00:51:29.005
另一个是不相关的反应。00:51:29.005 --> 00:51:30.175
所以那时，呃，00:51:30.175 --> 00:51:32.200
对话代理说了些什么00:51:32.200 --> 00:51:35.065
这和用户说的无关。00:51:35.065 --> 00:51:36.700
另一个是重复，00:51:36.700 --> 00:51:38.080
这是很基本的，00:51:38.080 --> 00:51:39.640
这种事经常发生。00:51:39.640 --> 00:51:44.275
这也是话语中的重复，也可能是跨话语的重复。00:51:44.275 --> 00:51:47.485
另一个困难是，00:51:47.485 --> 00:51:48.910
缺乏背景，00:51:48.910 --> 00:51:50.800
比如不记得谈话的历史。00:51:50.800 --> 00:51:53.710
显然，如果你不把整个谈话历史作为条件，00:51:53.710 --> 00:51:57.190
你的对话代理不可能使用它，但这是一个挑战，尤其是如果你00:51:57.190 --> 00:52:01.315
有很长一段对话的历史来找出有效的条件。00:52:01.315 --> 00:52:04.060
另一个问题是缺乏一致的人格。00:52:04.060 --> 00:52:05.380
所以如果你，呃，00:52:05.380 --> 00:52:09.970
就像我在上一张幻灯片中提到的那两篇论文一样，00:52:09.970 --> 00:52:14.395
如果你天真地训练一种标准的seq2seq模型，00:52:14.395 --> 00:52:16.480
你知道用户的最后一句话，然后回答，00:52:16.480 --> 00:52:18.955
或者甚至是整个对话的历史，然后说些什么。00:52:18.955 --> 00:52:22.675
通常你的对话代理会有一个完全不一致的角色，00:52:22.675 --> 00:52:26.800
就像某一刻他们会说它生活在欧洲然后它会说它生活在，00:52:26.800 --> 00:52:29.770
我不知道，在中国或者其他地方，这根本说不通。00:52:29.770 --> 00:52:31.915
所以我要讲一下00:52:31.915 --> 00:52:34.810
这些问题中的一些，给你们更多的细节。00:52:34.810 --> 00:52:37.870
首先，这个无关的反应问题。00:52:37.870 --> 00:52:40.960
更详细地说，你的问题是seq2seq00:52:40.960 --> 00:52:44.080
生成一些与用户话语无关的响应。00:52:44.080 --> 00:52:47.155
所以它可以是无关的，因为它只是一般的，00:52:47.155 --> 00:52:49.150
这意味着这有点像重叠00:52:49.150 --> 00:52:51.610
一般的反应问题，也可能是00:52:51.610 --> 00:52:54.160
有点不相关，因为模型选择改变，00:52:54.160 --> 00:52:56.845
把话题转移到不相干的事情上。00:52:56.845 --> 00:52:59.020
这里有很多解中的一个，00:52:59.020 --> 00:53:00.880
有很多不同的论文，00:53:00.880 --> 00:53:04.735
攻击这个无关的反应问题，但只有一个，00:53:04.735 --> 00:53:07.015
举个例子，00:53:07.015 --> 00:53:09.835
你应该改变培训目标。00:53:09.835 --> 00:53:12.760
所以我们不去优化，00:53:12.760 --> 00:53:15.520
从输入S映射到响应T00:53:15.520 --> 00:53:18.625
你要最大化T (S)的条件概率，00:53:18.625 --> 00:53:22.435
相反，你应该最大化最大的相互信息。00:53:22.435 --> 00:53:24.235
这就是为什么它在这里。00:53:24.235 --> 00:53:26.530
所以最大互信息，00:53:26.530 --> 00:53:29.140
你可以这样重写目标，00:53:29.140 --> 00:53:31.915
如果你想看到更多的细节，你可以看这篇论文。00:53:31.915 --> 00:53:35.830
但关键是你要找到你的反应T，00:53:35.830 --> 00:53:38.680
最大化这个东西，就像说，00:53:38.680 --> 00:53:41.680
它需要在给定输入条件下是可能的00:53:41.680 --> 00:53:44.920
就像它本身的概率之比。00:53:44.920 --> 00:53:49.510
所以如果T是非常高的可能性，00:53:49.510 --> 00:53:52.600
然后它就会受到惩罚，就像比例一样00:53:52.600 --> 00:53:56.440
给定输入的概率它只是一个独立的概率。00:53:56.440 --> 00:53:59.650
所以这个想法是为了阻止，00:53:59.650 --> 00:54:04.240
只是说一些一般性的东西，它们本身就有很高的PT值。00:54:04.240 --> 00:54:08.995
这就是不相关的反应问题。00:54:08.995 --> 00:54:10.780
正如我刚才暗示的，00:54:10.780 --> 00:54:12.040
两者之间肯定有很强的联系00:54:12.040 --> 00:54:16.870
无关的反应问题和一般或无聊的反应问题。00:54:16.870 --> 00:54:21.490
我们来看看一般性或无聊的反应问题。00:54:21.490 --> 00:54:27.730
(噪音)我想是的00:54:27.730 --> 00:54:32.230
有一些很简单的修复方法，00:54:32.230 --> 00:54:35.470
在一定程度上改善了单调响应问题。00:54:35.470 --> 00:54:38.410
你是否真正触及问题的核心是另一个问题。00:54:38.410 --> 00:54:42.310
但是你可以做一些简单的测试时间修正，例如，00:54:42.310 --> 00:54:46.885
在波束搜索过程中，您可以直接提升稀有字的速率和权重。00:54:46.885 --> 00:54:49.675
所以你可以说，所有罕见的词都能促进他们，00:54:49.675 --> 00:54:51.880
log概率，然后00:54:51.880 --> 00:54:54.220
更有可能在光束搜索时产生它们。00:54:54.220 --> 00:54:56.410
你可以做的另一件事是，00:54:56.410 --> 00:55:00.535
采样解码算法而不是波束搜索我们之前讲过，00:55:00.535 --> 00:55:02.350
或者你也可以用，00:55:02.350 --> 00:55:04.195
你也可以使用softmax温度。00:55:04.195 --> 00:55:07.105
这是另一件事。这些都是00:55:07.105 --> 00:55:12.040
这是一种测试时间的修正你可以把这些看作是一种后期干预，对吧?00:55:12.040 --> 00:55:16.000
所以早期的干预可能是用不同的方法训练你的模型。00:55:16.000 --> 00:55:20.005
所以我称这些为条件修正因为这些修正与，00:55:20.005 --> 00:55:23.965
把你的模型设定在能让它不那么无聊的东西上。00:55:23.965 --> 00:55:26.320
举个例子，也许你应该设置条件00:55:26.320 --> 00:55:28.930
解码器在某种附加的上下文中。00:55:28.930 --> 00:55:31.150
比如，有一些研究表明，00:55:31.150 --> 00:55:33.625
如果你在聊天，也许你应该，00:55:33.625 --> 00:55:36.280
去试一些与之相关的单词00:55:36.280 --> 00:55:38.710
用户说了什么，然后你就去关注他们00:55:38.710 --> 00:55:41.350
生成，然后你更有可能说一些类似内容的东西00:55:41.350 --> 00:55:44.080
和你之前说的那些无聊的事情相比，这是很有趣的。00:55:44.080 --> 00:55:46.870
另一个选择是你可以训练00:55:46.870 --> 00:55:50.770
检索和细化模型，而不是从头生成模型。00:55:50.770 --> 00:55:53.440
所以通过检索和改进，我的意思是，00:55:53.440 --> 00:55:55.825
假设你有某种语料库，00:55:55.825 --> 00:55:57.400
一般来说，00:55:57.400 --> 00:56:00.460
你可以说的话，然后你可以试一试，00:56:00.460 --> 00:56:01.855
从那个测试集中，00:56:01.855 --> 00:56:03.775
th-训练集，00:56:03.775 --> 00:56:06.895
然后根据当前的情况进行编辑。00:56:06.895 --> 00:56:10.630
这是一种非常强的生成方法00:56:10.630 --> 00:56:14.800
更多样化，更人性化，更有趣的话语，00:56:14.800 --> 00:56:19.555
因为你可以从样本中得到所有的细颗粒细节，00:56:19.555 --> 00:56:23.665
然后根据你目前的情况进行必要的修改。00:56:23.665 --> 00:56:26.740
我的意思是，这类方法也有缺点，就像它可能是00:56:26.740 --> 00:56:30.145
很难编辑它来适应实际情况，00:56:30.145 --> 00:56:32.410
但这确实是一种有效的方式00:56:32.410 --> 00:56:36.530
更多的多样性和兴趣。00:56:37.170 --> 00:56:40.810
关于重复问题，00:56:40.810 --> 00:56:43.105
这是我们注意到的另一个主要问题，00:56:43.105 --> 00:56:45.790
用seq2seq软件聊天00:56:45.790 --> 00:56:48.970
再一次，有简单的解和更复杂的解。00:56:48.970 --> 00:56:52.150
一个简单的解决方法是你可以阻止重复00:56:52.150 --> 00:56:55.915
在波束搜索过程中，这通常是非常有效的。00:56:55.915 --> 00:56:57.595
我的意思是，00:56:57.595 --> 00:56:59.815
在波束搜索过程中，当你考虑，00:56:59.815 --> 00:57:01.510
我的K个假设是什么?00:57:01.510 --> 00:57:05.110
也就是概率分布的前K，00:57:05.110 --> 00:57:09.535
任何构成重复n克的元素都会被抛出。00:57:09.535 --> 00:57:11.590
所以当我说构成一个重复的n克时，00:57:11.590 --> 00:57:14.470
我的意思是，如果你真的相信这个词，00:57:14.470 --> 00:57:19.630
你现在会创造一个重复的吗比如说2克，2克，00:57:19.630 --> 00:57:23.500
如果我们决定禁止所有重复的二叉或三角函数，00:57:23.500 --> 00:57:26.620
然后你只需要检查每一个可能的单词00:57:26.620 --> 00:57:30.700
在波束搜索中观察是否会产生一个重复的n-g。00:57:30.700 --> 00:57:32.440
这很好，我的意思是，00:57:32.440 --> 00:57:34.780
这绝不是一种原则性的解决方案，对吧?00:57:34.780 --> 00:57:37.495
如果我们觉得我们应该有更好的方法来学习不要重复，00:57:37.495 --> 00:57:39.790
但作为一种00:57:39.790 --> 00:57:42.535
有效的破解，我认为很有效，非常有效。00:57:42.535 --> 00:57:44.830
所以更复杂的解是，00:57:44.830 --> 00:57:47.920
例如，您可以培训一种称为覆盖机制的东西。00:57:47.920 --> 00:57:50.530
在seq2seq中，这主要是，00:57:50.530 --> 00:57:53.800
受到机器翻译设置的启发00:57:53.800 --> 00:57:56.440
覆盖机制是一种预防的目标00:57:56.440 --> 00:57:58.630
从注意到的注意机制00:57:58.630 --> 00:58:01.810
同样的话重复很多次或太多次。00:58:01.810 --> 00:58:03.655
直觉告诉我们，00:58:03.655 --> 00:58:06.595
也许重复是由重复的注意力引起的。00:58:06.595 --> 00:58:08.620
所以如果你多次关注同样的事情，00:58:08.620 --> 00:58:09.970
也许你会重复一遍00:58:09.970 --> 00:58:11.605
你知道，相同的输出很多次。00:58:11.605 --> 00:58:13.690
所以如果你能避免重复的注意力，00:58:13.690 --> 00:58:15.280
可以防止重复输出。00:58:15.280 --> 00:58:18.190
这确实很有效，但它确实，00:58:18.190 --> 00:58:21.490
实现起来更复杂，00:58:21.490 --> 00:58:23.635
不太方便，00:58:23.635 --> 00:58:25.120
我不知道，00:58:25.120 --> 00:58:28.075
在某些情况下，简单的解决方案是，00:58:28.075 --> 00:58:29.530
呃，更简单也更有效。00:58:29.530 --> 00:58:32.740
其他的复数解00:58:32.740 --> 00:58:35.800
也许你可以定义一个训练目标来阻止重复。00:58:35.800 --> 00:58:38.320
这个，你可以试着，00:58:38.320 --> 00:58:41.125
定义一些可微的东西，00:58:41.125 --> 00:58:45.600
困难在于你是在老师的强迫下接受培训的，对吧?00:58:45.600 --> 00:58:47.055
你总是看着，00:58:47.055 --> 00:58:48.435
到目前为止，00:58:48.435 --> 00:58:50.700
那你就永远不会真正去做00:58:50.700 --> 00:58:53.010
你产生你自己的输出，并开始重复你自己。00:58:53.010 --> 00:58:55.845
在这种情况下很难定义惩罚。00:58:55.845 --> 00:58:58.350
也许这是一个不可微函数。00:58:58.350 --> 00:59:00.255
就像，00:59:00.255 --> 00:59:03.745
保罗等人的论文是00:59:03.745 --> 00:59:06.250
优化胭脂，也许我们，00:59:06.250 --> 00:59:11.455
优化为不重复，这是一个离散函数的输入。00:59:11.455 --> 00:59:14.425
我要跳过讲故事了。00:59:14.425 --> 00:59:16.195
所以在讲故事的时候，00:59:16.195 --> 00:59:19.015
现在有很多有趣的神经讲故事的工作正在进行。00:59:19.015 --> 00:59:22.285
而且大部分都是用某种提示来写故事。00:59:22.285 --> 00:59:24.610
举个例子，00:59:24.610 --> 00:59:28.115
给图片或提示写故事00:59:28.115 --> 00:59:32.715
或者写出故事的下一个句子。00:59:32.715 --> 00:59:37.645
这是一个从图像生成故事的例子。00:59:37.645 --> 00:59:40.360
有趣的是我们有这个图像00:59:40.360 --> 00:59:42.940
这是一张爆炸的照片吗00:59:42.940 --> 00:59:44.740
这里是00:59:44.740 --> 00:59:48.475
一个关于这张图片的故事，但是是以泰勒・斯威夫特的歌词风格写的。00:59:48.475 --> 00:59:52.015
所以它说，你必须是夜空中唯一的灯泡，我想，00:59:52.015 --> 00:59:55.225
哦，上帝，我是如此的黑暗以至于我想念你，我保证。00:59:55.225 --> 00:59:58.690
有趣的是，这里没有任何直接的，有监督的，00:59:58.690 --> 01:00:02.620
你知道的，图片说明数据集的爆炸和泰勒斯威夫特的歌词。01:00:02.620 --> 01:00:05.890
他们是分开学的。01:00:05.890 --> 01:00:12.220
他们是这样做的他们使用了一种常见的句子编码空间。01:00:12.220 --> 01:00:15.160
所以他们使用了这种特殊的句子编码01:00:15.160 --> 01:00:18.205
跳跃思维载体，然后他们训练，01:00:18.205 --> 01:00:21.880
这个COCO的图片说明，01:00:21.880 --> 01:00:24.790
系统要从图像到编码01:00:24.790 --> 01:00:28.105
然后他们又分别训练，01:00:28.105 --> 01:00:30.370
一个语言模型，一个条件语言模型01:00:30.370 --> 01:00:33.010
泰勒・斯威夫特歌词的句子编码。01:00:33.010 --> 01:00:35.230
因为你有这个共享的编码空间，01:00:35.230 --> 01:00:38.305
现在你可以把这两个放在一起，01:00:38.305 --> 01:00:41.050
到嵌入，到泰勒・斯威夫特的风格输出，01:00:41.050 --> 01:00:43.790
我觉得这很神奇。01:00:44.220 --> 01:00:46.600
哇，我真的忘记时间了。01:00:46.600 --> 01:00:48.250
所以我想我得抓紧时间了。01:00:48.250 --> 01:00:55.140
所以，嗯，我们有一些令人印象深刻的故事，01:00:55.140 --> 01:00:57.900
发电系统，最近，01:00:57.900 --> 01:01:00.795
这是一个例子，01:01:00.795 --> 01:01:02.580
一个系统，01:01:02.580 --> 01:01:03.840
准备一个新的数据集01:01:03.840 --> 01:01:05.745
当你写一个故事的时候，01:01:05.745 --> 01:01:08.165
他们做的非常令人印象深刻，01:01:08.165 --> 01:01:10.900
非常强大的卷积语言模型，01:01:10.900 --> 01:01:13.975
生成给定输入的故事的seq-to-seq系统。01:01:13.975 --> 01:01:15.640
我就不细讲了01:01:15.640 --> 01:01:18.070
但我鼓励你如果你想退房的话01:01:18.070 --> 01:01:20.950
什么是故事生成的艺术状态，你应该看看这个。01:01:20.950 --> 01:01:23.110
有很多有趣的事情01:01:23.110 --> 01:01:25.780
非常奇特的注意力和卷积等等，01:01:25.780 --> 01:01:29.695
他们成功地创造了一些非常有趣，令人印象深刻的故事。01:01:29.695 --> 01:01:31.749
看这个例子，01:01:31.749 --> 01:01:36.280
我们有一些非常有趣的，01:01:36.280 --> 01:01:39.324
故事的产生是多样化的，不是一般的，01:01:39.324 --> 01:01:41.320
在风格上很戏剧化，这很好，01:01:41.320 --> 01:01:42.925
和提示相关。01:01:42.925 --> 01:01:46.330
但是我想你可以看到这里的极限01:01:46.330 --> 01:01:49.855
最先进的故事生成系统可以做到的就是，01:01:49.855 --> 01:01:51.715
虽然有点时髦，01:01:51.715 --> 01:01:54.625
主要是大气和描述性的。01:01:54.625 --> 01:01:56.140
这并没有真正推动情节向前发展。01:01:56.140 --> 01:01:57.940
这里没有什么事件，对吧?01:01:57.940 --> 01:02:02.305
问题是，当你生产的时间更长时，情况会更糟。01:02:02.305 --> 01:02:04.180
当你生成一个很长的文本时，01:02:04.180 --> 01:02:08.755
然后，它基本上只会停留在同一个想法上，而不会提出新的想法。01:02:08.755 --> 01:02:11.500
好。我要跳过很多，01:02:11.500 --> 01:02:13.510
抱歉，本来应该计划得更好的。01:02:13.510 --> 01:02:15.160
这里有很多信息需要检查01:02:15.160 --> 01:02:17.335
关于诗歌创作和其他事情。01:02:17.335 --> 01:02:19.690
我要跳过，因为我想要01:02:19.690 --> 01:02:23.320
NLG评估部分，因为这很重要。01:02:23.320 --> 01:02:27.655
我们讲过NLG的自动评估指标，01:02:27.655 --> 01:02:30.760
我们知道这些词与基于度量的词重叠，比如BLEU，01:02:30.760 --> 01:02:32.155
胭脂和流星01:02:32.155 --> 01:02:34.360
我们知道它们不适合机器翻译。01:02:34.360 --> 01:02:37.780
对总结来说，它们更糟糕01:02:37.780 --> 01:02:41.770
因为摘要比机器翻译更开放。01:02:41.770 --> 01:02:44.170
这意味着有了这种刚性的概念，01:02:44.170 --> 01:02:45.835
如果你要匹配n克，01:02:45.835 --> 01:02:47.380
就更没用了。01:02:47.380 --> 01:02:49.930
然后是更开放的对话，01:02:49.930 --> 01:02:51.580
那就是一场灾难。01:02:51.580 --> 01:02:54.220
它甚至不是一个给你一个好的信号的度规，01:02:54.220 --> 01:02:58.045
这也适用于任何其他开放式的东西，比如故事生成。01:02:58.045 --> 01:03:01.225
它已经显示出来了，你可以看看底部的那张纸，01:03:01.225 --> 01:03:05.125
重叠度量这个词并不适合对话。01:03:05.125 --> 01:03:07.480
橙色的盒子在向你展示，01:03:07.480 --> 01:03:13.855
对话类的人类得分与BLEU-2之间的一些关联图，01:03:13.855 --> 01:03:15.415
蓝色的一些变化。01:03:15.415 --> 01:03:18.490
这里的问题是你根本没有看到太多的相关性，对吧?01:03:18.490 --> 01:03:21.420
似乎尤其是在这个对话场景中，01:03:21.420 --> 01:03:23.370
蓝色度规与01:03:23.370 --> 01:03:26.565
人类对于这是否是一个好的对话反应的判断是，01:03:26.565 --> 01:03:28.020
我的意思是，01:03:28.020 --> 01:03:29.040
它看起来不存在。01:03:29.040 --> 01:03:30.720
它至少很弱。01:03:30.720 --> 01:03:35.120
这是很不幸的，其他一些论文也显示了同样的情况。01:03:35.120 --> 01:03:36.640
你可能会想，01:03:36.640 --> 01:03:38.920
我们还可以使用哪些其他自动度量?01:03:38.920 --> 01:03:40.600
“什么困惑呢?01:03:40.600 --> 01:03:45.820
困惑当然抓住了你的语言模型有多强大，01:03:45.820 --> 01:03:48.085
但它没有告诉你任何关于世代的事情。01:03:48.085 --> 01:03:51.970
举个例子，如果你的解码算法在某种程度上很糟糕，01:03:51.970 --> 01:03:54.700
那么困惑并不能告诉你任何事情，对吧?01:03:54.700 --> 01:03:57.640
因为解码是你应用到你训练过的语言模型中的东西。01:03:57.640 --> 01:04:00.130
困惑可以告诉你是否有一个强大的语言模型，01:04:00.130 --> 01:04:01.840
但它不会告诉你01:04:01.840 --> 01:04:04.165
你们这一代有多优秀。01:04:04.165 --> 01:04:07.330
关于自动评估，你可能还有其他想法，01:04:07.330 --> 01:04:09.460
那么，基于单词嵌入的度量标准呢?01:04:09.460 --> 01:04:12.145
基于词嵌入度量的主要思想是，01:04:12.145 --> 01:04:14.515
你想计算它们的相似度，01:04:14.515 --> 01:04:18.220
单词embeddings或者是一个句子中单词embeddings的平均值，01:04:18.220 --> 01:04:20.530
不仅仅是单词本身的重叠。01:04:20.530 --> 01:04:22.780
所以我的想法是，不仅仅是存在01:04:22.780 --> 01:04:25.510
非常严格，只说完全一样的话才算数，01:04:25.510 --> 01:04:28.885
你会说，“好吧，如果单词相似，并且在单词嵌入空间中，那么它们就算数。”01:04:28.885 --> 01:04:31.900
这当然更灵活，但不幸的是，01:04:31.900 --> 01:04:34.360
我之前展示过的同一篇论文表明这不是01:04:34.360 --> 01:04:37.315
无论是与人类对质量的判断，01:04:37.315 --> 01:04:39.955
至少对于他们正在研究的对话任务来说是这样的。01:04:39.955 --> 01:04:43.285
这里，中间一栏显示了人类之间的关系，01:04:43.285 --> 01:04:47.515
判断，以及某种基于平均嵌入词的度量。01:04:47.515 --> 01:04:49.540
所以，嗯，这看起来也不太好，01:04:49.540 --> 01:04:51.400
相关性不大。01:04:51.400 --> 01:04:54.970
所以如果我们没有足够的自动度量01:04:54.970 --> 01:04:58.435
捕捉自然语言生成的整体质量，01:04:58.435 --> 01:05:00.460
我们能做些什么?01:05:00.460 --> 01:05:02.590
所以我认为通常的策略是，01:05:02.590 --> 01:05:06.280
您最终将定义一些更集中的自动度量01:05:06.280 --> 01:05:10.705
捕获您可能感兴趣的生成文本的特定方面。01:05:10.705 --> 01:05:13.640
比如说，你可能对流利感兴趣，01:05:13.640 --> 01:05:15.540
你可以通过运行来计算01:05:15.540 --> 01:05:18.734
训练有素的语言模型，生成概率，01:05:18.734 --> 01:05:23.505
这也可以反映出它写得有多好，你知道，好的，流利的，合乎语法的文本。01:05:23.505 --> 01:05:28.000
如果你对以特定的样式生成文本特别感兴趣，01:05:28.000 --> 01:05:29.860
然后你可以取一个语言模型01:05:29.860 --> 01:05:32.185
通过训练代表这种风格的语料库，01:05:32.185 --> 01:05:35.110
现在概率不仅告诉你这是一篇好文章，01:05:35.110 --> 01:05:36.685
但是款式对吗?01:05:36.685 --> 01:05:38.875
还有一些其他的事情，比如，01:05:38.875 --> 01:05:41.185
多样性，01:05:41.185 --> 01:05:43.900
你可以很容易地通过一些统计数据，01:05:43.900 --> 01:05:45.940
你知道，你用了多少罕见的词。01:05:45.940 --> 01:05:48.250
与输入相关，01:05:48.250 --> 01:05:50.710
你可以用输入计算相似度评分，01:05:50.710 --> 01:05:52.555
有一些简单的事情，比如，01:05:52.555 --> 01:05:55.480
长度和重复，你肯定能数出来，是的，01:05:55.480 --> 01:05:58.075
它并没有告诉你整体的质量，01:05:58.075 --> 01:06:00.535
但这些都是值得衡量的。01:06:00.535 --> 01:06:02.410
所以我想我的主要观点是，是的，01:06:02.410 --> 01:06:04.960
我们在评估NLG时遇到了一个非常困难的情况。01:06:04.960 --> 01:06:06.400
没有一个整体的度量标准。01:06:06.400 --> 01:06:08.860
通常，它们会捕捉到这种整体质量。01:06:08.860 --> 01:06:11.365
但是如果你测量这些东西，01:06:11.365 --> 01:06:16.075
然后他们当然可以帮助你跟踪一些重要的事情，你应该知道。01:06:16.075 --> 01:06:21.895
所以我们讨论了NLG的自动评估指标是多么的困难。01:06:21.895 --> 01:06:23.710
我们来谈谈人类评估。01:06:23.710 --> 01:06:27.400
人类的判断被认为是黄金标准，对吧?01:06:27.400 --> 01:06:30.865
但是我们已经知道人类的评估是缓慢而昂贵的，01:06:30.865 --> 01:06:34.165
呃，但这些是人类进化的唯一问题吗?01:06:34.165 --> 01:06:36.910
假设你有权限01:06:36.910 --> 01:06:40.060
比如说，你做人工评估需要的时间或金钱。01:06:40.060 --> 01:06:42.115
这能解决你所有的问题吗?01:06:42.115 --> 01:06:43.480
假设你有无限的人类年龄，01:06:43.480 --> 01:06:44.980
这真的能解决你的问题吗?01:06:44.980 --> 01:06:47.590
我的回答是，不。01:06:47.590 --> 01:06:49.630
这是我个人的经历。01:06:49.630 --> 01:06:53.590
进行人体评估本身是非常困难的。01:06:53.590 --> 01:06:57.280
这一点都不容易，部分原因是人类做了很多奇怪的事情。01:06:57.280 --> 01:06:59.905
人类不像度规01:06:59.905 --> 01:07:02.125
一个自动的度量，它们是不一致的，01:07:02.125 --> 01:07:03.670
他们可能不合逻辑。01:07:03.670 --> 01:07:05.290
有时候，他们只是厌倦了你的工作，01:07:05.290 --> 01:07:06.760
他们也不再认真听讲了。01:07:06.760 --> 01:07:09.580
他们可能会误解你的问题01:07:09.580 --> 01:07:12.400
有时他们会做一些无法解释的事情。01:07:12.400 --> 01:07:14.440
作为一个案例研究01:07:14.440 --> 01:07:16.540
我要告诉你们的是，01:07:16.540 --> 01:07:18.010
我在那里做的一个项目，01:07:18.010 --> 01:07:19.540
建造一些聊天机器人01:07:19.540 --> 01:07:23.485
结果是人类的评估是项目中最难的部分。01:07:23.485 --> 01:07:26.230
所以我试图为个人聊天数据构建这些聊天机器人01:07:26.230 --> 01:07:29.635
设置，特别是研究可控性。01:07:29.635 --> 01:07:32.875
所以我们试图控制生成文本的各个方面，比如，01:07:32.875 --> 01:07:34.045
无论你是否重复，01:07:34.045 --> 01:07:35.395
你是多么的普通，01:07:35.395 --> 01:07:37.615
和我们之前提到的问题差不多。01:07:37.615 --> 01:07:40.180
所以我们建立了这些模型来控制，01:07:40.180 --> 01:07:42.085
我们所讲内容的特殊性01:07:42.085 --> 01:07:44.740
我们所说的和用户所说的有多相关。01:07:44.740 --> 01:07:46.090
你可以看到，01:07:46.090 --> 01:07:48.880
我们的搭档说"是的01:07:48.880 --> 01:07:51.745
我现在正在学习法律，我们可以控制01:07:51.745 --> 01:07:54.715
转动这个控制旋钮，让我们说一些很普通的话，01:07:54.715 --> 01:07:57.010
“哦，”然后大概20个点01:07:57.010 --> 01:07:59.470
完全疯了这是你知道的所有罕见的词。01:07:59.470 --> 01:08:01.510
在你说的话之间有一个甜蜜的点，01:08:01.510 --> 01:08:03.955
“听起来很有趣。你学习多长时间了?”01:08:03.955 --> 01:08:06.955
同样的，我们有一个旋钮，01:08:06.955 --> 01:08:11.260
确定我们所说的和他们所说的在语义上有多相关。01:08:11.260 --> 01:08:13.540
所以，嗯，你知道，这有点有趣。01:08:13.540 --> 01:08:16.615
这是一种控制NLG系统输出的方法。01:08:16.615 --> 01:08:19.525
但实际上，我想告诉你们人类的评估是如何的困难，01:08:19.525 --> 01:08:22.885
我们有这些系统，我们想用人类的eval来生成。01:08:22.885 --> 01:08:26.230
所以问题是，你如何要求这里的人类品质判断?01:08:26.230 --> 01:08:29.800
你可以问一些简单的总体质量问题，01:08:29.800 --> 01:08:31.975
比如，你知道，谈话进行得怎么样?01:08:31.975 --> 01:08:33.670
用户参与了吗?01:08:33.670 --> 01:08:34.990
或者比较一下，01:08:34.990 --> 01:08:38.605
这些用户中哪一个给出了最好的响应?像这样的问题。01:08:38.605 --> 01:08:40.330
我们尝试了很多，01:08:40.330 --> 01:08:43.000
但所有这些都存在一些主要问题。01:08:43.000 --> 01:08:46.960
这些问题都很主观，01:08:46.960 --> 01:08:49.150
不同的受访者有不同的期望，01:08:49.150 --> 01:08:50.620
这影响了他们的判断。01:08:50.620 --> 01:08:53.695
例如，如果你问，你认为这个用户是人类还是机器人?01:08:53.695 --> 01:08:55.645
那完全取决于01:08:55.645 --> 01:09:00.220
受访者对机器人的了解或对机器人的看法，以及他们认为自己能做什么。01:09:00.220 --> 01:09:03.940
另一个例子是，你会对这个问题产生灾难性的误解。01:09:03.940 --> 01:09:05.140
例如，如果我们问，01:09:05.140 --> 01:09:07.675
这个用户――这个聊天机器人吸引人吗?01:09:07.675 --> 01:09:09.475
然后有人回应说:“是的，01:09:09.475 --> 01:09:11.020
它很吸引人，因为它总是回信，01:09:11.020 --> 01:09:12.310
这显然不是我们的意思。01:09:12.310 --> 01:09:14.605
我们的意思是，他们是一个迷人的谈话伙伴吗，01:09:14.605 --> 01:09:16.765
但是他们采用了一个非常实际的假设，01:09:16.765 --> 01:09:19.075
呃，迷人的意思。01:09:19.075 --> 01:09:22.975
所以这里的问题是整体质量取决于很多潜在因素，01:09:22.975 --> 01:09:25.390
很难找到一个单身的人，01:09:25.390 --> 01:09:28.720
这个问题抓住了整体的质量。01:09:28.720 --> 01:09:31.030
所以我们最终做了这个，我们最终分解了这个01:09:31.030 --> 01:09:34.270
变成更多的质量因素。01:09:34.270 --> 01:09:36.205
我们是这么看的01:09:36.205 --> 01:09:39.820
你可能对聊天机器人的质量有了全面的衡量，01:09:39.820 --> 01:09:41.380
比如它有多吸引人，01:09:41.380 --> 01:09:43.135
和他谈话是多么愉快啊，01:09:43.135 --> 01:09:45.685
也许这是多么令人信服，它是人类。01:09:45.685 --> 01:09:47.230
在这些下面，01:09:47.230 --> 01:09:49.810
当这些更低的水平时，我们就崩溃了，01:09:49.810 --> 01:09:51.685
质量的组成部分，例如，01:09:51.685 --> 01:09:53.290
你知道吗，你有意思吗?01:09:53.290 --> 01:09:55.150
你是在表明你在听吗?01:09:55.150 --> 01:09:56.935
你问的问题够多了吗?01:09:56.935 --> 01:09:59.620
在这下面，我们有一些可控的属性01:09:59.620 --> 01:10:02.470
是我们转动的旋钮我们的目标是弄清楚，01:10:02.470 --> 01:10:04.525
这些是如何影响输出的。01:10:04.525 --> 01:10:09.745
我们来看看。01:10:09.745 --> 01:10:13.120
我们有很多发现，我想，01:10:13.120 --> 01:10:16.440
也许我要强调的是，01:10:16.440 --> 01:10:18.060
这两个在中间。01:10:18.060 --> 01:10:19.980
所以总体的度规参与度，01:10:19.980 --> 01:10:23.085
这意味着享受，这很容易最大化。01:10:23.085 --> 01:10:24.420
结果，呃，01:10:24.420 --> 01:10:28.300
我们的机器人在吸引人方面的表现接近人类。01:10:28.300 --> 01:10:30.730
但是总的衡量人性的标准，01:10:30.730 --> 01:10:32.440
这就是图灵测试度规，01:10:32.440 --> 01:10:34.405
这一点都不容易最大化。01:10:34.405 --> 01:10:35.920
我们所有的机器人01:10:35.920 --> 01:10:38.020
在人性方面远远低于人类，对吧?01:10:38.020 --> 01:10:40.300
所以我们根本不相信自己是人类，01:10:40.300 --> 01:10:41.785
这很有趣，对吧?01:10:41.785 --> 01:10:44.035
我们就像人类一样喜欢交谈，01:10:44.035 --> 01:10:46.630
但我们显然不是人类，对吧?01:10:46.630 --> 01:10:50.245
所以，人性化和对话质量不是一回事。01:10:50.245 --> 01:10:52.615
我们从中发现了一件有趣的事情，01:10:52.615 --> 01:10:55.390
研究中，我们不仅评估了聊天机器人，01:10:55.390 --> 01:10:57.655
我们还让人类互相评价，01:10:57.655 --> 01:11:00.925
人类是次优会话者。01:11:00.925 --> 01:11:05.170
嗯，他们在趣味性、流畅性和倾听方面得分很低。01:11:05.170 --> 01:11:06.895
他们互相问的问题不够多，01:11:06.895 --> 01:11:09.460
这就是我们喜欢这种方法的原因01:11:09.460 --> 01:11:13.150
人类的表现是愉快的，因为我们，01:11:13.150 --> 01:11:16.500
例如，打开问问题的旋钮，问更多的问题，01:11:16.500 --> 01:11:19.875
人们对此反应很好，因为人们喜欢谈论自己。01:11:19.875 --> 01:11:22.290
所以,嗯,是的。01:11:22.290 --> 01:11:23.610
我觉得这很有趣，对吧?01:11:23.610 --> 01:11:26.820
因为这表明没有一个明显的问题可以问，对吧?01:11:26.820 --> 01:11:27.990
因为如果你只是觉得，01:11:27.990 --> 01:11:31.870
要问的一个问题很明显是吸引人的或者说很明显是人性化的，01:11:31.870 --> 01:11:35.365
然后我们会得到完全不同的解读关于我们做得有多好，对吧?01:11:35.365 --> 01:11:39.770
而问这些问题能让你有更全面的了解。01:11:41.730 --> 01:11:46.780
我要跳过这个，因为时间不多了。01:11:46.780 --> 01:11:48.595
好。这是最后一部分。01:11:48.595 --> 01:11:51.670
这是我对NLG研究的总结，01:11:51.670 --> 01:11:54.340
当前的趋势和我们未来的发展方向。01:11:54.340 --> 01:11:59.020
这是NLG中三个令人兴奋的趋势。01:11:59.020 --> 01:12:00.985
当然你的里程可能不同，01:12:00.985 --> 01:12:02.860
你可能认为其他事情更有趣。01:12:02.860 --> 01:12:05.110
所以，呃，我在想的那些01:12:05.110 --> 01:12:08.635
首先将离散的潜在变量合并到NLG中。01:12:08.635 --> 01:12:11.140
你应该去结账01:12:11.140 --> 01:12:13.585
我跳过这些幻灯片，因为有一些例子。01:12:13.585 --> 01:12:16.960
但这个想法是用一些任务，比如01:12:16.960 --> 01:12:19.360
讲故事或以任务为导向的对话01:12:19.360 --> 01:12:21.100
你想要做的事情。01:12:21.100 --> 01:12:22.810
你可能想要更多的01:12:22.810 --> 01:12:25.540
你所谈论的事情的具体概念01:12:25.540 --> 01:12:30.160
比如实体，人物，事件，谈判等等。01:12:30.160 --> 01:12:33.805
所以，呃，有，有提到什么样的建模01:12:33.805 --> 01:12:38.860
这些离散的潜在变量在这些连续的NLG方法中。01:12:38.860 --> 01:12:42.520
第二种方法是严格的从左到右生成的替代方法。01:12:42.520 --> 01:12:44.440
我真的很抱歉[笑声]我跳过了这么多东西。01:12:44.440 --> 01:12:47.020
最近有一些有趣的研究在尝试01:12:47.020 --> 01:12:49.810
以非从左到右的方式生成文本。01:12:49.810 --> 01:12:51.160
举个例子01:12:51.160 --> 01:12:55.735
并行生成的东西或者写一些东西，然后不断地改进它，01:12:55.735 --> 01:12:59.815
还有一种自上而下的生成方法01:12:59.815 --> 01:13:02.800
尤其是较长的文本，比如决定内容01:13:02.800 --> 01:13:06.385
在写单词之前，把每个句子分开写。01:13:06.385 --> 01:13:08.620
第三个是01:13:08.620 --> 01:13:11.530
教师强迫下的最大可能性训练的替代方案。01:13:11.530 --> 01:13:14.320
提醒大家，教师强迫下的最大可能训练是01:13:14.320 --> 01:13:16.420
只是标准的训练方法01:13:16.420 --> 01:13:19.210
这个语言模型我们已经在课堂上讲过了。01:13:19.210 --> 01:13:20.755
所以，你知道，01:13:20.755 --> 01:13:23.200
有一些有趣的工作是关于整体的，01:13:23.200 --> 01:13:25.735
句子层次而不是单词层次的目标。01:13:25.735 --> 01:13:27.550
不幸的是，我没时间了01:13:27.550 --> 01:13:29.860
这张幻灯片，我没有时间把参考资料放进去，但是我会的01:13:29.860 --> 01:13:31.990
把参考文献放在后面01:13:31.990 --> 01:13:34.945
会在课程网站上，你们可以稍后去看看。01:13:34.945 --> 01:13:39.820
好。所以，作为一种概述，NLG研究，我们在哪里，我们要去哪里?01:13:39.820 --> 01:13:41.950
我的比喻是01:13:41.950 --> 01:13:46.210
大约五年前，NLP和深度学习研究是一种狂野的西部。01:13:46.210 --> 01:13:50.770
对吧?一切都是新的，我们不确定，01:13:50.770 --> 01:13:52.300
NLP的研究不确定是哪种01:13:52.300 --> 01:13:55.660
新的研究领域是因为，01:13:55.660 --> 01:13:58.645
神经方法改变了很多机器翻译，01:13:58.645 --> 01:14:01.675
看起来他们可能会改变其他领域，但不确定会改变多少。01:14:01.675 --> 01:14:04.690
但是现在五年过去了，01:14:04.690 --> 01:14:06.475
嗯，它没有那么狂野。01:14:06.475 --> 01:14:09.125
我想说的是，事情已经解决了很多01:14:09.125 --> 01:14:13.140
标准实践已经出现，当然还有很多事情在改变。01:14:13.140 --> 01:14:15.240
但是你知道社区里有更多的人01:14:15.240 --> 01:14:16.500
有更多的标准做法，01:14:16.500 --> 01:14:18.240
我们有TensorFlow和PyTorch。01:14:18.240 --> 01:14:20.085
所以你不用再用梯度了。01:14:20.085 --> 01:14:22.665
所以，我想说的是，现在的情况已经不那么疯狂了01:14:22.665 --> 01:14:26.370
但我认为NLG似乎是最狂野的部件之一01:14:26.370 --> 01:14:29.880
剩下的部分原因是因为01:14:29.880 --> 01:14:33.915
缺乏评估指标使得我们很难知道我们在做什么。01:14:33.915 --> 01:14:37.260
很难确定主要的方法是什么01:14:37.260 --> 01:14:41.810
当我们没有任何指标可以清楚地告诉我们发生了什么时，我们就开始工作。01:14:41.880 --> 01:14:44.710
我很高兴看到的另一件事是01:14:44.710 --> 01:14:47.830
神经NLG群落正在迅速扩大。01:14:47.830 --> 01:14:51.040
早年的时候01:14:51.040 --> 01:14:55.390
人们大多将成功的NMT方法转移到各种NLG任务中。01:14:55.390 --> 01:14:58.870
但现在我看到，你知道，越来越多的创新NLG技术01:14:58.870 --> 01:15:02.725
合并是特定于非nmt生成设置的。01:15:02.725 --> 01:15:05.845
我再次敦促你们回到我跳过的幻灯片。01:15:05.845 --> 01:15:08.650
我还想说的是01:15:08.650 --> 01:15:11.590
特别是神经NLG工作坊和比赛01:15:11.590 --> 01:15:14.470
专注于开放式NLG就像我们做的那些任务01:15:14.470 --> 01:15:18.055
know不适合用于为NMT工作的自动度量。01:15:18.055 --> 01:15:22.720
所以，有一个神经生成研讨会，一个讲故事的研讨会，01:15:22.720 --> 01:15:26.470
还有各种各样的挑战比如，01:15:26.470 --> 01:15:28.870
会话对话代理，01:15:28.870 --> 01:15:31.495
对彼此进行评估。01:15:31.495 --> 01:15:33.520
所以我认为这些不同，01:15:33.520 --> 01:15:35.350
社区组织研讨会01:15:35.350 --> 01:15:38.710
竞赛在组织社区方面做得很好，01:15:38.710 --> 01:15:44.080
提高重复性和标准化评价、标准化评价。01:15:44.080 --> 01:15:46.300
这很好，但我想说01:15:46.300 --> 01:15:50.230
进展的最大障碍无疑仍是评估。01:15:50.230 --> 01:15:53.305
好。所以，我想和你们分享的最后一件事01:15:53.305 --> 01:15:56.260
是我在NLG工作中学到的八件事。01:15:56.260 --> 01:15:58.930
第一个是任务的开放性，01:15:58.930 --> 01:16:00.535
一切都变得越困难。01:16:00.535 --> 01:16:03.655
评估变得更难，定义你在做什么变得更难，01:16:03.655 --> 01:16:05.905
当你在做一件好工作的时候，要想说出来就更难了。01:16:05.905 --> 01:16:09.130
因此，出于这个原因，约束有时会让事情变得更受欢迎。01:16:09.130 --> 01:16:15.370
所以，如果你决定限制你的任务，那么有时候完成它会更容易。01:16:15.370 --> 01:16:19.120
下一个目标是改进罐子01:16:19.120 --> 01:16:22.675
通常比提高整体发电质量更容易管理。01:16:22.675 --> 01:16:25.285
举个例子，如果你决定要这么做01:16:25.285 --> 01:16:27.865
比如增加模型的多样性01:16:27.865 --> 01:16:31.270
更有趣的事情比简单的事情更容易实现和衡量01:16:31.270 --> 01:16:35.875
说我们想做整体的发电质量因为评估问题。01:16:35.875 --> 01:16:40.285
下一个是如果你用你的语言模型做NLG，01:16:40.285 --> 01:16:44.860
然后改进语言模型，这是越来越好与困惑会给你01:16:44.860 --> 01:16:46.960
也许更好的一代质量，因为你有01:16:46.960 --> 01:16:51.220
一个更强大的语言模型，但这不是提高生成质量的唯一方法，01:16:51.220 --> 01:16:53.065
我们之前说过，01:16:53.065 --> 01:16:56.995
除了语言模型之外，还有其他组件可以影响生成，01:16:56.995 --> 01:17:00.340
问题的一部分在于这并不在培训目标之内。01:17:00.340 --> 01:17:03.655
我的下一个建议是你应该多看看你的输出，01:17:03.655 --> 01:17:07.150
部分原因是你没有任何单一的指标可以告诉你发生了什么。01:17:07.150 --> 01:17:10.270
为了形成自己的观点，多看看自己的工作成果是非常重要的。01:17:10.270 --> 01:17:12.745
这可能很费时，但可能是值得做的。01:17:12.745 --> 01:17:14.575
最后我和这些聊天机器人聊天01:17:14.575 --> 01:17:17.440
在我做这个项目的时候，我花了很多时间。01:17:17.440 --> 01:17:21.640
好。差不多完成了，所以，5你需要一个自动的度量，即使它不完美。01:17:21.640 --> 01:17:23.050
我知道你们已经知道了，因为我们01:17:23.050 --> 01:17:25.135
写满了项目说明。01:17:25.135 --> 01:17:29.200
但我可能会修改一下，比如你需要几个自动指标。01:17:29.200 --> 01:17:30.760
我之前说过你可能会追踪01:17:30.760 --> 01:17:33.445
为了全面了解情况，01:17:33.445 --> 01:17:36.100
我认为你的NLG任务越开放，01:17:36.100 --> 01:17:39.175
您越可能需要多个指标。01:17:39.175 --> 01:17:43.195
如果你做人类评估，你想让问题尽可能集中。01:17:43.195 --> 01:17:45.100
所以，我发现如果你01:17:45.100 --> 01:17:47.785
把问题定义为一种非常模糊的东西，01:17:47.785 --> 01:17:49.885
然后你就会敞开心扉01:17:49.885 --> 01:17:52.975
受访者对你有误解01:17:52.975 --> 01:17:54.670
如果他们那样做了，那就不是他们的错，01:17:54.670 --> 01:17:57.475
这是你的错，你需要回答你的问题，这就是我学到的。01:17:57.475 --> 01:17:59.860
下一个是可重复性01:17:59.860 --> 01:18:03.580
在今天的NLP和深度学习中，01:18:03.580 --> 01:18:06.130
而NLG的问题更大，01:18:06.130 --> 01:18:08.380
我想这是西部蛮荒的另一种方式。01:18:08.380 --> 01:18:10.930
所以，我想说，这真的很棒，01:18:10.930 --> 01:18:13.300
如果每个人都能公开发布所有这些01:18:13.300 --> 01:18:15.985
他们写NLG论文时产生的输出。01:18:15.985 --> 01:18:20.155
我认为这是一个很好的实践，因为如果你发布你生成的输出，01:18:20.155 --> 01:18:23.875
如果有人后来提出了一个很好的自动度量，01:18:23.875 --> 01:18:27.985
然后他们就可以得到你生成的输出然后计算它的度规。01:18:27.985 --> 01:18:30.040
然而，如果他从来没有释放你的输出或你01:18:30.040 --> 01:18:32.470
用某种不完全公制数表示，01:18:32.470 --> 01:18:35.020
那么未来的研究人员就没有什么可以与之相比的了。01:18:35.020 --> 01:18:38.575
最后，我的最后一个想法01:18:38.575 --> 01:18:42.790
在NLG工作有时很令人沮丧，01:18:42.790 --> 01:18:45.745
因为事情可能会很困难，很难知道你什么时候在进步。01:18:45.745 --> 01:18:48.805
但好处是，它也可以非常有趣。01:18:48.805 --> 01:18:52.740
这是我最后一张幻灯片，这是我和聊天机器人之间的一些奇怪对话。01:18:52.740 --> 01:18:54.000
(笑声)谢谢。01:18:54.000 --> 01:19:37.000
[NOISE] [LAUGHTER] All right, thanks.

