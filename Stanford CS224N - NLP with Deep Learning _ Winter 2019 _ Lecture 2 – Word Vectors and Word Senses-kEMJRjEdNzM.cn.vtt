WEBVTT
Kind: captions
Language: en

00:00:04.160 --> 00:00:07.125可以。大家好。00:00:07.125 --> 00:00:12.915嗯，欢迎回到第二节课，嗯，CS224N。00:00:12.915 --> 00:00:17.310好吧，上一次结束的时候我只是给你看一点，00:00:17.310 --> 00:00:18.975嗯，从这个，嗯，00:00:18.975 --> 00:00:22.140ipython记事本，你可以用单词向量来做一些事情。00:00:22.140 --> 00:00:25.380但我有点快没时间了。00:00:25.380 --> 00:00:28.050所以，我先再花几分钟，00:00:28.050 --> 00:00:29.850嗯，这一切都结束了。00:00:29.850 --> 00:00:33.360我把这本ipython笔记本贴在课程页上。00:00:33.360 --> 00:00:37.595所以，在第一堂课下，你可以找到一份副本，然后下载。00:00:37.595 --> 00:00:41.840所以，我都只提供了它的HTML版本和一个zip文件。00:00:41.840 --> 00:00:44.240像HTML文件一样，只需要看看。00:00:44.240 --> 00:00:45.470你不能用它做任何事。00:00:45.470 --> 00:00:48.275所以，如果你想一个人玩，嗯，00:00:48.275 --> 00:00:52.015下载zip文件，然后把ipython笔记本从中取出。00:00:52.015 --> 00:00:53.390可以。所以我们在看00:00:53.390 --> 00:00:57.965这些手套词载体，我今天将进一步讨论，因此00:00:57.965 --> 00:01:04.930在这个向量空间中，这些相似性的基本结果非常适用于：00:01:04.930 --> 00:01:10.550嗯，发现相似的词然后从中继续，00:01:10.550 --> 00:01:15.830有一个想法就是我们今天要多花些时间，嗯，00:01:15.830 --> 00:01:20.160也许这个向量空间不仅是一个相似空间，00:01:20.160 --> 00:01:25.760紧密相连的事物有着相似的含义，但实际上它抓住了意义。00:01:25.760 --> 00:01:30.800以一种更深刻的方式，也就是说00:01:30.800 --> 00:01:37.280实际上，在空间中你可以指向的方向有一定的意义。00:01:37.280 --> 00:01:43.970所以，如果你指向一个方向，这意味着情况更是如此，00:01:43.970 --> 00:01:48.170如果你指向不同的方向和意义空间，这可能是00:01:48.170 --> 00:01:50.780这个国家的首都或00:01:50.780 --> 00:01:54.205各种不同的含义可以在空间中编码。00:01:54.205 --> 00:01:56.655一种测试方法，00:01:56.655 --> 00:01:59.680就是用这些类比，嗯，问题。00:01:59.680 --> 00:02:03.080最后我很快就展示了这个，但只是为了确定你是否00:02:03.080 --> 00:02:06.890因为它有点-有点聪明，所以没有人看守，对吧？00:02:06.890 --> 00:02:14.600所以，我们的想法是从一对像“国王和男人”这样的词开始。00:02:14.600 --> 00:02:17.150所以我们要做的就是说，00:02:17.150 --> 00:02:23.150空间中有一个国王的矢量，还有一个人类的矢量。00:02:23.150 --> 00:02:26.840空间，但我们要做的是00:02:26.840 --> 00:02:32.000减法，就像你希望学过的向量减法一样，00:02:32.000 --> 00:02:33.750嗯，线性代数课。00:02:33.750 --> 00:02:36.020我们要从00:02:36.020 --> 00:02:40.490主向量和我们头脑中的想法是如果我们这样做的话00:02:40.490 --> 00:02:47.965会发生的事情是，我们将被留下王权的意义，而没有女仆。00:02:47.965 --> 00:02:54.270嗯，还有一个女人的直接向量。00:02:54.270 --> 00:02:59.810所以，我们可以把女性向量加到结果向量上，然后我们可以说，00:02:59.810 --> 00:03:04.220在向量中，我们会在向量空间的某个点结束，然后我们会说，00:03:04.220 --> 00:03:07.340你能找到的最接近的词是什么？00:03:07.340 --> 00:03:11.960它会打印出最接近的单词，正如我们看到的，00:03:11.960 --> 00:03:14.765嗯，上次，嗯，00:03:14.765 --> 00:03:17.660你看，如果你这样做，00:03:17.660 --> 00:03:20.565嗯，你知道答案了。00:03:20.565 --> 00:03:22.260我是说你明白，00:03:22.260 --> 00:03:32.440嗯，国王，男人，女人。00:03:33.530 --> 00:03:36.420不？好吧。[笑声]。00:03:36.420 --> 00:03:39.195你必须扭转国王和男人。00:03:39.195 --> 00:03:40.860我必须扭转国王，00:03:40.860 --> 00:03:42.900啊，当然，当然，当然。我很抱歉。00:03:42.900 --> 00:03:49.470哎呀。是的，好吧，我有点像男人一样，金。00:03:49.470 --> 00:03:55.260啊，[笑声]好的。是的，没错。00:03:55.260 --> 00:03:57.480对不起的。可以。是的，因为应该是00:03:57.480 --> 00:04:00.385男人对国王就像女人对什么都很抱歉是的。00:04:00.385 --> 00:04:03.680我的组件顺序不对。00:04:03.680 --> 00:04:06.260可以。嗯，而且，你知道，00:04:06.260 --> 00:04:10.790因为我有点像，我想我上一次展示了一些例子00:04:10.790 --> 00:04:18.620国籍词，但我的意思是，在某种程度上令人惊讶，令人震惊，00:04:18.620 --> 00:04:23.870这实际上适用于在这个空间中你能得到意义的各种事物。00:04:23.870 --> 00:04:29.030所以，我可以问各种各样的类比。00:04:29.030 --> 00:04:33.380所以我可以说澳大利亚要啤酒就像法国要啤酒一样。00:04:33.380 --> 00:04:34.280葡萄酒。00:04:34.280 --> 00:04:35.720葡萄酒。你可能会想到酒。00:04:35.720 --> 00:04:38.600它会以香槟的形式回报什么，这似乎是一个很好的答案。00:04:38.600 --> 00:04:40.535[笑声]嗯，我会同意的。00:04:40.535 --> 00:04:44.490嗯，嗯，你可以做更多的句法事实。00:04:44.490 --> 00:04:51.000所以，我可以说塔高是对最高的，只要是对最长的，它得到设置。00:04:51.000 --> 00:04:56.745嗯，如果我说“好”就是“好”，“坏”就是“坏”。00:04:56.745 --> 00:05:00.440似乎有一种观念00:05:00.440 --> 00:05:04.880朝着更极端的方向走，然后朝这个方向走。00:05:04.880 --> 00:05:06.275我跳过了一个。00:05:06.275 --> 00:05:10.565对克林顿来说，轰炸机就像里根那样。00:05:10.565 --> 00:05:13.675你可能喜欢，也可能不喜欢它给你的答案00:05:13.675 --> 00:05:17.615就像奥巴马对里根对尼克松那样。00:05:17.615 --> 00:05:20.420嗯，现在有件事你可能会注意到00:05:20.420 --> 00:05:24.350这一点，这是我最后想要回到的东西。00:05:24.350 --> 00:05:28.160嗯，好吧，这是因为克林顿模棱两可，对吧？00:05:28.160 --> 00:05:29.840有比尔，也有希拉里。00:05:29.840 --> 00:05:32.925嗯，而且，嗯，我忘了，00:05:32.925 --> 00:05:36.860你知道，我说的这些数据是几年前的。00:05:36.860 --> 00:05:39.320所以，这些数据是在2014年完成的。00:05:39.320 --> 00:05:42.230所以，在某种程度上，它肯定不会00:05:42.230 --> 00:05:45.170作为一个政治家，特朗普真的在里面，但是，你知道，00:05:45.170 --> 00:05:50.420这两种克林顿都有可能，但如果可能是这样的话，这是有道理的。00:05:50.420 --> 00:05:53.565为了证明2014年的数据，00:05:53.565 --> 00:05:55.575嗯，那是比尔・克林顿主宰的。00:05:55.575 --> 00:05:57.510所以，我想我们得到的，嗯，00:05:57.510 --> 00:06:03.330正因为如此，克林顿和尼克松有点像处于危险中的人，00:06:03.330 --> 00:06:05.790嗯，被弹劾。00:06:05.790 --> 00:06:11.695嗯，而且，呃，在过道的两边，我们主要考虑的是比尔・克林顿。00:06:11.695 --> 00:06:14.500但是，嗯，如果这种情况出现了00:06:14.500 --> 00:06:17.380我会在结束时回到右边，嗯，00:06:17.380 --> 00:06:21.370看起来我们有点问题，因为我们00:06:21.370 --> 00:06:25.690把这根细绳按字面上的克林顿和那个，嗯，00:06:25.690 --> 00:06:35.275弦是任何可能的意义和意义的弦克林顿和如此最低的嗯，00:06:35.275 --> 00:06:38.800比尔・克林顿和希拉里・克林顿就在附近。00:06:38.800 --> 00:06:41.240也许你也有一些朋友叫克林顿，对吧，00:06:41.240 --> 00:06:43.920在这个克林顿时代，他们都混在一起了。00:06:43.920 --> 00:06:47.200所以这似乎有点问题，这是个问题00:06:47.200 --> 00:06:50.905这已经讨论过了，一些关于向量这个词的内容，我将回到这里。00:06:50.905 --> 00:06:53.780嗯，你能做的另一件事就是00:06:53.780 --> 00:06:56.705一组单词，说出哪个是奇数。00:06:56.705 --> 00:07:00.020也许你以前在中学的时候做过这样的拼图。00:07:00.020 --> 00:07:02.630嗯，所以你可以这样做，它决定00:07:02.630 --> 00:07:06.080那谷类食品是那一套中的一种。看来没关系。00:07:06.080 --> 00:07:09.950嗯，然后我给你看的另一件事是，00:07:09.950 --> 00:07:13.250看看我画的这些词会有点好00:07:13.250 --> 00:07:16.220在一些幻灯片中。00:07:16.220 --> 00:07:18.680所以，这是说把一个PCA或00:07:18.680 --> 00:07:22.025主成分分析，嗯，散点图。00:07:22.025 --> 00:07:26.490嗯，所以，我可以这样做，然后我可以说，“嗯，00:07:26.490 --> 00:07:33.480给它一组词，把它们画成散点图。00:07:33.480 --> 00:07:36.375希望我能把它装进去，00:07:36.375 --> 00:07:38.475这是我的散点图。00:07:38.475 --> 00:07:40.090而且效果很好，对吗？00:07:40.090 --> 00:07:41.510我有酒，香槟，00:07:41.510 --> 00:07:43.910在这里喝啤酒，然后喝咖啡和茶。00:07:43.910 --> 00:07:45.770嗯，这是国家。00:07:45.770 --> 00:07:49.055这是学校，学院，大学。00:07:49.055 --> 00:07:51.380嗯，动物在下面。00:07:51.380 --> 00:07:55.410嗯，那里有食物。00:07:55.410 --> 00:08:00.290所以，是的，这种方法确实适用于这个二维显示。00:08:00.290 --> 00:08:02.810它基本上显示了你的相似性。00:08:02.810 --> 00:08:06.355现在，嗯，有，你知道，00:08:06.355 --> 00:08:10.790在某种程度上，尽管你想用这些PCA显示器保存你的钱包。00:08:10.790 --> 00:08:13.160所以，这是我之前讨论过的，因为你00:08:13.160 --> 00:08:16.070做一些100维的事情，我们只是在做00:08:16.070 --> 00:08:20.720这个二维投影捕捉了00:08:20.720 --> 00:08:25.430空间，但它只是失去了大量的信息。00:08:25.430 --> 00:08:27.760所以，当事情接近尾声时，00:08:27.760 --> 00:08:30.890它们可能在原始空间非常接近，或者00:08:30.890 --> 00:08:33.680他们可能只是迷失在00:08:33.680 --> 00:08:37.640二维投影是因为它们-还有其他模式00:08:37.640 --> 00:08:41.690更具优势，被选为前两个主要成分。00:08:41.690 --> 00:08:44.000所以，你不想过分信任00:08:44.000 --> 00:08:47.510如果你喜欢infoviz，你可能会想00:08:47.510 --> 00:08:50.270关于我还有其他的方法00:08:50.270 --> 00:08:53.690以更精确的方式表示距离。00:08:53.690 --> 00:08:56.770嗯，但不管怎样，这很简单，我只是00:08:56.770 --> 00:09:00.650一个PCA，用于降低矩阵的维数，然后，00:09:00.650 --> 00:09:04.585嗯，用它转换这些词向量并打印出来。00:09:04.585 --> 00:09:07.380嗯，这主要是很容易做到的。00:09:07.380 --> 00:09:09.755这对我来说不容易，嗯，00:09:09.755 --> 00:09:14.780但是如果有人有一些聪明的巨蟒，嗯，我想要一个技巧，00:09:14.780 --> 00:09:17.480如果有人想在下课后给我发个信息。00:09:17.480 --> 00:09:20.990我本以为会有一种默认的方式，你可以00:09:20.990 --> 00:09:25.310在散点图上标记点，但我找不到。00:09:25.310 --> 00:09:27.230所以，我所做的，嗯，00:09:27.230 --> 00:09:29.360我只是在设计文本00:09:29.360 --> 00:09:31.580我正在从这些点上稍微补偿一下。00:09:31.580 --> 00:09:33.650嗯，现在工作有点糟糕了00:09:33.650 --> 00:09:36.245因为它们就像你看到的那样互相碰撞。00:09:36.245 --> 00:09:40.970嗯，所以，如果有更好的方法在Python图中做点标记，那就更好了。00:09:40.970 --> 00:09:44.915所以，如果有人知道答案，你可以把它寄给我。00:09:44.915 --> 00:09:49.715嗯，好吧。就是这样。啊。00:09:49.715 --> 00:09:52.370如果你没用过ipython笔记本00:09:52.370 --> 00:09:55.670以前也不想让你的电脑运行得很慢，00:09:55.670 --> 00:09:56.960停下来是个好主意00:09:56.960 --> 00:10:00.095当你不再使用ipython笔记本的时候，00:10:00.095 --> 00:10:02.525嗯，尤其是当他们在计算什么的时候。00:10:02.525 --> 00:10:04.250嗯，好吧。[噪音]嗯。00:10:04.250 --> 00:10:26.480[噪音]00:10:26.480 --> 00:10:28.220可以。[噪音]那么现在，00:10:28.220 --> 00:10:32.180[噪音]嗯，今天的第二课等等，00:10:32.180 --> 00:10:34.400我们会继续谈论你能做的事情00:10:34.400 --> 00:10:38.870单词向量，在结尾说一点单词传感器。00:10:38.870 --> 00:10:41.570所以，更详细地说，[噪音]嗯，00:10:41.570 --> 00:10:45.530我要多说一点，嗯，word2vec。00:10:45.530 --> 00:10:50.300我要做一个关于优化的短途旅行，嗯，00:10:50.300 --> 00:10:55.010但我还是想解释一下00:10:55.010 --> 00:11:00.515人们已经做过并且可以用密集的词表示。00:11:00.515 --> 00:11:02.450所以我想谈谈00:11:02.450 --> 00:11:07.205基于计数的方法来获取意义以及它们是如何工作的。00:11:07.205 --> 00:11:08.870我想谈谈A，00:11:08.870 --> 00:11:12.755一个不同的词向量模型，就是手套模型，00:11:12.755 --> 00:11:16.055作为我的博士后，00:11:16.055 --> 00:11:18.305Jeffrey Pennington和，呃，00:11:18.305 --> 00:11:20.615我几年前工作过，00:11:20.615 --> 00:11:22.700嗯，谈谈评估，00:11:22.700 --> 00:11:25.070在很多事情上都占据主导地位00:11:25.070 --> 00:11:27.785我们在自然语言处理上所做的就是，00:11:27.785 --> 00:11:32.105我们如何评价事物，我们有多信任我们的评价，00:11:32.105 --> 00:11:35.120嗯，然后说一点，嗯，词传感器。00:11:35.120 --> 00:11:38.540我有一个目标，就是在课堂结束时，00:11:38.540 --> 00:11:41.900嗯，你应该理解，嗯，00:11:41.900 --> 00:11:43.910足够的土地，你可以00:11:43.910 --> 00:11:47.090阅读有关单词向量的论文，例如00:11:47.090 --> 00:11:49.310都在教学大纲里，并且真正理解00:11:49.310 --> 00:11:52.430他们，他们来自哪里，大致工作原理。00:11:52.430 --> 00:11:54.410所以，你知道，如果你真的想最小化00:11:54.410 --> 00:11:56.690为你的C班工作，你可以认为，“我，00:11:56.690 --> 00:11:59.780我知道第一周后我需要知道的一切，我会00:11:59.780 --> 00:12:03.260做一个关于矢量词的最后一个项目，我会没事的。”00:12:03.260 --> 00:12:05.150嗯，你知道，你真的可以这么做，00:12:05.150 --> 00:12:06.650我在工作中提到过，00:12:06.650 --> 00:12:11.585类，嗯，最近关于单词向量的一些工作。00:12:11.585 --> 00:12:13.055另一方面，嗯，00:12:13.055 --> 00:12:16.310把单词向量作为一个相当复杂的挖掘区域，00:12:16.310 --> 00:12:18.200所以你可能会过得更好，嗯，00:12:18.200 --> 00:12:21.260同时也听一些课后部分。00:12:21.260 --> 00:12:24.650可以。所以，记住我们对word2vec有一个想法，00:12:24.650 --> 00:12:29.975所以这是一个迭代更新算法，00:12:29.975 --> 00:12:32.450这些词的矢量表示，00:12:32.450 --> 00:12:36.290然后在某种意义上捕捉到它们的意义，它的工作方式是我们00:12:36.290 --> 00:12:40.580通过语料库和每个时间点逐位置移动，00:12:40.580 --> 00:12:45.170我们这里有一个中心词，它试图预测00:12:45.170 --> 00:12:47.330通过00:12:47.330 --> 00:12:50.945一个词的概率分布会出现在这附近，00:12:50.945 --> 00:12:55.385概率分布的定义很简单00:12:55.385 --> 00:13:00.275通过SoftMax函数得到字向量的.product。00:13:00.275 --> 00:13:03.620所以，我们要做的是改变00:13:03.620 --> 00:13:06.920一种很好的概率预测方法，00:13:06.920 --> 00:13:12.440这就给了你在上下文中倾向于看到的单词尽可能高的概率。00:13:12.440 --> 00:13:15.770所以，只要再多钻一点，你知道，00:13:15.770 --> 00:13:21.305我们实际上有两个矩阵，对吗？00:13:21.305 --> 00:13:23.525我们有一些中心词，00:13:23.525 --> 00:13:26.930我们有一个矩阵，对于词汇表中的每个单词，00:13:26.930 --> 00:13:29.525我们有一个向量，在这里，00:13:29.525 --> 00:13:32.450这可能是一个很好的观点00:13:32.450 --> 00:13:36.065结果发现所有主要的深度学习包，00:13:36.065 --> 00:13:40.115TensorFlow、PyTorch等，对于它们的字向量，00:13:40.115 --> 00:13:43.040字向量表示为行。00:13:43.040 --> 00:13:44.780如果你上过很多数学课，00:13:44.780 --> 00:13:46.790这可能不是你所期望的。00:13:46.790 --> 00:13:48.830你可能已经预料到了另一个方向，00:13:48.830 --> 00:13:50.540但他们都把它们排成一排。00:13:50.540 --> 00:13:53.285所以我们可以为我们的，00:13:53.285 --> 00:13:56.915我们有六个词，每个词有一个五维向量。00:13:56.915 --> 00:14:00.035可以。然后，我们在外面，嗯，00:14:00.035 --> 00:14:02.825我们还有一秒钟的矩阵，嗯，00:14:02.825 --> 00:14:07.625在上下文中表示的每个单词的向量。00:14:07.625 --> 00:14:11.345当我们有一个特别的中心词时，00:14:11.345 --> 00:14:12.950第四个字，你知道，00:14:12.950 --> 00:14:14.930当我们进行计算时，00:14:14.930 --> 00:14:19.640我们在V_4和每行00:14:19.640 --> 00:14:25.655然后给我们一个点积分数的向量。00:14:25.655 --> 00:14:27.350所以，在那之后，00:14:27.350 --> 00:14:31.190我们在每个数字上运行SoftMaxes00:14:31.190 --> 00:14:33.350从元素上看，这给了我们00:14:33.350 --> 00:14:37.080上下文中词的概率分布。00:14:37.080 --> 00:14:41.650嗯，还有一些值得注意的事情，00:14:41.650 --> 00:14:43.360希望你上次注意到，00:14:43.360 --> 00:14:45.020但为了确保你注意到，00:14:45.020 --> 00:14:49.280嗯，你知道，我们只有一个概率分布，对吧？00:14:49.280 --> 00:14:51.440所以从我们预测的单词来看，00:14:51.440 --> 00:14:55.640我们预测每个位置的概率分布完全相同。00:14:55.640 --> 00:14:58.640我们有点像是在左边说了一个最可能的词00:14:58.640 --> 00:15:02.630不管它是房子还是最有可能左边的词是房子，00:15:02.630 --> 00:15:04.070左边三个是豪斯，00:15:04.070 --> 00:15:06.470右边的那个也应该是豪斯，对吧？00:15:06.470 --> 00:15:09.320所以，这并不是给我们一个预测，00:15:09.320 --> 00:15:11.345这只是一种整体的00:15:11.345 --> 00:15:16.160在我的上下文中可能出现的词的概率分布。00:15:16.160 --> 00:15:19.685所以，我们只需要一个模型00:15:19.685 --> 00:15:23.480对所有那些00:15:23.480 --> 00:15:27.845在这个词的上下文中出现相对频繁，00:15:27.845 --> 00:15:29.975没有比这更重要的了。00:15:29.975 --> 00:15:32.450这就是为什么当你00:15:32.450 --> 00:15:36.380太简单了，一天结束的时候，00:15:36.380 --> 00:15:38.915它最终会捕捉到很多关于00:15:38.915 --> 00:15:42.770词义和词义方面，00:15:42.770 --> 00:15:47.345就像我刚才在ipython笔记本上展示的例子一样。00:15:47.345 --> 00:15:53.300嗯，还有一件事我想说，哦，是的，00:15:53.300 --> 00:15:59.254我要说的另一件事是，你可能会从这件事上发生的另一件事是，00:15:59.254 --> 00:16:00.905嗯，好吧，等一下，00:16:00.905 --> 00:16:03.185就像那样，而且，00:16:03.185 --> 00:16:06.125这种情况一直都在发生。00:16:06.125 --> 00:16:16.219嗯，这意味着每个单词都必须有一个高点积，其中包含类似于和的单词，00:16:16.219 --> 00:16:18.605嗯，他们的概率是对的。00:16:18.605 --> 00:16:23.015第一个答案是，“是的，那是真的。”00:16:23.015 --> 00:16:25.985结果是，所有的词向量，[噪音]嗯，00:16:25.985 --> 00:16:31.010有一个很强的prob-word概率成分，反映了这一点。00:16:31.010 --> 00:16:34.910我的意思是，一些工人讨论的事情之一，00:16:34.910 --> 00:16:36.980所以在读数上，00:16:36.980 --> 00:16:41.150普林斯顿的桑吉耶夫・阿罗拉小组发表了两篇论文，其中一篇是00:16:41.150 --> 00:16:45.485那些论文讨论了，嗯，这种可能性，00:16:45.485 --> 00:16:49.490实际上，高频效应和你粗暴的[噪音]方式00:16:49.490 --> 00:16:53.960固定这种高频效应的方法通常是，嗯，00:16:53.960 --> 00:16:55.685第一个，嗯，00:16:55.685 --> 00:16:58.460最大的组成部分00:16:58.460 --> 00:17:02.390你的词矢量实际上是一种频率效应，如果你只是去掉它，00:17:02.390 --> 00:17:05.000你可以使你的语义相似性更好。00:17:05.000 --> 00:17:09.860嗯，但是我们还有其他的事情要处理高频。00:17:09.860 --> 00:17:14.615好吧，我们得到了我展示过的这些可爱的空间。00:17:14.615 --> 00:17:17.510但我会再说一句。嗯。00:17:17.510 --> 00:17:21.500是的，上次我说了吗？哦，哦。00:17:21.500 --> 00:17:24.155嗯，我的话是，00:17:24.155 --> 00:17:28.580嗯，我们展示所有这些二维图片。00:17:28.580 --> 00:17:33.455它们非常，非常误导，因为在这些图片中，00:17:33.455 --> 00:17:36.410二维图片，你知道，00:17:36.410 --> 00:17:39.530如果你知道，00:17:39.530 --> 00:17:42.470三星离诺基亚很近，00:17:42.470 --> 00:17:47.690它必须在这里，然后它必须远离这里的单词。00:17:47.690 --> 00:17:50.660嗯，但是你可能也希望00:17:50.660 --> 00:17:54.619诺基亚离芬兰很近，原因不同，00:17:54.619 --> 00:17:57.245嗯，你不能在二维空间里这么做，嗯，00:17:57.245 --> 00:18:00.665向量空间，但是，你知道，其中一个，嗯，00:18:00.665 --> 00:18:05.120高维向量空间的大多数性质都是非常非直观的。00:18:05.120 --> 00:18:09.800其中一种非直觉的方法是在高维向量空间中，00:18:09.800 --> 00:18:15.275一个词可以在不同的方向上接近许多其他词。00:18:15.275 --> 00:18:18.545嗯，好吧。所以，嗯，00:18:18.545 --> 00:18:24.980我们开始讨论如何学习这些词向量。00:18:24.980 --> 00:18:31.580我大概要花五分钟的时间来进行优化。00:18:31.580 --> 00:18:33.950现在，这不是一个优化类，00:18:33.950 --> 00:18:36.350如果你想了解很多关于优化的知识。00:18:36.350 --> 00:18:38.870如果你这样做的话，你可以了解更多关于优化的知识。00:18:38.870 --> 00:18:42.830如果你做了斯蒂芬・博伊德的优化课，00:18:42.830 --> 00:18:45.800你可以学到很多优化，但这是00:18:45.800 --> 00:18:49.790有点像婴儿优化，但为了确保每个人都在同一个页面上，00:18:49.790 --> 00:18:52.085这是三张幻灯片。00:18:52.085 --> 00:18:53.960对，所以我们最后做的，00:18:53.960 --> 00:18:55.490我们在那边做的，00:18:55.490 --> 00:18:59.090我道歉我的文章太小了，00:18:59.090 --> 00:19:03.500但这也会给你做家庭作业的机会，你必须00:19:03.500 --> 00:19:08.495写下来，为你们自己解决，在这个过程中学习更多。00:19:08.495 --> 00:19:11.780对，所以我们有一个成本函数00:19:11.780 --> 00:19:15.440最小化，我们所做的就是00:19:15.440 --> 00:19:20.465计算成本函数梯度的微积分00:19:20.465 --> 00:19:26.570对于我们的向量，就是我们的变量θ，然后我们要做的就是说，00:19:26.570 --> 00:19:30.050好吧，如果我们采取一个小步骤00:19:30.050 --> 00:19:35.000把我们带下去的梯度的负方向，00:19:35.000 --> 00:19:38.615在这个地方下山，我们想继续00:19:38.615 --> 00:19:42.410这样做的话，我们的空间就越小。00:19:42.410 --> 00:19:45.499我是说，当然在我们的高多维空间里，00:19:45.499 --> 00:19:48.140你知道，像这样的曲线可能不是很平滑。00:19:48.140 --> 00:19:52.865这可能是一个可怕的非凸曲线，但这只是一个想法。00:19:52.865 --> 00:19:56.510所以，本质上，我们是说，我们得到了旧的参数，00:19:56.510 --> 00:20:01.525我们用这些旧参数求出目标函数的梯度。00:20:01.525 --> 00:20:05.830我们用一个小α乘以它00:20:05.830 --> 00:20:08.680因为我们只想移动00:20:08.680 --> 00:20:11.860每次一点点因为如果回到这里，00:20:11.860 --> 00:20:15.250如果我们说下坡是这样的话，00:20:15.250 --> 00:20:16.960“太好了，我们走很远的路吧。”00:20:16.960 --> 00:20:18.790你可能有点过头了，00:20:18.790 --> 00:20:21.220所以我们每次只想去一点点。00:20:21.220 --> 00:20:24.890所以我们通常有一个很小的学习率α，所以我们00:20:24.890 --> 00:20:28.745减去梯度的小倍数，我们，00:20:28.745 --> 00:20:30.470从旧的参数我们得到00:20:30.470 --> 00:20:34.580我们的新参数和有效的计算方法，00:20:34.580 --> 00:20:37.190如下图所示，00:20:37.190 --> 00:20:40.925我们只是对每一个偏导数这样做，然后，00:20:40.925 --> 00:20:45.485我们的希望是，这将让我们逐渐走下这个表面。00:20:45.485 --> 00:20:48.050如果你真的这么做了，00:20:48.050 --> 00:20:50.840这对那种00:20:50.840 --> 00:20:54.080我们建立的系统，有很多工作要做00:20:54.080 --> 00:20:57.890聪明的优化但是最基本的东西00:20:57.890 --> 00:21:02.015你肯定需要知道的是，00:21:02.015 --> 00:21:04.895我们在这里的目标功能，00:21:04.895 --> 00:21:09.230theta的j是我们整个语料库的函数，对吗？00:21:09.230 --> 00:21:11.000为了让这个工作顺利进行，00:21:11.000 --> 00:21:12.980你要做的第一件事是，00:21:12.980 --> 00:21:17.600你知道，收集你最喜欢的语言中的几十亿个单词然后说，00:21:17.600 --> 00:21:21.245“去为我建立一个word2vec模型，”然后，00:21:21.245 --> 00:21:23.945如果你必须评估00:21:23.945 --> 00:21:30.11010亿个中心词，然后可能是-100亿个上下文词中的每一个，00:21:30.110 --> 00:21:35.690如果你有一个5号的窗户，那么你必须做100亿嗯，00:21:35.690 --> 00:21:40.775在你计算出你的梯度之前，软最大值的计算，00:21:40.775 --> 00:21:45.350很长一段时间以前你要用电脑计算00:21:45.350 --> 00:21:50.045你在梯度上走了一小步，事情就会进展得很慢。00:21:50.045 --> 00:21:53.705所以，没有人在深度学习系统中这样做。00:21:53.705 --> 00:21:56.270嗯，所以人们-每个人都用00:21:56.270 --> 00:22:00.680随机梯度下降和随机梯度下降，00:22:00.680 --> 00:22:05.325我们在最简单的情况下对窗口进行采样。00:22:05.325 --> 00:22:08.170我们，就为了这扇窗户，00:22:08.170 --> 00:22:13.900求出一个梯度的估计值，并用它作为参数更新。00:22:13.900 --> 00:22:16.060所以，这有点令人惊讶，00:22:16.060 --> 00:22:18.970令人惊讶的嘈杂估计00:22:18.970 --> 00:22:23.600但是梯度并不重要，因为一旦我们完成了它，00:22:23.600 --> 00:22:26.750我们将选择一个不同的中心词，然后一次又一次地做，00:22:26.750 --> 00:22:30.830因此，我们逐渐接近如果我们能00:22:30.830 --> 00:22:34.999在我们采取任何措施之前，先看了所有的中心词，00:22:34.999 --> 00:22:37.280但因为我们在前进的过程中采取了措施，00:22:37.280 --> 00:22:43.655我们更快地得到最小的函数阶数和数量级。00:22:43.655 --> 00:22:48.995所以这是最简单的例子，我们只是从一个窗口取样。00:22:48.995 --> 00:22:51.980实际上，这不是我们通常做的。00:22:51.980 --> 00:22:55.895我们通常以-一小束的形式取样，00:22:55.895 --> 00:22:59.210你知道，大约32或64个。00:22:59.210 --> 00:23:03.185嗯，如果我们有一个更大的样本，00:23:03.185 --> 00:23:05.765这通常被称为小批量，我们00:23:05.765 --> 00:23:09.110从小批量计算梯度估计。00:23:09.110 --> 00:23:12.050嗯，这有两个优点。00:23:12.050 --> 00:23:16.550一个好处是你可以得到较少的噪音估计00:23:16.550 --> 00:23:18.590因为你的平均值00:23:18.590 --> 00:23:21.830通过一系列的例子而不是仅仅使用一个，00:23:21.830 --> 00:23:23.870但第二个优势是，00:23:23.870 --> 00:23:25.955这就是我们真正关心的原因，00:23:25.955 --> 00:23:30.815如果我们想在使用GPU时加快计算速度，00:23:30.815 --> 00:23:36.050你需要把做相同操作的并行化00:23:36.050 --> 00:23:38.960时间，然后通过使用00:23:38.960 --> 00:23:42.275一小批64个例子或类似的东西。00:23:42.275 --> 00:23:44.510嗯，你不必，但你知道，00:23:44.510 --> 00:23:47.780你知道的硬件的细节，00:23:47.780 --> 00:23:50.600不是-GPU，你知道，他们有这些，00:23:50.600 --> 00:23:52.040不管里面有什么，00:23:52.040 --> 00:23:53.495有两种力量。00:23:53.495 --> 00:23:57.965所以，如果你使用像32或64这样的批次，你会得到更好的加速，00:23:57.965 --> 00:24:01.070而不是仅仅决定42仍然是你最喜欢的数字00:24:01.070 --> 00:24:06.060高中的时候（笑声），你会把它当作你的小批量。00:24:07.000 --> 00:24:10.610可以。嗯，是的，这是00:24:10.610 --> 00:24:13.730还有一件有趣的事00:24:13.730 --> 00:24:17.270事实证明，它实际上有一些优化细节。00:24:17.270 --> 00:24:19.625嗯，如果你想到这些嗯，00:24:19.625 --> 00:24:22.955用词向量做随机梯度，00:24:22.955 --> 00:24:24.500实际上这和00:24:24.500 --> 00:24:28.385其他一些深度学习问题，如视觉深度学习问题。00:24:28.385 --> 00:24:33.740因为对于单个窗口，甚至是一种大小合理的小批量，00:24:33.740 --> 00:24:36.575结果是那些小批量，00:24:36.575 --> 00:24:39.260小批量只有，你知道，00:24:39.260 --> 00:24:41.990相对来说，里面有几个词，对吧？00:24:41.990 --> 00:24:45.710所以，如果你有32号的小批量和10号的窗户，00:24:45.710 --> 00:24:50.000你知道，可能里面只有大约100150个不同的词。00:24:50.000 --> 00:24:53.030嗯，但是我们正在建立这个模型00:24:53.030 --> 00:24:56.135有25万个单词或类似的词汇。00:24:56.135 --> 00:25:00.290所以，向量中的所有元素都是零。00:25:00.290 --> 00:25:03.440嗯，所以，嗯，00:25:03.440 --> 00:25:06.935我们真的有点稀少，嗯，00:25:06.935 --> 00:25:10.565周界更新等等，嗯，00:25:10.565 --> 00:25:14.300这说明我们可能，嗯，00:25:14.300 --> 00:25:17.780只想更新00:25:17.780 --> 00:25:21.380然后问题是你是否能做到，对吗？00:25:21.380 --> 00:25:25.040做这件事的愚蠢的方法是你只需要这个矩阵，通常，00:25:25.040 --> 00:25:27.830几乎所有的零，你说加00:25:27.830 --> 00:25:33.230这两个矩阵放在一起，然后问题是，00:25:33.230 --> 00:25:38.030你能不能有一个稀疏矩阵更新，它只更新00:25:38.030 --> 00:25:41.030矩阵中包含00:25:41.030 --> 00:25:44.645你输入的单词做得更快？00:25:44.645 --> 00:25:47.240如果你在做一些更聪明的事情00:25:47.240 --> 00:25:52.220在多台计算机上进行分布式计算并共享参数，00:25:52.220 --> 00:25:54.620那么你肯定只是想更新一下00:25:54.620 --> 00:25:58.660你得到参数估计的向量这个词。00:25:58.660 --> 00:26:00.850所以，这里有一些细节，但我00:26:00.850 --> 00:26:03.990我要跳过他们了解更多细节，嗯。00:26:03.990 --> 00:26:08.570正确的。后来有几个人问，是的，00:26:08.570 --> 00:26:14.525为什么有这两个词的向量是中心向量和外部向量？00:26:14.525 --> 00:26:16.670我的意思是答案是，00:26:16.670 --> 00:26:19.520我给你看的数学很容易，对吧？00:26:19.520 --> 00:26:21.875如果，嗯，00:26:21.875 --> 00:26:25.445如果你照我说的做，00:26:25.445 --> 00:26:28.130你知道，为了锻炼，嗯，00:26:28.130 --> 00:26:31.985中心词的偏导数。00:26:31.985 --> 00:26:35.210就像我给你看的，很简单。00:26:35.210 --> 00:26:39.980但是如果你只用一组词向量，00:26:39.980 --> 00:26:41.810那么同样的词，00:26:41.810 --> 00:26:43.235这是中心词，00:26:43.235 --> 00:26:45.440将是00:26:45.440 --> 00:26:49.895当你计算出上下文词的SoftMax时的上下文词。00:26:49.895 --> 00:26:52.550然后你会得到这些条款00:26:52.550 --> 00:26:55.820两个参考文献中的平方项，00:26:55.820 --> 00:26:57.290所以这个词，00:26:57.290 --> 00:26:59.930这使你的数学更难。00:26:59.930 --> 00:27:03.725嗯，这是一个实际的事情，00:27:03.725 --> 00:27:05.375嗯，最后。00:27:05.375 --> 00:27:08.600我的意思是这没什么区别，00:27:08.600 --> 00:27:12.005因为如果你从经历了所有的00:27:12.005 --> 00:27:13.625嗯，位置，你知道。00:27:13.625 --> 00:27:17.000什么是一个中心词，在某一点上，是立即后00:27:17.000 --> 00:27:20.510一个过去是上下文的词，00:27:20.510 --> 00:27:21.890现在是中心词。00:27:21.890 --> 00:27:26.240所以，做同样的计算，因为，你知道，00:27:26.240 --> 00:27:28.504点积实际上是对称的，00:27:28.504 --> 00:27:30.770嗯，又来了。00:27:30.770 --> 00:27:34.100所以，它们得到了非常相似的向量表示。00:27:34.100 --> 00:27:36.530所以，一般来说，你能得到最好的结果00:27:36.530 --> 00:27:39.349求出两个向量的平均值，00:27:39.349 --> 00:27:41.990最后每个单词只有一个向量。00:27:41.990 --> 00:27:45.740好吧，更实质地说，嗯，00:27:45.740 --> 00:27:48.620如果你去看word2vec论文，00:27:48.620 --> 00:27:51.920你会发现Word2vec还有很多00:27:51.920 --> 00:27:55.565定义为Word2vec模型系列。00:27:55.565 --> 00:27:59.045这个家庭有两个主要部分。00:27:59.045 --> 00:28:03.455嗯，首先，在连续的单词袋模型之间有一个选择，00:28:03.455 --> 00:28:05.345以及跳克模型。00:28:05.345 --> 00:28:07.940以及我介绍的跳克模型。00:28:07.940 --> 00:28:09.650所以，在跳克模型中，00:28:09.650 --> 00:28:12.170你有一个中心词，你想00:28:12.170 --> 00:28:15.515一次预测一个上下文中的所有单词。00:28:15.515 --> 00:28:19.225对于连续的单词袋模型来说，情况正好相反。00:28:19.225 --> 00:28:23.980你已经掌握了所有的外来词，而且你正在努力使用它们，00:28:23.980 --> 00:28:29.715虽然独立考虑像一个幼稚的贝叶斯模型来预测中心词。00:28:29.715 --> 00:28:34.325嗯，第二个是，嗯，00:28:34.325 --> 00:28:37.685我提出的学习方法是00:28:37.685 --> 00:28:41.240使用所谓的naive softmax的方法。00:28:41.240 --> 00:28:44.315所以，当我们想解决问题的时候，00:28:44.315 --> 00:28:49.354我们有点说，好吧，我们需要上下文词的概率估计，00:28:49.354 --> 00:28:50.990所以我们要总结一下00:28:50.990 --> 00:28:55.550整个词汇表，我们将得出这些概率估计。00:28:55.550 --> 00:28:59.540实际上，这是一种00:28:59.540 --> 00:29:03.860一个坏主意，因为这也会使事情变得非常缓慢。00:29:03.860 --> 00:29:05.884所以，在家庭作业二，00:29:05.884 --> 00:29:08.000下周就来，嗯，00:29:08.000 --> 00:29:11.585你会实现一个更实际的，嗯，00:29:11.585 --> 00:29:15.530他们在word2vec论文中提出的方法，对吗？00:29:15.530 --> 00:29:16.730所以，问题是，00:29:16.730 --> 00:29:21.035如果我们用这个方程来做微积分，00:29:21.035 --> 00:29:23.225在这个分母里，00:29:23.225 --> 00:29:26.420我们在做整个词汇表的总和。00:29:26.420 --> 00:29:29.120所以，如果你有25万个单词的词汇表，00:29:29.120 --> 00:29:31.610我们正在做25万个点的产品，00:29:31.610 --> 00:29:36.170指数加上它们就可以算出分母。00:29:36.170 --> 00:29:38.180这似乎是呃，00:29:38.180 --> 00:29:41.240如果你想快点的话，这真是个坏主意。00:29:41.240 --> 00:29:44.360嗯，那么，嗯，托马斯・米科洛夫和00:29:44.360 --> 00:29:48.830同事们提出了负抽样就足够了。00:29:48.830 --> 00:29:51.035所以负抽样的想法，00:29:51.035 --> 00:29:55.745我们要训练二元逻辑回归吗？00:29:55.745 --> 00:29:59.525所以，我们要训练一个二元逻辑回归00:29:59.525 --> 00:30:04.055对于观察到分子中的实际单词，00:30:04.055 --> 00:30:09.140你想给这个词很高的概率，它是实际观察到的。00:30:09.140 --> 00:30:11.825然后，我们要做的，00:30:11.825 --> 00:30:15.680我们要随机抽取一些其他的单词，00:30:15.680 --> 00:30:21.920他们是阴性样本，并说他们不是真正看到的。00:30:21.920 --> 00:30:26.795所以，你应该尽量给他们一个尽可能低的概率。00:30:26.795 --> 00:30:30.920好吧，那么，嗯，这种符号00:30:30.920 --> 00:30:35.195它们在纸上的用法和我用过的稍有不同。00:30:35.195 --> 00:30:38.360它们实际上是最大化而不是最小化，00:30:38.360 --> 00:30:42.380这就是我要回到的方程。00:30:42.380 --> 00:30:46.700嗯，在我们这样做之前，这里是乙状结肠的功能。00:30:46.700 --> 00:30:49.310所以，sigmoid函数通常是这样写的，00:30:49.310 --> 00:30:51.905一对一加上e到负x。00:30:51.905 --> 00:30:54.230但实际上，00:30:54.230 --> 00:30:59.600sigmoid函数类似于softmax函数的二进制大小写，对吧？00:30:59.600 --> 00:31:03.020我们有两个可能的结果，是或否，00:31:03.020 --> 00:31:07.610你又得到了一个输入，任何实数，00:31:07.610 --> 00:31:11.300它把它映射到00:31:11.300 --> 00:31:14.8700和1代表这两个二元结果。00:31:14.870 --> 00:31:16.790如果这个数字是正数，00:31:16.790 --> 00:31:21.215它的上限是1，负值是0。00:31:21.215 --> 00:31:23.165好吧，这次，00:31:23.165 --> 00:31:26.239我们要把圆点作为一个好词，00:31:26.239 --> 00:31:29.330我们要取两个向量的点积，00:31:29.330 --> 00:31:31.835把它推过我们的乙状结肠功能00:31:31.835 --> 00:31:34.280然后我们需要概率估计，00:31:34.280 --> 00:31:36.710嗯，尽可能高。00:31:36.710 --> 00:31:38.810所以，如果我给你看这个版本，00:31:38.810 --> 00:31:41.840只是写得有点不同，嗯，00:31:41.840 --> 00:31:46.730为了尽可能的像我们上次使用的符号，00:31:46.730 --> 00:31:50.675这是我们使用负抽样的新目标函数。00:31:50.675 --> 00:31:52.415我们有两个条件，00:31:52.415 --> 00:31:54.500第一个，嗯，00:31:54.500 --> 00:31:59.285是观察到的上下文单词的sigmoid的日志，00:31:59.285 --> 00:32:02.810外面的单词，中间的单词产生的圆点，00:32:02.810 --> 00:32:05.180我们希望它能变得更大。00:32:05.180 --> 00:32:08.435嗯，另一方面，00:32:08.435 --> 00:32:12.785嗯，我们有，嗯，那个，00:32:12.785 --> 00:32:16.850嗯，随机选择的K字，00:32:16.850 --> 00:32:18.530换句话说，00:32:18.530 --> 00:32:21.905我们将计算出它们和中心词之间的点积。00:32:21.905 --> 00:32:25.100我们希望这些尽可能小。00:32:25.100 --> 00:32:28.340嗯，注意这里有多余的减号00:32:28.340 --> 00:32:31.550这两件事的标志是不同的，对吧？00:32:31.550 --> 00:32:34.355所以，这些是我们的阴性样本。00:32:34.355 --> 00:32:37.220对于大K来说，这个数字可能是一个相当适中的数字，00:32:37.220 --> 00:32:38.600你只要吃10种，00:32:38.600 --> 00:32:42.66515个阴性样本，效果很好。00:32:42.665 --> 00:32:46.670嗯，我说我们抽了一些单词，00:32:46.670 --> 00:32:48.755嗯，作为阴性样本。00:32:48.755 --> 00:32:54.260他们特别提出了一个抽样分布来帮助他们00:32:54.260 --> 00:33:00.260在一定程度上处理这个经常出现的词的问题。00:33:00.260 --> 00:33:03.980嗯，那么你如何取样的起点是你00:33:03.980 --> 00:33:07.460使用我们称之为的-单格分布。00:33:07.460 --> 00:33:11.390所以，这就意味着你把单词放在一个大语料库里，然后数数00:33:11.390 --> 00:33:16.010每一个独立词的出现频率，00:33:16.010 --> 00:33:18.125所以这就是所谓的单位数。00:33:18.125 --> 00:33:20.480所以你从单位数开始，00:33:20.480 --> 00:33:23.585但是你把它们提升到四分之三的能量。00:33:23.585 --> 00:33:25.730提高到四分之三的权力，00:33:25.730 --> 00:33:27.395具有，嗯，00:33:27.395 --> 00:33:30.785减少常用词的抽样频率，00:33:30.785 --> 00:33:35.195增加你对较少见词的取样频率。00:33:35.195 --> 00:33:39.395好吧，嗯，就是这样。00:33:39.395 --> 00:33:43.955好吧，这就是我要说的关于word2vec的一切。00:33:43.955 --> 00:33:48.560任何人都有最后一件事。00:33:48.560 --> 00:33:52.245对.[噪音]00:33:52.245 --> 00:33:55.275哦，哦[噪音]。这是-对不起Z，00:33:55.275 --> 00:34:00.660资本z经常被用作一个标准化术语，所以这是说，00:34:00.660 --> 00:34:03.705如果你想知道单词的概率分布，00:34:03.705 --> 00:34:06.960你能算出这个四分之三的数数吗00:34:06.960 --> 00:34:10.230在词汇表中的每一个词中，然后这些词00:34:10.230 --> 00:34:13.800数字你只要把它们加在词汇表上就可以了00:34:13.800 --> 00:34:17.670总的，我们除以它，得到一个概率分布。00:34:17.670 --> 00:34:19.845好问题，因为我没解释过。00:34:19.845 --> 00:34:23.610在这节课上，当你看到字母Z没有解释的时候，00:34:23.610 --> 00:34:28.740它通常意味着我是一个标准化的术语，可以把事情变成00:34:28.740 --> 00:34:31.200概率和你的迭代00:34:31.200 --> 00:34:34.800分子项，求和并除以。00:34:34.800 --> 00:34:40.530还有其他我没解释的问题吗？对。00:34:40.530 --> 00:34:45.000所以窗户[听不见]是[听不见]00:34:45.000 --> 00:34:46.290是的，是的。00:34:46.290 --> 00:34:48.480那么，[噪音]你用多大的窗户？00:34:48.480 --> 00:34:52.515实际上，我会再回到这个问题上，展示一些关于这个问题的数据，00:34:52.515 --> 00:34:54.480但是，是的，我们没有做任何事情。00:34:54.480 --> 00:34:57.270现在我们猜测窗户的尺寸是五号，00:34:57.270 --> 00:34:58.995这不是一个坏消息，嗯，00:34:58.995 --> 00:35:04.305但你知道这背后没有任何科学，嗯，00:35:04.305 --> 00:35:09.405人们把它当作超参数，也就是说，00:35:09.405 --> 00:35:12.720你试着用几个不同的数字看看哪一个00:35:12.720 --> 00:35:17.310最好，这就是你在未来工作中用到的。是啊。00:35:17.310 --> 00:35:19.485嗯，[听不见]四分之三的功率00:35:19.485 --> 00:35:24.180是因为任何理论上的原因，还是仅仅因为它似乎在实践中起作用？00:35:24.180 --> 00:35:27.060嗯，不，嗯，那个，00:35:27.060 --> 00:35:33.120也被选为超参数，提高了性能。00:35:33.120 --> 00:35:35.490我是说，实际上，嗯，你知道，00:35:35.490 --> 00:35:38.580对于这个词2vec论文，我的意思是，00:35:38.580 --> 00:35:40.995你知道，事实证明，嗯，00:35:40.995 --> 00:35:43.320在实际的论文中，嗯，00:35:43.320 --> 00:35:47.490模型看起来很干净，但是00:35:47.490 --> 00:35:51.449当人们开始挖掘代码的时候，00:35:51.449 --> 00:35:56.325他们确实提供了可重复的研究，00:35:56.325 --> 00:35:59.175实际上有很多技巧00:35:59.175 --> 00:36:03.300不同的东西，比如超参数，嗯，00:36:03.300 --> 00:36:08.475你如何取样，你如何等待窗口和各种各样的东西使数字更好。00:36:08.475 --> 00:36:11.190所以，你知道，人们会玩很多把戏00:36:11.190 --> 00:36:15.205数字上升了，这并不是特别理论上的。00:36:15.205 --> 00:36:17.530我们好吗？00:36:17.530 --> 00:36:33.540是啊。00:36:33.540 --> 00:36:39.230[听不见][噪音]。00:36:39.230 --> 00:36:42.325啊，有时候。00:36:42.325 --> 00:36:48.510我-我-你-所以一般来说，对于很多这样的抽样调查，00:36:48.510 --> 00:36:53.145如果你要多次传球，如果你只是去布卢姆，那是个坏主意，00:36:53.145 --> 00:36:55.530盛开，盛开，然后盛开，盛开，再次盛开，00:36:55.530 --> 00:36:56.670这是个坏主意，00:36:56.670 --> 00:36:59.580但是很多软件包使用的一种常见技术00:36:59.580 --> 00:37:02.820他们在开始时确实使用了这种洗牌操作。00:37:02.820 --> 00:37:04.320所以对于每个时代，00:37:04.320 --> 00:37:08.880他们会随机地移动数据，然后按顺序进行，这样00:37:08.880 --> 00:37:13.905从局域等方面快速计算的好处，00:37:13.905 --> 00:37:16.590虽然哈-意思是当你做不同的时代，00:37:16.590 --> 00:37:18.720结果会有所不同。00:37:18.720 --> 00:37:28.650嗯，是的，是的。00:37:28.650 --> 00:37:28.950[听不见][噪音][听不见]。00:37:28.950 --> 00:37:34.185我认为最后一个问题是从语料库中抽取小批量00:37:34.185 --> 00:37:37.140对比你是否说样本2000:37:37.140 --> 00:37:41.535从整个语料库中随机抽取，而不是从左到右进行排序。00:37:41.535 --> 00:37:43.230是的，你有问题吗？00:37:43.230 --> 00:37:55.770嗯，是的[听不见][噪音]。00:37:55.770 --> 00:37:59.190是啊。所以-所以你可以争论-你可以争论00:37:59.190 --> 00:38:02.895不管这是不是用最清楚的方式写的，但是，是的。00:38:02.895 --> 00:38:07.110所以，我们要做这个点积，然后我们要否定它。00:38:07.110 --> 00:38:11.510哪个是翻转空间的哪一边，对吗？00:38:11.510 --> 00:38:15.890因为乙状结肠是在零附近对称的。00:38:15.890 --> 00:38:19.215所以，如果我们有点积，嗯，00:38:19.215 --> 00:38:20.790然后我们否定了它，00:38:20.790 --> 00:38:25.095我们正在计算一个负概率，所以00:38:25.095 --> 00:38:30.225这就是我们第一次的方式，嗯，00:38:30.225 --> 00:38:32.910我们第一次希望00:38:32.910 --> 00:38:35.640高，然后对于阴性样品，00:38:35.640 --> 00:38:38.520我们希望他们的概率很低。00:38:38.520 --> 00:38:42.495好吧，我现在就跑吧。00:38:42.495 --> 00:38:47.810嗯，这是一个算法，嗯，00:38:47.810 --> 00:38:53.870有点像你在一个位置一个位置地浏览这个语料库00:38:53.870 --> 00:38:56.780对单词的预测，然后你00:38:56.780 --> 00:39:00.200更新一些参数，你会学到一些东西，00:39:00.200 --> 00:39:04.400根据Job，它似乎是基于我们在示例中看到的内容而起作用的，00:39:04.400 --> 00:39:07.155但你知道，你可能会想，00:39:07.155 --> 00:39:08.955那有点奇怪，对吧？00:39:08.955 --> 00:39:12.600看，我们有一大堆数据，00:39:12.600 --> 00:39:16.560有点传统，我在考虑统计学，对吧？00:39:16.560 --> 00:39:18.180所以你有一大堆数据，00:39:18.180 --> 00:39:22.530你把它加起来，看起来这里有一些明显的事情你可以做。00:39:22.530 --> 00:39:24.930你可以说，嗯，有一个词是，00:39:24.930 --> 00:39:27.030不管我们用什么词，香蕉。00:39:27.030 --> 00:39:31.140让我们看看在香蕉肠的上下文中会出现什么单词，然后数数。00:39:31.140 --> 00:39:35.370然后我们就可以用它们来预测，你知道，00:39:35.370 --> 00:39:38.940这些方法是传统的00:39:38.940 --> 00:39:43.350甚至与分布式表示技术一起使用。00:39:43.350 --> 00:39:45.210嗯，所以我想说一点，00:39:45.210 --> 00:39:49.815所以你受过充分的教育，听起来不像00:39:49.815 --> 00:39:55.440注意到2013年网络起飞之前没有工作。00:39:55.440 --> 00:39:59.310嗯，好吧。所以，我们可以做的是00:39:59.310 --> 00:40:03.300做与word2vec类似的事情。00:40:03.300 --> 00:40:07.185我们可以说周围有一个五个字的窗口00:40:07.185 --> 00:40:11.160通常被称为单词标记的每个单词实例，对吗？00:40:11.160 --> 00:40:17.250所以在NLP，我们经常想区分像香蕉这样的特殊类型00:40:17.250 --> 00:40:20.400或苹果与特定实例00:40:20.400 --> 00:40:23.985通常在文本中，这被称为某种类型标记区别。00:40:23.985 --> 00:40:26.160所以我们可以，嗯，00:40:26.160 --> 00:40:29.670看看每个带有单词的um标记，00:40:29.670 --> 00:40:31.290在那周围写着五个字，00:40:31.290 --> 00:40:34.680然后我们就可以开始计算哪些单词会出现，00:40:34.680 --> 00:40:40.845与它一起发生，这样我们就可以得到一个共发生计数矩阵。00:40:40.845 --> 00:40:42.930嗯，好吧。00:40:42.930 --> 00:40:44.415所以，我们再来一次，00:40:44.415 --> 00:40:46.080我要举个例子。00:40:46.080 --> 00:40:49.350所以，通常情况下你用5到10，但你知道我可以00:40:49.350 --> 00:40:53.055用一个窗口来保持我的计数非常简单和小。00:40:53.055 --> 00:40:56.400我忽略左右，就像Word2vec一样，00:40:56.400 --> 00:40:59.475所以如果我有一个像这样的小宝宝语料库，00:40:59.475 --> 00:41:00.840你知道，我能做的，00:41:00.840 --> 00:41:05.445就是说这里是“共现账户”这个词的矩阵。00:41:05.445 --> 00:41:08.295所以，在我一号窗户的范围内，00:41:08.295 --> 00:41:10.860我有过两次这样的经历，00:41:10.860 --> 00:41:14.490这就意味着接下来的两次，它是对称的，00:41:14.490 --> 00:41:18.720我这里所有的账户都是单件的，嗯。00:41:18.720 --> 00:41:25.580所以这给了我一个巨大的稀疏矩阵词共现帐户。00:41:25.580 --> 00:41:30.000你能做的一件事就是直接使用这个矩阵，00:41:30.000 --> 00:41:32.870因为我没有足够的数据。00:41:32.870 --> 00:41:35.180但是，你知道，如果你有点，00:41:35.180 --> 00:41:38.125嗯，决定了，你知道，00:41:38.125 --> 00:41:40.915单词like就像单词learning，00:41:40.915 --> 00:41:42.760你会做的是你会期待00:41:42.760 --> 00:41:46.880这两个向量最终会有点相似。00:41:46.880 --> 00:41:48.345他们是这样做的。00:41:48.345 --> 00:41:50.015所以，你可以测量，嗯，00:41:50.015 --> 00:41:55.580向量的相似性直接与这些共现计数有关。00:41:55.580 --> 00:41:59.980但是，你知道，这样做有点不吸引人，对吧？00:41:59.980 --> 00:42:03.160如果你有25万个词汇表，那就是00:42:03.160 --> 00:42:06.545你在我数学不好的地方，00:42:06.545 --> 00:42:09.850但它在这个矩阵的数万亿个细胞中，00:42:09.850 --> 00:42:11.930可能需要大量的存储空间。00:42:11.930 --> 00:42:15.490不过，如果你很聪明，注意到大多数细胞都是零的，可以做到。00:42:15.490 --> 00:42:20.040一些聪明的稀疏矩阵表示可能会少一些。00:42:20.040 --> 00:42:23.710嗯，你的分类模型可能有稀疏性问题，你知道，00:42:23.710 --> 00:42:27.460很多这样的细胞都不存在，所以可能不是很健壮。00:42:27.460 --> 00:42:31.980所以这些都是传统的答案00:42:31.980 --> 00:42:36.105也许我们可以有一个大的共现计数矩阵00:42:36.105 --> 00:42:40.930以某种方式降低它的维数，嗯，00:42:40.930 --> 00:42:45.545找到相应的低维矩阵00:42:45.545 --> 00:42:47.915大部分信息，嗯，00:42:47.915 --> 00:42:50.780在原始矩阵中，你知道，00:42:50.780 --> 00:42:56.140也许我们会把尺寸缩小到25到1000左右，00:42:56.140 --> 00:42:59.195嗯，就像word2vec一样。00:42:59.195 --> 00:43:02.620所以，有一种标准的最常见的做法00:43:02.620 --> 00:43:07.210这个降维法，你不需要理解所有的数学知识，00:43:07.210 --> 00:43:10.715但你可以玩这个和家庭作业，也就是，00:43:10.715 --> 00:43:15.665对于任何矩阵，你都可以做所谓的奇异值分解，嗯，00:43:15.665 --> 00:43:23.505这是一种将任意矩阵分解成三个矩阵的方法。00:43:23.505 --> 00:43:27.250中间的那个是对角线，里面有什么00:43:27.250 --> 00:43:31.320称为奇异向量，是不同维度的权重。00:43:31.320 --> 00:43:34.895所以，当你往下走的时候，它们的大小会减小。00:43:34.895 --> 00:43:37.540然后这两个u和v是00:43:37.540 --> 00:43:41.945与行和列对应的正交基。00:43:41.945 --> 00:43:43.435尤其是，00:43:43.435 --> 00:43:47.315它比我们只有这些词向量的情况更简单，00:43:47.315 --> 00:43:51.095因为你有一个正方形的矩阵，所以它们实际上是一样的。00:43:51.095 --> 00:43:53.490但是，你知道，对于一般情况，嗯，00:43:53.490 --> 00:43:57.089尽管你得到了这些完全正交的基，00:43:57.089 --> 00:44:00.065那你就有点不太对劲了00:44:00.065 --> 00:44:03.855重要的是，当你设计产品的时候，它们最终会毫无用处。00:44:03.855 --> 00:44:09.180如果你想减少维度，你说的是，00:44:09.180 --> 00:44:14.675丢弃最小的奇异值，记住在递减中存在的奇异值。00:44:14.675 --> 00:44:17.740尺寸，这意味着你有效00:44:17.740 --> 00:44:22.040丢弃其他矩阵的行和列。00:44:22.040 --> 00:44:23.140然后它说，00:44:23.140 --> 00:44:25.860我现在把这些东西简化成00:44:25.860 --> 00:44:28.360二维表示法00:44:28.360 --> 00:44:32.440原始的三维表示，这被称为00:44:32.440 --> 00:44:39.275减少的SVD和经典的结果是在最小平方误差方面00:44:39.275 --> 00:44:47.189估计这三个东西的乘积会得到x k，这是最好的，00:44:47.189 --> 00:44:52.210嗯，k-k阶k与原始x的近似关系00:44:52.210 --> 00:44:55.155x平方最小二乘准则。00:44:55.155 --> 00:44:58.975所以，我们可以这样做，我们可以构建单词向量。00:44:58.975 --> 00:45:00.750所以，我可以，嗯，00:45:00.750 --> 00:45:02.445利用，嗯，00:45:02.445 --> 00:45:07.175numpy的svd函数，我可以投入其中，00:45:07.175 --> 00:45:11.525矩阵和，嗯，00:45:11.525 --> 00:45:14.040我可以做矢量词。00:45:14.040 --> 00:45:16.175这些看起来很糟糕，但是嘿，00:45:16.175 --> 00:45:20.825我给它一个由三个中心（笑声）组成的数据集，这并不是一个公平的比较。00:45:20.825 --> 00:45:23.420但是-所以这项技术是，嗯，00:45:23.420 --> 00:45:28.925普及到了，嗯，千年之交。00:45:28.925 --> 00:45:30.990一般来说，嗯，是为了00:45:30.990 --> 00:45:34.900以潜在语义分析或00:45:34.900 --> 00:45:38.585潜在的语义索引和想法是你可以00:45:38.585 --> 00:45:41.490这些语义方向00:45:41.490 --> 00:45:44.610在这个低维空间中发现有意义。00:45:44.610 --> 00:45:47.380人们在技术方面也做了不少工作00:45:47.380 --> 00:45:50.440像T-试图做信息检索00:45:50.440 --> 00:45:56.945使用这些LSA近似，它有点起作用。00:45:56.945 --> 00:46:02.140我想，它从来没有真正起到很好的作用，00:46:02.140 --> 00:46:06.565嗯，所以它从来没有被大范围地吸引住。00:46:06.565 --> 00:46:11.770嗯，但事实上，这种方法仍在继续探索，主要是00:46:11.770 --> 00:46:17.135cog psych―人们在那里做有词义的事情的cogs psych社区。00:46:17.135 --> 00:46:20.140有点有趣，嗯，00:46:20.140 --> 00:46:24.955文献上说有个叫道格・罗德的家伙，嗯，00:46:24.955 --> 00:46:31.055他，嗯，2005年在加州大学攻读博士学位。00:46:31.055 --> 00:46:34.225基本上他发现的是，00:46:34.225 --> 00:46:37.715看，如果不只是使用原始计数，00:46:37.715 --> 00:46:42.175我开始做更多的事情，00:46:42.175 --> 00:46:44.565你知道，玩弄计数，00:46:44.565 --> 00:46:47.525我可以开始产生更好的结果。00:46:47.525 --> 00:46:49.620所以，与其用低计数，00:46:49.620 --> 00:46:53.450你必须做些事情来处理那些高频词。00:46:53.450 --> 00:46:55.960所以，一个想法是，你可以用对数秤来测量它们，00:46:55.960 --> 00:46:58.505也常用于信息检索。00:46:58.505 --> 00:47:01.575另一个想法是你可以用，00:47:01.575 --> 00:47:03.450呃，天花板的功能，00:47:03.450 --> 00:47:09.905所以对于t集，取x，t的最小值，这个数字大约是100。00:47:09.905 --> 00:47:15.430嗯，他有――他使用了另一个想法，这也是被放入00:47:15.430 --> 00:47:21.215“2vec”这个词并不仅仅是对整个窗口的处理和你应该做的一样，00:47:21.215 --> 00:47:24.045嗯，数数更接近的单词。00:47:24.045 --> 00:47:29.745因此，在word2vec中，它们比远离单词更常见地采样更近的单词。00:47:29.745 --> 00:47:31.715嗯，在他的系统里，你必须00:47:31.715 --> 00:47:34.925近词和其他词的差异计数。00:47:34.925 --> 00:47:40.435然后，嗯，与任何一种方法相比，根本不用计数，00:47:40.435 --> 00:47:43.775然后他开始使用皮尔逊相关性00:47:43.775 --> 00:47:48.995帮助和设置他们有时是消极的，他决定帮助，00:47:48.995 --> 00:47:52.310嗯，如果你能去掉负值。00:47:52.310 --> 00:47:54.030所以，在某种意义上，00:47:54.030 --> 00:47:56.045这听起来像是一袋黑客，00:47:56.045 --> 00:47:58.459但另一方面，00:47:58.459 --> 00:48:00.555他能证明，你知道，00:48:00.555 --> 00:48:03.520这些转换的计数实际上可以给出00:48:03.520 --> 00:48:07.150正如我即将展示的，你是非常有用的词向量。00:48:07.150 --> 00:48:13.120而且-好吧，我们必须认识到，实际上在形式上略有不同，00:48:13.120 --> 00:48:17.165在word2vec中也使用了几个完全相同的计数。00:48:17.165 --> 00:48:21.110你听到了吗？00:48:21.110 --> 00:48:26.885是啊。他们[听不见]。00:48:26.885 --> 00:48:31.265是啊。所以，这是一个-我正准备展示这个。00:48:31.265 --> 00:48:34.370嗯，这真的很有趣，00:48:34.370 --> 00:48:35.800嗯，数据位。00:48:35.800 --> 00:48:39.215所以，你知道，什么，嗯，是的，00:48:39.215 --> 00:48:42.005所以，问题是-如果你这样做，00:48:42.005 --> 00:48:44.915你不仅得到了很好的单词相似性。00:48:44.915 --> 00:48:48.360我来给你举个更清楚的例子。00:48:48.360 --> 00:48:52.685嗯，所以这个-关于00:48:52.685 --> 00:48:57.390用类比法进行评估并不是真正发展起来的。00:48:57.390 --> 00:49:01.520所以，这是马什・米科洛夫，嗯，建议的。00:49:01.520 --> 00:49:05.655但事实上，Doug Rohde做了这个，嗯，00:49:05.655 --> 00:49:11.100非常有趣的观察，他说，看，00:49:11.100 --> 00:49:14.440“一旦我对00:49:14.440 --> 00:49:17.925改进我的词向量的语义表示，00:49:17.925 --> 00:49:21.705看，这真的很有趣。00:49:21.705 --> 00:49:27.240嗯，你发现有语义向量00:49:27.240 --> 00:49:32.870在我精心构建的空间中，基本上是线性组件。00:49:32.870 --> 00:49:35.170所以，这里我们有一种，嗯，00:49:35.170 --> 00:49:38.055动词方向的执行者，00:49:38.055 --> 00:49:40.740开车，开车，嗯，干净，00:49:40.740 --> 00:49:43.505看门人，游泳，游泳，学习，00:49:43.505 --> 00:49:45.645老师还是老师，老师，00:49:45.645 --> 00:49:48.705医生，治疗，牧师，祈祷。00:49:48.705 --> 00:49:51.000我是说，你知道，这并不完美，00:49:51.000 --> 00:49:52.910你知道，那里有点摇晃，对吧？00:49:52.910 --> 00:49:57.740但是，你知道，大致上很明显，有一个方向00:49:57.740 --> 00:50:03.320在对应于-从一个动词到一个动词的执行者的空格中。00:50:03.320 --> 00:50:06.430嗯，是的，所以他[听不见]―他-00:50:06.430 --> 00:50:10.450没有人想到要做类比和测试。00:50:10.450 --> 00:50:14.895但回顾起来，显而易见的是，00:50:14.895 --> 00:50:20.195如果你能构造一个具有这种线性性质的向量空间，00:50:20.195 --> 00:50:23.740那么你肯定会做得很好。00:50:23.740 --> 00:50:26.610实际上，他发明了一个向量空间00:50:26.610 --> 00:50:29.855在类比中，因为这意味着00:50:29.855 --> 00:50:33.845这个方向就是行动者，然后你可以立即00:50:33.845 --> 00:50:38.155假设这是从游泳者身上减去净身可以得到的实干向量。00:50:38.155 --> 00:50:40.745还有-对。所以，从看门人那里是干净的。00:50:40.745 --> 00:50:45.385然后我们可以加上它去游泳，我们就可以接近游泳运动员了。00:50:45.385 --> 00:50:48.410嗯，他的空间确实做到了。00:50:48.410 --> 00:50:51.930所以，嗯，这是-所以，00:50:51.930 --> 00:50:53.705从某种意义上讲，道德是，00:50:53.705 --> 00:50:58.545如果你有――如果你真的仔细控制账户等等，00:50:58.545 --> 00:51:04.715传统的方法也能给你很好的词向量空间，我的意思是，00:51:04.715 --> 00:51:09.110所以这实际上是我们手套工作的起点。00:51:09.110 --> 00:51:11.010嗯，从本质上说，00:51:11.010 --> 00:51:13.440曾经有过这两所学校。00:51:13.440 --> 00:51:16.775嗯，曾经有一所工作学校00:51:16.775 --> 00:51:20.160比其他任何地方都更深入地探索了COG心理，00:51:20.160 --> 00:51:23.870这是基于计数和转换计数。00:51:23.870 --> 00:51:29.795而且，你知道，它有一些优势，或者看起来有一些优势，对吧？00:51:29.795 --> 00:51:33.670你在利用统计数据00:51:33.670 --> 00:51:38.045整个矩阵的全局统计直接对事物进行估计。00:51:38.045 --> 00:51:41.310嗯，在那之前，00:51:41.310 --> 00:51:45.575它只是用来捕捉单词的相似性，嗯，00:51:45.575 --> 00:51:51.675而且，由于大量的统计数字，许多国家遭受了不相称的重视。00:51:51.675 --> 00:51:56.210但道格罗德，他已经开始展示如何解决这两个问题。00:51:56.210 --> 00:51:57.700另一方面，00:51:57.700 --> 00:51:59.760曾经有过这些神经网络方法00:51:59.760 --> 00:52:02.145哪种直接预测方法00:52:02.145 --> 00:52:07.110我们正在定义概率分布，并试图预测发生的单词。00:52:07.110 --> 00:52:09.970他们有一些优势，对吗？00:52:09.970 --> 00:52:15.030事实上，你的抽样意味着你不会耗尽记忆的希望。00:52:15.030 --> 00:52:18.425我知道我们有一些关于家庭作业的记忆问题，但原则上，00:52:18.425 --> 00:52:21.460你的记忆力没有那么差，如果必须的话00:52:21.460 --> 00:52:24.690构造一个巨大的矩阵，因为你是线性的，00:52:24.690 --> 00:52:27.360嗯，但是，你知道，因为你在做抽样00:52:27.360 --> 00:52:31.340举例来说，统计数据的使用效率很低。00:52:31.340 --> 00:52:37.255可以。所以，但另一方面，米科洛夫的作品表现得很完美。00:52:37.255 --> 00:52:39.145不是很完美，但很好。00:52:39.145 --> 00:52:42.880嗯，所以这是导致这项工作的原因，00:52:42.880 --> 00:52:45.340嗯，杰弗里・彭宁顿，嗯，00:52:45.340 --> 00:52:50.155我们能把这些想法结合起来吗？00:52:50.155 --> 00:52:55.445有点像神经网络方法的优点，00:52:55.445 --> 00:53:00.535嗯，在尝试使用某种计数矩阵的时候。00:53:00.535 --> 00:53:02.650尤其是，嗯，00:53:02.650 --> 00:53:04.480我们想得到结果00:53:04.480 --> 00:53:10.240一种稍微不那么粗俗的方式，你想要有意义的成分00:53:10.240 --> 00:53:13.540线性操作-线性操作00:53:13.540 --> 00:53:18.085向量空间，它们只是一些有效的或者加上的或者类似的。00:53:18.085 --> 00:53:22.540所以这个模型的关键观察是我们可以使用00:53:22.540 --> 00:53:27.685共现概率与编码意义成分的比率。00:53:27.685 --> 00:53:29.320所以这里的想法是，00:53:29.320 --> 00:53:32.020如果你有一个词像冰和00:53:32.020 --> 00:53:35.215你说这件事多久会发生一次，00:53:35.215 --> 00:53:38.965井内固体应共存，气体不应共存。00:53:38.965 --> 00:53:45.545但井水也会同时出现很多，一些随机词不会出现太多。00:53:45.545 --> 00:53:49.525如果有的话，哎呀。00:53:49.525 --> 00:53:51.820如果你有蒸汽，00:53:51.820 --> 00:53:56.230你用固体和气体得到了相反的模式，对吗？00:53:56.230 --> 00:53:58.835但要注意的是，00:53:58.835 --> 00:54:02.320仅仅靠自己大是不够的，因为大00:54:02.320 --> 00:54:06.320出现在这里和这里或小出现在那里和那里，00:54:06.320 --> 00:54:09.490有趣的事情和00:54:09.490 --> 00:54:12.925其中的这些成分表示意义成分。00:54:12.925 --> 00:54:20.275所以我们可以得到，如果我们看共存概率的比率。00:54:20.275 --> 00:54:25.810所以对于共发生概率的比率，这是00:54:25.810 --> 00:54:33.200其他单词的意思和位置，以及这类比例抵消了约一个。00:54:33.200 --> 00:54:36.970所以在这张幻灯片中，我移动了，所以这不是我00:54:36.970 --> 00:54:40.840小的和大的，它们实际上是来自语料库的实际计数。00:54:40.840 --> 00:54:43.660所以我们粗略地得到00:54:43.660 --> 00:54:46.240固体和气体是出来的00:54:46.240 --> 00:54:49.540因为它们不是意义的维度。00:54:49.540 --> 00:54:53.960所以，我们想要的是00:54:53.960 --> 00:54:58.660共现概率变成线性的，我们的空间。00:54:58.660 --> 00:55:00.515然后我们生意兴隆。00:55:00.515 --> 00:55:03.340所以这就是我们要做的。00:55:03.340 --> 00:55:05.410嗯，你怎么能做到？00:55:05.410 --> 00:55:07.360好吧，你这样做的方式，00:55:07.360 --> 00:55:15.775如果你能使点积等于同现概率的对数，00:55:15.775 --> 00:55:19.450然后你马上就会发现00:55:19.450 --> 00:55:26.605一个向量差，它变成一个共同发生概率的比率。00:55:26.605 --> 00:55:30.640所以，基本上整个模型是00:55:30.640 --> 00:55:34.630想要有点积或共同发生概率的日志。00:55:34.630 --> 00:55:36.940所以，这就是我们要做的。00:55:36.940 --> 00:55:39.700所以，这里是我们的目标函数00:55:39.700 --> 00:55:43.325它看起来有点复杂。00:55:43.325 --> 00:55:47.095但本质上我们这里有平方损失00:55:47.095 --> 00:55:51.820然后我们想说点产品应该是相似的00:55:51.820 --> 00:55:53.980尽可能记录00:55:53.980 --> 00:55:57.190同时发生的概率，所以你会00:55:57.190 --> 00:56:00.745如果他们不一样的话就会迷失方向，00:56:00.745 --> 00:56:07.180但我们对这两个词都加上了偏袒的词条，使它稍微复杂了一点。00:56:07.180 --> 00:56:10.180因为这个词可能只是一般的，而且喜欢00:56:10.180 --> 00:56:13.660共同发生或不寻常或确实结束。00:56:13.660 --> 00:56:18.160然后我们再做一个小把戏，因为每个[听不见的]人都会做把戏来表演。00:56:18.160 --> 00:56:23.185更好的是我们也在前面使用这个f函数，00:56:23.185 --> 00:56:25.510这样我们就可以限制00:56:25.510 --> 00:56:30.370非常常见的词对会对系统的性能产生影响。00:56:30.370 --> 00:56:35.155可以。这给了我们词汇向量的手套模型。00:56:35.155 --> 00:56:40.165从理论上讲，这件事的好处是，00:56:40.165 --> 00:56:43.240你知道，以前有很多文献00:56:43.240 --> 00:56:46.515有没有这些计数方法，有没有这些预测方法。00:56:46.515 --> 00:56:49.900希望这能使00:56:49.900 --> 00:56:53.245第二，向你展示一种方法，00:56:53.245 --> 00:56:58.675只是简单地用一个计数矩阵来估计，但它是用同样的00:56:58.675 --> 00:57:01.510基于迭代损失的估计方法00:57:01.510 --> 00:57:04.960用于神经方法得到良好的词向量。00:57:04.960 --> 00:57:07.390这也为我们提供了良好的词汇载体。00:57:07.390 --> 00:57:10.490下面是青蛙这个词的手套结果。00:57:10.490 --> 00:57:13.985青蛙和蟾蜍很明显。00:57:13.985 --> 00:57:16.675但是有这些不同的词，呃，00:57:16.675 --> 00:57:21.205各种各样漂亮的树蛙之类的。00:57:21.205 --> 00:57:26.440可以。嗯，那我就从这走，说一点00:57:26.440 --> 00:57:31.180关于评估单词向量的一些工作，请多了解一些。00:57:31.180 --> 00:57:36.505也许这也是一个机会，一起谈谈评估。00:57:36.505 --> 00:57:40.165所以，通常在NLP中，当我们进行估价时，00:57:40.165 --> 00:57:45.320首先是内在评价和外在评价。00:57:45.320 --> 00:57:50.170所以，通常情况下，如果我们想做点什么，比如模型，嗯，00:57:50.170 --> 00:57:54.790词与词向量相似度，或者我们想，嗯，00:57:54.790 --> 00:57:57.889把演讲的一部分放在文字或其他东西上，00:57:57.889 --> 00:58:02.860我们可以有一个内在的评价，说你得到了多好的工作。00:58:02.860 --> 00:58:05.105你在猜测演讲的正确部分吗？00:58:05.105 --> 00:58:07.480你把同义词放在一起吗？00:58:07.480 --> 00:58:12.265这通常很容易做到，计算也很快。00:58:12.265 --> 00:58:16.585这样做很有用，因为它能帮助我们理解这个系统。00:58:16.585 --> 00:58:20.440另一方面，很多时候，这些内在的评价，00:58:20.440 --> 00:58:26.285不太清楚他们在那项任务上做得很好会去哪里00:58:26.285 --> 00:58:30.460帮助我们构建令人惊叹的自然语言理解机器人00:58:30.460 --> 00:58:32.230我们如此渴望。00:58:32.230 --> 00:58:37.660嗯，所以，人们对外部评价也很感兴趣。00:58:37.660 --> 00:58:41.470所以外在的意思是，假设你使用00:58:41.470 --> 00:58:47.455在一个真实的系统中，这种新的东西并不能提高性能。00:58:47.455 --> 00:58:50.200然后就有点确定了什么是重要的00:58:50.200 --> 00:58:52.835作为一个真正的系统00:58:52.835 --> 00:58:58.675这意味着它是人类真正关心和喜欢使用的应用程序。00:58:58.675 --> 00:59:02.980这就像是网络搜索或者问答，00:59:02.980 --> 00:59:07.030或者电话对话系统之类的，嗯，00:59:07.030 --> 00:59:11.135你可以把它放进那个系统，数字就会上升。00:59:11.135 --> 00:59:13.300所以，这似乎是你想要做的。00:59:13.300 --> 00:59:15.575你想要的是在真正的任务中工作的东西。00:59:15.575 --> 00:59:20.500当然，另一方面，很多事情都比这困难得多。00:59:20.500 --> 00:59:27.340做这样一个评估和运行一个系统的不同方差的工作要多得多。00:59:27.340 --> 00:59:30.580即使结果，呃，00:59:30.580 --> 00:59:34.600有时很难诊断。00:59:34.600 --> 00:59:39.095你知道，如果你的新词载体在系统中不能更好地工作，00:59:39.095 --> 00:59:41.800可能是因为某种无关的原因00:59:41.800 --> 00:59:44.825系统是如何建立起来的，隐藏了你所有的魔法。00:59:44.825 --> 00:59:49.300如果你改变系统的其他部分，突然显示出它的良好效果。00:59:49.300 --> 00:59:51.130所以很难做到，00:59:51.130 --> 00:59:53.035嗯，有点，嗯，00:59:53.035 --> 00:59:58.060善与恶的分配好吧。00:59:58.060 --> 01:00:01.780所以，嗯，所以，今天我主要想说的是01:00:01.780 --> 01:00:05.980我们已经讨论过的这些内在的词向量评估。01:00:05.980 --> 01:00:09.835所以我们已经讨论了很多类似的东西。01:00:09.835 --> 01:00:12.370所以如果我们真的在做类比，01:00:12.370 --> 01:00:16.210事实证明，通常人们所做的是01:00:16.210 --> 01:00:20.890余弦距离和夹角01:00:20.890 --> 01:00:23.740不同的候选词，嗯，01:00:23.740 --> 01:00:26.950找出哪一个词能解出01:00:26.950 --> 01:00:30.565那是一个诺伯特式的小皱纹吗？01:00:30.565 --> 01:00:34.000还有一个人们通常使用的技巧。01:00:34.000 --> 01:00:37.620他们禁止系统返回她输入的三个字中的一个。01:00:37.620 --> 01:00:41.620类比一下，好吧。01:00:41.620 --> 01:00:45.265不过，这是你可以评估的。01:00:45.265 --> 01:00:48.520下面是一些手套的可视化效果。01:00:48.520 --> 01:00:52.240所以这些手套的图像显示了01:00:52.240 --> 01:00:57.970DougRohde发现的线性特性，这意味着类比的工作。01:00:57.970 --> 01:00:59.259按结构分类，01:00:59.259 --> 01:01:03.295因为我们的向量空间想使意义分量线性化。01:01:03.295 --> 01:01:05.275所以，这就是，嗯，01:01:05.275 --> 01:01:08.425显示性别显示。01:01:08.425 --> 01:01:13.450这是在公司和CEO之间展示的，有点酷。01:01:13.450 --> 01:01:16.150你也可以做更多的句法事实。01:01:16.150 --> 01:01:17.770这是在展示，嗯，01:01:17.770 --> 01:01:21.440形容词的正比较级和最高级。01:01:21.440 --> 01:01:28.645是啊。所以，托马斯米科洛夫提出了做这些类比任务的想法。01:01:28.645 --> 01:01:32.815所以他建立了一个数据集，里面有很多类似的东西。01:01:32.815 --> 01:01:37.330这有点-这有点奇怪的数据集，因为它是一些测试01:01:37.330 --> 01:01:42.380随机的不同的事情，可能是他的系统运行良好的事情，嗯，01:01:42.380 --> 01:01:46.185但你知道，它测试国家和首都，01:01:46.185 --> 01:01:52.780国家、城市和国家、国家和货币。01:01:52.780 --> 01:01:55.535所以有很多语义的东西需要测试。01:01:55.535 --> 01:01:58.630还有一些，嗯，01:01:58.630 --> 01:02:01.360测试如此糟糕、最糟糕的句法问题，01:02:01.360 --> 01:02:04.535最高级的速度最快。01:02:04.535 --> 01:02:07.780但是，你知道，即使是我之前展示的那些，01:02:07.780 --> 01:02:10.210没有-没有奥巴马要做的01:02:10.210 --> 01:02:15.400克林顿是那种真正在这个评估中的人。01:02:15.400 --> 01:02:18.579嗯，这是一个大的结果表，01:02:18.579 --> 01:02:20.830嗯，那是我们的手套纸。01:02:20.830 --> 01:02:25.270所以手套纸在这个评估中表现最好并不奇怪。01:02:25.270 --> 01:02:28.150因为那是我们的论文。嗯，[笑声]01:02:28.150 --> 01:02:31.450[笑声]但我的意思是-你知道，01:02:31.450 --> 01:02:34.180也许开始注意到的是，01:02:34.180 --> 01:02:37.660是的，如果你只是做一个简单的SVD计数。01:02:37.660 --> 01:02:44.170你知道，这对于这些，嗯，类比任务来说是非常糟糕的。01:02:44.170 --> 01:02:46.825但是，你知道，就像道格・罗德展示的那样，01:02:46.825 --> 01:02:53.875如果在进行SVD之前开始对计数矩阵进行操作，01:02:53.875 --> 01:02:55.690你可以开始生产01:02:55.690 --> 01:03:01.090一个基于SVD的系统，在这些任务上实际执行得相当好。01:03:01.090 --> 01:03:05.410嗯，你知道，与其他事情相比还不错。01:03:05.410 --> 01:03:07.990嗯，你会发现的其他事情，01:03:07.990 --> 01:03:10.480在顶部有100个维度，01:03:10.480 --> 01:03:13.360底部有1000个维度，01:03:13.360 --> 01:03:15.355以及其他300维的。01:03:15.355 --> 01:03:17.725至少当你在训练大量的文字时，01:03:17.725 --> 01:03:20.530更大的维度肯定更好。01:03:20.530 --> 01:03:22.420我马上就回来。01:03:22.420 --> 01:03:25.630嗯，文本的数量也会有所不同，对吗？01:03:25.630 --> 01:03:30.520所以我们要从一开始的1到15亿个单词，01:03:30.520 --> 01:03:34.480下面这些人正在接受超过420亿字的文字训练，01:03:34.480 --> 01:03:40.450或许不出所料的是，420亿字的文本效果更好。01:03:40.450 --> 01:03:42.660嗯，这是大数据。01:03:42.660 --> 01:03:45.525嗯，这篇论文还有几个步骤。01:03:45.525 --> 01:03:50.100这是一个维度图，以及性能是什么。01:03:50.100 --> 01:03:52.979所以对于这三条线，绿色的语义，01:03:52.979 --> 01:03:57.715蓝色的是句法类比，红色的是总分。01:03:57.715 --> 01:04:00.790所以你所看到的都是维度01:04:00.790 --> 01:04:04.375300件事情明显增加了不少，01:04:04.375 --> 01:04:06.085然后变得相当平坦，01:04:06.085 --> 01:04:09.325这就是为什么你会发现很多单词向量，01:04:09.325 --> 01:04:11.770嗯，这是维度300。01:04:11.770 --> 01:04:15.505嗯，这个显示的是什么窗口大小。01:04:15.505 --> 01:04:20.680所以这就是我们所说的两边对称的窗口大小，01:04:20.680 --> 01:04:23.455从246810开始。01:04:23.455 --> 01:04:25.240你看到的是，01:04:25.240 --> 01:04:29.635如果你使用一个非常小的窗口，比如两个，那实际上是可行的。01:04:29.635 --> 01:04:33.610这句话的句法预测能力更强，因为，01:04:33.610 --> 01:04:35.770句法效果非常局部。01:04:35.770 --> 01:04:37.345而当你出去的时候，01:04:37.345 --> 01:04:39.805语义预测越来越好。01:04:39.805 --> 01:04:42.370事实上，这种句法也变得更好了，01:04:42.370 --> 01:04:45.100但是，尤其是语义的进步。01:04:45.100 --> 01:04:50.080右图显示，如果只在一侧使用上下文，01:04:50.080 --> 01:04:52.570嗯，你的数字不太好。01:04:52.570 --> 01:05:00.700好吧，嗯，所以，我有点想偷偷摸摸地拍几张，01:05:00.700 --> 01:05:03.040嗯，最近的一些工作，01:05:03.040 --> 01:05:05.815作为人们做的第一件事，01:05:05.815 --> 01:05:07.570嗯，用词向量。01:05:07.570 --> 01:05:09.670嗯，这个，嗯，01:05:09.670 --> 01:05:12.475实际上是两个斯坦福人。01:05:12.475 --> 01:05:15.010嗯，现在最好了-这是最好的故事。01:05:15.010 --> 01:05:17.995如果我能说这是最后一个项目，01:05:17.995 --> 01:05:19.540嗯，去年在这节课上，01:05:19.540 --> 01:05:21.025但不幸的是，这不是真的。01:05:21.025 --> 01:05:25.330这篇论文与这门课[笑声]无关。01:05:25.330 --> 01:05:26.710但它――对。01:05:26.710 --> 01:05:30.490嗯，津隐和元元，01:05:30.490 --> 01:05:33.910嗯，事实上，嗯，01:05:33.910 --> 01:05:37.690一些聪明的，非常数学的想法，01:05:37.690 --> 01:05:41.200他们使用的是矩阵摄动理论。01:05:41.200 --> 01:05:44.140嗯，有点像是在展示，01:05:44.140 --> 01:05:49.030词向量中的维数实际上是偏倚-方差权衡的结果。01:05:49.030 --> 01:05:50.335如果你看到了，01:05:50.335 --> 01:05:52.825嗯，在机器学习的其他部分。01:05:52.825 --> 01:05:56.185我甚至不打算解释他们的论文。01:05:56.185 --> 01:05:57.535嗯，但在这里，01:05:57.535 --> 01:05:59.230他们在这篇论文上做得很好，01:05:59.230 --> 01:06:01.570他们在欧洲的谈话都是从那里开始的。01:06:01.570 --> 01:06:03.580嗯，所以-但是有点01:06:03.580 --> 01:06:07.720一个有趣的结果，你看到这些词向量，01:06:07.720 --> 01:06:10.015这在某种程度上令人惊讶。01:06:10.015 --> 01:06:16.210所以这是做字向量维从零到10000。01:06:16.210 --> 01:06:19.270所以我们要比以前谈的要高得多。01:06:19.270 --> 01:06:23.440所以你发现哪些人已经认识很久了，01:06:23.440 --> 01:06:27.745有点像是在两三点左右，01:06:27.745 --> 01:06:30.235这似乎优化了性能。01:06:30.235 --> 01:06:32.275所以，我用了这些尺寸。01:06:32.275 --> 01:06:35.920但他们所做的很多理论，01:06:35.920 --> 01:06:38.185令人惊讶的是，01:06:38.185 --> 01:06:42.250当然，如果你有一个非常庞大的数字，比如，01:06:42.250 --> 01:06:44.290如果你用10000，01:06:44.290 --> 01:06:46.720嗯，维向量，你知道，01:06:46.720 --> 01:06:53.320你想估计的是每个单词再多出两个数量级的数字，01:06:53.320 --> 01:06:56.455当然事情会分崩离析，嗯，01:06:56.455 --> 01:06:59.920因为相对于01:06:59.920 --> 01:07:04.060您尝试从中估计这些数字的培训数据量。01:07:04.060 --> 01:07:07.330所以他们展示的有趣的结果是，01:07:07.330 --> 01:07:10.270事情不会破裂的。01:07:10.270 --> 01:07:15.280嗯，你基本上可以到这些巨大的维度，01:07:15.280 --> 01:07:17.365性能保持平稳。01:07:17.365 --> 01:07:19.285他们有很多理论，01:07:19.285 --> 01:07:24.670有点像是在预测为什么最终会出现这种情况。01:07:24.670 --> 01:07:26.785嗯，是的。01:07:26.785 --> 01:07:28.975为了反复训练这些模型，01:07:28.975 --> 01:07:33.775这是-橙色显示，嗯，手套训练。01:07:33.775 --> 01:07:36.370你知道，他们有一段时间一直在好转。01:07:36.370 --> 01:07:37.870所以你知道，出去吧，01:07:37.870 --> 01:07:41.020早上去睡觉看看怎么样，对吧？01:07:41.020 --> 01:07:42.550所以如果你在运行它，嗯，01:07:42.550 --> 01:07:47.29024小时内，你的数据比只运行6小时的数据要好。01:07:47.290 --> 01:07:51.610嗯，对于很多深度学习的模特来说，这是真的，对不起。01:07:51.610 --> 01:07:54.730所以这就是你不想要的关键原因01:07:54.730 --> 01:07:57.910提前一晚开始你的任务。01:07:57.910 --> 01:08:00.880因为即使你对它进行了完美的编程，01:08:00.880 --> 01:08:03.865你可能只是没有足够的时间让它运行，01:08:03.865 --> 01:08:07.430嗯，这样你就可以在最后产生好的数字。01:08:08.160 --> 01:08:12.280嗯，好吧。嗯，是的，所以，01:08:12.280 --> 01:08:14.965所以再多一点，嗯，01:08:14.965 --> 01:08:17.710事情，关于那个，嗯。01:08:17.710 --> 01:08:21.850对.那么，嗯，我们在这里展示什么？01:08:21.850 --> 01:08:26.185所以这些又是策略和整体数字的语义。01:08:26.185 --> 01:08:29.950所以这里有两种混合在一起的东西。01:08:29.950 --> 01:08:33.010一个是，如果我们只看总的数字，01:08:33.010 --> 01:08:35.095他们是这里最高的，嗯，01:08:35.095 --> 01:08:39.730这是420亿个常见的爬行网页语料库，01:08:39.730 --> 01:08:42.085这给了我们最高的总数。01:08:42.085 --> 01:08:46.735但这张图中还有一些有趣的东西，也就是说，01:08:46.735 --> 01:08:51.730嗯，使用维基百科经常很有效。01:08:51.730 --> 01:08:57.040所以你会发现16亿个维基百科的代币是有效的01:08:57.040 --> 01:09:03.640超过43亿代币的新闻专线报纸文章数据。01:09:03.640 --> 01:09:08.125所以我觉得这是有道理的，01:09:08.125 --> 01:09:09.280很好，你知道，01:09:09.280 --> 01:09:11.620百科全书的工作就是01:09:11.620 --> 01:09:14.860解释概念以及它们如何相互关联，对吗？01:09:14.860 --> 01:09:16.870所以百科全书是01:09:16.870 --> 01:09:22.000只是更多的说明性文字，显示事物之间的所有联系，01:09:22.000 --> 01:09:27.730然而，一般来说，报纸并没有试图揭露事物是如何结合在一起的。01:09:27.730 --> 01:09:29.530他们只是告诉你，你知道，01:09:29.530 --> 01:09:32.980昨晚是谁被枪杀的，对吧？01:09:32.980 --> 01:09:37.600所以，嗯，这是一个有趣的事实，嗯，01:09:37.600 --> 01:09:39.955这个维基百科的数据真的，01:09:39.955 --> 01:09:42.655它有点不同的用处，嗯，01:09:42.655 --> 01:09:46.180对于，嗯，制作单词向量。01:09:46.180 --> 01:09:48.385你知道，事实上，你知道，01:09:48.385 --> 01:09:53.935当我们做得很好，没有手套词载体和许多人使用这些。01:09:53.935 --> 01:09:58.060你知道，我认为他们工作如此出色的原因之一是01:09:58.060 --> 01:10:03.880google发布的word2vec向量最初只建立在google新闻数据上，01:10:03.880 --> 01:10:05.770在别的什么地方有这个，01:10:05.770 --> 01:10:07.915嗯，里面有维基百科的数据。01:10:07.915 --> 01:10:11.650好吧，嗯，快点。01:10:11.650 --> 01:10:14.500嗯，是的，所以所有的类比都有，01:10:14.500 --> 01:10:20.380但另一个更基本的评估是捕获相似性判断。01:10:20.380 --> 01:10:23.020我没说太多，但你知道，01:10:23.020 --> 01:10:28.135心理学界有这样的大型亚文学，01:10:28.135 --> 01:10:32.935在那里人们想要模拟人类对相似性的判断。01:10:32.935 --> 01:10:36.265就像一个好的心理医生，你做的，01:10:36.265 --> 01:10:39.655你是不是找到了心理课的本科生？01:10:39.655 --> 01:10:42.520你给他们看成对的单词，然后说速率01:10:42.520 --> 01:10:45.745这些东西的相似程度是1到10。01:10:45.745 --> 01:10:48.265大量的数据被收集，01:10:48.265 --> 01:10:50.875你计算出人类的平均值，01:10:50.875 --> 01:10:55.255他们给出了老虎和猫这样的数字，7.35。01:10:55.255 --> 01:10:58.945老虎类似于老虎10，书和纸，01:10:58.945 --> 01:11:01.225飞机和汽车，股票和电话，01:11:01.225 --> 01:11:03.895股票和CD，你会得到数字。01:11:03.895 --> 01:11:07.000那么，我们要做的就是想说，01:11:07.000 --> 01:11:13.140让我们用空间中的距离直接映射到这些相似性判断上，01:11:13.140 --> 01:11:15.540地图绘制得有多好？01:11:15.540 --> 01:11:18.570所以这就是相似性判断01:11:18.570 --> 01:11:22.350然后被用于评估这些系统。01:11:22.350 --> 01:11:24.570所以，这里有很多模型。01:11:24.570 --> 01:11:26.550这又是我们手套纸上写的。01:11:26.550 --> 01:11:29.500但是有这些不同的相似性数据集。01:11:29.500 --> 01:11:36.190所以我在幻灯片上看到的最著名的是这个，嗯，Wordsim353。01:11:36.190 --> 01:11:38.140它有353，嗯，01:11:38.140 --> 01:11:40.120里面有不同的，01:11:40.120 --> 01:11:44.140因此，你可以对相关性进行建模。01:11:44.140 --> 01:11:49.225在你对相似性的判断和来自人类的判断之间。01:11:49.225 --> 01:11:52.960可以。还有两件事我想说。嗯，是的。01:11:52.960 --> 01:11:56.860所以，我们刚开始就有这个问题01:11:56.860 --> 01:12:01.765关于克林顿，怎么可能是各种各样的人。01:12:01.765 --> 01:12:07.030从某种意义上说，这可能是最简单的含糊不清的情况，01:12:07.030 --> 01:12:10.585当你有提到不同人的名字时。01:12:10.585 --> 01:12:13.810嗯，但名字不仅如此。01:12:13.810 --> 01:12:15.595总的来说，01:12:15.595 --> 01:12:21.820人类语言中的词是模棱两可的，有很多含义。01:12:21.820 --> 01:12:24.640嗯，这在常用词中尤其适用。01:12:24.640 --> 01:12:26.500它们总是有很多意义。01:12:26.500 --> 01:12:30.625尤其是那些已经存在了很长时间的单词。01:12:30.625 --> 01:12:34.315你知道，癌症这个新的专业词汇是不正确的。01:12:34.315 --> 01:12:35.965我认为这只有一个意义。01:12:35.965 --> 01:12:37.090但是，你知道，01:12:37.090 --> 01:12:40.585如果你想到任何相对的，嗯，01:12:40.585 --> 01:12:43.690常用词和开头，嗯，01:12:43.690 --> 01:12:45.955抓你的头一会儿，01:12:45.955 --> 01:12:48.430你会发现它有很多含义。01:12:48.430 --> 01:12:50.680我-也许这不是一个常见的词，01:12:50.680 --> 01:12:53.110但我的随机词是派克。01:12:53.110 --> 01:12:55.270嗯，派克有很多意思，01:12:55.270 --> 01:12:57.610它的含义是什么？01:12:57.610 --> 01:12:59.230鱼。01:12:59.230 --> 01:13:00.760鱼，是一种鱼，是的。01:13:00.760 --> 01:13:02.140所以有一条鱼是梭子鱼。01:13:02.140 --> 01:13:03.220还有什么是梭子鱼？01:13:03.220 --> 01:13:04.480一把大矛。01:13:04.480 --> 01:13:05.740一把大矛。01:13:05.740 --> 01:13:08.200是的，所以长矛就是长矛。01:13:08.200 --> 01:13:09.940其他类型的梭子鱼？01:13:09.940 --> 01:13:10.535体操动作。01:13:10.535 --> 01:13:10.920这是一条路。01:13:10.920 --> 01:13:13.350体操运动或跳水运动。01:13:13.350 --> 01:13:14.595这是一条路。01:13:14.595 --> 01:13:18.240嗯，是的。嗯，所以有很多意思。01:13:18.240 --> 01:13:19.845嗯，还有其他的意思。01:13:19.845 --> 01:13:21.240嗯，澳大利亚英语，01:13:21.240 --> 01:13:23.760派克也用作动词来表示，01:13:23.760 --> 01:13:25.935嗯，从做事中解脱出来。01:13:25.935 --> 01:13:31.590比如，“我们都打算晚些时候去夜总会，但乔生气了。”01:13:31.590 --> 01:13:35.160[笑声]嗯，我不认为在这个国家这种用法很普遍，01:13:35.160 --> 01:13:38.255但是，嗯，你可以试试看，嗯。[笑声]01:13:38.255 --> 01:13:41.355正确的。但是很多意思，你知道，01:13:41.355 --> 01:13:44.940派克这个词不仅如此，对吧？01:13:44.940 --> 01:13:47.540选择其他简单的词，对吗？01:13:47.540 --> 01:13:52.040你可以选择一个词，比如shell，field，house，make，01:13:52.040 --> 01:13:55.065你知道，当涉及到它的时候，它们有很多意义。01:13:55.065 --> 01:13:56.550所以，你知道，但是，呃，01:13:56.550 --> 01:14:01.069如果我们只有一个有意义的词，这怎么能起作用？01:14:01.069 --> 01:14:04.380这是个有趣的问题01:14:04.380 --> 01:14:08.120[噪音]我们很早就对它感兴趣了。01:14:08.120 --> 01:14:13.815所以，我甚至在2012年世界经济论坛的论文发表之前，01:14:13.815 --> 01:14:15.910嗯，我们在玩，嗯，01:14:15.910 --> 01:14:20.370用神经词向量，嗯，我们想，01:14:20.370 --> 01:14:23.790天哪，只有一个就这么破碎了，01:14:23.790 --> 01:14:26.605嗯，分，一句话。01:14:26.605 --> 01:14:30.045为什么我们不想出一个有多个传感器的模型？01:14:30.045 --> 01:14:33.565所以我们做了，而且我们做得很粗糙，01:14:33.565 --> 01:14:34.920我猜[噪音]嗯，01:14:34.920 --> 01:14:36.710我们的做法是说，01:14:36.710 --> 01:14:40.145好吧，让我们来看看每个常用词，01:14:40.145 --> 01:14:44.285让我们将它发生的所有上下文集群起来。01:14:44.285 --> 01:14:47.365然后我们看看是否有01:14:47.365 --> 01:14:51.980根据这个词的某种标准，多个清晰的簇。01:14:51.980 --> 01:14:56.220如果是这样的话，我们将把这个词分成伪词。01:14:56.220 --> 01:14:59.180所以，如果看起来有五个星系团，01:14:59.180 --> 01:15:00.960嗯，说实话，01:15:00.960 --> 01:15:03.020我想在这里使用的例子是捷豹。01:15:03.020 --> 01:15:05.090“捷豹”这个词有五个分类，01:15:05.090 --> 01:15:08.240我就叫他们捷豹1号，捷豹2号，捷豹3号，4号，01:15:08.240 --> 01:15:10.570五个，所以这只是字面上的变化01:15:10.570 --> 01:15:13.945这个词在我们的语料库中是根据它的簇号而定的。01:15:13.945 --> 01:15:16.990然后我们运行我们的词矢量算法，所以我们得到01:15:16.990 --> 01:15:20.870一种表示，表示每个字的传感器。01:15:20.870 --> 01:15:22.380基本上，这是可行的，01:15:22.380 --> 01:15:24.570就在顶部，接下来是捷豹1号，01:15:24.570 --> 01:15:26.825嗯，豪华和敞篷车。01:15:26.825 --> 01:15:33.260嗯，这是，我想有一种非常古老的MacOS版本叫捷豹，01:15:33.260 --> 01:15:34.860还记得那个吗？01:15:34.860 --> 01:15:40.435嗯。正确的。所以，捷豹就在软件和微软旁边，所以这是有希望的。01:15:40.435 --> 01:15:44.715嗯，这是猎手旁边的美洲虎，嗯，01:15:44.715 --> 01:15:47.070我对这件事感到困惑，01:15:47.070 --> 01:15:50.550捷豹是不是接近独奏音乐键盘和弦乐。01:15:50.550 --> 01:15:53.545有没有一个叫捷豹的键盘品牌？01:15:53.545 --> 01:15:55.015我对那个不太确定，01:15:55.015 --> 01:15:57.380但不管怎样，这基本上是可行的。01:15:57.380 --> 01:16:02.080嗯，但那有点粗糙，而且可能也有问题，01:16:02.080 --> 01:16:06.450所以很多时候，传感器之间的划分不是很清楚，对吗？01:16:06.450 --> 01:16:10.330很多传感器实际上是相互关联和重叠的，因为01:16:10.330 --> 01:16:14.790当传感器通常到达的时候，人们会拉伸单词的含义。01:16:14.790 --> 01:16:16.860不是他们只是随机的01:16:16.860 --> 01:16:19.685第二天早上醒来说：“我知道地毯。01:16:19.685 --> 01:16:23.005我也可以把它称为石头，“嗯，01:16:23.005 --> 01:16:25.990给“石头”这个词一个新的含义，对吧？01:16:25.990 --> 01:16:28.600你带上你知道的东西01:16:28.600 --> 01:16:33.085一个网络，你可以把它比喻为其他的网络用途。01:16:33.085 --> 01:16:37.000嗯，这可能是更有趣的事情，01:16:37.000 --> 01:16:39.475这是另一个桑吉夫・阿罗拉，01:16:39.475 --> 01:16:41.980嗯，我要提到的文件。01:16:41.980 --> 01:16:44.710所以，如果你不这样做会发生什么，01:16:44.710 --> 01:16:50.095嗯，如果你每个字不超过一美分？01:16:50.095 --> 01:16:53.100实际上，你得到的是01:16:53.100 --> 01:16:57.260你所学的矢量这个词是指01:16:57.260 --> 01:17:01.040物理学家和幻想中的人是一个叠加态01:17:01.040 --> 01:17:05.910不同句子的词向量，不同的传感器。01:17:05.910 --> 01:17:09.995所谓的超初叠加，就是指加权平均数。01:17:09.995 --> 01:17:14.930嗯，嗯，[笑声]所以有效地01:17:14.930 --> 01:17:17.040我对派克的理解是01:17:17.040 --> 01:17:20.865不同派克传感器矢量的加权平均值，01:17:20.865 --> 01:17:24.645这些分量的权重就是它们的频率。01:17:24.645 --> 01:17:28.265嗯，所以这部分可能并不太令人惊讶，01:17:28.265 --> 01:17:31.830但真正令人惊讶的是，01:17:31.830 --> 01:17:34.375如果我们只求这些词向量的平均值，01:17:34.375 --> 01:17:38.380你会认为你不能从平均水平中得到任何东西，对吗？01:17:38.380 --> 01:17:43.180如果我告诉你我在想两个数字，它们就在这里，01:17:43.180 --> 01:17:46.010加权和为54，01:17:46.010 --> 01:17:47.870我的两个号码是什么，对吗？01:17:47.870 --> 01:17:51.990你真的有点缺乏信息来回答我的问题。01:17:51.990 --> 01:17:53.610但是，你知道，01:17:53.610 --> 01:17:56.820对于这些词向量，嗯，01:17:56.820 --> 01:18:02.625我们有这些高维空间，即使在那里01:18:02.625 --> 01:18:07.980是很多词，空间如此之大，思想维度，01:18:07.980 --> 01:18:13.535实际的单词或传感器在那个空间中非常稀少。01:18:13.535 --> 01:18:17.650结果发现，这本书里有关于，嗯，01:18:17.650 --> 01:18:20.330稀疏编码，压缩传感，01:18:20.330 --> 01:18:23.740嗯，有些是统计部门的人做的，01:18:23.740 --> 01:18:29.325嗯，这表明在这些情况下，你有这种稀疏，01:18:29.325 --> 01:18:32.115嗯，这些高维空间的代码，01:18:32.115 --> 01:18:36.885实际上，你可以重建叠加的分量，01:18:36.885 --> 01:18:40.240尽管你所做的只是这个加权平均数，01:18:40.240 --> 01:18:45.010所以，这篇论文着眼于你如何做到这一点，所以他们做到了，01:18:45.010 --> 01:18:48.105嗯，这些潜在的意义成分，01:18:48.105 --> 01:18:49.910他们有点分离了。01:18:49.910 --> 01:18:52.810所以，领带有一个意义成分，01:18:52.810 --> 01:18:55.975在这个空间里有裤子，衬衫，背心，01:18:55.975 --> 01:19:00.300这是有道理的，还有一个就是经验丰富的团队的组成部分，01:19:00.300 --> 01:19:02.645赢得联赛是有道理的。01:19:02.645 --> 01:19:06.015嗯，得分线进球，均衡器得分，01:19:06.015 --> 01:19:08.355这个似乎和这个有点重叠。01:19:08.355 --> 01:19:10.045嗯，但这里打领带，01:19:10.045 --> 01:19:13.800这是一种电缆扎带和电线扎带之类的东西。01:19:13.800 --> 01:19:17.245所以，他们实际上能够提取出不同的意义，01:19:17.245 --> 01:19:21.055嗯，从外面，从这个词的意思。01:19:21.055 --> 01:19:24.045嗯，那是件很酷的事情。01:19:24.045 --> 01:19:26.085我只是想，嗯，01:19:26.085 --> 01:19:28.195再说一句。01:19:28.195 --> 01:19:32.010可以。[噪音]到目前为止所有的评估都是内在的，01:19:32.010 --> 01:19:35.735嗯，你也可能想做外在的评估。01:19:35.735 --> 01:19:40.010为什么，为什么矢量词在NLP上如此激动人心？01:19:40.010 --> 01:19:42.300原来有这个意思吗？01:19:42.300 --> 01:19:45.965有了这个表象意味着01:19:45.965 --> 01:19:49.810非常有用，并且在那之后改进了你的所有任务。01:19:49.810 --> 01:19:52.280嗯，所以，嗯，01:19:52.280 --> 01:19:55.360这是执行命名实体识别，它正在标记01:19:55.360 --> 01:19:58.860人、地点和组织，但是，你知道，01:19:58.860 --> 01:20:01.755这是人们发现的许多任务的典型特征，01:20:01.755 --> 01:20:06.610如果你从一个没有任何文字表达的模型开始01:20:06.610 --> 01:20:11.875你把你的文字载体扔进去，不管它们是不是汽车手套的载体，01:20:11.875 --> 01:20:15.505只是你的数字上升了百分之几或更多？01:20:15.505 --> 01:20:19.570所以单词向量只是一种有用的来源，你可以01:20:19.570 --> 01:20:24.190投入到你建立的任何NLP系统中，你的数字就会上升。01:20:24.190 --> 01:20:27.335所以，只有非常有效的技术，嗯，01:20:27.335 --> 01:20:28.980实际上它在01:20:28.980 --> 01:20:34.490
basically any extrinsic tasks you type tried it on. Okay. Thanks a lot.

