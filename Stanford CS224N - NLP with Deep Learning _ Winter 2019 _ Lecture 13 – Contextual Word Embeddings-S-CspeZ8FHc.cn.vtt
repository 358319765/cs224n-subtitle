WEBVTT
Kind: captions
Language: en

00:00:05.660 --> 00:00:08.145
好,大家好。00:00:08.145 --> 00:00:11.110
让我们重新开始。00:00:11.210 --> 00:00:17.010
嗯。好。首先是一些公告。00:00:17.010 --> 00:00:20.040
首先感谢大家00:00:20.040 --> 00:00:23.955
谁填写了我们的季度中期调查，00:00:23.955 --> 00:00:27.320
很好的参与其中。00:00:27.320 --> 00:00:29.855
这是我的两个小精灵。00:00:29.855 --> 00:00:31.640
所以，吃豆人认为，00:00:31.640 --> 00:00:34.580
意思是几乎每个人都认为讲座在00:00:34.580 --> 00:00:38.390
正确的节奏和不正确的节奏几乎平分了。00:00:38.390 --> 00:00:42.680
如果我们来看看作业三的挑战性，00:00:42.680 --> 00:00:46.190
略多的人认为这太简单而不是太难。00:00:46.190 --> 00:00:48.680
所以，我想我们要开始纠正这个问题了00:00:48.680 --> 00:00:53.460
作业4和5，嗯，[噪音]。00:00:53.460 --> 00:00:56.030
尽管还有很多其他的问题00:00:56.030 --> 00:00:59.180
试图吸收所有的反馈。00:00:59.180 --> 00:01:03.980
我的意思是其中一个问题是人们最想从剩下的课程中得到什么。00:01:03.980 --> 00:01:09.260
我想好消息是我们很擅长预测，00:01:09.260 --> 00:01:11.780
人们想要什么，不是这个就是那个00:01:11.780 --> 00:01:14.660
看一下教学大纲，把它写下来00:01:14.660 --> 00:01:19.160
在教学大纲的前面，但我想是最受欢迎的四个答案00:01:19.160 --> 00:01:23.810
在接下来的课程中，他们想要的主题是变形金刚和伯特，00:01:23.810 --> 00:01:25.860
这两项都将在这周讨论。00:01:25.860 --> 00:01:29.780
我们上周讨论过的问答，00:01:29.780 --> 00:01:33.230
然后是文本生成和总结00:01:33.230 --> 00:01:38.275
你们下个星期再找艾比谈这个。00:01:38.275 --> 00:01:42.380
也有很多人回答了这个问题00:01:42.380 --> 00:01:46.400
这是一种不同的方式，00:01:46.400 --> 00:01:50.960
有些人强调新的研究和该领域的最新进展。00:01:50.960 --> 00:01:53.300
我想我们今天也会学到一些，00:01:53.300 --> 00:01:55.235
有些人更感兴趣00:01:55.235 --> 00:02:00.000
在工业上的成功应用或尝试做一点，00:02:00.000 --> 00:02:02.840
很酷的新神经结构。00:02:02.840 --> 00:02:05.660
底部的答案不是最受欢迎的，00:02:05.660 --> 00:02:08.390
我承认但至少有几个人00:02:08.390 --> 00:02:11.810
真希望我们能教更多语言方面的东西。00:02:11.810 --> 00:02:14.300
我是说，这是我的真实感受00:02:14.300 --> 00:02:18.580
CS224N的合并方式有点尴尬，00:02:18.580 --> 00:02:20.100
通过这种深入的学习，00:02:20.100 --> 00:02:22.370
我的意思是，事实似乎是这样的00:02:22.370 --> 00:02:24.850
就像在课程的早期，00:02:24.850 --> 00:02:27.210
有很多东西要讲，00:02:27.210 --> 00:02:29.660
神经网络，反向传播，00:02:29.660 --> 00:02:34.250
不同的，嗯，神经网络结构等等，现实是我们00:02:34.250 --> 00:02:39.890
教的语言比我们以前在课堂上教的要少。00:02:39.890 --> 00:02:43.160
我的意思是，在这门课的最后四周，我们真的很努力00:02:43.160 --> 00:02:46.730
涵盖更多的语言话题。00:02:46.730 --> 00:02:49.255
所以期待着吧。00:02:49.255 --> 00:02:51.255
嗯,公告。00:02:51.255 --> 00:02:54.365
好。所以我们对截止日期做了一些修改。00:02:54.365 --> 00:02:57.410
首先，有很多人00:02:57.410 --> 00:03:00.920
提到他们认为作业五有点难。00:03:00.920 --> 00:03:04.160
所以，我们给人们多一天的时间，00:03:04.160 --> 00:03:06.105
做作业五。00:03:06.105 --> 00:03:09.830
从某种意义上说，我意识到多一天并不算多00:03:09.830 --> 00:03:13.760
但是你知道这里有一种复杂的平衡因为另一方面，00:03:13.760 --> 00:03:18.695
我们并不想破坏人们完成最终项目的时间。00:03:18.695 --> 00:03:23.010
如果你是还没有开始做作业五的同学之一，00:03:23.010 --> 00:03:26.075
我们真的鼓励你去做这件事。00:03:26.075 --> 00:03:29.955
嗯，对，在相反的方向00:03:29.955 --> 00:03:34.160
我们认为这个项目的里程碑实在是太迟了。00:03:34.160 --> 00:03:38.060
如果我们能给你有用的反馈00:03:38.060 --> 00:03:42.080
所以我们把项目的里程碑日期提前了两天。00:03:42.080 --> 00:03:45.935
所以，我们也得到了每个人的项目建议和我们的00:03:45.935 --> 00:03:50.270
计划中的希望是在周五把它们发给所有人。00:03:50.270 --> 00:03:52.310
是的，很多东西在移动。00:03:52.310 --> 00:03:56.975
最后还有其他的通知00:03:56.975 --> 00:04:01.640
本周四是我们的第一位受邀演讲者，00:04:01.640 --> 00:04:05.165
如果你是学生，你就应该在这里，00:04:05.165 --> 00:04:08.930
如果你不能来这里00:04:08.930 --> 00:04:12.350
你应该知道我们的反应段落政策00:04:12.350 --> 00:04:16.460
我在广场上张贴了一些海报，00:04:16.460 --> 00:04:21.375
反应片段和出勤率，反应片段的一个例子，00:04:21.375 --> 00:04:27.320
从上节课开始，让它更加具体。00:04:27.320 --> 00:04:31.850
但是，你知道，这个想法是我们所希望的，而不是大量的工作。00:04:31.850 --> 00:04:35.855
你可以写100个，150个单词，几个句子，00:04:35.855 --> 00:04:40.040
而是想让你挑出一个具体的东西00:04:40.040 --> 00:04:42.410
写几个有趣的句子00:04:42.410 --> 00:04:45.140
关于它是什么以及你的想法。00:04:45.140 --> 00:04:50.270
我，不只是一些非常普通的陈述这是一个关于变形金刚的讲座。00:04:50.270 --> 00:04:52.805
他谈到了变形金刚，这很有趣，00:04:52.805 --> 00:04:58.635
这不是我们想要的反应产物。嗯,好吧。00:04:58.635 --> 00:05:01.140
这是今天的计划。00:05:01.140 --> 00:05:04.955
今天，我想讲的是，00:05:04.955 --> 00:05:09.785
最近令人兴奋的关于语境词表示的研究。00:05:09.785 --> 00:05:15.620
我是说，我在想我要说什么我想说，哦，这是00:05:15.620 --> 00:05:18.770
最令人兴奋的事情在深度学习的NLP在00:05:18.770 --> 00:05:22.040
在过去的五年里，有些事情是完全错误的，00:05:22.040 --> 00:05:27.080
因为这真的是2018年发生的深度学习中最令人兴奋的事情。00:05:27.080 --> 00:05:29.960
我是说，我想事情进展得很快，00:05:29.960 --> 00:05:33.530
在目前的深度学习中，我不这么认为00:05:33.530 --> 00:05:38.130
公平地说，你知道它只有5年的寿命。00:05:38.130 --> 00:05:40.490
但是去年发生了一件非常令人兴奋的事情，00:05:40.490 --> 00:05:42.655
我们会讲到的。00:05:42.655 --> 00:05:45.720
好。我们会讲一些早期的东西，00:05:45.720 --> 00:05:47.455
ULMfit,埃尔莫00:05:47.455 --> 00:05:50.630
变压器架构，然后继续00:05:50.630 --> 00:05:55.320
说到伯特模型，它最近很突出。00:05:55.670 --> 00:05:58.215
让我们来回顾一下，00:05:58.215 --> 00:06:02.605
我们先倒回去想想，00:06:02.605 --> 00:06:08.075
我们曾经做过什么，现在做了什么，为什么我们想要更多。00:06:08.075 --> 00:06:09.695
到目前为止，00:06:09.695 --> 00:06:11.060
我们刚刚，00:06:11.060 --> 00:06:16.255
单词的一种表示就是我们在课开始时学过的，00:06:16.255 --> 00:06:22.085
有一个词，你训练了一个词向量这就是你在模型中使用的。00:06:22.085 --> 00:06:24.774
你可以用Word2vec这样的算法，00:06:24.774 --> 00:06:28.075
手套，或者我上周提到的快速文本。00:06:28.075 --> 00:06:34.460
一些关于深度学习的想法，00:06:34.460 --> 00:06:39.050
当深度学习为NLP或一般00:06:39.050 --> 00:06:42.065
神经网络在NLP领域的复兴00:06:42.065 --> 00:06:45.620
大约是在本世纪初出现的。00:06:45.620 --> 00:06:50.640
这些预先训练好的单词向量。00:06:50.640 --> 00:06:54.790
所以，在没有监督的情况下对大量的文本进行预先训练。00:06:54.790 --> 00:06:58.270
他们完全被视为秘密武器00:06:58.270 --> 00:07:00.805
它们是改变的东西00:07:00.805 --> 00:07:04.795
神经网络从NLP到一些不太好用的东西，00:07:04.795 --> 00:07:06.650
去做一些很棒的事情。00:07:06.650 --> 00:07:09.910
这是我的一张旧幻灯片。00:07:09.910 --> 00:07:12.670
这张幻灯片是我第一次做的00:07:12.670 --> 00:07:18.490
2012年ACL教程，然后在课堂上使用。00:07:18.490 --> 00:07:22.995
大概是在2013年和2014年。嗯,。00:07:22.995 --> 00:07:26.460
这就是当时的情况。00:07:26.460 --> 00:07:28.000
这是两个任务，00:07:28.000 --> 00:07:33.115
这是语音标记和命名实体识别的一部分，我今天会经常用到。00:07:33.115 --> 00:07:38.275
你知道，最上面一行显示的是一种艺术的状态00:07:38.275 --> 00:07:42.780
一种传统的基于分类特征的分类器00:07:42.780 --> 00:07:47.445
在21世纪头10年，这一点主导了NLP的表现。00:07:47.445 --> 00:07:53.215
下一行显示的是，如果取相同的数据集00:07:53.215 --> 00:07:59.515
你在上面训练了一个监督神经网络然后问它你的表现有多好?00:07:59.515 --> 00:08:01.845
故事不是很好。00:08:01.845 --> 00:08:06.720
词性标注之所以有这么高的数字是因为各种各样的原因。00:08:06.720 --> 00:08:11.395
因此，可能更具有指示性的是这些命名实体识别数字。00:08:11.395 --> 00:08:14.530
所以，你知道，这有点像神经网络，对吧?00:08:14.530 --> 00:08:18.310
为什么在过去的十年里，00:08:18.310 --> 00:08:20.790
基于分类特征，00:08:20.790 --> 00:08:23.340
CRF, SVM一类分类器。00:08:23.340 --> 00:08:26.995
如果你看，它比神经网络好8%00:08:26.995 --> 00:08:28.330
为什么不有吗?00:08:28.330 --> 00:08:32.845
但是后来发生的事情是人们提出了我们可以做到的想法00:08:32.845 --> 00:08:37.515
对单词表示进行无监督的预习，00:08:37.515 --> 00:08:40.770
用单词向量来表示单词。00:08:40.770 --> 00:08:42.060
你知道，在那些日子里，00:08:42.060 --> 00:08:45.620
这是非常难做的alg-都因为00:08:45.620 --> 00:08:49.500
那种算法，那种可用的机器，对吧?00:08:49.500 --> 00:08:51.990
胶体和韦斯顿，2011年，00:08:51.990 --> 00:08:56.980
花了7周时间训练他们的无监督文字表达。00:08:56.980 --> 00:08:58.105
在一天结束的时候，00:08:58.105 --> 00:09:01.795
只有100维的单词表示。00:09:01.795 --> 00:09:03.865
但这是一个奇迹般的突破，对吧?00:09:03.865 --> 00:09:08.965
你在无监督的文字表达上取得了奇迹般的突破。00:09:08.965 --> 00:09:12.215
现在，神经网络达到88。87。00:09:12.215 --> 00:09:15.380
它几乎和基于特征的分类器一样好，00:09:15.380 --> 00:09:17.510
然后像所有优秀的工程师一样，00:09:17.510 --> 00:09:19.500
他们做了一些额外的功能，00:09:19.500 --> 00:09:21.175
因为他们有这样的东西。00:09:21.175 --> 00:09:26.720
他们得到了一个比基于功能的系统稍好一点的系统。00:09:26.720 --> 00:09:29.520
好。这就是我们的想法，00:09:29.520 --> 00:09:33.180
有了这些预培训，00:09:33.180 --> 00:09:36.560
单词表示的非超和非监督方式，00:09:36.560 --> 00:09:39.000
这是一个很大的突破00:09:39.000 --> 00:09:42.280
秘密酱汁，让所有的魅力，00:09:42.280 --> 00:09:44.765
神经网络很有竞争力。00:09:44.765 --> 00:09:46.200
但是，你知道，00:09:46.200 --> 00:09:51.565
这是一件很有趣的事情发生在人们有了00:09:51.565 --> 00:09:54.430
一些最初的突破00:09:54.430 --> 00:09:57.510
所有关于培训前的无监督方法，00:09:57.510 --> 00:09:59.030
在视觉上也是一样的。00:09:59.030 --> 00:10:00.625
这是一个充满幻想的时代，00:10:00.625 --> 00:10:03.380
你在哪里建造受限制的玻尔兹曼机器00:10:03.380 --> 00:10:07.410
复杂的无监督的培训技术，以及对他们。00:10:07.410 --> 00:10:13.425
不知何故，当人们发现并开始适应之后，00:10:13.425 --> 00:10:16.260
人们开始发现，00:10:16.260 --> 00:10:20.280
实际上我们有一些新的非线性技术，00:10:20.280 --> 00:10:22.665
正则化等等。00:10:22.665 --> 00:10:25.830
如果我们继续使用同样的技术，00:10:25.830 --> 00:10:29.770
我们可以回到以前的监督学习。00:10:29.770 --> 00:10:34.510
令人震惊的是，现在它在神经网络中工作得更好了。00:10:34.510 --> 00:10:37.440
所以如果你继续我所说的，00:10:37.440 --> 00:10:43.615
从2014年到2018年，00:10:43.615 --> 00:10:46.445
这幅图其实很不一样。00:10:46.445 --> 00:10:48.270
图像是这样的，00:10:48.270 --> 00:10:51.295
我要给你们看的结果来自陈和曼宁，00:10:51.295 --> 00:10:54.550
我们几周前讲过的神经依赖解析器。00:10:54.550 --> 00:10:56.695
那张照片是00:10:56.695 --> 00:10:59.065
你可以――尽管事实如此00:10:59.065 --> 00:11:02.995
这个依赖解析器是在一个很小的语料库上训练的，00:11:02.995 --> 00:11:05.640
一百万字的监督数据，00:11:05.640 --> 00:11:09.250
你可以用随机的词向量初始化它，00:11:09.250 --> 00:11:11.830
训练一个依赖解析器。00:11:11.830 --> 00:11:13.740
对于第一个近似，00:11:13.740 --> 00:11:15.060
它很好用。00:11:15.060 --> 00:11:17.935
你的准确率达到了90%00:11:17.935 --> 00:11:20.425
英语依赖解析器。00:11:20.425 --> 00:11:23.740
现在的情况是，00:11:23.740 --> 00:11:27.475
您可以使用预先训练好的word embeddings，这样可以做得更好。00:11:27.475 --> 00:11:29.230
你的成绩提高了百分之一。00:11:29.230 --> 00:11:31.550
这就是，00:11:31.550 --> 00:11:35.725
新的世界秩序是，嗯，00:11:35.725 --> 00:11:40.860
这些预先训练的无监督单词嵌入非常有用，因为您可以这样做00:11:40.860 --> 00:11:46.840
训练他们从更多的数据，他们可以知道一个更大的词汇。00:11:46.840 --> 00:11:48.025
这意味着它们是有用的。00:11:48.025 --> 00:11:51.805
他们会用一些罕见的词语来帮助你，他们会给你一个百分比，00:11:51.805 --> 00:11:55.160
但它们绝对不再是那种日夜兼程，00:11:55.160 --> 00:11:59.910
让神经网络工作的东西，我们曾经相信的。00:11:59.910 --> 00:12:04.465
我要离开这里，00:12:04.465 --> 00:12:08.455
从主要叙述到，嗯，00:12:08.455 --> 00:12:13.490
还有一个处理未知单词和单词向量的技巧，00:12:13.490 --> 00:12:16.290
以防对某些人有用00:12:16.290 --> 00:12:19.350
建立问题回答系统，对吧?00:12:19.350 --> 00:12:24.450
对于未知单词上的单词向量，00:12:24.450 --> 00:12:29.695
历史上最常见的是你有你的监督训练数据，00:12:29.695 --> 00:12:32.790
你定义一个词汇，它可能是出现的单词00:12:32.790 --> 00:12:36.255
在你的监督训练数据中，五倍或更多。00:12:36.255 --> 00:12:39.040
而你却把其他的一切都当成是扣篮。00:12:39.040 --> 00:12:42.085
每一次扣篮训练一个向量。00:12:42.085 --> 00:12:46.140
但那有一些你无法解决的问题00:12:46.140 --> 00:12:51.250
区分不同的扣篮词的身份或意义。00:12:51.250 --> 00:12:54.745
这对于问答系统来说是有问题的。00:12:54.745 --> 00:12:58.140
解决这个问题的一种方法是我们上周讲过的，00:12:58.140 --> 00:13:00.630
你只要说，“哦，单词是由字符组成的。00:13:00.630 --> 00:13:05.655
我可以用字符表示来学习其他单词的单词向量。”00:13:05.655 --> 00:13:06.960
你当然可以这么做。00:13:06.960 --> 00:13:08.230
你可以试试。00:13:08.230 --> 00:13:10.210
这增加了一些复杂性。00:13:10.210 --> 00:13:14.380
尤其是像问答系统这样的东西，00:13:14.380 --> 00:13:16.380
你还可以做一些其他的事情00:13:16.380 --> 00:13:18.720
这样做效果更好，而且他们一直都是这样00:13:18.720 --> 00:13:23.365
Dhingra等人于2017年在本文中进行了探索。00:13:23.365 --> 00:13:26.695
第一个问题是，00:13:26.695 --> 00:13:34.255
当你在考试时遇到新单词，可能是你的非监督单词，00:13:34.255 --> 00:13:39.335
预先训练好的单词嵌入比实际系统中的词汇量要大得多。00:13:39.335 --> 00:13:42.090
所以当你遇到一个不存在的单词时00:13:42.090 --> 00:13:44.955
你的词汇是预习好的嵌入式词汇，00:13:44.955 --> 00:13:48.985
用这个词的向量，然后开始用它。00:13:48.985 --> 00:13:51.745
这样用起来会更有用。00:13:51.745 --> 00:13:53.850
还有第二个建议00:13:53.850 --> 00:13:56.305
看到一些未知的单词，00:13:56.305 --> 00:13:57.925
与其把它当作扣篮，00:13:57.925 --> 00:14:00.030
你只要当场分配，00:14:00.030 --> 00:14:01.750
一个随机的字向量。00:14:01.750 --> 00:14:06.810
所以每个单词都有一个唯一的标识。00:14:06.810 --> 00:14:09.360
也就是说如果你在问题中看到同样的词，00:14:09.360 --> 00:14:11.065
一个可能的答案是，00:14:11.065 --> 00:14:14.940
它们会以一种精确的方式完美地结合在一起00:14:14.940 --> 00:14:19.895
而不仅仅是扣篮匹配这些都是很有用的想法。00:14:19.895 --> 00:14:25.275
好了,题外话结束。到目前为止，00:14:25.275 --> 00:14:28.225
我们只是用这个词来表示，00:14:28.225 --> 00:14:31.600
我们运行Word2vec，得到一个单词向量，00:14:31.600 --> 00:14:33.725
每一个字。00:14:33.725 --> 00:14:37.570
这很有用。00:14:37.570 --> 00:14:39.105
效果很好。00:14:39.105 --> 00:14:41.545
但它有00:14:41.545 --> 00:14:46.860
一些大的问题。那么这样做的最大问题是什么呢?00:14:48.530 --> 00:14:50.785
当我们，00:14:50.785 --> 00:14:53.475
每个单词中都有一个向量，是的。00:14:53.475 --> 00:14:56.825
很多单词只有一种拼写，但是有很多含义。00:14:56.825 --> 00:15:00.550
一个词可以有，00:15:00.550 --> 00:15:05.620
你有一串有很多含义的字母。00:15:05.620 --> 00:15:09.220
所以，单词有很多含义。00:15:09.220 --> 00:15:11.350
对，就是这样00:15:11.350 --> 00:15:13.405
这是我们面临的最大也是最明显的问题00:15:13.405 --> 00:15:15.550
把所有单词的意思合在一起。00:15:15.550 --> 00:15:18.130
我们讲了一点00:15:18.130 --> 00:15:20.290
一个解决办法是你可以分辨出来00:15:20.290 --> 00:15:23.680
词的意义，并有不同的词向量。00:15:23.680 --> 00:15:27.700
然后我说了一些你能想到的00:15:27.700 --> 00:15:31.750
向量这个词是它们的混合物也许你的模型可以把它们分开。00:15:31.750 --> 00:15:35.065
但似乎我们应该更认真地对待这个问题。00:15:35.065 --> 00:15:37.420
一种方法是，00:15:37.420 --> 00:15:43.345
我们可以更认真地对待这个问题，我们可以开始说，00:15:43.345 --> 00:15:50.065
真的，你知道，传统的语感列表本身就是一个粗略的近似。00:15:50.065 --> 00:15:57.925
我们真正想知道的是这个词在特定语境中的意义。00:15:57.925 --> 00:16:00.400
我的意思是，00:16:00.400 --> 00:16:04.570
我们区分一个词的不同意义，对吧?00:16:04.570 --> 00:16:08.380
比如说恒星这个词有天文学上的意义00:16:08.380 --> 00:16:12.415
有一种好莱坞的感觉，他们明显不同。00:16:12.415 --> 00:16:16.270
但是你知道，如果我们接着看我所说的好莱坞感，00:16:16.270 --> 00:16:18.370
然后我可以说，等一下。00:16:18.370 --> 00:16:21.520
有电影明星，也有摇滚明星，00:16:21.520 --> 00:16:24.070
还有R&B明星，00:16:24.070 --> 00:16:25.825
还有乡村明星。00:16:25.825 --> 00:16:29.275
现在，所有这些不同的感觉，00:16:29.275 --> 00:16:33.025
然而，在某些情况下，它们中的一个或另一个会被唤起。00:16:33.025 --> 00:16:34.210
所以，你知道，00:16:34.210 --> 00:16:36.879
如果你想要枚举，这是非常困难的00:16:36.879 --> 00:16:40.825
一个词的不同或相同的意义。00:16:40.825 --> 00:16:44.785
所以，你想知道一个词在上下文中是什么意思。00:16:44.785 --> 00:16:50.515
这些词的第二个限制是，00:16:50.515 --> 00:16:53.710
我们还没有真正讨论过，也不那么明显，00:16:53.710 --> 00:16:56.770
但这也是我们想要解决的问题，至少是其中之一00:16:56.770 --> 00:17:00.070
我们今天讨论的模型针对这一点，00:17:00.070 --> 00:17:04.045
也就是说，我们只有一个向量来表示一个单词。00:17:04.045 --> 00:17:07.465
但是一个词有不同的维度。00:17:07.465 --> 00:17:10.390
所以，单词可以有不同的意思，00:17:10.390 --> 00:17:14.605
某种真正的语义学或词汇00:17:14.605 --> 00:17:19.765
不同的句法行为，如不同的词性或语法行为。00:17:19.765 --> 00:17:23.065
所以，在某种意义上，到达和到达，00:17:23.065 --> 00:17:25.675
它们的语义几乎是一样的，00:17:25.675 --> 00:17:28.990
但它们是不同的词性。00:17:28.990 --> 00:17:32.410
一个是动词，一个是名词，00:17:32.410 --> 00:17:35.380
所以它们可以出现在不同的地方。00:17:35.380 --> 00:17:39.130
你想在依赖解析器中对它们做不同的事情。00:17:39.130 --> 00:17:41.290
甚至还有其他维度。00:17:41.290 --> 00:17:47.200
因此，词语也存在着语域和内涵的差异。00:17:47.200 --> 00:17:52.270
所以，你可能会想到很多不同的词来形容浴室，00:17:52.270 --> 00:17:56.170
很多这些词在语义上都是一样的，00:17:56.170 --> 00:17:58.330
但是有不同的寄存器00:17:58.330 --> 00:18:01.225
什么时候使用合适的含义。00:18:01.225 --> 00:18:04.900
所以，我们可能也想在这个基础上区分单词。00:18:04.900 --> 00:18:08.350
这些就是我们想要的soluti00:18:08.350 --> 00:18:11.845
解决与我们新的上下文词嵌入。00:18:11.845 --> 00:18:16.240
我之前说过，00:18:16.240 --> 00:18:21.670
哦，我们只是用了这些向量，00:18:21.670 --> 00:18:24.205
单词只有一个向量。00:18:24.205 --> 00:18:29.170
但是如果你仔细想想，也许这是错的。00:18:29.170 --> 00:18:34.270
我的意思是，也许我们从来没有遇到过问题，或者至少，我们在六节课之前就解决了它。00:18:34.270 --> 00:18:36.205
因为如果你还记得，00:18:36.205 --> 00:18:39.460
当我们开始讨论神经语言模型时，00:18:39.460 --> 00:18:41.950
那么，神经语言模型做了什么呢?00:18:41.950 --> 00:18:45.100
在底部，你输入向量这个词。00:18:45.100 --> 00:18:49.599
但当你遇到一个或多个循环层时，00:18:49.599 --> 00:18:51.565
比如LSTM层，00:18:51.565 --> 00:18:57.430
计算每个单词上面的这些表示，00:18:57.430 --> 00:19:00.760
你知道，这些隐藏状态的作用有点矛盾。00:19:00.760 --> 00:19:02.260
它们是用来预测的。00:19:02.260 --> 00:19:06.670
它们用于下一个隐藏状态和输出状态等等。00:19:06.670 --> 00:19:09.205
但从很多方面来说，00:19:09.205 --> 00:19:16.045
这些表示实际上是一个单词在上下文中的表示。00:19:16.045 --> 00:19:18.895
如果你想想发生了什么00:19:18.895 --> 00:19:21.310
问答系统，00:19:21.310 --> 00:19:23.620
这就是它们的用法，对吧?00:19:23.620 --> 00:19:26.200
我们来回运行LSTM，00:19:26.200 --> 00:19:29.320
通过文章中的一个问题，然后我们说，00:19:29.320 --> 00:19:33.085
这些都很好地表达了单词的意思和上下文。00:19:33.085 --> 00:19:36.205
让我们开始将它们与注意力功能等进行匹配。00:19:36.205 --> 00:19:41.470
所以，似乎我们已经发明了一种方法，00:19:41.470 --> 00:19:47.185
特定上下文的单词表示。00:19:47.185 --> 00:19:50.035
实际上，你知道，00:19:50.035 --> 00:19:55.450
这堂课剩下的内容基本上没有比这更复杂。00:19:55.450 --> 00:20:02.245
过了一段时间，但有些人醒来后开始注意到，00:20:02.245 --> 00:20:04.600
当你运行任何语言模型时，00:20:04.600 --> 00:20:08.365
生成特定于上下文的单词表示。00:20:08.365 --> 00:20:11.364
也许，如果我们只考虑那些特定的上下文00:20:11.364 --> 00:20:16.810
表示单词，它们在做其他事情时很有用。00:20:16.810 --> 00:20:18.595
这有点，你知道，00:20:18.595 --> 00:20:19.780
还有一些细节，00:20:19.780 --> 00:20:23.980
这就是整节课的总结。00:20:23.980 --> 00:20:34.165
首先要做的是Matt Peters在2017年写的一篇论文，00:20:34.165 --> 00:20:36.265
嗯，前年。00:20:36.265 --> 00:20:41.080
这是现代社会的前身，00:20:41.080 --> 00:20:46.720
这些上下文敏感词嵌入的版本。00:20:46.720 --> 00:20:49.840
所以，嗯，和合著者一起，00:20:49.840 --> 00:20:53.185
他写了一篇论文叫TagLM，00:20:53.185 --> 00:20:56.920
但它基本上已经有了所有的主要思想。00:20:56.920 --> 00:21:01.255
所以，我们想要的是可以的。00:21:01.255 --> 00:21:05.620
我们希望在诸如名称实体识别等任务上做得更好。00:21:05.620 --> 00:21:10.945
我们想做的是了解一个单词在语境中的意思。00:21:10.945 --> 00:21:14.500
但是你知道，如果我们做命名实体识别，00:21:14.500 --> 00:21:18.175
我们只是在50万字的监督数据上训练它。00:21:18.175 --> 00:21:20.230
这不是一个很大的来源00:21:20.230 --> 00:21:23.950
学习单词和上下文的含义。00:21:23.950 --> 00:21:28.810
所以，为什么我们不采用半监督的方法，这就是我们所做的。00:21:28.810 --> 00:21:32.740
我们从大量未标记的数据开始。00:21:32.740 --> 00:21:35.545
从这些未标记的数据中，00:21:35.545 --> 00:21:39.850
我们可以训练传统的word嵌入模型，如Word2vec。00:21:39.850 --> 00:21:43.810
但我们也可以同时训练一个神经语言模型。00:21:43.810 --> 00:21:47.590
比如bi-LSTM语言模型。00:21:47.590 --> 00:21:55.190
好。那么，对于第二步，当我们使用监督数据时，00:21:55.740 --> 00:21:58.900
实际上，我想这是第三步。00:21:58.900 --> 00:22:05.965
好。那么，当我们想要学习顶部有监督的词性标记时，00:22:05.965 --> 00:22:09.190
我们要做的是说，00:22:09.190 --> 00:22:13.420
对于输入单词New what York is located，00:22:13.420 --> 00:22:18.340
我们不仅可以使用与上下文无关的嵌入这个词，00:22:18.340 --> 00:22:24.505
但是我们可以使用我们训练过的递归语言模型并运行它，00:22:24.505 --> 00:22:31.180
然后我们会在bi-LSTM语言模型中生成隐藏状态我们也可以00:22:31.180 --> 00:22:38.380
把这些作为特征输入到我们的序列标签模型中，00:22:38.380 --> 00:22:41.335
这些功能会让它工作得更好。00:22:41.335 --> 00:22:47.095
这是第二张图片，它更详细地展示了这一点。00:22:47.095 --> 00:22:52.885
所以，所以，我们假设我们受过训练，00:22:52.885 --> 00:22:56.875
bi-LSTM语言模型，00:22:56.875 --> 00:22:59.755
在很多非监督数据上。00:22:59.755 --> 00:23:06.370
然后我们要做的是为New York is locate做命名实体识别。00:23:06.370 --> 00:23:09.160
所以，我们做的第一件事是说，00:23:09.160 --> 00:23:16.150
让我们运行纽约通过我们单独训练的神经语言模型。00:23:16.150 --> 00:23:18.925
因此，我们通过一个正向语言模型来运行它。00:23:18.925 --> 00:23:21.490
我们通过一个向后的语言模型来运行它。00:23:21.490 --> 00:23:23.830
我们从中得到，00:23:23.830 --> 00:23:26.515
一个隐藏状态表示，00:23:26.515 --> 00:23:28.750
每个字，00:23:28.750 --> 00:23:31.644
我们把前向和后向连接起来，00:23:31.644 --> 00:23:35.529
这就给出了一个集合，一个连接语言模型的嵌入00:23:35.529 --> 00:23:40.090
我们将在命名实体识别器中使用它作为特性。00:23:40.090 --> 00:23:43.870
然后对于命名实体识别器本身00:23:43.870 --> 00:23:48.700
我们有同样的句子，00:23:48.700 --> 00:23:55.390
因此，我们都可以为它查找一个word2vecstyle令牌嵌入。00:23:55.390 --> 00:24:01.315
我们可以使用我们所学到的字符级CNNs和RNNs，我们可以建立00:24:01.315 --> 00:24:04.450
一个字符级的表示，我们也00:24:04.450 --> 00:24:07.795
连接到两个表示。00:24:07.795 --> 00:24:15.685
因此，我们将这些表示提供给bi-LSTM层。00:24:15.685 --> 00:24:19.945
但是当我们得到这个bi-LSTM层的输出时，00:24:19.945 --> 00:24:22.180
以及这个正常的输出，00:24:22.180 --> 00:24:28.285
我们可以把每个输出连接起来，00:24:28.285 --> 00:24:30.730
神经语言模型。00:24:30.730 --> 00:24:33.370
所以，每一个都变成了一对状态。00:24:33.370 --> 00:24:36.490
一个是从第一个bi-LSTM层吐出来的00:24:36.490 --> 00:24:39.760
然后它与神经语言模型中的一些东西连接起来。00:24:39.760 --> 00:24:46.450
这样，连接起来的表示法就被输入到bi-LSTM的第二层。00:24:46.450 --> 00:24:48.265
然后从它的输出，00:24:48.265 --> 00:24:51.310
我们做的是通常的softmax分类00:24:51.310 --> 00:24:54.790
然后我们给标签，比如位置开始，00:24:54.790 --> 00:24:59.380
结束位置，假设纽约是一个位置，然后是00:24:59.380 --> 00:25:05.540
另一个标签表示它不是一个位置。这说得通吗?00:25:07.860 --> 00:25:14.305
对，所以，中心问题是00:25:14.305 --> 00:25:20.455
我们从Bi-LSTMs中得到的这些表示是有用的。00:25:20.455 --> 00:25:24.580
我们只是在训练它们的时候把它们放进监督模型中，00:25:24.580 --> 00:25:28.600
这个想法会给我们更好的词语特征。00:25:28.600 --> 00:25:32.305
某种意义和语境的表达，00:25:32.305 --> 00:25:39.610
这将使我们学习更好的命名实体识别器或它-无论它是什么。00:25:39.610 --> 00:25:42.580
也许我应该把这张幻灯片放早一点，00:25:42.580 --> 00:25:45.955
但这张幻灯片是想提醒你们什么是命名实体识别器。00:25:45.955 --> 00:25:47.305
我希望你们能记住，00:25:47.305 --> 00:25:50.605
我们要在哪里找到并标记00:25:50.605 --> 00:25:54.850
个人、位置、日期、组织等的实体。00:25:54.850 --> 00:25:57.625
总之，这样做是有效的。00:25:57.625 --> 00:25:59.905
这是一段历史。00:25:59.905 --> 00:26:07.285
最著名的命名实体识别数据集是这个CoNLL 2003数据集，00:26:07.285 --> 00:26:10.180
它实际上存在于多种语言中。00:26:10.180 --> 00:26:14.770
但每当人们说CoNLL 2003而不提一种语言时，00:26:14.770 --> 00:26:17.665
他们指的是英文版。00:26:17.665 --> 00:26:20.410
世界就是这样运转的。00:26:20.410 --> 00:26:24.430
在这个数据集中。00:26:24.430 --> 00:26:29.035
大概有15年了。00:26:29.035 --> 00:26:32.920
这本来是一场比赛，对吧?00:26:32.920 --> 00:26:36.235
这是2003年最初的烘焙。00:26:36.235 --> 00:26:38.455
我的小组就在那里。00:26:38.455 --> 00:26:42.055
参与其中。我想我们获得了第三或第四名，00:26:42.055 --> 00:26:46.720
我们的F1得分是86。00:26:46.720 --> 00:26:52.810
获奖者来自IBM研究实验室，00:26:52.810 --> 00:26:55.870
他们得到88分，接近89分。00:26:55.870 --> 00:27:00.490
但这两者的区别在于我们的系统是00:27:00.490 --> 00:27:05.290
一个干净的机器学习模型，00:27:05.290 --> 00:27:08.800
而IBM并不仅仅是一个整体00:27:08.800 --> 00:27:13.600
四种不同的机器学习模型，加上地名辞典。00:27:13.600 --> 00:27:16.090
它也适合于。的输出00:27:16.090 --> 00:27:22.450
另外两个旧的NER系统是IBM人员多年前针对不同的数据进行培训的。00:27:22.450 --> 00:27:25.030
我想这对他们很有效，00:27:25.030 --> 00:27:27.100
这是一个相当复杂的系统。00:27:27.100 --> 00:27:29.170
这是斯坦福的另一个系统。00:27:29.170 --> 00:27:33.910
这是我们的经典斯坦福NER系统被广泛使用。00:27:33.910 --> 00:27:39.475
这是使用条件随机场模型，它通常占主导地位00:27:39.475 --> 00:27:46.935
在2000年下半年和2010年上半年，00:27:46.935 --> 00:27:52.800
它有点，你知道，有点，但通常不会比2003年的系统好。00:27:52.800 --> 00:27:59.910
这个系统是有史以来最好的分类CRF系统。00:27:59.910 --> 00:28:06.105
但不是像这个系统那样仅仅使用训练数据来建立模型，00:28:06.105 --> 00:28:11.065
它加入了维基百科和其他一些东西来使它工作得更好，00:28:11.065 --> 00:28:14.725
这就得到了90。8 F1。00:28:14.725 --> 00:28:23.770
所以，本质上，一旦BiLSTM风格的模型开始在NLP中被了解和使用。00:28:23.770 --> 00:28:28.060
那时人们可以训练，建立训练00:28:28.060 --> 00:28:33.175
仅仅在训练数据系统上工作得更好。00:28:33.175 --> 00:28:38.440
因为本质上你是从同一个数据从这个系统到那个系统。00:28:38.440 --> 00:28:41.530
所以，你可以从中获得4%的收益，00:28:41.530 --> 00:28:45.835
因为它没有使用维基百科之类的东西;00:28:45.835 --> 00:28:51.805
这个Ma和Hovy系统非常有名，得到了91。21。00:28:51.805 --> 00:28:56.140
好的，但是如果我们进入这个TagLM系统，00:28:56.140 --> 00:29:00.610
Matt Peters和他的同事有一个系统00:29:00.610 --> 00:29:05.590
有点类似于Ma和Hovy系统，有点糟糕。00:29:05.590 --> 00:29:12.670
但是重点是这个BiLSTM使用了不好意思，使用了神经语言模型，00:29:12.670 --> 00:29:17.080
是一种很有用的魅力给予者，它会接受结果。00:29:17.080 --> 00:29:18.610
不是日日夜夜，00:29:18.610 --> 00:29:24.160
略高于1%然后给他们最好的NER系统。00:29:24.160 --> 00:29:25.990
这就证明了00:29:25.990 --> 00:29:33.380
语境词的表达确实有一定的力量，并且开始变得有用，00:29:33.660 --> 00:29:38.620
然后在顶部有一个空白因为我们稍后会详细讨论。00:29:38.620 --> 00:29:43.240
嗯，他们的语言模型有一些细节。00:29:43.240 --> 00:29:46.330
他们的一些细节是有用的00:29:46.330 --> 00:29:49.285
一个双向的语言模型，而不是单向的。00:29:49.285 --> 00:29:51.640
有一个大的嗯，00:29:51.640 --> 00:29:55.510
语言模型获得了很大的收益，00:29:55.510 --> 00:30:01.960
你需要在更多的数据上训练这个语言模型。00:30:01.960 --> 00:30:07.070
如果你只是在你的监督训练数据上训练它，它就不起作用了。00:30:08.160 --> 00:30:11.140
另一个模型是CoVe，00:30:11.140 --> 00:30:12.610
但我想跳过这个。00:30:12.610 --> 00:30:15.895
好。第二年，00:30:15.895 --> 00:30:18.865
马特・彼得斯和另一组同事00:30:18.865 --> 00:30:23.410
然后提出了一个改进的系统，叫做ELMo，00:30:23.410 --> 00:30:27.610
实际上，这是一个突破性的系统。00:30:27.610 --> 00:30:30.960
这是一种每个人都有的系统00:30:30.960 --> 00:30:35.880
注意到并说"哇，这些上下文相关的词向量很棒。00:30:35.880 --> 00:30:37.680
每个人都应该使用它们，00:30:37.680 --> 00:30:41.620
而不是传统的单词载体。”是吗?00:30:41.790 --> 00:30:51.490
我有一个简单的问题，想象重新训练一个系统，到底是什么00:30:59.330 --> 00:31:02.910
什么措施(听不清)00:31:02.910 --> 00:31:06.250
这是预先训练过的，因为这一块;00:31:06.250 --> 00:31:11.035
首先训练一个大的神经语言模型，00:31:11.035 --> 00:31:13.270
还有一件重要的事我忘了说。00:31:13.270 --> 00:31:15.280
谢谢你的问题。00:31:15.280 --> 00:31:20.020
主要原因是，在某种程度上，00:31:20.020 --> 00:31:21.670
这是第一次训练。00:31:21.670 --> 00:31:26.245
但人们认为这是训练前的主要原因00:31:26.245 --> 00:31:30.985
在你训练好这个之后，它就被冻住了。00:31:30.985 --> 00:31:35.680
这是你可以用参数来运行的东西00:31:35.680 --> 00:31:40.840
你有一个向量，它代表你的上下文单词的每个位置，00:31:40.840 --> 00:31:43.960
然后它会被用在这个系统中。00:31:43.960 --> 00:31:46.420
所以，当你训练这个系统时，00:31:46.420 --> 00:31:48.580
没有梯度流回00:31:48.580 --> 00:31:52.885
这个神经语言模型正在改变和更新它;只是固定的。00:31:52.885 --> 00:31:56.260
这就是人们谈论训练前的感觉。00:31:56.260 --> 00:31:59.185
这是一种你训练过的模型00:31:59.185 --> 00:32:02.680
在其他地方，你用来提供功能，00:32:02.680 --> 00:32:06.280
但不是你现在训练的模型的一部分。是吗?00:32:06.280 --> 00:32:12.060
(听不清)00:32:12.060 --> 00:32:16.650
嗯，我想，我不会把它叫做重建。00:32:16.650 --> 00:32:20.190
是的，从某种意义上说，它是无监督的，因为这是一个语言模型，00:32:20.190 --> 00:32:22.470
你训练它预测下一个单词。00:32:22.470 --> 00:32:28.335
这是单词1到k，在交叉熵损失中，k + 1是多少，00:32:28.335 --> 00:32:30.150
每个姿势都要重复。00:32:30.150 --> 00:32:37.530
[噪音]是的，所以我的意思是，00:32:37.530 --> 00:32:45.240
我的意思是，在详细讨论了TagLM之后，00:32:45.240 --> 00:32:52.350
在某种意义上，TagLM和ELMo之间的区别很小，00:32:52.350 --> 00:32:54.090
这取决于细节。00:32:54.090 --> 00:32:56.385
我的意思是，对于第一个近似，00:32:56.385 --> 00:32:58.890
他们又在做同样的事情，00:32:58.890 --> 00:33:00.675
但是好多了。00:33:00.675 --> 00:33:06.360
我希望上次讲得通00:33:06.360 --> 00:33:09.015
我的意思是，有什么不同之处?00:33:09.015 --> 00:33:13.710
他们的双向语言模型有点不同，00:33:13.710 --> 00:33:16.800
实际上，他们关心的问题之一是尝试提出00:33:16.800 --> 00:33:21.435
一个简洁的语言模型，便于人们使用，00:33:21.435 --> 00:33:27.390
即使他们没有世界上最强大的计算机硬件，他们也能完成其他任务。00:33:27.390 --> 00:33:29.940
所以他们决定放弃00:33:29.940 --> 00:33:34.185
完全用单词表示，00:33:34.185 --> 00:33:38.610
字符CNNs用于构建单词表示，00:33:38.610 --> 00:33:42.045
因为这减少了必须存储的参数的数量，00:33:42.045 --> 00:33:45.510
你要用到的大矩阵。00:33:45.510 --> 00:33:50.280
他们将隐藏维度扩展到4096，00:33:50.280 --> 00:33:52.020
然后他们把它投射到00:33:52.020 --> 00:33:57.450
512维，带有一种前馈投影层，00:33:57.450 --> 00:34:00.300
这是一种很常见的方法00:34:00.300 --> 00:34:03.360
模型的参数化，所以你有很多00:34:03.360 --> 00:34:06.060
参数在它们当前的方向，而不是你00:34:06.060 --> 00:34:09.315
需要更小的矩阵，00:34:09.315 --> 00:34:11.400
下一层的输入。00:34:11.400 --> 00:34:13.530
在图层之间，00:34:13.530 --> 00:34:18.300
它们现在使用剩余连接，并进行一些参数绑定。00:34:18.300 --> 00:34:21.615
这些都是细节。00:34:21.615 --> 00:34:25.200
但还有一件有趣的事00:34:25.200 --> 00:34:28.890
这是埃尔默的一个重要创新，00:34:28.890 --> 00:34:30.405
所以我们应该得到这个。00:34:30.405 --> 00:34:32.400
所以在TagLM,00:34:32.400 --> 00:34:36.930
从预先训练的LM输入什么00:34:36.930 --> 00:34:43.695
主模型只是神经语言模型栈的顶层，00:34:43.695 --> 00:34:47.040
这在当时是完全标准的社交礼节，00:34:47.040 --> 00:34:49.800
你可能有三层00:34:49.800 --> 00:34:53.790
你把顶层的神经语言模型作为你的分类00:34:53.790 --> 00:34:57.120
一个真正抓住了00:34:57.120 --> 00:35:01.185
句子和较低层次的处理导致了它。00:35:01.185 --> 00:35:05.295
他们的想法是00:35:05.295 --> 00:35:09.780
使用所有的层，00:35:09.780 --> 00:35:12.960
神经语言模型的biLSTM。00:35:12.960 --> 00:35:16.935
所以不仅仅是顶层，所有层都是有用的。00:35:16.935 --> 00:35:20.760
有一些复杂的方程，00:35:20.760 --> 00:35:24.480
但本质上，这里的重点是，00:35:24.480 --> 00:35:27.360
对于一个特定的位置，00:35:27.360 --> 00:35:29.505
语言模型中的word 7，00:35:29.505 --> 00:35:33.930
我们要取每一层的隐状态，00:35:33.930 --> 00:35:36.599
我们的神经语言模型栈，00:35:36.599 --> 00:35:40.545
我们要给这个水平一个权重，00:35:40.545 --> 00:35:42.540
我们把它们加起来，00:35:42.540 --> 00:35:47.190
这是每个位置隐藏层的加权平均值，00:35:47.190 --> 00:35:51.225
这就是我们的基本表示。00:35:51.225 --> 00:35:55.785
所以，他们发现这给了很多00:35:55.785 --> 00:36:00.480
对于不同的任务可能更喜欢不同的层。00:36:00.480 --> 00:36:03.045
这里还有一点，00:36:03.045 --> 00:36:08.625
他们学习特定任务的全局缩放因子Gamma。00:36:08.625 --> 00:36:13.665
这让他们能够控制一些任务，嗯，00:36:13.665 --> 00:36:16.080
上下文单词嵌入可能是真的00:36:16.080 --> 00:36:19.515
对于其他任务，它们可能不是很有用，00:36:19.515 --> 00:36:21.449
所以你只是在学习一个特定的，00:36:21.449 --> 00:36:25.095
对整个任务都有用。00:36:25.095 --> 00:36:30.285
好。这是一种新的语言模型。00:36:30.285 --> 00:36:33.390
但是这个，这个允许这个想法，00:36:33.390 --> 00:36:36.750
也许有更多的句法意义00:36:36.750 --> 00:36:39.855
一个单词和更多的语义意义，00:36:39.855 --> 00:36:43.380
可能它们可以用不同的层来表示00:36:43.380 --> 00:36:45.510
你的神经语言模型00:36:45.510 --> 00:36:48.330
不同的任务你可以有不同的权重。00:36:48.330 --> 00:36:51.330
这就是基本模型。00:36:51.330 --> 00:36:56.850
所以你运行你的biLSTM之前，以g et表示每个单词。00:36:56.850 --> 00:36:59.610
一般的埃尔莫食谱是，00:36:59.610 --> 00:37:03.215
有了这个固定的语言模型，00:37:03.215 --> 00:37:08.540
你想要根据任务的不同将它输入到某个监督模型中，00:37:08.540 --> 00:37:10.070
他们在论文中说，00:37:10.070 --> 00:37:12.500
你怎么做可能取决于任务。00:37:12.500 --> 00:37:15.965
你可能想把它连接到中间层，00:37:15.965 --> 00:37:17.660
正如TagLM所做的，00:37:17.660 --> 00:37:19.085
这可能没问题。00:37:19.085 --> 00:37:22.220
但是你知道它也可能有用00:37:22.220 --> 00:37:25.700
当产生输出时，这些ELMo表示，00:37:25.700 --> 00:37:28.910
如果你在做a00:37:28.910 --> 00:37:35.210
生成系统或者你也可以输入ELMo表示，00:37:35.210 --> 00:37:38.630
在你用softmax计算输出之前，00:37:38.630 --> 00:37:41.580
他们让它灵活地使用，00:37:41.580 --> 00:37:42.960
但总的来说，00:37:42.960 --> 00:37:45.960
就像我们之前看到的那样。00:37:45.960 --> 00:37:49.590
实际上我在重复使用你们计算过的图00:37:49.590 --> 00:37:54.105
每个位置的ELMo表示为加权平均值，00:37:54.105 --> 00:37:57.360
然后你把它连接到00:37:57.360 --> 00:38:01.125
你的监督系统，生成你的输出。00:38:01.125 --> 00:38:04.890
不管怎样，不管怎样，00:38:04.890 --> 00:38:07.920
他们做到了00:38:07.920 --> 00:38:11.925
这是一个小小的进步，给了他们一个额外的00:38:11.925 --> 00:38:16.770
命名实体识别的0.3%。00:38:16.770 --> 00:38:21.165
嗯，听起来不太像。00:38:21.165 --> 00:38:26.055
你可能会从这里得出为什么兴奋[笑声]，00:38:26.055 --> 00:38:28.695
在某种意义上，00:38:28.695 --> 00:38:33.720
这是对的，因为在某种程度上，这里有一个有趣的想法00:38:33.720 --> 00:38:39.060
这就为TagLM论文提供了更好的收益。00:38:39.060 --> 00:38:45.254
但是，大家都很兴奋的原因是在ELMo的论文中，00:38:45.254 --> 00:38:48.030
然后他们证明这不是你能做到的00:38:48.030 --> 00:38:50.910
一次性改进命名实体识别器，00:38:50.910 --> 00:38:58.035
你可以把这些ELMo表示用于任何NLP任务，00:38:58.035 --> 00:39:01.695
它们非常有用，能带来很好的收益。00:39:01.695 --> 00:39:08.340
所以，人们兴奋的主要原因是这个表格中的数据。00:39:08.340 --> 00:39:11.250
这里我们要做的是一系列非常不同的任务，00:39:11.250 --> 00:39:13.620
这是小组问答，00:39:13.620 --> 00:39:16.380
有自然语言推理，00:39:16.380 --> 00:39:18.345
有语义角色标签，00:39:18.345 --> 00:39:23.760
有共同参照，命名实体识别，做情感分析，00:39:23.760 --> 00:39:26.730
所以很多不同的NLP任务，00:39:26.730 --> 00:39:30.315
他们有一个先进的系统。00:39:30.315 --> 00:39:34.860
他们制作了自己的基线，00:39:34.860 --> 00:39:40.080
你知道，通常有点类似于以前的技术水平，00:39:40.080 --> 00:39:43.620
但通常比00:39:43.620 --> 00:39:45.360
因为它是00:39:45.360 --> 00:39:48.315
无论他们想出什么更简单更干净的系统，00:39:48.315 --> 00:39:51.345
然后他们会说，00:39:51.345 --> 00:39:55.260
哦，把这个系统加起来00:39:55.260 --> 00:39:59.985
ELMo向量到中间隐藏的表示，00:39:59.985 --> 00:40:02.040
让这些帮助你预测。00:40:02.040 --> 00:40:04.710
总的来说，在所有情况下，00:40:04.710 --> 00:40:08.970
这给了你3%左右的绝对收益00:40:08.970 --> 00:40:13.470
然后产生了巨大的性能提升，00:40:13.470 --> 00:40:18.450
在所有情况下，性能都远高于之前，00:40:18.450 --> 00:40:20.040
最先进的系统。00:40:20.040 --> 00:40:24.000
所以你知道，这让它看起来像魔法精灵尘，00:40:24.000 --> 00:40:28.050
因为，你知道，在NLP会议的土地上，00:40:28.050 --> 00:40:30.960
很多人过去都是这样尝试的00:40:30.960 --> 00:40:34.500
明年的论文就会好一个百分点00:40:34.500 --> 00:40:37.080
完成一项任务，然后写下来00:40:37.080 --> 00:40:41.715
他们今年的重大突破是发表了他们的新论文。00:40:41.715 --> 00:40:44.355
它的意思是00:40:44.355 --> 00:40:48.045
这种创建上下文敏感的方法，00:40:48.045 --> 00:40:51.660
你可以在任何任务中使用它们，00:40:51.660 --> 00:40:55.245
他们会给你3%的利息，然后带你通过最先进的技术，00:40:55.245 --> 00:40:58.395
这看起来真的很棒。00:40:58.395 --> 00:41:01.800
所以人们对这个很兴奋，那个赢了00:41:01.800 --> 00:41:06.390
2018年NAACL大会最佳论文奖。00:41:06.390 --> 00:41:10.590
然后，就像我模糊提到的，00:41:10.590 --> 00:41:14.370
所以他们实际使用的模型并不是一个深度堆栈，00:41:14.370 --> 00:41:17.520
实际上只有两层biLSTMs，00:41:17.520 --> 00:41:22.620
但他们确实展示了一个有趣的结果，即较低的层次能更好地捕捉00:41:22.620 --> 00:41:26.790
低级语法字属性00:41:26.790 --> 00:41:30.389
它最有用的是词性标注，句法00:41:30.389 --> 00:41:33.210
依赖项，NER，在顶层00:41:33.210 --> 00:41:35.310
他们的语言模型更适合00:41:35.310 --> 00:41:38.940
更高层次的语义对情感之类的东西更有用，00:41:38.940 --> 00:41:42.495
语义角色标记和问题回答。00:41:42.495 --> 00:41:45.150
这看起来很有趣，00:41:45.150 --> 00:41:47.940
不过看看结果如何会很有趣00:41:47.940 --> 00:41:51.820
如果你有更多的图层可以使用，就会有更多。00:41:52.100 --> 00:41:55.875
好。艾尔摩,完成。00:41:55.875 --> 00:41:58.590
嗯，我继续。00:41:58.590 --> 00:42:05.550
还有一件事我想我应该提一下，00:42:05.550 --> 00:42:09.270
与此同时，另一件作品，00:42:09.270 --> 00:42:12.450
几个月后，也许是，也许不是，00:42:12.450 --> 00:42:14.430
差不多是在同一时间出版的00:42:14.430 --> 00:42:18.420
在2018年，这项工作开始了00:42:18.420 --> 00:42:23.025
用于文本分类的通用语言模型微调，00:42:23.025 --> 00:42:25.995
霍华德和鲁德的《乌尔姆菲特》00:42:25.995 --> 00:42:31.335
本质上，这和，嗯，00:42:31.335 --> 00:42:39.370
我们想做的是转移学习在那里我们可以学习一个大的语言模型。00:42:40.560 --> 00:42:43.075
一个大的语言模型，00:42:43.075 --> 00:42:48.220
然后是我们的目标任务它可能被命名为实体识别。00:42:48.220 --> 00:42:50.200
这是文本分类，00:42:50.200 --> 00:42:55.690
我们可以传递这种语言模型信息，帮助我们更好地完成任务。00:42:55.690 --> 00:42:58.690
因此，他们提出了一个架构来实现这一点。00:42:58.690 --> 00:43:00.640
所以，他们的建筑是，00:43:00.640 --> 00:43:07.960
你有一个很大的无监督语料库，你可以从中训练一个神经语言模型。00:43:07.960 --> 00:43:12.775
他们使用了更深层次的神经语言模型，有三个隐藏层。00:43:12.775 --> 00:43:14.920
嗯，那你调得真好00:43:14.920 --> 00:43:19.660
你感兴趣的领域的神经语言模型。00:43:19.660 --> 00:43:22.255
所以，这是他们做的一个额外的阶段。00:43:22.255 --> 00:43:24.730
最后，00:43:24.730 --> 00:43:28.960
现在介绍分类目标。00:43:28.960 --> 00:43:31.930
他们要做的是创建文本分类器。00:43:31.930 --> 00:43:33.535
所以，我们现在想，00:43:33.535 --> 00:43:39.280
将这个模型从语言模型转换为文本分类器。00:43:39.280 --> 00:43:42.340
但有一点他们做得不一样00:43:42.340 --> 00:43:43.720
从某种意义上说，00:43:43.720 --> 00:43:46.840
预示着变形金刚的后期工作。00:43:46.840 --> 00:43:52.210
所以，不仅仅是把这些功能输入到一个完全不同的网络中，00:43:52.210 --> 00:43:58.710
他们继续使用相同的网络，但他们在顶层引入了不同的目标。00:43:58.710 --> 00:44:01.710
所以，你可以用这个网络做的一件事就是使用00:44:01.710 --> 00:44:05.015
它将预测下一个单词作为一种语言模型。00:44:05.015 --> 00:44:06.460
所以在这一点上，00:44:06.460 --> 00:44:09.820
他们冻结了顶部softmax的参数，00:44:09.820 --> 00:44:11.455
这就是为什么它是黑色的。00:44:11.455 --> 00:44:14.935
但相反，他们可以坚持下去00:44:14.935 --> 00:44:19.825
一个不同的预测单元它预测特定任务的内容。00:44:19.825 --> 00:44:21.610
所以，这可能是预测00:44:21.610 --> 00:44:26.680
文本分类任务中的积极或消极情绪。00:44:26.680 --> 00:44:27.760
在他们的模型中，00:44:27.760 --> 00:44:31.915
他们在某种程度上重复使用同一个网络，00:44:31.915 --> 00:44:36.205
另一层，做新的分类任务。00:44:36.205 --> 00:44:39.700
他们也对小玩意感兴趣，00:44:39.700 --> 00:44:43.615
一种GPU的研究模型，00:44:43.615 --> 00:44:47.620
这篇论文有很多细节和技巧00:44:47.620 --> 00:44:52.150
并照顾和喂养你的神经模型，以最大限度地提高性能。00:44:52.150 --> 00:44:56.245
如果你对此感兴趣，你可以查一些相关的细节。00:44:56.245 --> 00:45:00.250
但是他们再次展示的是，00:45:00.250 --> 00:45:03.820
是利用这种语言模型进行预培训的吗00:45:03.820 --> 00:45:07.495
一个非常有效的提高绩效的方法，00:45:07.495 --> 00:45:09.865
这次是文本分类。00:45:09.865 --> 00:45:12.520
这些是文本分类数据集，00:45:12.520 --> 00:45:14.260
IMDb代表情感，00:45:14.260 --> 00:45:18.970
TREC用于主题文本分类，00:45:18.970 --> 00:45:22.780
之前有一些系统是其他人开发的00:45:22.780 --> 00:45:26.620
通过使用这种语言模型，00:45:26.620 --> 00:45:31.390
他们能够显著提高这些错误率的水平，00:45:31.390 --> 00:45:33.590
所以这个低点很好。00:45:33.900 --> 00:45:39.715
他们还展示了另一个有趣的结果，00:45:39.715 --> 00:45:44.395
你希望通过这种转移学习，00:45:44.395 --> 00:45:46.330
他们能够证明的是，00:45:46.330 --> 00:45:51.205
如果你能在大量数据上训练这个神经语言模型，00:45:51.205 --> 00:45:54.430
这意味着你将能够做得很好00:45:54.430 --> 00:45:59.110
你的监督任务，即使是在很小的数据上训练。00:45:59.110 --> 00:46:01.780
这是错误率，00:46:01.780 --> 00:46:03.355
如此之低是好事。00:46:03.355 --> 00:46:05.170
这是00:46:05.170 --> 00:46:08.815
在对数尺度上的训练例子。00:46:08.815 --> 00:46:11.710
所以蓝线是如果你只是在训练00:46:11.710 --> 00:46:15.730
一种基于监督数据的文本分类器。00:46:15.730 --> 00:46:19.765
所以，你需要大量的数据来开始做得很好。00:46:19.765 --> 00:46:24.715
但是如果你利用这种转移学习，00:46:24.715 --> 00:46:27.894
从一个预先训练好的语言模型来看，00:46:27.894 --> 00:46:30.310
你做得很漂亮00:46:30.310 --> 00:46:33.700
用更少的训练例子。00:46:33.700 --> 00:46:35.889
本质上，一个数量级，00:46:35.889 --> 00:46:39.655
较少的训练例子会给你同样的表现。00:46:39.655 --> 00:46:44.020
这两条线之间的差对应于额外的，00:46:44.020 --> 00:46:48.670
他们中间的阶段，也就是，00:46:48.670 --> 00:46:53.920
无论你是在你的目标域上做这种额外的微调，00:46:53.920 --> 00:46:58.690
这是你过程的一部分，他们发现这很有帮助。00:46:58.690 --> 00:47:05.215
好。这是另一个前兆。00:47:05.215 --> 00:47:11.545
所以，从那以后发生的一件事，00:47:11.545 --> 00:47:15.820
实际上人们说这是个好主意00:47:15.820 --> 00:47:21.910
如果我们把东西做大一点，也许会变成一个非常非常好的主意。00:47:21.910 --> 00:47:24.250
所以ULMfit00:47:24.250 --> 00:47:28.045
你可以在GPU的一天内训练，00:47:28.045 --> 00:47:31.870
听起来对CS224N期末项目很有吸引力，00:47:31.870 --> 00:47:34.930
记住这一点，但是，00:47:34.930 --> 00:47:39.115
然后OpenAI的人决定，00:47:39.115 --> 00:47:43.300
我们可以建立一个训练前的语言模型并训练它00:47:43.300 --> 00:47:47.590
在更大的计算量上有更多的数据，00:47:47.590 --> 00:47:54.130
使用242天GPU就会好很多，它确实做到了。00:47:54.130 --> 00:47:57.190
然后谷歌的人说，00:47:57.190 --> 00:48:00.445
我们可以训练一个模型，00:48:00.445 --> 00:48:04.660
在256天内，00:48:04.660 --> 00:48:07.645
这意味着计算量可能是原来的两倍。00:48:07.645 --> 00:48:09.565
很难精确地算出来，00:48:09.565 --> 00:48:12.175
这可能会做一些令人兴奋的事情，00:48:12.175 --> 00:48:14.950
这就是伯特模型，它做到了。00:48:14.950 --> 00:48:18.370
然后如果你顺着这些东西，00:48:18.370 --> 00:48:20.110
就在上周，00:48:20.110 --> 00:48:22.270
OpenAI的人说，00:48:22.270 --> 00:48:26.845
我们可以再做大一点我们可以训练一个模型，00:48:26.845 --> 00:48:32.830
大约2000 TPU版三天。00:48:32.830 --> 00:48:36.340
它将能够，00:48:36.340 --> 00:48:39.294
再做大一点，00:48:39.294 --> 00:48:41.080
又好多了，00:48:41.080 --> 00:48:44.410
这是GP2，00:48:44.410 --> 00:48:47.800
GPT-2语言模型，00:48:47.800 --> 00:48:50.680
这是OpenAI上周发布的。00:48:50.680 --> 00:48:56.740
嗯，它们实际上是非常令人印象深刻的结果，嗯，00:48:56.740 --> 00:49:00.730
当他们展示如果你在建造一个，00:49:00.730 --> 00:49:05.155
庞大的语言模型，包含大量的数据。00:49:05.155 --> 00:49:09.745
然后你说，语言模型开始生成一些文本，00:49:09.745 --> 00:49:11.800
在这个特殊的话题上，00:49:11.800 --> 00:49:15.100
它实际上可以很好地生成文本。00:49:15.100 --> 00:49:17.125
所以，这种做法，00:49:17.125 --> 00:49:19.930
是一个人文主义者写了几句话;00:49:19.930 --> 00:49:21.190
令人震惊的发现是，00:49:21.190 --> 00:49:23.515
科学家们发现了一群独角兽，00:49:23.515 --> 00:49:27.700
生活在安第斯山脉的偏远山谷，以前从未有人探索过。00:49:27.700 --> 00:49:29.905
所以，我们，00:49:29.905 --> 00:49:33.700
利用我们的神经语言模型，00:49:33.700 --> 00:49:35.680
这就给了我们背景，00:49:35.680 --> 00:49:37.765
然后生成更多的文本，00:49:37.765 --> 00:49:39.760
它开始产生科学家00:49:39.760 --> 00:49:42.160
以它们独特的角命名，00:49:42.160 --> 00:49:44.320
奥维德的独角兽，这些四角兽，00:49:44.320 --> 00:49:47.815
银白色的四粒玉米以前是科学界所不知道的。00:49:47.815 --> 00:49:50.080
它产生了惊人的，00:49:50.080 --> 00:49:52.735
好的文本，或者至少在，00:49:52.735 --> 00:49:57.220
在科技新闻中精心挑选的例子[笑声]中，00:49:57.220 --> 00:49:59.920
它能产生非常好的文本。00:49:59.920 --> 00:50:04.960
所以我觉得大家应该谨慎一点00:50:04.960 --> 00:50:07.930
以及它的一些随机输出00:50:07.930 --> 00:50:10.900
虽然没有那么好，但是你知道，00:50:10.900 --> 00:50:12.895
我认为这是戏剧性的00:50:12.895 --> 00:50:16.540
一旦你开始训练，你的语言模型会变得多好00:50:16.540 --> 00:50:23.210
就像我们可以用现代模型在大量数据上做的那样。00:50:23.280 --> 00:50:27.430
然后，OpenAI的人决定00:50:27.430 --> 00:50:31.720
这个语言模型太棒了以至于他们不打算把它发布出去，00:50:31.720 --> 00:50:34.480
然后变成了头条新闻00:50:34.480 --> 00:50:39.265
埃隆・马斯克的OpenAI构建了如此强大的人工智能，00:50:39.265 --> 00:50:41.980
为了人类的利益，必须把它锁起来。00:50:41.980 --> 00:50:46.660
[笑声]嗯，有合适的图片总是在00:50:46.660 --> 00:50:52.075
屏幕底部的这些瞬间00:50:52.075 --> 00:50:57.520
嗯，我想这就是埃隆・马斯克想要澄清和说的00:50:57.520 --> 00:51:03.020
他不再是OpenAI的导演了。00:51:03.020 --> 00:51:06.355
不管怎样，我们继续。00:51:06.355 --> 00:51:09.760
嗯，故事的一部分是这样的00:51:09.760 --> 00:51:14.635
只是这些东西变得越来越大，00:51:14.635 --> 00:51:18.760
但是故事的另一部分是这三个00:51:18.760 --> 00:51:23.785
这些系统使用transformer体系结构。00:51:23.785 --> 00:51:27.700
变压器的结构不仅非常强大，00:51:27.700 --> 00:51:32.575
但从技术上讲，它可以扩展到更大的尺寸。00:51:32.575 --> 00:51:35.575
为了理解剩下的部分，00:51:35.575 --> 00:51:39.055
我们应该多了解变形金刚。00:51:39.055 --> 00:51:42.610
所以，我打算这么做，00:51:42.610 --> 00:51:46.495
但我的意思是，在不同的顺序下，00:51:46.495 --> 00:51:50.200
我们周四请来的演讲者是00:51:50.200 --> 00:51:52.420
变压器论文的作者之一，00:51:52.420 --> 00:51:54.490
他要谈谈变形金刚。00:51:54.490 --> 00:51:57.430
所以我想我要做的是00:51:57.430 --> 00:52:01.000
快速介绍一下变形金刚，00:52:01.000 --> 00:52:04.090
但并不是说所有的细节00:52:04.090 --> 00:52:06.265
但希望这只是一个介绍，00:52:06.265 --> 00:52:10.360
你可以在周四了解更多细节00:52:10.360 --> 00:52:15.190
然后在结束之前，再谈谈伯特模型。00:52:15.190 --> 00:52:19.450
变形金刚的动机是00:52:19.450 --> 00:52:23.245
我们希望事情进展得更快，这样我们就能建立更大的模型，00:52:23.245 --> 00:52:26.125
正如我们提到的，00:52:26.125 --> 00:52:31.060
LSTM或者一般来说任何一个递归模型都是递归的。00:52:31.060 --> 00:52:36.190
你需要生成一到n个状态的时间，00:52:36.190 --> 00:52:41.275
这意味着你不能做同样的并行计算，00:52:41.275 --> 00:52:46.970
gpu喜欢卷积神经网络。00:52:46.970 --> 00:52:48.855
但是，另一方面，00:52:48.855 --> 00:52:51.210
我们发现即使，00:52:51.210 --> 00:52:56.005
这些门控复发单位，比如LSTMs和GRUs都很棒，00:52:56.005 --> 00:53:00.070
为了从这些重复的模型中得到更好的性能，00:53:00.070 --> 00:53:05.680
我们发现我们想-我们有一个问题在这些长序列长度内，00:53:05.680 --> 00:53:09.010
我们可以通过增加注意力机制来改善。00:53:09.010 --> 00:53:12.070
这就引出了，00:53:12.070 --> 00:53:14.425
既然注意力如此有效，00:53:14.425 --> 00:53:17.440
也许我们可以用注意力，00:53:17.440 --> 00:53:22.195
我们实际上可以完全消除模型中重复出现的部分[噪声]。00:53:22.195 --> 00:53:27.625
这就引出了变压器结构的概念，00:53:27.625 --> 00:53:32.545
关于这个的原始论文实际上叫做《注意力是你所需要的》00:53:32.545 --> 00:53:36.700
这反映了我们要保持注意力的部分，00:53:36.700 --> 00:53:40.000
我们要去掉，嗯，00:53:40.000 --> 00:53:43.960
递归部分，我们就能建立一个很好的模型。00:53:43.960 --> 00:53:45.310
所以在最初的工作中，00:53:45.310 --> 00:53:48.790
他们做的是机器翻译00:53:48.790 --> 00:53:52.720
注意神经机器翻译，00:53:52.720 --> 00:53:56.185
但他们想做的是建造00:53:56.185 --> 00:54:03.625
一个复杂的编码器和一个非递归工作的复杂解码器，00:54:03.625 --> 00:54:07.659
尽管如此，还是能够翻译句子00:54:07.659 --> 00:54:13.075
通过使用大量的注意力分配。00:54:13.075 --> 00:54:18.070
所以，我想更快地说一下，00:54:18.070 --> 00:54:20.965
希望周四我们能学到更多。00:54:20.965 --> 00:54:24.685
首先作为一种推荐资源00:54:24.685 --> 00:54:26.545
如果你想看，00:54:26.545 --> 00:54:29.695
回家了解更多关于00:54:29.695 --> 00:54:34.000
变压器的结构非常棒，00:54:34.000 --> 00:54:39.100
Sasha Rush做了一点工作，叫做带注释的Transformer00:54:39.100 --> 00:54:45.025
整篇变压器论文连同PyTorch代码一起放在木星笔记本上，00:54:45.025 --> 00:54:48.220
这实际上是非常有用的，00:54:48.220 --> 00:54:54.235
现在我要讲一些基本的操作方法。00:54:54.235 --> 00:54:57.460
基本思路是，00:54:57.460 --> 00:55:03.385
他们会用注意力来计算。00:55:03.385 --> 00:55:07.540
我们之前讲过不同种类的00:55:07.540 --> 00:55:12.520
注意是线性注意的乘法，00:55:12.520 --> 00:55:15.490
前馈网络加法注意。00:55:15.490 --> 00:55:18.670
他们追求的是最简单的注意力，00:55:18.670 --> 00:55:23.035
这里的注意力只是两件事之间的点积。00:55:23.035 --> 00:55:26.860
但是出于不同的目的，00:55:26.860 --> 00:55:32.830
他们在两个东西之间做更复杂的点积，00:55:32.830 --> 00:55:36.280
当他们在查的东西是00:55:36.280 --> 00:55:40.375
假设为键值对、键和值，00:55:40.375 --> 00:55:46.765
所以你在计算一个查询和键之间的点积相似性，00:55:46.765 --> 00:55:48.415
基于此，00:55:48.415 --> 00:55:52.060
你要用这个向量来求对应的值。00:55:52.060 --> 00:55:55.795
我们计算的方程在这里00:55:55.795 --> 00:56:00.130
使用softmax over查询，00:56:00.130 --> 00:56:03.610
关键的相似性，并利用它来给予00:56:03.610 --> 00:56:08.680
权重是相对于相应值的基于注意的权重。00:56:08.680 --> 00:56:12.220
这就是基本的注意力模型。00:56:12.220 --> 00:56:15.985
这样说的话，00:56:15.985 --> 00:56:18.100
增加了一点复杂性，00:56:18.100 --> 00:56:21.145
但对于他们的编码器来说，这是最简单的部分。00:56:21.145 --> 00:56:26.065
实际上，所有的查询键和值都是完全相同的。00:56:26.065 --> 00:56:28.225
就是这些词，嗯，00:56:28.225 --> 00:56:32.620
他们把这些作为他们的原始语言。00:56:32.620 --> 00:56:38.000
所以，它增加了一些并不存在的复杂性。00:56:38.340 --> 00:56:42.280
嗯,好吧。我就跳过这个了。00:56:42.280 --> 00:56:48.175
嗯，他们还做了一些其他的事情。00:56:48.175 --> 00:56:52.165
他们注意到的一件事是，00:56:52.165 --> 00:56:57.745
你得到的值，QTK，00:56:57.745 --> 00:57:03.280
在方差中，维度变得很大00:57:03.280 --> 00:57:08.230
所以它们通过隐藏状态维的大小进行标准化，00:57:08.230 --> 00:57:12.280
但我也会把它留到细节部分。00:57:12.280 --> 00:57:13.945
在编码器中，00:57:13.945 --> 00:57:17.020
所有的都是向量，00:57:17.020 --> 00:57:20.380
有查询、键和值。00:57:20.380 --> 00:57:23.785
我们要在整个系统中使用注意力。00:57:23.785 --> 00:57:29.860
哦。好。第二个新想法是，00:57:29.860 --> 00:57:36.115
注意力很好，但如果你只有一个注意力分布，那就不好了，00:57:36.115 --> 00:57:39.190
因为你只会用一种方式处理事情。00:57:39.190 --> 00:57:42.415
也许对于不同的用户来说，它会很棒00:57:42.415 --> 00:57:45.760
如果你能从一个职位参加到不同的事情。00:57:45.760 --> 00:57:51.190
所以，如果你在考虑语法和我们用依赖解析器做了什么。00:57:51.190 --> 00:57:54.970
如果你是一个单词，你可能要注意你的头词，00:57:54.970 --> 00:57:59.155
但你也要注意，注意你的从属词。00:57:59.155 --> 00:58:01.689
如果你是代词，00:58:01.689 --> 00:58:06.010
你可能需要注意代词指的是什么。00:58:06.010 --> 00:58:07.855
你可能想要吸引更多的注意力。00:58:07.855 --> 00:58:12.010
所以他们引入了多头注意力的概念。00:58:12.010 --> 00:58:16.360
所以你用多头注意力做的是，00:58:16.360 --> 00:58:18.130
你的隐藏状态，00:58:18.130 --> 00:58:20.170
在你的系统中，00:58:20.170 --> 00:58:23.800
你可以通过投影层来映射它们，00:58:23.800 --> 00:58:27.670
哪些是不同W矩阵的乘法00:58:27.670 --> 00:58:32.350
线性投影到不同的低维空间，00:58:32.350 --> 00:58:37.030
然后你用它们中的每一个来计算点积注意力，00:58:37.030 --> 00:58:40.270
所以你可以同时处理不同的事情。00:58:40.270 --> 00:58:42.670
这种多头注意力是其中之一00:58:42.670 --> 00:58:48.655
变形金刚非常成功的想法使他们成为一个更强大的架构。00:58:48.655 --> 00:58:54.715
好。那么，对于我们的整个变压器组，00:58:54.715 --> 00:59:00.505
它开始建造复杂的建筑就像我们开始看到的，00:59:00.505 --> 00:59:02.200
嗯，前一周。00:59:02.200 --> 00:59:05.320
嗯,所以,好的。00:59:05.320 --> 00:59:06.969
是的。所以,开始,00:59:06.969 --> 00:59:10.060
从向量这个词，00:59:10.060 --> 00:59:16.915
我们要关注多种不同的事物，00:59:16.915 --> 00:59:19.900
我们同时会有00:59:19.900 --> 00:59:23.530
使周围短路的残余连接。00:59:23.530 --> 00:59:28.045
然后我们要把这两个加起来，00:59:28.045 --> 00:59:33.115
然后在这一点上进行标准化。00:59:33.115 --> 00:59:36.400
我之前讲过批量标准化，00:59:36.400 --> 00:59:38.020
他们不做批量标准化，00:59:38.020 --> 00:59:41.200
他们做的另一种变体是层标准化，00:59:41.200 --> 00:59:43.855
这是一种不同的归一化方法，00:59:43.855 --> 00:59:45.625
但我先跳过这个。00:59:45.625 --> 00:59:49.000
然后对于一个变压器块，00:59:49.000 --> 00:59:52.045
然后你去关注多头注意力，00:59:52.045 --> 00:59:56.755
你把东西放到一个前馈层它也有一个剩余的连接，00:59:56.755 --> 00:59:58.810
把它们的输出相加，00:59:58.810 --> 01:00:03.790
然后再做另一个层标准化。01:00:03.790 --> 01:00:08.965
这是他们要用到的基本的变压器模块。01:00:08.965 --> 01:00:11.320
为了做出完整的结构，01:00:11.320 --> 01:00:13.210
然后它们开始堆积01:00:13.210 --> 01:00:17.050
这些变压器块产生一个非常深的网络。01:00:17.050 --> 01:00:18.160
在某种意义上，01:00:18.160 --> 01:00:22.780
人们发现变压器的性能非常好。01:00:22.780 --> 01:00:25.000
但是，你知道，没有免费的午餐，01:00:25.000 --> 01:00:26.440
你不能。01:00:26.440 --> 01:00:28.150
你现在，再也不能01:00:28.150 --> 01:00:31.450
重复的信息实际上是按顺序传送的。01:00:31.450 --> 01:00:36.280
你在某个位置有一个词可以吸引注意力，01:00:36.280 --> 01:00:38.035
换句话说。01:00:38.035 --> 01:00:41.560
所以如果你想让信息在链中传递，01:00:41.560 --> 01:00:44.980
你得先迈出链条的第一步01:00:44.980 --> 01:00:46.690
然后你需要另一层01:00:46.690 --> 01:00:49.690
垂直方向可以走下一个环节，01:00:49.690 --> 01:00:53.800
然后你需要有另一层垂直于链的下一步。01:00:53.800 --> 01:00:57.520
这样就可以消去序列上的递归式，01:00:57.520 --> 01:01:03.220
但是你要用一些深度来代替让物体沿着多个跃点移动。01:01:03.220 --> 01:01:07.885
但无论如何，这在GPU架构中是非常有利的01:01:07.885 --> 01:01:13.300
因为它允许你使用并行化来计算每个点的所有值，01:01:13.300 --> 01:01:16.400
同时还有深度。嗯。01:01:19.290 --> 01:01:22.900
也许我也会解释一下。01:01:22.900 --> 01:01:25.420
他们使用字节对编码。01:01:25.420 --> 01:01:27.490
但如果你什么都不做01:01:27.490 --> 01:01:30.850
你只需要在这个向量中输入一些单词01:01:30.850 --> 01:01:34.765
不知道你是在句子的开头还是结尾。01:01:34.765 --> 01:01:38.680
虽然，他们有一个消息的方法做位置编码，这给01:01:38.680 --> 01:01:42.865
你有一些想法来支持你的词在句子中的位置。01:01:42.865 --> 01:01:47.950
好。这是一种编码系统。01:01:47.950 --> 01:01:49.540
所以从这句话中，01:01:49.540 --> 01:01:51.550
它们有一个初始的单词嵌入，01:01:51.550 --> 01:01:54.085
加上它们的位置编码，01:01:54.085 --> 01:01:58.105
你进入其中一个变压器组，01:01:58.105 --> 01:02:01.030
然后重复n次。01:02:01.030 --> 01:02:03.835
你会有一堆这样的变压器。01:02:03.835 --> 01:02:06.775
所以你做了很多次，01:02:06.775 --> 01:02:11.590
多注意句子的其他部分，计算值，01:02:11.590 --> 01:02:12.940
输入一个值，01:02:12.940 --> 01:02:14.860
通过一个完全连接的层，01:02:14.860 --> 01:02:19.735
然后你只是重复，注意句子中的不同地方。01:02:19.735 --> 01:02:21.310
获取你所有的信息，01:02:21.310 --> 01:02:23.275
通过一个完全连接的图层，01:02:23.275 --> 01:02:26.755
然后上升，嗯，继续上升。01:02:26.755 --> 01:02:31.000
这听起来有点神秘，01:02:31.000 --> 01:02:34.215
但事实证明，这种方法非常有效。01:02:34.215 --> 01:02:36.600
我们可以这样想，01:02:36.600 --> 01:02:39.900
我认为在每个阶段，01:02:39.900 --> 01:02:44.760
你可以用你的多头注意力和句子中的其他地方，01:02:44.760 --> 01:02:48.210
积累信息，把它推到下一层。01:02:48.210 --> 01:02:51.255
如果你这样做五六次，01:02:51.255 --> 01:02:55.530
你可以开始逐步推进信息01:02:55.530 --> 01:03:01.455
在任意方向计算有意义的值的序列。01:03:01.455 --> 01:03:08.605
有趣的是，这些模型是有效的01:03:08.605 --> 01:03:15.970
非常擅长学习语言结构中有趣的东西。01:03:15.970 --> 01:03:19.810
这些只是一些暗示图，01:03:19.810 --> 01:03:24.190
这是变压器堆栈的第5层01:03:24.190 --> 01:03:28.945
观察不同的注意头在注意哪些单词。01:03:28.945 --> 01:03:33.010
这些不同的颜色对应着不同的注意力头。01:03:33.010 --> 01:03:35.050
所以这句话是，01:03:35.050 --> 01:03:39.010
“本着这种精神，01:03:39.010 --> 01:03:42.310
自那以后，大多数美国政府都通过了新的法律01:03:42.310 --> 01:03:47.064
2009年使得登记或投票过程更加困难。”01:03:47.064 --> 01:03:53.275
所以我们看到的是大多数注意力的头部，01:03:53.275 --> 01:03:58.840
从制造到制造更困难，这似乎很有用。01:03:58.840 --> 01:04:03.700
其中一个注意力集中的人似乎在看这个词本身是否合适。01:04:03.700 --> 01:04:10.570
其他的研究则是关于法律和2009年。01:04:10.570 --> 01:04:14.530
所以这是在挑选论点，01:04:14.530 --> 01:04:18.910
以及修饰词，并以一种类似语法的方式。01:04:18.910 --> 01:04:21.880
有趣的是，对于代词，01:04:21.880 --> 01:04:26.770
注意力头似乎能学会回头看参考。01:04:26.770 --> 01:04:28.795
所以法律永远不会完美，01:04:28.795 --> 01:04:35.185
但它的应用应该是，01:04:35.185 --> 01:04:39.055
查看它在应用程序中修改的内容。01:04:39.055 --> 01:04:40.930
但是另一个注意头，01:04:40.930 --> 01:04:45.640
its强烈关注的是其所指的法律。01:04:45.640 --> 01:04:47.740
这看起来很酷。01:04:47.740 --> 01:04:49.810
嗯,是的。01:04:49.810 --> 01:04:52.870
嗯,好吧。01:04:52.870 --> 01:04:56.035
然后，对于模型的其余部分，01:04:56.035 --> 01:04:58.990
然后，如何使用就更加复杂了01:04:58.990 --> 01:05:05.020
变压器译码器给你一个完整的神经机器翻译系统。01:05:05.020 --> 01:05:08.770
但我想我还是跳过这部分，直接讲吧01:05:08.770 --> 01:05:13.750
在我剩下的时间里，我想谈谈伯特。01:05:13.750 --> 01:05:18.490
好。所以，嗯，最新和最伟大的背景01:05:18.490 --> 01:05:23.590
单词表示来帮助你流动你的任务是这些伯特向量，01:05:23.590 --> 01:05:29.965
其中BERT为变压器的双向编码器表示。01:05:29.965 --> 01:05:35.095
本质上，它使用的是变压器网络的编码器。01:05:35.095 --> 01:05:40.195
这个深度的多头注意力堆栈用来计算，01:05:40.195 --> 01:05:43.615
一个句子的表达，01:05:43.615 --> 01:05:49.750
这是一个很好的句子的通用表达，你可以用它来完成任务。01:05:49.750 --> 01:05:54.054
无论是实体识别还是小组问答。”01:05:54.054 --> 01:05:59.320
所以这些人有了一个有趣的新想法。01:05:59.320 --> 01:06:04.990
他们的想法是标准语言模型01:06:04.990 --> 01:06:08.230
单向的，这很有用01:06:08.230 --> 01:06:11.755
因为它给出了一个语言模型的概率分布。01:06:11.755 --> 01:06:16.210
但这很糟糕，因为你想做01:06:16.210 --> 01:06:21.190
从两方面预测单词的意思和上下文。01:06:21.190 --> 01:06:23.725
还有第二种选择01:06:23.725 --> 01:06:29.185
你可以做双向模型，01:06:29.185 --> 01:06:31.705
两种方式的信息。01:06:31.705 --> 01:06:35.050
但这也有问题，01:06:35.050 --> 01:06:37.480
因为这样就有了串音。01:06:37.480 --> 01:06:40.615
如果你经营BiLSTM，01:06:40.615 --> 01:06:43.090
然后合并表示01:06:43.090 --> 01:06:46.765
将它们连接起来，然后将它们提供给下一层。01:06:46.765 --> 01:06:48.655
当你运行下一层时，01:06:48.655 --> 01:06:51.430
LSTM已经得到了转发01:06:51.430 --> 01:06:54.385
来自第一层的关于未来的信息。01:06:54.385 --> 01:06:56.545
所以它有点，01:06:56.545 --> 01:07:00.490
以已经看到未来的文字结束。01:07:00.490 --> 01:07:03.685
这是一种复杂的非生成模型。01:07:03.685 --> 01:07:08.005
所以他们想要做一些不同的事情，01:07:08.005 --> 01:07:13.600
所以他们可以有双向的语境，而文字却不能看到他们自己。01:07:13.600 --> 01:07:16.915
他们的想法是，01:07:16.915 --> 01:07:21.430
我们要用变压器编码器来训练。01:07:21.430 --> 01:07:26.515
但是我们要做的是把句子中的一些单词蒙起来，01:07:26.515 --> 01:07:30.160
比如，也许我们可以在这里储存和加加仑。01:07:30.160 --> 01:07:34.180
然后，我们的语言模化我们的语言模型01:07:34.180 --> 01:07:36.130
目标将不复存在01:07:36.130 --> 01:07:40.090
一个真正的语言模型是生成一个句子的概率，01:07:40.090 --> 01:07:43.705
通常是从左到右，01:07:43.705 --> 01:07:49.390
但这将是一个疯狂的Libs风格填补空白的目标。01:07:49.390 --> 01:07:52.120
你会看到这样的背景，01:07:52.120 --> 01:07:53.800
也就是说，01:07:53.800 --> 01:07:56.965
“那个人去面具店买了一副牛奶面具。”01:07:56.965 --> 01:08:00.790
你的培训目标是说，01:08:00.790 --> 01:08:03.430
试着预测这个词是什么，01:08:03.430 --> 01:08:08.035
你可以用交叉熵损失来达到你猜不到的程度。01:08:08.035 --> 01:08:12.880
然后，它会尝试猜这个词是什么你想让它猜加仑数。01:08:12.880 --> 01:08:14.995
你在训练一个模型，01:08:14.995 --> 01:08:17.920
填空。01:08:17.920 --> 01:08:22.840
他们的单词空白率基本上是七分之一，01:08:22.840 --> 01:08:25.225
他们讨论这是如何权衡的。01:08:25.225 --> 01:08:28.540
因为如果你空白的单词太少，01:08:28.540 --> 01:08:30.700
培训非常昂贵。01:08:30.700 --> 01:08:32.590
如果你空白了很多单词，01:08:32.590 --> 01:08:35.545
你已经把一个词的大部分语境都删掉了，01:08:35.545 --> 01:08:38.064
这就意味着它对培训不是很有用，01:08:38.064 --> 01:08:42.325
他们发现大约七分之一的人似乎对他们很有效。01:08:42.325 --> 01:08:46.585
但是他们想说的是，01:08:46.585 --> 01:08:51.220
对于OpenAI的GPT，01:08:51.220 --> 01:08:53.470
这也是一个变压器模型。01:08:53.470 --> 01:08:56.845
这是一种经典的语言模型01:08:56.845 --> 01:09:00.700
从左到右，你只能得到左上下文。01:09:00.700 --> 01:09:03.805
对于伯特语言模型，01:09:03.805 --> 01:09:07.285
抱歉，上面显示的是ELMo语言模型。01:09:07.285 --> 01:09:11.680
他们在运行一个从左到右的语言模型，01:09:11.680 --> 01:09:13.990
从右到左的语言模型。01:09:13.990 --> 01:09:16.030
所以在某种意义上，01:09:16.030 --> 01:09:18.295
他们有来自双方的背景。01:09:18.295 --> 01:09:22.690
但是这两种语言模型是完全独立训练的01:09:22.690 --> 01:09:27.265
然后你只是把它们的表示连接在一起。01:09:27.265 --> 01:09:32.170
因此，我们实际上并没有建立一个联合的模型01:09:32.170 --> 01:09:37.930
利用当时双方的背景，01:09:37.930 --> 01:09:40.930
建立上下文单词表示。01:09:40.930 --> 01:09:45.940
所以他们希望在变压器模型中使用01:09:45.940 --> 01:09:47.980
这个划掉单词的技巧，01:09:47.980 --> 01:09:53.290
使用整个上下文来预测会让他们使用双面上下文，01:09:53.290 --> 01:09:55.540
更有效率。01:09:55.540 --> 01:10:00.025
这就是它们所显示的。01:10:00.025 --> 01:10:03.835
还有一个复杂的问题，01:10:03.835 --> 01:10:05.485
我的意思是，我待会再展示。01:10:05.485 --> 01:10:09.835
最后一个问题有点用，01:10:09.835 --> 01:10:12.999
但这对他们的主要观点不是很重要，01:10:12.999 --> 01:10:14.845
他们认为，01:10:14.845 --> 01:10:18.550
他们脑子里的目标之一就是能够01:10:18.550 --> 01:10:22.660
这对回答问题，01:10:22.660 --> 01:10:25.080
任务，或者01:10:25.080 --> 01:10:26.770
自然语言推理任务，01:10:26.770 --> 01:10:30.640
以及两句话之间的关系。01:10:30.640 --> 01:10:32.260
所以他们的想法是，01:10:32.260 --> 01:10:36.430
一个很好的目标是填补空白的单词objective，01:10:36.430 --> 01:10:39.085
有点像语言建模目标。01:10:39.085 --> 01:10:42.310
但是他们认为有第二个目标是有用的01:10:42.310 --> 01:10:45.925
预测句子之间的关系。01:10:45.925 --> 01:10:51.415
其次，它们有一个损失函数，01:10:51.415 --> 01:10:54.670
让我们用两句话01:10:54.670 --> 01:10:58.359
句子可以是课文中的两个连续的句子，01:10:58.359 --> 01:11:02.650
或者一个句子后面跟着一个来自其他地方的随机句子。01:11:02.650 --> 01:11:06.475
我们想训练系统来预测，01:11:06.475 --> 01:11:10.930
看到一个正确的下一个句子和一个随机的句子。01:11:10.930 --> 01:11:16.330
所以你也在训练一个基于下一个句子预测任务的损失。01:11:16.330 --> 01:11:19.660
就像这样:这个男人去了商店。01:11:19.660 --> 01:11:21.430
他买了一加仑牛奶。01:11:21.430 --> 01:11:24.610
你要预测的是正确的，01:11:24.610 --> 01:11:26.740
嗯，那个男人去了商店。01:11:26.740 --> 01:11:28.090
企鹅不会飞的。01:11:28.090 --> 01:11:29.515
你的意思是说错了。01:11:29.515 --> 01:11:31.285
这不是下一句。01:11:31.285 --> 01:11:33.580
所以它们同时也是，01:11:33.580 --> 01:11:36.325
用这种方式训练。01:11:36.325 --> 01:11:40.345
它们最终的样子是这样的。01:11:40.345 --> 01:11:44.245
他们有，01:11:44.245 --> 01:11:45.490
对于输入，01:11:45.490 --> 01:11:47.170
他们会有两句话。01:11:47.170 --> 01:11:48.700
我的狗很可爱。01:11:48.700 --> 01:11:50.095
嗯,分隔符。01:11:50.095 --> 01:11:51.925
他喜欢玩。01:11:51.925 --> 01:11:57.955
就像我们上周说的，单词被表示为单词片段。01:11:57.955 --> 01:12:01.570
每个词块都嵌入了一个令牌。01:12:01.570 --> 01:12:05.350
然后是位置嵌入01:12:05.350 --> 01:12:09.535
每个单词片段都要用嵌入的令牌求和。01:12:09.535 --> 01:12:14.470
最后，每个词块都有一个嵌入段，很简单01:12:14.470 --> 01:12:17.050
无论是来自第一句话还是01:12:17.050 --> 01:12:19.915
分隔符前后的第二句话。01:12:19.915 --> 01:12:24.940
你把这三样东西加起来就得到了令牌表示。01:12:24.940 --> 01:12:28.914
然后在变压器模型中使用这些01:12:28.914 --> 01:12:33.835
你会损失到无法预测假话的程度。01:12:33.835 --> 01:12:38.410
然后是二元预测函数01:12:38.410 --> 01:12:43.525
下一句是否正确，这就是训练架构。01:12:43.525 --> 01:12:47.485
好。它和之前一样是一个变压器，01:12:47.485 --> 01:12:50.740
它是在维基百科和书库上训练的。01:12:50.740 --> 01:12:52.720
他们建立了两个模型。01:12:52.720 --> 01:12:57.175
贝斯伯特模型是一个十二层变压器。01:12:57.175 --> 01:13:02.470
这和之前的变压器论文中使用的是一致的，对吧?01:13:02.470 --> 01:13:09.190
这两个层变压器块重复了6次，得到了12层768隐藏，01:13:09.190 --> 01:13:14.665
维度隐藏状态和12个头，用于多头注意。01:13:14.665 --> 01:13:16.479
然后它们变得更大，01:13:16.479 --> 01:13:18.610
训练有素的伯特玛，01:13:18.610 --> 01:13:20.620
层数翻倍，01:13:20.620 --> 01:13:23.485
更大的隐藏状态，更多的注意力。01:13:23.485 --> 01:13:26.410
训练它们，01:13:26.410 --> 01:13:29.185
TPUs的豆荚。01:13:29.185 --> 01:13:33.850
首先，你在接受培训01:13:33.850 --> 01:13:38.260
在此基础上，01:13:38.260 --> 01:13:40.375
嗯，下一句话还是不说。01:13:40.375 --> 01:13:45.940
然后他们想说的是这个训练前的模型，01:13:45.940 --> 01:13:51.685
评估这些损失，掩蔽语言模型和下一句预测。01:13:51.685 --> 01:13:54.925
我们可以用这个模型，01:13:54.925 --> 01:13:59.050
fr-冻结它的大部分什么弱。不，对不起，那是错的。01:13:59.050 --> 01:14:01.270
我们可以用这个模型，01:14:01.270 --> 01:14:06.610
预先训练过的，对各种不同的任务都非常有用。01:14:06.610 --> 01:14:08.800
我们可以用它来进行命名实体识别，01:14:08.800 --> 01:14:12.310
回答问题，自然语言推理等等。01:14:12.310 --> 01:14:14.890
我们要做的是，01:14:14.890 --> 01:14:18.550
执行与ULMFit模型相同的操作。01:14:18.550 --> 01:14:20.755
我们不只是说这是我们的，01:14:20.755 --> 01:14:25.240
这是一个上下文相关的单词表示，就像ELMo所做的那样。01:14:25.240 --> 01:14:29.560
相反，我们要说的是继续用这个，01:14:29.560 --> 01:14:32.230
继续用这个，01:14:32.230 --> 01:14:36.880
我们训练的变压器网络，01:14:36.880 --> 01:14:42.535
语言模型，但对特定任务进行微调。01:14:42.535 --> 01:14:45.190
现在你要运行这个变压器01:14:45.190 --> 01:14:49.180
计算特定任务的表示形式。01:14:49.180 --> 01:14:55.990
我们要改变的是我们要移除最顶层的预测。01:14:55.990 --> 01:15:00.415
预测大众语言模型和下一个句子预测的位元。01:15:00.415 --> 01:15:02.770
我们要代入它，01:15:02.770 --> 01:15:08.080
在最上面，嗯，最后一个预测层是适合这项任务的。01:15:08.080 --> 01:15:11.005
所以，如果我们的任务是回答小组问题，01:15:11.005 --> 01:15:16.344
我们的最终预测层将预测跨度的开始和结束，01:15:16.344 --> 01:15:20.740
就像我们几周前看到的DrQA一样。01:15:20.740 --> 01:15:23.979
如果我们要做的是NER任务，01:15:23.979 --> 01:15:26.889
最后一个预测层是预测01:15:26.889 --> 01:15:33.895
每个令牌的网络命名实体识别类就像一个标准的NER系统。01:15:33.895 --> 01:15:42.775
他们建立了这个系统并在一系列数据集上进行了测试。01:15:42.775 --> 01:15:45.610
他们测试的主要内容之一是01:15:45.610 --> 01:15:48.625
这个GLUE数据集有很多任务。01:15:48.625 --> 01:15:50.170
很多任务，01:15:50.170 --> 01:15:53.530
自然语言推理任务。01:15:53.530 --> 01:15:57.205
这节课我一直在说这个短语，但我还没有真正定义它。01:15:57.205 --> 01:16:00.820
用自然语言推理，你得到了两个句子01:16:00.820 --> 01:16:05.935
比如:在耆那教中，山和山是特别神圣的。01:16:05.935 --> 01:16:09.550
然后你可以写一个假设:耆那教憎恨自然。01:16:09.550 --> 01:16:11.530
你想说的是，01:16:11.530 --> 01:16:13.570
假设，嗯，01:16:13.570 --> 01:16:15.505
从前提出发，01:16:15.505 --> 01:16:19.240
与前提相矛盾，或者与前提没有关系。01:16:19.240 --> 01:16:21.265
这是一个三向分类。01:16:21.265 --> 01:16:23.845
这与前提相矛盾。01:16:23.845 --> 01:16:30.115
还有很多其他的任务，比如语言可接受性任务。01:16:30.115 --> 01:16:33.550
但是如果我们看看这些，粘任务。01:16:33.550 --> 01:16:37.735
这些展示了开放人工智能之前的状态。01:16:37.735 --> 01:16:40.735
埃尔默的工作很好。01:16:40.735 --> 01:16:43.900
OpenAI GPT工作得有多好，01:16:43.900 --> 01:16:48.415
然后是伯特模型的大小。01:16:48.415 --> 01:16:53.290
实际上，你会发现，01:16:53.290 --> 01:16:57.370
OpenAI GPT是这样的，01:16:57.370 --> 01:16:58.495
你知道，很好。01:16:58.495 --> 01:17:02.455
在这些任务中，大多数都取得了很好的进展。01:17:02.455 --> 01:17:05.890
对很多人来说，但不是所有人都打破了之前的技术水平，01:17:05.890 --> 01:17:08.995
展示了这些上下文语言模型的强大功能。01:17:08.995 --> 01:17:15.205
但伯特预测的双向形式似乎又好得多。01:17:15.205 --> 01:17:19.180
所以，从这条线到这条线取决于01:17:19.180 --> 01:17:23.185
该任务的性能大约提高了2%。01:17:23.185 --> 01:17:27.010
所以伯特家的人做实验很仔细。01:17:27.010 --> 01:17:30.430
所以，这些模型在尺寸上非常相似，01:17:30.430 --> 01:17:33.775
但双向背景似乎真的很有帮助。01:17:33.775 --> 01:17:35.470
然后他们发现，01:17:35.470 --> 01:17:37.570
通过一个更大的模型，01:17:37.570 --> 01:17:41.545
同样，您可以获得性能上的另一个大提升。01:17:41.545 --> 01:17:44.740
所以很多任务都是关于01:17:44.740 --> 01:17:48.145
另外2%的性能提升进入了更大的模型。01:17:48.145 --> 01:17:51.010
所以，这真的产生了超强的效果。01:17:51.010 --> 01:17:54.085
总的来说，人们发现，01:17:54.085 --> 01:17:57.400
伯特继续给出非常好的结果。01:17:57.400 --> 01:18:01.480
如果我回到ConLL NER任务，01:18:01.480 --> 01:18:05.260
ELMo给了你92。2，01:18:05.260 --> 01:18:06.640
而你呢01:18:06.640 --> 01:18:08.050
继续获得收益。01:18:08.050 --> 01:18:13.900
BERT Base得到92。4,BERT Large得到92。8。01:18:13.900 --> 01:18:17.650
虽然，嗯，真实的，真实的描述，01:18:17.650 --> 01:18:23.125
现在有一个打败伯特的系统很大程度上是一个角色级的，01:18:23.125 --> 01:18:25.990
Flair的transformer语言模型。01:18:25.990 --> 01:18:27.835
但是，你知道，01:18:27.835 --> 01:18:30.790
这种情况一直延续到很多其他事情上。01:18:30.790 --> 01:18:33.865
在1。1单元，01:18:33.865 --> 01:18:36.370
伯特马上就超越了01:18:36.370 --> 01:18:39.745
人们为球队工作了很长时间的其他事情。01:18:39.745 --> 01:18:42.610
特别引人注目的是，01:18:42.610 --> 01:18:45.985
是唱歌，一个伯特模型，01:18:45.985 --> 01:18:50.770
击败了之前在1.1版中所做的一切，01:18:50.770 --> 01:18:53.575
即使他们也能证明01:18:53.575 --> 01:18:59.815
伯特模型的集合可以进一步提高性能。01:18:59.815 --> 01:19:03.055
正如我之前提到的，01:19:03.055 --> 01:19:05.980
基本上，如果你看一下0。0版，01:19:05.980 --> 01:19:08.935
排行榜，所有排名靠前的系统，01:19:08.935 --> 01:19:12.280
在某个地方用伯特。01:19:12.280 --> 01:19:14.590
所以，01:19:14.590 --> 01:19:16.060
就这样，01:19:16.060 --> 01:19:19.570
有点像新世界秩序，嗯，好吧，01:19:19.570 --> 01:19:22.735
现在NLP的状态是，01:19:22.735 --> 01:19:25.240
如果你想要有最好的表现，01:19:25.240 --> 01:19:26.410
你想要使用01:19:26.410 --> 01:19:31.855
这些经过深度预培训的变压器堆栈可以获得最佳性能。01:19:31.855 --> 01:19:33.220
这就是，01:19:33.220 --> 01:19:35.410
NLP更像是视觉。01:19:35.410 --> 01:19:38.560
因为真正的远见卓识已经有五年了01:19:38.560 --> 01:19:42.730
这些深度预训练的神经网络栈，就像ResNets。01:19:42.730 --> 01:19:47.124
对于大多数视觉任务，你要做的是使用一个预先训练过的ResNet，01:19:47.124 --> 01:19:49.870
然后在顶部微调一个图层01:19:49.870 --> 01:19:52.870
做一些你感兴趣的分类任务。01:19:52.870 --> 01:19:54.970
这是，现在，嗯，01:19:54.970 --> 01:19:57.520
也开始成为NLP中正在发生的事情。01:19:57.520 --> 01:20:00.280
你可以通过下载来做同样的事情01:20:00.280 --> 01:20:05.875
您预先训练的BERT并对其进行微调，以执行某些特定的性能任务。01:20:05.875 --> 01:20:09.400
好了，今天就讲到这里01:20:09.400 --> 01:20:18.330
transformers on Thursday [NOISE].

