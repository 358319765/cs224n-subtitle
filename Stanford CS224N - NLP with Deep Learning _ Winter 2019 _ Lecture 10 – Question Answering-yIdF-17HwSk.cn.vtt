WEBVTT
Kind: captions
Language: en

00:00:05.480 --> 00:00:11.115
好。你好,每个人。我们今天再开始吧。00:00:11.115 --> 00:00:14.610
今天我要做的是，00:00:14.610 --> 00:00:16.710
是在说，嗯，00:00:16.710 --> 00:00:19.060
通过短信回答问题。00:00:19.060 --> 00:00:22.025
这是另一个巨大的成功00:00:22.025 --> 00:00:25.655
在自然语言处理中运用深度学习，00:00:25.655 --> 00:00:30.140
这也是一项具有明显商业用途的技术。00:00:30.140 --> 00:00:32.660
这是一个吸引人的区域00:00:32.660 --> 00:00:36.265
在过去的几年里得到了很多关注。00:00:36.265 --> 00:00:38.790
这就是整个计划。00:00:38.790 --> 00:00:43.970
在期末项目开始的时候提醒大家几件事，00:00:43.970 --> 00:00:48.875
然后，基本上所有的内容都是关于问答的，00:00:48.875 --> 00:00:53.000
动机历史，谈到球队数据，00:00:53.000 --> 00:00:56.390
一个特别简单的模型，我们斯坦福的细心读者。00:00:56.390 --> 00:00:58.940
然后再讲一些更复杂的，00:00:58.940 --> 00:01:02.460
把东西变成最现代的东西。00:01:02.460 --> 00:01:05.815
在人口普查中，00:01:05.815 --> 00:01:09.365
讲座有双重目的，因为如果你要做，00:01:09.365 --> 00:01:11.390
默认的期末项目，00:01:11.390 --> 00:01:13.410
这是关于文本问题的回答，00:01:13.410 --> 00:01:17.855
这是你们学习文本问答的机会，00:01:17.855 --> 00:01:21.410
以及你可能想要思考和建立的模型。00:01:21.410 --> 00:01:24.890
但是这节课的主要内容是00:01:24.890 --> 00:01:28.915
没有办法和默认的期末项目联系起来，00:01:28.915 --> 00:01:32.720
除了它告诉你的主题之外00:01:32.720 --> 00:01:37.580
人们如何使用神经网络来建立问答系统。00:01:37.580 --> 00:01:41.205
好。首先是提醒，00:01:41.205 --> 00:01:43.050
嗯,季中调查。00:01:43.050 --> 00:01:45.149
我的意思是，很多人，00:01:45.149 --> 00:01:47.330
实际上已经填好了。00:01:47.330 --> 00:01:51.140
我们已经有超过60%了00:01:51.140 --> 00:01:54.170
以人们的标准来衡量它的填充率00:01:54.170 --> 00:01:57.245
他们已经取得了巨大的成功。00:01:57.245 --> 00:01:59.510
但如果你不属于那个百分比，00:01:59.510 --> 00:02:03.480
我们仍然希望得到您的反馈，现在正是这样做的最佳时机。00:02:03.480 --> 00:02:05.515
嗯,是的。00:02:05.515 --> 00:02:09.455
我只是想对定制的期末项目做个说明。00:02:09.455 --> 00:02:11.390
总的来说，00:02:11.390 --> 00:02:14.645
在定制的最终项目上得到反馈是很好的。00:02:14.645 --> 00:02:16.910
这有一个正式的机制00:02:16.910 --> 00:02:19.625
我上次提到的项目建议书。00:02:19.625 --> 00:02:22.330
和别人聊天也很棒，00:02:22.330 --> 00:02:25.930
非正式地谈谈期末项目。00:02:25.930 --> 00:02:28.685
所以我是其中的一员，我也有过这样的经历00:02:28.685 --> 00:02:31.610
和很多人谈过期末项目，00:02:31.610 --> 00:02:33.455
很高兴这么做。00:02:33.455 --> 00:02:36.500
但有个问题，我只有一个。00:02:36.500 --> 00:02:38.630
我也是，00:02:38.630 --> 00:02:42.080
鼓励你们意识到在不同的助教中00:02:42.080 --> 00:02:46.070
很多人都有不同深度学习项目的经验，00:02:46.070 --> 00:02:48.620
尤其是在办公时间页面，00:02:48.620 --> 00:02:53.420
有一张这样的表格，但如果你在自己的笔记本电脑上看，00:02:53.420 --> 00:02:57.125
讲的是不同助教的经历。00:02:57.125 --> 00:02:59.930
他们中的许多人在不同领域都有经验，00:02:59.930 --> 00:03:04.930
他们中的许多人也是谈论期末项目的好朋友。00:03:04.930 --> 00:03:10.860
好。对于默认的期末项目，文本问答。00:03:10.860 --> 00:03:15.194
所以，嗯，为今天的应用程序起草材料，00:03:15.194 --> 00:03:17.340
实际上，现在就在网站上。00:03:17.340 --> 00:03:20.720
我们叫他们选秀是因为我们认为还有00:03:20.720 --> 00:03:24.230
也许下周会有一些改变，00:03:24.230 --> 00:03:29.840
所以，就代码而言，不要认为这是最终的，00:03:29.840 --> 00:03:32.120
你知道，这是百分之九十的决赛。00:03:32.120 --> 00:03:35.405
所以在决定你是否要做的时候，00:03:35.405 --> 00:03:38.495
自定义期末项目或默认期末项目，00:03:38.495 --> 00:03:41.465
然后计算出你在项目建议书中写了什么。00:03:41.465 --> 00:03:42.755
应该是，你知道的，00:03:42.755 --> 00:03:44.270
比，嗯，00:03:44.270 --> 00:03:46.470
你今年需要什么。00:03:46.470 --> 00:03:48.670
好。另一个，嗯，00:03:48.670 --> 00:03:52.040
最后一点，我只是想说我还没讲到00:03:52.040 --> 00:03:55.520
上节课是关于期末项目的，00:03:55.520 --> 00:03:58.055
不管你在做什么00:03:58.055 --> 00:04:00.750
部分原因是00:04:00.750 --> 00:04:02.545
做一些实验00:04:02.545 --> 00:04:04.700
处理数据和代码，00:04:04.700 --> 00:04:06.880
得到一些数字之类的。00:04:06.880 --> 00:04:08.480
但我真的00:04:08.480 --> 00:04:11.630
鼓励人们也记住这一点很重要00:04:11.630 --> 00:04:15.515
期末项目是写期末项目报告。00:04:15.515 --> 00:04:20.900
这和任何研究项目没有什么不同，00:04:20.900 --> 00:04:25.590
学生们为会议或期刊之类的事情做准备，对吧?00:04:25.590 --> 00:04:30.020
您通常要花费几个月的时间来研究代码和实验。00:04:30.020 --> 00:04:31.970
但在大多数情况下，00:04:31.970 --> 00:04:36.365
对你作品的主要评价来自于人们的阅读，00:04:36.365 --> 00:04:39.200
一个书面的纸质输出版本的东西。00:04:39.200 --> 00:04:41.420
所以很重要的是，00:04:41.420 --> 00:04:44.480
纸上的版本反映了作品的风格00:04:44.480 --> 00:04:47.840
你所做的和你提出的有趣的想法，00:04:47.840 --> 00:04:50.720
解释清楚并展示你的实验，00:04:50.720 --> 00:04:52.100
所有这些。00:04:52.100 --> 00:04:56.670
所以我们鼓励你们把项目写好。00:04:56.670 --> 00:04:59.680
这只是一个模糊的轮廓，00:04:59.680 --> 00:05:03.320
一个典型的项目报告应该是什么样的。00:05:03.320 --> 00:05:06.620
现在，并没有一种尺寸可以完全适合所有人00:05:06.620 --> 00:05:09.950
因为这取决于你做了什么不同的事情可能是合适的。00:05:09.950 --> 00:05:11.990
但是，你知道，通常在第一页，00:05:11.990 --> 00:05:15.905
你将会有论文摘要和论文简介。00:05:15.905 --> 00:05:19.220
你将花一些时间谈论一下相关的前期工作。00:05:19.220 --> 00:05:23.615
嗯，你会谈论一下你做过的模型。00:05:23.615 --> 00:05:28.565
可能会有一些关于你在项目中使用什么数据的讨论。00:05:28.565 --> 00:05:34.920
通常用一些表格和数字来做实验。00:05:34.920 --> 00:05:39.740
嗯，更多的表格和数字是关于你的系统如何工作的结果。00:05:39.740 --> 00:05:43.010
做一些误差分析是很好的00:05:43.010 --> 00:05:46.290
你做对了和错了什么00:05:46.290 --> 00:05:48.500
也许在最后00:05:48.500 --> 00:05:51.965
对未来的计划，结论等等。00:05:51.965 --> 00:05:59.475
好。这是我额外的行政提醒。00:05:59.475 --> 00:06:03.470
大家对期末项目有什么问题吗?00:06:03.470 --> 00:06:09.800
(噪音)。祝你好运。00:06:09.800 --> 00:06:10.925
我只是想说祝你好运。00:06:10.925 --> 00:06:13.470
是的。祝你期末项目顺利。(笑)好。00:06:13.470 --> 00:06:15.375
现在，00:06:15.375 --> 00:06:18.550
嗯，是的，回答问题。00:06:18.550 --> 00:06:23.165
好。所以，我的意思是，问题的回答是00:06:23.165 --> 00:06:28.610
一个非常直接的应用，00:06:28.610 --> 00:06:30.095
嗯，想做。00:06:30.095 --> 00:06:33.620
嗯，也许人类一般不想知道这个。00:06:33.620 --> 00:06:37.355
我的问题是“谁是澳大利亚第三任总理?”00:06:37.355 --> 00:06:39.500
嗯，也许，是的，那不是真正的那种00:06:39.500 --> 00:06:41.645
你要在查询中输入的东西，00:06:41.645 --> 00:06:43.145
也许你会问，00:06:43.145 --> 00:06:45.110
“谁是《大盗》的主唱?”00:06:45.110 --> 00:06:46.745
或者类似的东西。我不知道。00:06:46.745 --> 00:06:48.050
你是，但是你知道，00:06:48.050 --> 00:06:51.770
很多――网络上的很多东西[噪音]00:06:51.770 --> 00:06:56.090
人们实际上是在问问题的答案。00:06:56.090 --> 00:06:59.120
如果我把这个查询输入谷歌，00:06:59.120 --> 00:07:00.530
它实际上是有效的。00:07:00.530 --> 00:07:03.920
它告诉我答案是约翰・克里斯蒂安・华生。00:07:03.920 --> 00:07:08.915
这就是在现实世界中如何回答问题。00:07:08.915 --> 00:07:11.540
如果你在谷歌中尝试不同的问题，00:07:11.540 --> 00:07:14.585
你会发现其中一些有用，而很多没用。00:07:14.585 --> 00:07:15.770
当它们不起作用时，00:07:15.770 --> 00:07:20.090
你只是得到了某种信息检索，网络搜索结果。00:07:20.090 --> 00:07:23.315
有一点我很想说，00:07:23.315 --> 00:07:25.130
在这里提一下。00:07:25.130 --> 00:07:28.790
谷歌还有一个知识图，00:07:28.790 --> 00:07:32.225
它是知识的结构化图形表示。00:07:32.225 --> 00:07:35.405
还有一些问题，00:07:35.405 --> 00:07:39.080
从结构化的知识表示中得到答案。00:07:39.080 --> 00:07:40.430
所以，我的意思是，00:07:40.430 --> 00:07:43.025
很多时候，比如看电影，00:07:43.025 --> 00:07:44.870
它来自于那个结构化的图。00:07:44.870 --> 00:07:47.690
如果你问，“谁是电影导演?”00:07:47.690 --> 00:07:48.890
或者类似的东西。00:07:48.890 --> 00:07:51.050
但这个答案不是来自于那个。00:07:51.050 --> 00:07:53.000
这个答案是真实的，00:07:53.000 --> 00:07:55.400
我们今天要讲的内容。00:07:55.400 --> 00:07:59.360
这是一个文本问题的回答从一个网页00:07:59.360 --> 00:08:01.580
谷歌的问答系统有00:08:01.580 --> 00:08:04.505
把答案提取出来，贴在上面。00:08:04.505 --> 00:08:06.365
如果你是00:08:06.365 --> 00:08:09.485
想要探索这些东西，00:08:09.485 --> 00:08:14.735
如果你有一个这样的盒子在我把它剪掉的地方，00:08:14.735 --> 00:08:16.340
有一点灰色表示，00:08:16.340 --> 00:08:17.990
“我是怎么得到这个结果的?”00:08:17.990 --> 00:08:19.415
如果你点击它，00:08:19.415 --> 00:08:23.300
它会告诉你它是从哪来的你可以看到它是否在做00:08:23.300 --> 00:08:28.130
从文本的问题回答系统或类似于知识图的东西。00:08:28.130 --> 00:08:31.040
好。总的来说，00:08:31.040 --> 00:08:35.600
现在回答问题的动机是00:08:35.600 --> 00:08:40.355
就是这些大量的全文文档，00:08:40.355 --> 00:08:42.110
即。，这就是网络。00:08:42.110 --> 00:08:46.580
所以有数十亿的信息文件。00:08:46.580 --> 00:08:49.730
传统上，当人们刚开始的时候00:08:49.730 --> 00:08:53.330
把搜索信息检索看作一个领域，00:08:53.330 --> 00:08:59.020
你知道，那种数量和大小是不存在的，对吧?00:08:59.020 --> 00:09:02.320
当人们开始建立搜索系统时，00:09:02.320 --> 00:09:05.200
索引是不可想象的00:09:05.200 --> 00:09:09.340
整个文件，因为那时候没有人有足够大的硬盘，对吧?00:09:09.340 --> 00:09:15.335
他们在索引标题或标题和摘要之类的东西。00:09:15.335 --> 00:09:19.925
所以，在那个年代，说“好吧”似乎已经足够了。00:09:19.925 --> 00:09:22.760
我们会把结果发给你。”00:09:22.760 --> 00:09:24.680
至于“这是一份文件清单。”00:09:24.680 --> 00:09:27.440
因为这些文件只有一百字长。00:09:27.440 --> 00:09:31.010
但这显然不是现在的情况当我们有，你知道，00:09:31.010 --> 00:09:36.275
十分钟的阅读，中等大小的帖子，可能有问题的答案。00:09:36.275 --> 00:09:39.080
所以，有必要说，00:09:39.080 --> 00:09:43.205
我们能不能有一个系统能给我们问题的答案?”00:09:43.205 --> 00:09:49.730
最近的许多技术变革都极大地强调了这一需求。00:09:49.730 --> 00:09:54.950
所以，如果你坐在笔记本电脑前，返回文档是可以的，00:09:54.950 --> 00:09:59.150
但如果你用手机的话，效果会很糟糕，而且效果会更好00:09:59.150 --> 00:10:04.040
如果你想在数字助理设备上处理语音，00:10:04.040 --> 00:10:06.110
有点像Alexa系统。00:10:06.110 --> 00:10:08.840
所以，我们真的希望能够生产00:10:08.840 --> 00:10:12.260
能够回答人们问题的系统。00:10:12.260 --> 00:10:16.865
一般来说，这分为两部分。00:10:16.865 --> 00:10:21.500
第一部分是我们仍然做信息检索。00:10:21.500 --> 00:10:26.270
我们使用通常比较标准的信息检索技术00:10:26.270 --> 00:10:32.150
找到那些很可能包含答案的文档。00:10:32.150 --> 00:10:36.200
这通常是由传统的技术来完成的原因是00:10:36.200 --> 00:10:41.390
传统的技术可以扩展到数十亿个文档，00:10:41.390 --> 00:10:43.790
而现在的神经系统00:10:43.790 --> 00:10:46.225
无法扩展到数十亿个文档。00:10:46.225 --> 00:10:50.380
但这是一个正在进行研究的领域。00:10:50.380 --> 00:10:53.920
但是一旦我们有了一些候选文件，00:10:53.920 --> 00:10:55.645
我们想要找到，00:10:55.645 --> 00:10:57.369
它们包含答案吗，00:10:57.369 --> 00:10:59.305
如果是的话，答案是什么?00:10:59.305 --> 00:11:00.520
所以在那个时候，00:11:00.520 --> 00:11:03.275
我们有一个文件或一段，00:11:03.275 --> 00:11:07.445
我们说，“我们能从那里回答这个问题吗?”00:11:07.445 --> 00:11:11.345
这个问题通常被称为阅读理解问题。00:11:11.345 --> 00:11:14.705
这就是我今天的重点。00:11:14.705 --> 00:11:19.535
阅读理解并不是一个新问题。00:11:19.535 --> 00:11:26.345
我的意思是――你可以追溯到人工智能和NLP的早期。00:11:26.345 --> 00:11:28.295
回到70年代，00:11:28.295 --> 00:11:31.520
很多NLP的工作都是为了做阅读理解。00:11:31.520 --> 00:11:35.420
我的意思是其中一条著名的线索是，00:11:35.420 --> 00:11:38.435
罗杰・沙克爵士很有名，00:11:38.435 --> 00:11:41.030
早期的NLP人员。00:11:41.030 --> 00:11:42.650
虽然他不是一个非常好的人。00:11:42.650 --> 00:11:43.985
实际上我不这么认为。00:11:43.985 --> 00:11:48.440
但是耶鲁大学的人工智能学院非常有名，00:11:48.440 --> 00:11:51.830
NLP方法，00:11:51.830 --> 00:11:55.385
它非常注重阅读理解。00:11:55.385 --> 00:11:58.205
但这有点，00:11:58.205 --> 00:12:01.070
你知道，我认为那是一种时间，无论如何都太早了。00:12:01.070 --> 00:12:03.725
它消失了。没有什么结果。00:12:03.725 --> 00:12:07.670
但是，就在千禧年来临之前，00:12:07.670 --> 00:12:11.150
Lynette Hirschman重新提出了这个想法，她说，00:12:11.150 --> 00:12:14.000
也许一个很好的挑战就是找到那种00:12:14.000 --> 00:12:18.155
小学生做的阅读理解题，00:12:18.155 --> 00:12:19.700
我们看看能不能得到，00:12:19.700 --> 00:12:21.500
用电脑来做。00:12:21.500 --> 00:12:24.530
有些人尝试了相当简单的方法，00:12:24.530 --> 00:12:26.690
这种方法只能在一般情况下奏效。00:12:26.690 --> 00:12:29.180
之后，嗯，00:12:29.180 --> 00:12:31.460
Chris Burges是一个在00:12:31.460 --> 00:12:34.610
他不是一个真正的NLP人。00:12:34.610 --> 00:12:36.335
他是一个学习机器的人，00:12:36.335 --> 00:12:39.065
但他脑子里想的是00:12:39.065 --> 00:12:43.820
而真正需要解决的一个大问题是00:12:43.820 --> 00:12:49.115
机器理解，他建议你可以把它编成这样的代码。00:12:49.115 --> 00:12:52.715
这是一个特别干净的法典00:12:52.715 --> 00:12:55.340
这种情况一直存在，今天我们会进一步探讨。00:12:55.340 --> 00:12:58.880
好吧。因此，机器理解一段文字。00:12:58.880 --> 00:13:01.640
如果对这段文字有任何疑问的话00:13:01.640 --> 00:13:04.490
大多数以英语为母语的人都能正确回答，00:13:04.490 --> 00:13:06.890
那台机器可以提供一个字符串，00:13:06.890 --> 00:13:09.470
哪位发言者会同意这两个答案00:13:09.470 --> 00:13:13.565
并且不包含与该问题无关的信息。00:13:13.565 --> 00:13:17.750
他提出这是一个挑战性的问题00:13:17.750 --> 00:13:21.980
人工智能开始收集语料库，00:13:21.980 --> 00:13:27.410
MCTest语料库，这本来是一个简单的阅读理解挑战。00:13:27.410 --> 00:13:29.855
所以他们收集，00:13:29.855 --> 00:13:32.840
故事，嗯，嗯，00:13:32.840 --> 00:13:35.510
都是孩子们的故事。00:13:35.510 --> 00:13:37.790
阿丽莎经过长途旅行来到了海滩。00:13:37.790 --> 00:13:40.010
她从夏洛特。她是从亚特兰大来的。00:13:40.010 --> 00:13:41.570
她现在在迈阿密。”00:13:41.570 --> 00:13:43.505
很简单。00:13:43.505 --> 00:13:45.185
还有一些问题。00:13:45.185 --> 00:13:47.795
“为什么艾莉莎要去迈阿密?”00:13:47.795 --> 00:13:49.895
然后答案是，00:13:49.895 --> 00:13:51.320
“去拜访一些朋友”。00:13:51.320 --> 00:13:55.130
这就是这段文字的字符串。00:13:55.130 --> 00:13:57.515
这就是问题的答案。00:13:57.515 --> 00:14:00.950
MCTest是一个语料库00:14:00.950 --> 00:14:07.160
大约有600个这样的故事和挑战存在，有几个人在做。00:14:07.160 --> 00:14:11.240
但在接下来的几年里，这也没有走多远。00:14:11.240 --> 00:14:15.350
但真正改变的是2015年，00:14:15.350 --> 00:14:18.515
在2016年，00:14:18.515 --> 00:14:23.000
深度学习的人对这个想法很感兴趣，00:14:23.000 --> 00:14:27.620
“我们可以建立神经问题回答系统吗?”00:14:27.620 --> 00:14:30.965
如果你想这么做00:14:30.965 --> 00:14:33.980
像MCTest这样的东西只能是一个测试集00:14:33.980 --> 00:14:38.240
取得进展的方法将是做已经做过的事情00:14:38.240 --> 00:14:45.680
在其他领域，为了建立大量的训练通道，00:14:45.680 --> 00:14:50.870
这样的问题和答案将能够训练神经网络使用00:14:50.870 --> 00:14:53.600
我们所掌握的监督学习技术00:14:53.600 --> 00:14:56.540
集中在这门课上。00:14:56.540 --> 00:15:00.445
事实上，这种监督神经网络学习技术，00:15:00.445 --> 00:15:02.990
哪一种(噪音)才是真正成功的00:15:02.990 --> 00:15:06.500
几乎所有深度学习应用的动力，00:15:06.500 --> 00:15:07.955
不仅在NLP中，00:15:07.955 --> 00:15:10.200
但也在其他领域，如视觉。00:15:10.200 --> 00:15:15.680
第一个subs-第一个这样的数据集是由00:15:15.680 --> 00:15:20.990
DeepMind的工作人员通过CNN和每日邮报报道新闻故事。00:15:20.990 --> 00:15:23.300
但是第二年，00:15:23.300 --> 00:15:26.270
Pranav Rajpurkar是斯坦福大学的一名博士生00:15:26.270 --> 00:15:29.270
和珀西・梁以及其他几个学生一起工作，00:15:29.270 --> 00:15:31.055
生成了小队数据集，00:15:31.055 --> 00:15:34.580
哪个数据集设计得更好00:15:34.580 --> 00:15:38.120
更成功地推动了这一进程。00:15:38.120 --> 00:15:39.830
接着，00:15:39.830 --> 00:15:42.770
其他人开始生产很多其他的，00:15:42.770 --> 00:15:45.590
问题回答数据集，00:15:45.590 --> 00:15:48.215
其中许多都有有趣的优点00:15:48.215 --> 00:15:51.320
他们自己的缺点包括马可女士00:15:51.320 --> 00:15:53.810
琐事，种族，等等，等等，很多。00:15:53.810 --> 00:15:55.760
但是今天的课，00:15:55.760 --> 00:15:58.100
我要集中精力在球队上00:15:58.100 --> 00:16:03.890
因为到目前为止，使用最广泛的其实是小队。00:16:03.890 --> 00:16:10.145
因为它只是一个构造良好的干净的数据集，00:16:10.145 --> 00:16:13.670
事实证明，这是一种有利可图的合作方式。00:16:13.670 --> 00:16:17.260
(噪音)00:16:17.260 --> 00:16:20.230
好。这就是阅读理解。00:16:20.230 --> 00:16:23.050
我也会很快地告诉你们，00:16:23.050 --> 00:16:26.485
开放领域问题回答的历史。00:16:26.485 --> 00:16:29.080
这里的差值是00:16:29.080 --> 00:16:33.305
开放域问题的答案是，00:16:33.305 --> 00:16:37.350
有百科全书或者网络爬虫，00:16:37.350 --> 00:16:39.405
我要问一个问题，00:16:39.405 --> 00:16:40.560
你能回答吗?00:16:40.560 --> 00:16:43.555
所以，这是一个更大的问题回答任务。00:16:43.555 --> 00:16:46.570
你知道，这是我们再次思考的问题，00:16:46.570 --> 00:16:49.000
嗯，很早以前。00:16:49.000 --> 00:16:51.460
所以，这有点早，00:16:51.460 --> 00:16:56.170
西蒙斯的CACM论文，他探讨了如何做到这一点00:16:56.170 --> 00:17:00.940
把回答问题作为文本问题的回答，00:17:00.940 --> 00:17:03.010
他知道接下来会发生什么00:17:03.010 --> 00:17:05.755
你要做的是依赖分析问题，00:17:05.755 --> 00:17:08.470
以及文本的依赖分析语句，00:17:08.470 --> 00:17:11.830
然后对依赖项进行树匹配，00:17:11.830 --> 00:17:13.660
找出答案。00:17:13.660 --> 00:17:15.865
你知道，这在某种意义上00:17:15.865 --> 00:17:22.120
事实上，这预示着人们在35年后会做的工作。00:17:22.120 --> 00:17:25.570
变得更现代了，朱利安・库普塞克00:17:25.570 --> 00:17:28.000
当时她在施乐帕洛阿尔托研究中心工作，00:17:28.000 --> 00:17:31.240
提出了一个叫做MURAX的系统，00:17:31.240 --> 00:17:35.890
所以在90年代的这个阶段，00:17:35.890 --> 00:17:38.770
可用的数字百科全书，00:17:38.770 --> 00:17:41.275
所以他使用了格罗里埃的百科全书，00:17:41.275 --> 00:17:44.560
所以他说要建立一个能够回答这个问题的系统00:17:44.560 --> 00:17:47.980
关于百科全书的问题，00:17:47.980 --> 00:17:50.590
总的来说，有点肤浅，00:17:50.590 --> 00:17:55.435
语言处理方法。e,正则表达式。00:17:55.435 --> 00:17:58.210
因为，在[笑声]00:17:58.210 --> 00:18:01.555
在上面做信息检索搜索。00:18:01.555 --> 00:18:05.515
但这开始引起其他人更多的兴趣，00:18:05.515 --> 00:18:13.135
1999年，美国国家标准与技术研究院，00:18:13.135 --> 00:18:17.170
建立了一个TREC问答系统，00:18:17.170 --> 00:18:21.145
有大量的电报文件，00:18:21.145 --> 00:18:25.090
你可以提出他们的问题，00:18:25.090 --> 00:18:28.390
很多人开始建立问答系统。00:18:28.390 --> 00:18:30.850
的确，如果在某种意义上是这样的话00:18:30.850 --> 00:18:35.560
这场比赛是IBM员工开始的地方，00:18:35.560 --> 00:18:38.320
研究文本问题的回答，00:18:38.320 --> 00:18:42.010
十年后，00:18:42.010 --> 00:18:47.305
IBM将其重新组合成更性感的格式，00:18:47.305 --> 00:18:52.975
让我们做一个《危险边缘》的参赛者而不是回答新闻中的问题，00:18:52.975 --> 00:18:56.620
然后，他们在2011年推出了DeepQA系统。00:18:56.620 --> 00:18:59.155
我想你们很多人都看过，00:18:59.155 --> 00:19:02.545
这些人看了Jeopardy IBM?00:19:02.545 --> 00:19:04.120
是的，你们中的一些人。00:19:04.120 --> 00:19:07.195
好。所以他们成功了00:19:07.195 --> 00:19:13.180
建立一个问答系统，可以在Jeopardy，嗯，和赢。00:19:13.180 --> 00:19:17.710
嗯，你知道，就像很多这样的演示00:19:17.710 --> 00:19:23.950
技术上的成功你可以对它的建立方式吹毛求疵，00:19:23.950 --> 00:19:27.250
嗯，这真的是那种刚刚才有的电脑00:19:27.250 --> 00:19:32.260
速度上的优势，相对于人类，要回答这个问题。00:19:32.260 --> 00:19:34.945
但是，你知道，从根本上说，00:19:34.945 --> 00:19:37.540
文本问答必须奏效，00:19:37.540 --> 00:19:42.895
这是一个主要基于文本段落回答问题的系统，00:19:42.895 --> 00:19:47.079
它必须能够正确地找到这些问题的答案，00:19:47.079 --> 00:19:48.790
让系统正常工作。00:19:48.790 --> 00:19:52.090
最近，00:19:52.090 --> 00:19:55.990
第一个关于神经系统的研究是，00:19:55.990 --> 00:19:58.000
工作是，00:19:58.000 --> 00:19:59.650
斯坦福大学的一名博士生，00:19:59.650 --> 00:20:00.925
稍后我会讲到，00:20:00.925 --> 00:20:02.350
当时的想法是，00:20:02.350 --> 00:20:06.940
我们能否取代传统的复杂问答系统00:20:06.940 --> 00:20:09.954
通过使用神经阅读理解系统，00:20:09.954 --> 00:20:12.280
这被证明是非常成功的。00:20:12.280 --> 00:20:15.970
为了进一步解释这个问题，00:20:15.970 --> 00:20:20.410
如果你看看那些为TREC问答系统而建的系统，00:20:20.410 --> 00:20:24.640
它们是非常复杂的多部分系统。00:20:24.640 --> 00:20:27.565
真的，如果你再看看，00:20:27.565 --> 00:20:31.510
IBM的深度QA系统是这样的00:20:31.510 --> 00:20:35.950
乘以10，因为它们都有像这样非常复杂的系统，00:20:35.950 --> 00:20:40.465
但它在每个地方都由六种不同的成分组成，00:20:40.465 --> 00:20:41.860
然后，00:20:41.860 --> 00:20:45.220
在上面分类一个组合。00:20:45.220 --> 00:20:46.660
但到目前为止，电流-。00:20:46.660 --> 00:20:51.850
这有点像2003年的问答系统，00:20:51.850 --> 00:20:55.120
所以经历的事情是，00:20:55.120 --> 00:20:56.980
所以当有问题的时候，00:20:56.980 --> 00:20:59.470
它使用解析器解析问题00:20:59.470 --> 00:21:02.380
有点像我们在依赖解析器中看到的。00:21:02.380 --> 00:21:03.875
确实是这样00:21:03.875 --> 00:21:09.435
手写的语义规范化规则，试图使它们成为更好的语义形式。00:21:09.435 --> 00:21:13.140
然后它有一个问题类型分类器00:21:13.140 --> 00:21:16.890
找出这个问题要找的是哪种语义类型，00:21:16.890 --> 00:21:18.780
它是在寻找一个人的名字，00:21:18.780 --> 00:21:19.890
或者一个国家的名字，00:21:19.890 --> 00:21:22.860
或者温度，或者类似的东西。00:21:22.860 --> 00:21:27.825
它会，然后，00:21:27.825 --> 00:21:32.280
有一个信息检索系统从文件收集，00:21:32.280 --> 00:21:37.565
它会找到可能包含答案的段落。00:21:37.565 --> 00:21:40.510
然后它会有一个排序的方法00:21:40.510 --> 00:21:45.175
这些段落的选择，看看哪些可能有答案。00:21:45.175 --> 00:21:47.740
这样的话，00:21:47.740 --> 00:21:50.365
在那边的某个地方00:21:50.365 --> 00:21:56.320
在这些通道上运行命名实体识别，以查找其中的实体。00:21:56.320 --> 00:21:59.515
这些系统在很大程度上依赖于00:21:59.515 --> 00:22:02.350
很好的匹配实体，因为它可以寻找00:22:02.350 --> 00:22:05.755
与问题类型对应的实体。00:22:05.755 --> 00:22:09.970
一旦有了候选实体，00:22:09.970 --> 00:22:11.980
它必须试着确定00:22:11.980 --> 00:22:14.980
这些实体是否回答了这个问题。00:22:14.980 --> 00:22:18.745
这些人，这是LCC的系统，00:22:18.745 --> 00:22:21.100
Sanda Harabagiu和Dan Moldovan。00:22:21.100 --> 00:22:23.605
实际上他们有一些非常有趣的东西，00:22:23.605 --> 00:22:28.900
他们有一个松散的定理证明器试图证明，00:22:28.900 --> 00:22:31.510
一段文字的语义形式，00:22:31.510 --> 00:22:34.120
给出了问题的答案。00:22:34.120 --> 00:22:38.410
所以，你知道，这是一种很酷的东西，有一个公理知识库，00:22:38.410 --> 00:22:41.275
最终会有答案的。00:22:41.275 --> 00:22:44.305
所以，你知道，00:22:44.305 --> 00:22:46.300
我只是想强调，00:22:46.300 --> 00:22:50.050
有时候通过这些深度学习课程，00:22:50.050 --> 00:22:55.330
你给人的印象是2014年之前什么都没用，00:22:55.330 --> 00:22:57.445
当我们回到深度学习时00:22:57.445 --> 00:22:59.440
但事实并非如此。00:22:59.440 --> 00:23:01.570
所以，这类事实性的问题，00:23:01.570 --> 00:23:03.970
这些问题的回答系统00:23:03.970 --> 00:23:07.135
某一领域实际上运作得相当好。00:23:07.135 --> 00:23:10.690
所以我开始用Factoid这个词来回答问题，00:23:10.690 --> 00:23:13.120
我来解释一下，因为这就是秘密。00:23:13.120 --> 00:23:14.860
所以，至少在NLP中，00:23:14.860 --> 00:23:17.965
用“Factoid Question answer”来表示00:23:17.965 --> 00:23:21.790
您的答案是一个命名实体的情况。00:23:21.790 --> 00:23:23.890
所以，这有点像，你知道，00:23:23.890 --> 00:23:26.215
猫王是哪一年出生的，00:23:26.215 --> 00:23:32.050
碧昂丝的丈夫叫什么名字00:23:32.050 --> 00:23:35.320
你知道，哪个州，00:23:35.320 --> 00:23:38.740
嗯，有最多的猪肉之类的，我不知道。00:23:38.740 --> 00:23:40.240
对，任何东西，00:23:40.240 --> 00:23:45.205
任何答案都是某种明确的语义类型实体，00:23:45.205 --> 00:23:46.735
这就是你的答案。00:23:46.735 --> 00:23:50.935
我的意思是，在这些问题的范围内，00:23:50.935 --> 00:23:55.195
在网络搜索中，哪一个问题是很重要的?00:23:55.195 --> 00:23:58.630
很多网络搜索只是，00:23:58.630 --> 00:24:01.120
这部电影的主角是谁00:24:01.120 --> 00:24:03.355
或者某人是哪一年出生的，对吧?00:24:03.355 --> 00:24:05.785
这样的例子数不胜数。00:24:05.785 --> 00:24:08.710
这些系统确实运行得很好00:24:08.710 --> 00:24:12.070
他们可以答对70%的问题，00:24:12.070 --> 00:24:14.110
这一点都不坏00:24:14.110 --> 00:24:16.270
虽然他们并没有00:24:16.270 --> 00:24:19.380
扩展到其他种类的东西。00:24:19.380 --> 00:24:22.400
但不管他们有什么优点00:24:22.400 --> 00:24:28.280
它们是极其复杂的系统，人们花了很多年才把它们组装在一起，00:24:28.280 --> 00:24:32.885
它有很多组件和大量手工制作的东西。00:24:32.885 --> 00:24:39.035
大部分的东西都是分开建造的，绑在一起，00:24:39.035 --> 00:24:41.120
你只是希望它能成功，00:24:41.120 --> 00:24:44.045
嗯，当它们组合在一起的时候。00:24:44.045 --> 00:24:47.690
所以我们可以把它和我们后来看到的进行对比，00:24:47.690 --> 00:24:51.275
对于神经网络类型的系统。00:24:51.275 --> 00:24:57.350
好。现在让我再讲一些关于，00:24:57.350 --> 00:25:02.870
我刚才提到的斯坦福问答数据集或小组，00:25:02.870 --> 00:25:07.055
这也是默认期末项目的数据。00:25:07.055 --> 00:25:10.040
所以小队的情况是，00:25:10.040 --> 00:25:13.490
课堂上的问题有一段，00:25:13.490 --> 00:25:16.070
这是维基百科上的一段话。00:25:16.070 --> 00:25:18.425
还有一个问题，00:25:18.425 --> 00:25:21.755
这是“哪支球队赢得了超级碗50强?”00:25:21.755 --> 00:25:27.270
这个系统的目标就是找到这个问题的答案。00:25:27.270 --> 00:25:30.430
人类的阅读理解。00:25:30.430 --> 00:25:32.350
这个问题的答案是什么?00:25:32.350 --> 00:25:36.640
(噪音)00:25:36.640 --> 00:25:37.510
野马队。00:25:37.510 --> 00:25:39.130
野马队。(笑)好。00:25:39.130 --> 00:25:42.730
是的。这就是问题的答案。00:25:42.730 --> 00:25:47.060
所以通过建立小队，00:25:47.060 --> 00:25:53.570
一个问题的答案总是由文章中的单词组成的子序列，00:25:53.570 --> 00:25:56.345
通常，它最终被称为张成的空间，00:25:56.345 --> 00:25:58.580
文章中单词的子序列。00:25:58.580 --> 00:26:01.670
这是唯一的问题。00:26:01.670 --> 00:26:04.639
你不能用问题来计算问题，00:26:04.639 --> 00:26:07.130
或者是，没有问题，或者类似的问题。00:26:07.130 --> 00:26:10.475
你可以选择一个子序列。00:26:10.475 --> 00:26:12.260
嗯,好吧。00:26:12.260 --> 00:26:18.650
但是，他们在第一个版本中创造了大约10万个例子。00:26:18.650 --> 00:26:22.040
每一段都有很多问题。00:26:22.040 --> 00:26:24.200
所以这有点像，嗯，00:26:24.200 --> 00:26:28.580
我想大概每篇文章有五个问题，00:26:28.580 --> 00:26:32.315
维基百科使用了2万个不同的比特。00:26:32.315 --> 00:26:34.910
这种形式一定是张成的，00:26:34.910 --> 00:26:39.260
通常被称为抽取式问题回答。00:26:39.260 --> 00:26:43.520
好。这里还有一个例子00:26:43.520 --> 00:26:47.540
这能让你对其中的一些东西有更多的了解，00:26:47.540 --> 00:26:50.345
它还说明了其他几个因素。00:26:50.345 --> 00:26:52.760
所以，你知道，00:26:52.760 --> 00:26:56.360
即使是这个，我想前一个也不是，00:26:56.360 --> 00:26:59.600
答案很明显，因为00:26:59.600 --> 00:27:02.900
也许你会说答案应该是野马，00:27:02.900 --> 00:27:05.720
或者你可以说是丹佛野马队。00:27:05.720 --> 00:27:07.340
总的来说，00:27:07.340 --> 00:27:09.785
即使你用一个跨度来回答，00:27:09.785 --> 00:27:13.445
你选择的跨度会有变化。00:27:13.445 --> 00:27:16.040
所以他们做了什么00:27:16.040 --> 00:27:18.680
这是在土耳其的机械上完成的，00:27:18.680 --> 00:27:21.170
收集数据，或者提出问题，00:27:21.170 --> 00:27:25.790
得到答案，就是他们从三个不同的人那里得到答案。00:27:25.790 --> 00:27:26.900
这里有个问题，00:27:26.900 --> 00:27:29.810
“连同非政府和非公立学校，00:27:29.810 --> 00:27:32.029
私立学校的另一个名字是什么?”00:27:32.029 --> 00:27:35.585
根据这篇文章，三个人被问到这个问题的答案。00:27:35.585 --> 00:27:37.009
一个说独立，00:27:37.009 --> 00:27:39.485
两所是私立学校。00:27:39.485 --> 00:27:42.950
这一个，三个人都给出了相同的答案。00:27:42.950 --> 00:27:45.515
再一次，你会得到两个不同的答案，00:27:45.515 --> 00:27:48.020
所以他们抽样了三个答案。00:27:48.020 --> 00:27:52.670
基本上，如果你选择任何一个答案，你都可以是正确的。00:27:52.670 --> 00:27:59.330
这至少让你对人类答案的变化有一定的稳健性。00:27:59.330 --> 00:28:04.460
好。这让我开始了评估的话题。00:28:04.460 --> 00:28:05.855
嗯,是的。00:28:05.855 --> 00:28:08.450
这些幻灯片的题目是00:28:08.450 --> 00:28:12.140
因为这意味着在五分钟内，00:28:12.140 --> 00:28:14.600
我要给你们讲讲第二版的小队00:28:14.600 --> 00:28:16.640
这就增加了一些东西，00:28:16.640 --> 00:28:19.535
但我们先得到1。1。00:28:19.535 --> 00:28:22.895
好吧。所以我们收集了三个答案。00:28:22.895 --> 00:28:25.280
对于评估指标，00:28:25.280 --> 00:28:28.145
他们提出了两个评估指标。00:28:28.145 --> 00:28:31.340
第一个是精确匹配。00:28:31.340 --> 00:28:34.250
所以你要返回一个张成的空间。00:28:34.250 --> 00:28:37.970
如果张成的空间是这三个中的一个，00:28:37.970 --> 00:28:39.515
你得到1分，00:28:39.515 --> 00:28:40.820
如果扫描，00:28:40.820 --> 00:28:42.980
张成的空间不是这三个中的一个，00:28:42.980 --> 00:28:45.185
这个问题你得零分。00:28:45.185 --> 00:28:48.560
然后你的正确率就是正确的百分比，00:28:48.560 --> 00:28:50.345
这非常简单。00:28:50.345 --> 00:28:52.910
但是第二个度规，实际上，00:28:52.910 --> 00:28:55.984
被认为是主要度量标准的那个，00:28:55.984 --> 00:28:58.235
是F1度规。00:28:58.235 --> 00:29:01.504
对于这个F1度规00:29:01.504 --> 00:29:05.105
是你在单词层面上匹配不同的答案。00:29:05.105 --> 00:29:06.935
所以你治疗了每一个人，00:29:06.935 --> 00:29:12.275
你把系统跨度和每个黄金答案看作一个单词包，00:29:12.275 --> 00:29:14.930
然后求出一个精度，00:29:14.930 --> 00:29:22.780
系统答案中在张成的空间中的单词所占的百分比，00:29:22.780 --> 00:29:25.765
我-在一个金色的跨度里，回忆，00:29:25.765 --> 00:29:31.615
也就是在一个金跨度内的单词在系统跨度内的百分比。00:29:31.615 --> 00:29:34.720
然后计算这两个数的谐波均值00:29:34.720 --> 00:29:37.760
调和平均值是一种非常保守的平均值。00:29:37.760 --> 00:29:40.460
它接近这两个数的均值，00:29:40.460 --> 00:29:42.800
这就是分数。00:29:42.800 --> 00:29:47.375
然后你要做的是，对于每个问题，00:29:47.375 --> 00:29:50.090
你会返回，你说它的分数是00:29:50.090 --> 00:29:55.355
从人类采集的三个不同答案中F1值最大。00:29:55.355 --> 00:29:58.850
然后对于整个数据集，00:29:58.850 --> 00:30:05.195
然后你在所有问题中平均F1得分，这就是你最终的F1成绩。00:30:05.195 --> 00:30:08.345
这是一个更复杂的问题。00:30:08.345 --> 00:30:12.080
我们提供了一种val代码，00:30:12.080 --> 00:30:13.970
对你来说是这样的。00:30:13.970 --> 00:30:18.230
但是看起来F1实际上是00:30:18.230 --> 00:30:24.199
一个更可靠更好的测量方法，因为如果你使用精确匹配，00:30:24.199 --> 00:30:25.850
即使有，00:30:25.850 --> 00:30:29.525
三个人的回答都很有说服力，00:30:29.525 --> 00:30:31.940
3不是一个很大的样本，00:30:31.940 --> 00:30:34.310
所以大家可以猜测一下00:30:34.310 --> 00:30:37.760
和人类的寿命一样长，00:30:37.760 --> 00:30:41.180
而你会得到一个合理的分数00:30:41.180 --> 00:30:44.330
在F1中，即使你的边界有一点点偏离。00:30:44.330 --> 00:30:47.345
所以F1度规，00:30:47.345 --> 00:30:52.760
是更可靠的，并避免了各种工件的大小00:30:52.760 --> 00:30:58.295
或者在某些情况下，人类倾向于选择一个小的答案。00:30:58.295 --> 00:31:00.650
嗯，这是一种00:31:00.650 --> 00:31:04.955
人们在领导委员会给员工打分的主要标准。00:31:04.955 --> 00:31:07.970
最后一个细节，两个指标，00:31:07.970 --> 00:31:13.235
忽略标点符号和英语冠词a, an, the。00:31:13.235 --> 00:31:17.390
好。事情是怎么发展的?00:31:17.390 --> 00:31:21.170
对于球队版本1。1。00:31:21.170 --> 00:31:23.090
很久以前，00:31:23.090 --> 00:31:25.250
2016年底，00:31:25.250 --> 00:31:27.905
这是排行榜的样子。00:31:27.905 --> 00:31:30.680
这是排行榜的底部00:31:30.680 --> 00:31:34.145
时间允许我给你们展示一些东西。00:31:34.145 --> 00:31:36.890
所以在排行榜的底部，00:31:36.890 --> 00:31:40.520
所以他们测试了人类的表现，00:31:40.520 --> 00:31:42.830
在回答这些问题时，00:31:42.830 --> 00:31:45.875
人类也不擅长回答问题。00:31:45.875 --> 00:31:49.145
所以他们测量的人类表现，00:31:49.145 --> 00:31:52.895
F1的得分是91.2。00:31:52.895 --> 00:31:56.285
我待会再讲这个。00:31:56.285 --> 00:31:59.015
所以当他们建立数据集时，00:31:59.015 --> 00:32:04.790
他们建立了一个逻辑回归基线这是一种传统的NLP系统。00:32:04.790 --> 00:32:09.320
因此，他们依赖分析问题和句子的答案。00:32:09.320 --> 00:32:12.200
他们寻找依赖。00:32:12.200 --> 00:32:14.780
依赖链接匹配，00:32:14.780 --> 00:32:18.350
所以一个词的两端都有依赖关系in00:32:18.350 --> 00:32:23.615
在count和match之间并指向一个可能的答案。00:32:23.615 --> 00:32:29.795
这是一个相当完善的传统NLP系统00:32:29.795 --> 00:32:32.150
不像它那么复杂，但是有点00:32:32.150 --> 00:32:36.110
和我之前提到的那个问答系统是一样的。00:32:36.110 --> 00:32:39.410
F1是51。00:32:39.410 --> 00:32:41.225
所以不是没有希望，00:32:41.225 --> 00:32:43.985
但和人类比起来就没那么好了。00:32:43.985 --> 00:32:46.520
所以，在那之后不久，00:32:46.520 --> 00:32:48.635
人们开始建造00:32:48.635 --> 00:32:53.750
神经网络系统试图在这个数据集上做得更好。00:32:53.750 --> 00:32:58.040
所以，第一个成功做到这一点的人，00:32:58.040 --> 00:33:01.580
这些人来自新加坡管理大学，00:33:01.580 --> 00:33:05.150
也许不是你想到的第一个地方00:33:05.150 --> 00:33:08.870
他们是第一批证明这一点的人，是的，00:33:08.870 --> 00:33:12.320
你可以建立一个端到端的训练神经网络00:33:12.320 --> 00:33:15.320
做得更好。00:33:15.320 --> 00:33:18.935
他们达到67个F1。00:33:18.935 --> 00:33:22.100
然后他们有了第二个系统。00:33:22.100 --> 00:33:24.995
他们得到了70，然后事情开始了，00:33:24.995 --> 00:33:28.145
继续。00:33:28.145 --> 00:33:29.675
所以即使，00:33:29.675 --> 00:33:32.570
2016年底00:33:32.570 --> 00:33:38.180
开始有一些系统在这个任务上运行得很好。00:33:38.180 --> 00:33:40.985
这里，这次是，00:33:40.985 --> 00:33:42.815
嗯，在排行榜的顶端。00:33:42.815 --> 00:33:46.455
稍后我会讲到BiDAF系统，00:33:46.455 --> 00:33:48.380
人工智能,00:33:48.380 --> 00:33:51.800
艾伦人工智能研究所和华盛顿大学。00:33:51.800 --> 00:33:53.810
结果是77个a00:33:53.810 --> 00:33:57.770
在几乎所有的机器学习中，00:33:57.770 --> 00:34:00.260
人们很快就注意到如果你做了00:34:00.260 --> 00:34:03.440
一组相同结构的系统，00:34:03.440 --> 00:34:06.830
你可以把数字推得更高如果你把它们合起来，00:34:06.830 --> 00:34:11.090
然后你可以得到另一种形式，大概是4点00:34:11.090 --> 00:34:15.800
达到81，嗯，F1。00:34:15.800 --> 00:34:22.445
这大概是在2017年，00:34:22.445 --> 00:34:30.440
224N类，我们第一次使用小队版本1作为jus-作为一个默认的期末项目。00:34:30.440 --> 00:34:32.240
在那个时候，00:34:32.240 --> 00:34:36.470
事实上，最好的学生几乎达到了排行榜的顶端。00:34:36.470 --> 00:34:38.180
所以我们最好的，00:34:38.180 --> 00:34:44.239
2017年冬季CS224N期末项目，00:34:44.239 --> 00:34:47.690
相当于在排行榜上排名第四00:34:47.690 --> 00:34:51.080
他们的分数是77.5分。00:34:51.080 --> 00:34:52.790
这真的很酷。00:34:52.790 --> 00:34:56.105
但那是几年前的事了00:34:56.105 --> 00:34:58.100
人们开始建造，00:34:58.100 --> 00:35:02.780
越来越大，越来越复杂，嗯，系统。00:35:02.780 --> 00:35:06.140
所以，本质上，00:35:06.140 --> 00:35:10.790
你可以说第一个小队版本基本上已经解决了。00:35:10.790 --> 00:35:13.970
所以现在最好的系统00:35:13.970 --> 00:35:18.470
F1的分数在90分左右，00:35:18.470 --> 00:35:22.910
你可以看到，00:35:22.910 --> 00:35:25.895
系统有更高的1s和00:35:25.895 --> 00:35:31.250
比人类的精确匹配度要高。00:35:31.250 --> 00:35:34.145
但是就像很多人说的00:35:34.145 --> 00:35:37.310
深度学习更好，从人身上表现出来，00:35:37.310 --> 00:35:41.000
比起人类，你可以在后面加上星号。00:35:41.000 --> 00:35:43.520
我的意思是，特别是对于这个数据集，00:35:43.520 --> 00:35:48.125
他们测量人类表现的方法有点00:35:48.125 --> 00:35:53.870
这不公平，因为他们实际上只收集了三个人的答案。00:35:53.870 --> 00:35:58.340
所以，要判断人类的表现，00:35:58.340 --> 00:36:05.780
胡，那些胡，这些人中的每一个都被打分，而只有另外两个人得分。00:36:05.780 --> 00:36:08.780
所以，这意味着你只有两次机会去匹配，而不是三次。00:36:08.780 --> 00:36:13.820
所以，这实际上是对人类表现的一种系统的强调。00:36:13.820 --> 00:36:17.745
但是无论如何，系统在这方面做得很好。00:36:17.745 --> 00:36:20.960
所以下一步，00:36:20.960 --> 00:36:22.520
然后介绍，呃，00:36:22.520 --> 00:36:25.445
小队漫游者-第2版任务。00:36:25.445 --> 00:36:29.990
所以很多人觉得这是球队版的缺陷00:36:29.990 --> 00:36:34.985
1 .在所有情况下，问题都有答案。00:36:34.985 --> 00:36:40.445
所以，你只需要在段落中找到答案，00:36:40.445 --> 00:36:44.120
这就变成了一种排序任务。00:36:44.120 --> 00:36:48.355
你只需要找出最可能的答案。00:36:48.355 --> 00:36:50.500
我就把它退了00:36:50.500 --> 00:36:53.910
不知道这是不是问题的答案。00:36:53.910 --> 00:36:56.525
所以，对于第二版的小队，00:36:56.525 --> 00:36:58.790
对于开发和测试集，00:36:58.790 --> 00:37:01.760
一半的问题有答案，一半有00:37:01.760 --> 00:37:04.955
这些问题在文章中没有答案，00:37:04.955 --> 00:37:08.015
训练数据的分布略有不同。00:37:08.015 --> 00:37:12.785
得分的方法是，00:37:12.785 --> 00:37:18.920
没有答案就像一个单词一样是一种特殊的标记。00:37:18.920 --> 00:37:23.690
所以，如果它是，如果它应该是一个没有答案，而你说没有答案，00:37:23.690 --> 00:37:28.580
你在精确匹配或F-measure中得到1分。00:37:28.580 --> 00:37:30.560
如果你不这么做，00:37:30.560 --> 00:37:32.210
你得零分。00:37:32.210 --> 00:37:38.690
最简单的方法就是，00:37:38.690 --> 00:37:42.274
而不是总是返回系统中最好的匹配，00:37:42.274 --> 00:37:47.075
我将使用某种阈值，只有当分数高于阈值时，00:37:47.075 --> 00:37:48.785
我们的计数器和答案。00:37:48.785 --> 00:37:51.050
你可以做更复杂的事情。00:37:51.050 --> 00:37:54.080
我们在斯坦福研究的另一个领域是00:37:54.080 --> 00:37:58.520
这个自然语言推理任务，我将在以后的课程中讲到。00:37:58.520 --> 00:38:02.840
但这实际上是在说，00:38:02.840 --> 00:38:05.630
文本是另一个的结论，00:38:05.630 --> 00:38:06.890
嗯，一段文字。00:38:06.890 --> 00:38:10.670
这是一种方法，你可以试着看看，00:38:10.670 --> 00:38:17.120
一段文字实际上给了你一个理由和问题的答案。00:38:17.120 --> 00:38:21.530
但无论如何，这是在决定是否00:38:21.530 --> 00:38:27.005
在很多情况下，你是否得到了答案是一个相当困难的问题。00:38:27.005 --> 00:38:31.880
这是一个来自于SQuAD的例子，嗯，2。0。00:38:31.880 --> 00:38:35.120
所以成吉思汗统一了蒙古和突厥部落00:38:35.120 --> 00:38:38.855
在1206年成为大汗。00:38:38.855 --> 00:38:42.290
他和他的继任者将蒙古帝国扩展到了亚洲，00:38:42.290 --> 00:38:43.940
等等，等等，等等。00:38:43.940 --> 00:38:45.635
问题是，00:38:45.635 --> 00:38:48.260
成吉思汗什么时候杀的大汗?00:38:48.260 --> 00:38:50.480
答案是，00:38:50.480 --> 00:38:53.525
你知道，呃，没有答案，因为实际上，00:38:53.525 --> 00:38:59.150
成吉思汗是一个叫大汗的人，他没有杀死大汗。00:38:59.150 --> 00:39:01.835
这不是一个有答案的问题。00:39:01.835 --> 00:39:07.985
但这正是系统所发生的，00:39:07.985 --> 00:39:11.645
即使这些系统在分数方面得分很高，00:39:11.645 --> 00:39:15.980
他们其实不太懂人类语言。00:39:15.980 --> 00:39:17.615
所以他们看到的是，00:39:17.615 --> 00:39:20.855
成吉思汗什么时候杀的大汗?00:39:20.855 --> 00:39:23.930
这是一个寻找约会对象的东西00:39:23.930 --> 00:39:27.740
这篇文章中一些明显的日期是1206 1234，00:39:27.740 --> 00:39:31.835
1251年，00:39:31.835 --> 00:39:36.560
kill看起来有点像destroy。00:39:36.560 --> 00:39:38.645
我看到这个词被毁了。00:39:38.645 --> 00:39:41.345
所以这可能是匹配的。00:39:41.345 --> 00:39:43.400
然后我们说，嗯，00:39:43.400 --> 00:39:45.560
成吉思汗，00:39:45.560 --> 00:39:48.395
我可以在这段文字中看到成吉思汗和成吉思汗。00:39:48.395 --> 00:39:50.960
所以它把这些放在一起说00:39:50.960 --> 00:39:55.175
1234是答案，而不是答案。00:39:55.175 --> 00:39:59.870
这就是这些系统的典型行为。00:39:59.870 --> 00:40:03.560
所以，一方面，他们工作得很好。00:40:03.560 --> 00:40:06.155
另一方面，他们实际上并不了解那么多，00:40:06.155 --> 00:40:10.025
有效地问，00:40:10.025 --> 00:40:14.930
这个问题在文中实际上是被回答的一种方式00:40:14.930 --> 00:40:17.360
揭示了这些模型00:40:17.360 --> 00:40:20.945
做或不知道到底发生了什么。00:40:20.945 --> 00:40:23.915
好。所以，在那个时候，00:40:23.915 --> 00:40:27.095
他们建立了小队2.0版。00:40:27.095 --> 00:40:28.835
他们拿了一些，00:40:28.835 --> 00:40:32.090
现有的小队版本one的系统，00:40:32.090 --> 00:40:36.725
然后，呃，用一种非常简单的方式修改了它们。00:40:36.725 --> 00:40:39.275
我设置了一个门槛，00:40:39.275 --> 00:40:43.175
最后一场比赛被认为有多好，00:40:43.175 --> 00:40:47.645
然后说，你在2。0的表现如何?00:40:47.645 --> 00:40:50.825
我们之前看到的系统做得很好，00:40:50.825 --> 00:40:52.370
现在做得不太好，00:40:52.370 --> 00:40:58.820
所以像我们之前提到的BiDAF系统现在得分是62 F1，00:40:58.820 --> 00:41:01.370
这就大大降低了00:41:01.370 --> 00:41:05.210
它的表现和反映了认识的局限性。00:41:05.210 --> 00:41:09.650
但事实上这个问题并没有被证明00:41:09.650 --> 00:41:14.240
和数据集作者一样难，00:41:14.240 --> 00:41:16.820
也许我也这么想。00:41:16.820 --> 00:41:19.775
因为事实证明，00:41:19.775 --> 00:41:23.375
现在是2019年2月，00:41:23.375 --> 00:41:26.285
如果你看看排行榜的顶部，00:41:26.285 --> 00:41:29.465
我们又接近正题了00:41:29.465 --> 00:41:32.780
最好的系统几乎和人类一样好。00:41:32.780 --> 00:41:39.080
所以，嗯，你可以看到当前的最高利率系统是87.6 F1，00:41:39.080 --> 00:41:43.220
比人类落后不到两点。00:41:43.220 --> 00:41:47.510
在第二版中他们还合作更正了，00:41:47.510 --> 00:41:49.400
人类的得分，00:41:49.400 --> 00:41:52.805
所以这次的评估比较公平，00:41:52.805 --> 00:41:54.920
所以仍然有一些差距，但是，你知道，00:41:54.920 --> 00:41:58.010
系统运行得很好。00:41:58.010 --> 00:42:01.040
有趣的是，00:42:01.040 --> 00:42:04.625
你知道，一方面这些系统非常好。00:42:04.625 --> 00:42:06.890
你可以去球队网站看看00:42:06.890 --> 00:42:09.275
在好几个系统的输出中，00:42:09.275 --> 00:42:12.335
你可以看到他们做对了很多事情。00:42:12.335 --> 00:42:14.330
它们绝对不是坏系统。00:42:14.330 --> 00:42:18.980
你必须有一个好的系统才能答对6道题中的5道。00:42:18.980 --> 00:42:21.860
但是，另一方面，他们仍然00:42:21.860 --> 00:42:25.130
犯一些基本的自然语言理解错误。00:42:25.130 --> 00:42:28.295
这里有一个例子。00:42:28.295 --> 00:42:29.720
好的，这个，00:42:29.720 --> 00:42:32.540
元朝被认为是两者的继承者00:42:32.540 --> 00:42:36.155
蒙古帝国和中国的一个帝国王朝。00:42:36.155 --> 00:42:38.840
它是由汗国的继承者统治的00:42:38.840 --> 00:42:42.665
蒙古帝国分裂后的蒙古可汗。00:42:42.665 --> 00:42:46.730
在中国的官方历史中，元朝肩负着天命，00:42:46.730 --> 00:42:50.480
宋朝之后，明朝之前。00:42:50.480 --> 00:42:52.655
好。问题是，00:42:52.655 --> 00:42:55.760
元以前是哪个朝代?00:42:55.760 --> 00:42:58.490
这是个很简单的问题，00:42:58.490 --> 00:42:59.990
我希望，作为一个人。00:42:59.990 --> 00:43:02.790
每个人都能回答这个问题吗?00:43:02.830 --> 00:43:08.480
好的，嗯，是的，所以在中国的官方历史上，元朝说，00:43:08.480 --> 00:43:09.920
对不起，下一句。00:43:09.920 --> 00:43:12.560
嗯，对，元朝之后00:43:12.560 --> 00:43:15.245
宋朝及明朝以前。00:43:15.245 --> 00:43:17.555
但是，实际上，00:43:17.555 --> 00:43:20.960
这种领先的，00:43:20.960 --> 00:43:25.310
谷歌伯特模型说，这是明朝之前00:43:25.310 --> 00:43:29.450
元朝是一个基本的00:43:29.450 --> 00:43:33.320
这是错误的，它揭示了同样的问题00:43:33.320 --> 00:43:38.240
并没有真正理解所有的东西但是它仍然在做一个匹配的问题。00:43:38.240 --> 00:43:45.620
好。因此，这个阵容数据集非常有用。00:43:45.620 --> 00:43:48.860
它仍然有一些主要的限制，我只是想我会00:43:48.860 --> 00:43:52.370
提到了其中的一些，所以你们知道其中的一些问题。00:43:52.370 --> 00:43:54.950
其中一个我已经提过了，00:43:54.950 --> 00:44:00.740
你在这个空间里所有的答案都是由文章张成的。00:44:00.740 --> 00:44:03.890
这就限制了问题的种类00:44:03.890 --> 00:44:07.025
问一问，会有什么样的困难情况。00:44:07.025 --> 00:44:10.370
所以，不可能是-没有问题计数00:44:10.370 --> 00:44:15.785
问题，甚至是任何比较难的隐性问题。00:44:15.785 --> 00:44:21.185
所以，如果你回想一下你上中学时的阅读理解，00:44:21.185 --> 00:44:23.825
我的意思是，这不是典型的，00:44:23.825 --> 00:44:27.440
你被问到的那个案子00:44:27.440 --> 00:44:31.400
这些问题在，00:44:31.400 --> 00:44:34.880
苏要去迈阿密看她妈妈。00:44:34.880 --> 00:44:36.335
问题是，00:44:36.335 --> 00:44:38.315
谁在迈阿密访问?00:44:38.315 --> 00:44:43.730
那不是你被问到的那种问题你通常会被问到的问题，00:44:43.730 --> 00:44:46.310
比如，你知道，00:44:46.310 --> 00:44:52.505
苏今天早上要去面试工作00:44:52.505 --> 00:44:56.360
这对她未来的工作面试非常重要。00:44:56.360 --> 00:44:59.435
早餐时，00:44:59.435 --> 00:45:03.395
开始在烤面包片的两面涂黄油00:45:03.395 --> 00:45:06.410
有人问你一个问题，00:45:06.410 --> 00:45:11.320
为什么苏要在烤面包片的两面涂黄油?00:45:11.320 --> 00:45:13.420
你应该能够回答，00:45:13.420 --> 00:45:17.680
“她被当天晚些时候的重要工作面试分散了注意力。”00:45:17.680 --> 00:45:20.995
这不是你可以回答的问题，00:45:20.995 --> 00:45:23.505
只要选出一个张成的子空间。00:45:23.505 --> 00:45:31.055
第二个更大的问题是，00:45:31.055 --> 00:45:35.645
这支队伍的结构很简单00:45:35.645 --> 00:45:41.970
不要太贵，还有其他原因，00:45:41.970 --> 00:45:46.235
选择维基百科的段落，00:45:46.235 --> 00:45:48.680
雇佣机械工人的目的是说，00:45:48.680 --> 00:45:51.215
"提出一些问题，00:45:51.215 --> 00:45:56.210
这可以在1.1版的这篇文章中找到答案。”00:45:56.210 --> 00:45:59.315
在第二版中，他们被告知，00:45:59.315 --> 00:46:03.170
也要提出一些问题00:46:03.170 --> 00:46:07.385
看起来它们与这篇文章有关，但实际上并没有在这篇文章中得到回答。”00:46:07.385 --> 00:46:10.070
但是，在所有的案例中，人们都提出了00:46:10.070 --> 00:46:14.870
盯着文章的问题，如果你这样做，00:46:14.870 --> 00:46:18.260
这意味着你的问题很强烈00:46:18.260 --> 00:46:21.905
与这篇文章重叠的是，00:46:21.905 --> 00:46:26.630
所使用的单词，甚至是语法结构00:46:26.630 --> 00:46:31.520
用于你的问题趋向于符合文章的句法结构。00:46:31.520 --> 00:46:37.085
这样回答问题就很简单了。00:46:37.085 --> 00:46:39.125
现实世界中发生了什么，00:46:39.125 --> 00:46:42.260
这是人类想出来的问题吗00:46:42.260 --> 00:46:46.010
在搜索引擎里输入一些东西00:46:46.010 --> 00:46:49.355
他们输入的是完全不同的00:46:49.355 --> 00:46:53.075
从网站上的用词来看。00:46:53.075 --> 00:46:56.600
所以他们可能会说，00:46:56.600 --> 00:47:02.720
你知道，“哪一年硬盘的价格降到每兆字节1美元以下?”00:47:02.720 --> 00:47:07.220
网页上会这样写00:47:07.220 --> 00:47:12.050
多年来，硬盘的价格一直在下降，00:47:12.050 --> 00:47:18.470
在2004年的时候，00:47:18.470 --> 00:47:20.870
兆字节的屏障之类的。00:47:20.870 --> 00:47:24.785
但对这些想法的讨论却截然不同。00:47:24.785 --> 00:47:28.220
这种匹配要困难得多，这是其中之一00:47:28.220 --> 00:47:32.270
人们在其他数据集上所做的事情也尝试着有所不同。00:47:32.270 --> 00:47:35.960
另一个限制是这些问题00:47:35.960 --> 00:47:40.355
答案很重要，找到表达事实的句子，00:47:40.355 --> 00:47:42.545
把你的问题和句子搭配起来，00:47:42.545 --> 00:47:45.080
返回正确的东西，00:47:45.080 --> 00:47:49.400
没有什么比多句话更难的了，00:47:49.400 --> 00:47:53.210
把事实和推理方式结合起来，00:47:53.210 --> 00:47:57.050
交叉句的限制是非常有限的00:47:57.050 --> 00:48:01.295
解决共同参照，这是我们在以后的课上会讲到的，00:48:01.295 --> 00:48:04.310
这意味着你看到一个他或她或它，00:48:04.310 --> 00:48:09.125
你可以在这门课的早些时候算出这是谁。00:48:09.125 --> 00:48:12.590
尽管有这些缺点，00:48:12.590 --> 00:48:15.230
这就证明了这支球队，00:48:15.230 --> 00:48:20.180
它的难度目标明确，结构合理，00:48:20.180 --> 00:48:22.910
干净的数据集，它只是00:48:22.910 --> 00:48:27.140
这是大家最喜欢的回答问题的数据集。00:48:27.140 --> 00:48:30.080
它似乎也证明了这一点00:48:30.080 --> 00:48:33.530
在工业界工作的人想要建立一个问答系统，00:48:33.530 --> 00:48:36.005
从训练一个模型开始，00:48:36.005 --> 00:48:39.230
实际上效果很好。00:48:39.230 --> 00:48:41.420
我是说，这不是你想做的所有事情。00:48:41.420 --> 00:48:46.250
你肯定想要有相关的领域数据，并使用它，00:48:46.250 --> 00:48:50.450
但你知道，事实证明这似乎是一个非常有用的起点。00:48:50.450 --> 00:48:55.865
好。所以，我现在想给你们展示的是一个混凝土，00:48:55.865 --> 00:49:00.710
简单，神经问题回答系统。00:49:00.710 --> 00:49:08.300
嗯，这是这里做的模型，我猜她是00:49:08.300 --> 00:49:15.860
有点像Abby的前任因为她是CS 224N的前任助教。00:49:15.860 --> 00:49:18.650
所以这个系统，00:49:18.650 --> 00:49:21.830
斯坦福的细心读者，现在有点叫了。00:49:21.830 --> 00:49:24.575
我的意思是，这基本上是00:49:24.575 --> 00:49:29.990
最简单的神经问题回答系统工作得很好。00:49:29.990 --> 00:49:32.780
所以，这并不是一件坏事00:49:32.780 --> 00:49:36.320
一个基线，而且无论如何它都不是当前的技术状态。00:49:36.320 --> 00:49:40.790
但是如果你想知道我能做的最简单的东西是什么00:49:40.790 --> 00:49:45.215
这基本上相当于一个体面的问答系统，00:49:45.215 --> 00:49:47.315
基本上就是这样。00:49:47.315 --> 00:49:50.390
嗯,好吧。这是怎么做到的呢?00:49:50.390 --> 00:49:52.595
它是这样工作的。00:49:52.595 --> 00:49:53.930
首先，00:49:53.930 --> 00:49:58.205
我们有一个问题，哪支球队赢得了超级碗50强?00:49:58.205 --> 00:50:04.175
我们要做的是建立一个问题的向量表示。00:50:04.175 --> 00:50:06.920
我们可以这样做，00:50:06.920 --> 00:50:09.035
对于问题中的每个单词，00:50:09.035 --> 00:50:10.835
我们查找一个单词嵌入。00:50:10.835 --> 00:50:15.440
所以，它特别使用了手套-手套300维单词嵌入。00:50:15.440 --> 00:50:19.235
然后我们运行LSTM00:50:19.235 --> 00:50:23.330
然后就像艾比说的那样，00:50:23.330 --> 00:50:25.295
我们把它写成bi-LSTM。00:50:25.295 --> 00:50:29.030
所以，我们倒着运行第二个LSTM。00:50:29.030 --> 00:50:34.880
然后，我们获取两个LSTMs的最终状态00:50:34.880 --> 00:50:40.760
我们只要把它们连接到一个二维的向量中，00:50:40.760 --> 00:50:43.730
如果LSTM的隐藏状态是维度00:50:43.730 --> 00:50:48.425
这就是问题的表示。00:50:48.425 --> 00:50:51.245
好。一旦我们有了这个，00:50:51.245 --> 00:50:54.230
然后我们开始看文章。00:50:54.230 --> 00:50:57.635
所以，在处理这篇文章的开始，00:50:57.635 --> 00:50:59.180
我们做同样的事情。00:50:59.180 --> 00:51:03.110
我们，嗯，为每一个单词查找一个向量00:51:03.110 --> 00:51:07.340
通道和我们运行一个双向LSTM，00:51:07.340 --> 00:51:12.200
现在被更简洁地表示出来了。00:51:12.200 --> 00:51:15.710
但是我们还需要做更多的工作因为实际上00:51:15.710 --> 00:51:19.040
必须在短文中找到答案。00:51:19.040 --> 00:51:21.680
我们要做的是00:51:21.680 --> 00:51:28.175
用问题表示来找出答案在哪里使用注意力。00:51:28.175 --> 00:51:31.805
这是对机器翻译的另一种关注。00:51:31.805 --> 00:51:35.105
这种注意方程仍然是一样的。00:51:35.105 --> 00:51:39.170
但现在我们有了一个问题向量，我们要尝试00:51:39.170 --> 00:51:43.385
匹配以返回答案。00:51:43.385 --> 00:51:47.150
所以我们要做的是00:51:47.150 --> 00:51:51.125
计算出他们之间的注意力得分00:51:51.125 --> 00:51:57.575
每个单词的bi-LSTM表示和问题。00:51:57.575 --> 00:52:02.930
我们要做的就是用这种双线注意，00:52:02.930 --> 00:52:07.370
艾比简单地讨论了一下，我们今天会看到更多。00:52:07.370 --> 00:52:09.140
我们有了问题向量，00:52:09.140 --> 00:52:12.530
在段落中特定位置的向量00:52:12.530 --> 00:52:15.770
连接到两个LSTM隐藏状态。00:52:15.770 --> 00:52:17.930
所以它们是相同的维度。00:52:17.930 --> 00:52:21.020
我们有这个中间的学习W矩阵。00:52:21.020 --> 00:52:23.360
我们算出这个量，00:52:23.360 --> 00:52:25.115
对于每个位置，00:52:25.115 --> 00:52:27.890
然后我们把它放到softmax中00:52:27.890 --> 00:52:32.180
文章中不同单词的概率。00:52:32.180 --> 00:52:34.220
那些给了我们，00:52:34.220 --> 00:52:36.665
我们的注意力重量。00:52:36.665 --> 00:52:39.350
在这一点上我们有了注意力权重，00:52:39.350 --> 00:52:42.140
对于不同的位置，00:52:42.140 --> 00:52:45.410
在文章中，我们只是声明，00:52:45.410 --> 00:52:47.030
那就是，00:52:47.030 --> 00:52:49.610
答案开始了。00:52:49.610 --> 00:52:53.270
为了得到答案，00:52:53.270 --> 00:53:01.310
我们只是做同样的事情除了我们在这里训练一个不同的W矩阵，00:53:01.310 --> 00:53:02.840
我们有这个，00:53:02.840 --> 00:53:04.940
预测结束标记。00:53:04.940 --> 00:53:07.490
这里有一些微妙的东西。00:53:07.490 --> 00:53:10.610
因为，你知道，我们是在要求它排序00:53:10.610 --> 00:53:13.685
预测答案的开始和结束，00:53:13.685 --> 00:53:15.830
你可能会想，等等。00:53:15.830 --> 00:53:19.595
当然，我们也需要看看答案的中间部分，因为，00:53:19.595 --> 00:53:23.405
最具指示性的单词实际上是在答案的中间。00:53:23.405 --> 00:53:27.710
但是，你知道，我们是什么，00:53:27.710 --> 00:53:32.960
我们含蓄地告诉模型，00:53:32.960 --> 00:53:37.055
当你训练的时候，如果中间有有用的东西，00:53:37.055 --> 00:53:42.440
这是bi-LSTM的工作，把它推向跨度的极限，00:53:42.440 --> 00:53:47.075
这个简单的双线注意00:53:47.075 --> 00:53:51.950
将能够得到一个大的分数在开始的跨度。00:53:51.950 --> 00:53:55.040
你可能也认为有一些东西00:53:55.040 --> 00:53:58.370
有趣的是这个方程和那个方程完全一样。00:53:58.370 --> 00:54:02.270
那么，为什么其中一个要知道它开始回升，00:54:02.270 --> 00:54:04.400
最后呢?00:54:04.400 --> 00:54:07.475
再说一次，你知道，我们没有做任何事情来强加于人。00:54:07.475 --> 00:54:09.890
我们只是说，神经网络。00:54:09.890 --> 00:54:11.915
学习是你的工作。00:54:11.915 --> 00:54:16.115
你必须在这里学习一个矩阵，在那里学习另一个矩阵，00:54:16.115 --> 00:54:20.240
所以其中一个会挑出那个的部分表示00:54:20.240 --> 00:54:25.175
表示应答范围的开始和另一个应答范围的结束。00:54:25.175 --> 00:54:28.160
所以，这又会产生压力00:54:28.160 --> 00:54:31.550
神经网络可以自我组织00:54:31.550 --> 00:54:34.100
这样就会有一部分00:54:34.100 --> 00:54:38.270
这种隐藏的表现形式将有利于跨度的学习。00:54:38.270 --> 00:54:40.010
你知道，也许会有一些倒退00:54:40.010 --> 00:54:43.520
后面的LSTM和它的一些部分会很擅长00:54:43.520 --> 00:54:45.980
学习跨越的终点00:54:45.980 --> 00:54:50.610
W矩阵将能够找出表示的那些部分。00:54:50.610 --> 00:54:54.130
但是，是的，00:54:54.130 --> 00:54:58.360
这是系统。嗯,是的。00:54:58.360 --> 00:55:00.640
这是00:55:00.640 --> 00:55:05.980
基本的斯坦福专注读者模型，它并不比这更复杂。00:55:05.980 --> 00:55:08.770
有趣的是，00:55:08.770 --> 00:55:14.245
这个非常简单的模型实际上运行得很好。00:55:14.245 --> 00:55:16.360
这是时光倒流。00:55:16.360 --> 00:55:23.230
同样，这是2017年2月的球队版本1排行榜。00:55:23.230 --> 00:55:28.690
但在那个时候00:55:28.690 --> 00:55:32.680
它总是在神经网络中相当成功00:55:32.680 --> 00:55:39.280
训练你的超参数并优化你的模型。00:55:39.280 --> 00:55:41.260
有时候，你知道，00:55:41.260 --> 00:55:47.020
这在神经网络领域得到了反复的证明00:55:47.020 --> 00:55:50.170
比你想象的要好得多00:55:50.170 --> 00:55:53.845
非常简单的模型，如果你优化得很好。00:55:53.845 --> 00:55:57.280
所以有很多循环00:55:57.280 --> 00:55:59.830
那里有深度学习研究00:55:59.830 --> 00:56:02.950
是一篇做了一些事情的论文，然后下一个人说，00:56:02.950 --> 00:56:04.960
“这是一个越来越复杂的模型00:56:04.960 --> 00:56:07.540
然后另一个人发表了一篇论文，00:56:07.540 --> 00:56:09.640
“这里有一个比那个模型更复杂的模型00:56:09.640 --> 00:56:12.490
然后有人指出，“不。”00:56:12.490 --> 00:56:17.140
如果你回到第一个模型并很好地训练它的超参数，00:56:17.140 --> 00:56:19.375
你可以打败这两种模式。”00:56:19.375 --> 00:56:21.880
这实际上是关于什么的00:56:21.880 --> 00:56:24.610
发生在斯坦福大学专注的读者身上。00:56:24.610 --> 00:56:29.245
回到2017年2月，00:56:29.245 --> 00:56:32.920
如果你很好地训练这个模型，00:56:32.920 --> 00:56:37.990
它实际上可以超过大多数早期的阵容系统。00:56:37.990 --> 00:56:39.235
我的意思是，特别是，00:56:39.235 --> 00:56:41.875
它可能会胜过BiDAF，00:56:41.875 --> 00:56:46.390
2017年初推出的BiDAF版本，00:56:46.390 --> 00:56:49.315
你知道，这些不同的系统来自于其他人。00:56:49.315 --> 00:56:51.340
但实际上，在那个时候，00:56:51.340 --> 00:56:55.405
这几乎是世界上最好的系统了。00:56:55.405 --> 00:56:57.970
我已经说过了00:56:57.970 --> 00:57:00.280
从那以后，这个数字上升了很多。00:57:00.280 --> 00:57:02.500
所以我不是说，00:57:02.500 --> 00:57:08.785
这个系统仍然是您可以构建的最好的系统。好了。00:57:08.785 --> 00:57:13.000
这个简单的系统已经运行得很好了，00:57:13.000 --> 00:57:15.070
但是你当然希望这个系统能更好地工作。00:57:15.070 --> 00:57:19.690
丹琪在这方面做了很多工作。00:57:19.690 --> 00:57:23.305
在这里我要提几件事，00:57:23.305 --> 00:57:26.125
斯坦福专注的读者++ as00:57:26.125 --> 00:57:29.635
你能做些什么使模型更好。00:57:29.635 --> 00:57:34.705
这是一张图片，00:57:34.705 --> 00:57:37.960
改进后的系统，我们会讲到00:57:37.960 --> 00:57:41.290
一些不同之处，以及是什么让它变得更好。00:57:41.290 --> 00:57:45.190
嗯，有一些我以前没有的东西我应该提一下，对吗?00:57:45.190 --> 00:57:50.215
整个模型的所有参数都是从头到尾训练的，00:57:50.215 --> 00:57:53.980
你的培训目标很简单00:57:53.980 --> 00:57:56.380
计算出你的预测有多准确00:57:56.380 --> 00:57:59.050
开始位置和预测的准确性00:57:59.050 --> 00:58:02.680
结束的位置使注意力给予00:58:02.680 --> 00:58:06.505
得到开始位置和结束位置的概率分布。00:58:06.505 --> 00:58:09.820
题目问的是概率估计00:58:09.820 --> 00:58:13.330
你给出的是真实的开始位置和结束位置。00:58:13.330 --> 00:58:15.250
在某种程度上，00:58:15.250 --> 00:58:17.289
你知道，那不是一个，00:58:17.289 --> 00:58:22.375
然后你得到损失然后用对数概率来求和。00:58:22.375 --> 00:58:25.570
好。这个模型怎么样，00:58:25.570 --> 00:58:28.855
比我之前展示的更复杂?00:58:28.855 --> 00:58:31.945
主要有两种方式。00:58:31.945 --> 00:58:36.370
第一个问题是，00:58:36.370 --> 00:58:40.075
我们仍然像以前一样经营BiLSTM。00:58:40.075 --> 00:58:44.530
但是现在我们要做的是有点粗糙00:58:44.530 --> 00:58:48.850
将LSTM的结束状态连接起来。00:58:48.850 --> 00:58:54.280
事实证明，您可以更好地利用LSTM中的所有状态。00:58:54.280 --> 00:58:57.880
这适用于大多数任务00:58:57.880 --> 00:59:01.975
想要从序列模型中得到某种句子表示。00:59:01.975 --> 00:59:04.585
事实证明，你通常可以通过使用00:59:04.585 --> 00:59:07.510
而不仅仅是端点。00:59:07.510 --> 00:59:12.685
但是这是一个很有趣的常识因为，00:59:12.685 --> 00:59:18.415
这实际上是另一种变体――你如何使用注意力。00:59:18.415 --> 00:59:25.525
你知道，过去两年的神经NLP有很多可以总结00:59:25.525 --> 00:59:29.230
随着人们发现了许多聪明的使用方法00:59:29.230 --> 00:59:33.220
注意力与所有的进步相匹配。00:59:33.220 --> 00:59:41.890
我们想做的是我们想要关注LSTM中的位置。00:59:41.890 --> 00:59:46.255
但是，你知道，我们先处理查询。00:59:46.255 --> 00:59:51.355
所以我们似乎没有计算关于什么的注意。00:59:51.355 --> 00:59:55.150
所以我们要做的就是发明一些东西。00:59:55.150 --> 00:59:56.860
所以我们只是发明。00:59:56.860 --> 01:00:01.660
这是一个矢量，它有时被称为哨兵，或者类似的词，01:00:01.660 --> 01:00:03.850
但是，你知道，我们只是在火炬里说，01:00:03.850 --> 01:00:05.185
这是一个向量。01:00:05.185 --> 01:00:07.690
我们要计算，01:00:07.690 --> 01:00:09.460
我们随机初始化它，01:00:09.460 --> 01:00:13.495
我们要计算关于这个向量的注意，01:00:13.495 --> 01:00:20.950
我们要用这些注意力分数，01:00:20.950 --> 01:00:24.250
找出要注意的地方01:00:24.250 --> 01:00:30.625
在这个BiLSTM中，然后我们训练这个向量使它得到值。01:00:30.625 --> 01:00:34.270
然后我们得到时间的加权和01:00:34.270 --> 01:00:39.430
LSTM的步骤，然后形成问题表示。01:00:39.430 --> 01:00:42.370
第二次改变01:00:42.370 --> 01:00:45.400
照片上只有一个浅浅的BiLSTM，01:00:45.400 --> 01:00:48.940
事实证明，如果你有一个深刻的认识，你可以做得更好01:00:48.940 --> 01:00:53.005
使用三层的深度BiLSTM而不是单层。01:00:53.005 --> 01:00:56.200
好。然后其他的变化01:00:56.200 --> 01:01:02.350
文章的表述和这部分可能会变得有些粗糙，01:01:02.350 --> 01:01:06.520
嗯，但是我想，你可以做一些事情来让数字上升。01:01:06.520 --> 01:01:07.810
嗯,好吧。01:01:07.810 --> 01:01:13.840
So- So首先用于表示单词而不是仅仅使用01:01:13.840 --> 01:01:18.070
手套表示输入向量01:01:18.070 --> 01:01:24.055
展开后，运行一个命名实体识别器和部分语音标记器。01:01:24.055 --> 01:01:28.615
因为这些都是一组很小的值，01:01:28.615 --> 01:01:33.910
它们的输出只是一个热编码并连接到01:01:33.910 --> 01:01:36.490
向量这个词，如果它是01:01:36.490 --> 01:01:40.195
一个地点或一个人的名字，无论是名词还是动词。01:01:40.195 --> 01:01:44.080
词频是很有用的。01:01:44.080 --> 01:01:52.165
这就是你连接的单词频率的表示形式，01:01:52.165 --> 01:01:57.370
只是一种一元概率的浮点数。01:01:57.370 --> 01:02:05.335
这部分是取得进一步进展的关键，01:02:05.335 --> 01:02:11.140
事实证明，我们可以通过做一些事情来做得更好01:02:11.140 --> 01:02:16.945
更好地理解问题和文章之间的搭配。01:02:16.945 --> 01:02:20.170
这个功能看起来是01:02:20.170 --> 01:02:23.815
很简单，但实际上给了你很多价值。01:02:23.815 --> 01:02:28.420
所以你只是对问题中的每个单词说，01:02:28.420 --> 01:02:32.215
所以我说错了每个词01:02:32.215 --> 01:02:35.920
对于文章中的每个单词，01:02:35.920 --> 01:02:39.040
你刚才说，“这个词出现在问题中了吗?”01:02:39.040 --> 01:02:42.160
如果是这样的话，你就把一比特放入01:02:42.160 --> 01:02:46.105
输入有三种方式:精确匹配，01:02:46.105 --> 01:02:48.580
无大小写匹配和引理匹配。01:02:48.580 --> 01:02:51.655
这就意味着开车，01:02:51.655 --> 01:02:53.590
会匹配吗01:02:53.590 --> 01:02:56.755
这是问题所在。01:02:56.755 --> 01:02:59.230
理论上，系统应该能够解决这个问题01:02:59.230 --> 01:03:03.115
无论如何，这明确地表明，它给出了相当多的价值。01:03:03.115 --> 01:03:09.310
最后一个是一个更柔和的版本它使用的是文字01:03:09.310 --> 01:03:12.550
嵌入类似于排序的计算01:03:12.550 --> 01:03:16.210
问题和答案之间的相似性，01:03:16.210 --> 01:03:19.345
这是一个稍微复杂的方程。01:03:19.345 --> 01:03:26.035
但实际上，你得到的是词语的嵌入和问题的答案。01:03:26.035 --> 01:03:30.085
每一个，你运行在一个单独的隐藏层，01:03:30.085 --> 01:03:31.585
神经网络，01:03:31.585 --> 01:03:35.245
点积它，然后把所有这些都放到Softmax中，01:03:35.245 --> 01:03:40.820
这给了你一个单词相似度的评分，这也有帮助。01:03:41.040 --> 01:03:46.510
好。这是它给你的整体图像。01:03:46.510 --> 01:03:49.435
如果你还记得，01:03:49.435 --> 01:03:52.540
有一种经典的NLP01:03:52.540 --> 01:03:55.825
使用logistic回归基线，大约有51个。01:03:55.825 --> 01:03:58.810
对于一个相当简单的模型，01:03:58.810 --> 01:04:00.970
就像斯坦福专注的读者一样，01:04:00.970 --> 01:04:03.760
它会给你一个巨大的提升，对吧?01:04:03.760 --> 01:04:07.765
这使您的性能提高了近30%。01:04:07.765 --> 01:04:10.180
然后，你知道，从那里，01:04:10.180 --> 01:04:13.420
人们一直在推动神经系统的发展。01:04:13.420 --> 01:04:17.410
但是，你知道，这给了你某种程度上四分之三的01:04:17.410 --> 01:04:22.525
相对于传统的NLP系统，01:04:22.525 --> 01:04:26.080
复杂的神经系统。01:04:26.080 --> 01:04:27.145
嗯,是的。01:04:27.145 --> 01:04:28.555
在减少误差方面，01:04:28.555 --> 01:04:31.780
它们很大，但更像是给你，01:04:31.780 --> 01:04:34.880
之后是12%01:04:35.310 --> 01:04:43.030
为什么这些系统比传统系统运行得更好呢?01:04:43.030 --> 01:04:46.750
所以我们做了一些误差分析，01:04:46.750 --> 01:04:52.180
事实证明，他们的大部分收益是因为他们能做01:04:52.180 --> 01:04:56.890
更好的词语相似性语义匹配01:04:56.890 --> 01:05:02.080
或重新措辞，语义上相关，但不使用相同的词。01:05:02.080 --> 01:05:10.675
那么，在某种程度上，问题是克里斯托弗・曼宁是在哪里出生的?01:05:10.675 --> 01:05:15.595
判决称克里斯托弗・曼宁出生在澳大利亚，01:05:15.595 --> 01:05:18.790
传统的NLP系统也能做到这一点。01:05:18.790 --> 01:05:21.565
但只要你能做对，01:05:21.565 --> 01:05:23.980
取决于能否匹配，01:05:23.980 --> 01:05:29.575
更松散的语义匹配这样我们就能理解，01:05:29.575 --> 01:05:33.610
你知道，出生地点必须与出生地点相匹配。01:05:33.610 --> 01:05:37.750
这就是神经系统工作得更好的地方。01:05:37.750 --> 01:05:44.950
好。这还不是问答系统的全部内容。01:05:44.950 --> 01:05:48.400
我想说一点关于，01:05:48.400 --> 01:05:51.670
更复杂的系统给你一些概念，01:05:51.670 --> 01:05:53.725
之后会发生什么。01:05:53.725 --> 01:05:56.260
但在我深入之前，01:05:56.260 --> 01:05:59.980
有什么问题吗?01:05:59.980 --> 01:06:03.130
到目前为止，斯坦福专注的读者们?01:06:03.130 --> 01:06:09.760
(噪音)是的。01:06:09.760 --> 01:06:12.925
我有一个关于注意力的问题。01:06:12.925 --> 01:06:18.550
我们见过的每一个例子都是一个权矩阵的线性映射。01:06:18.550 --> 01:06:23.695
有人试过把它转换成深层神经网络看看会发生什么吗?01:06:23.695 --> 01:06:26.335
是的，他们有。01:06:26.335 --> 01:06:30.040
至少是一个浅层的神经网络。01:06:30.040 --> 01:06:33.010
我马上会展示一个例子。01:06:33.010 --> 01:06:35.800
也许我会把它保存到那个时候。01:06:35.800 --> 01:06:38.305
但绝对是，01:06:38.305 --> 01:06:43.520
是的，人们已经这样做了，这是一件好事。01:06:45.030 --> 01:06:52.060
还有别的事吗?好。嗯,好吧。01:06:52.060 --> 01:06:57.970
这是BiDAF系统的图片，01:06:57.970 --> 01:07:00.730
这是来自AI2 UDub的。01:07:00.730 --> 01:07:03.490
BiDAF系统非常有名。01:07:03.490 --> 01:07:06.880
这是另一种经典的版本01:07:06.880 --> 01:07:11.140
很多人使用并建立的问答系统。01:07:11.140 --> 01:07:14.260
你知道，01:07:14.260 --> 01:07:20.260
其中一些和我们之前看到的并没有完全不同，但是它有不同的补充。01:07:20.260 --> 01:07:23.980
和之前一样，也有嵌入词，01:07:23.980 --> 01:07:28.225
有一个biLSTM就像我们之前看到的一样，01:07:28.225 --> 01:07:31.435
这两项工作都在进行，01:07:31.435 --> 01:07:33.865
文章和问题。01:07:33.865 --> 01:07:37.210
但是也有一些不同的事情正在发生。01:07:37.210 --> 01:07:40.510
其中一个是，不仅仅是嵌入词，01:07:40.510 --> 01:07:45.085
它还在字符级处理问题和段落。01:07:45.085 --> 01:07:48.730
这也是我们接下来要讲的内容。01:07:48.730 --> 01:07:54.204
在最近的神经NLP中有很多关于字符级处理的工作，01:07:54.204 --> 01:07:56.365
但我现在不想谈这个。01:07:56.365 --> 01:08:00.460
BiDAF模型的主要技术创新01:08:00.460 --> 01:08:06.175
这就是注意力流动的布局因为它的名字叫双向注意力流动。01:08:06.175 --> 01:08:10.300
所以，有一个注意力流动的模型，你有注意力01:08:10.300 --> 01:08:14.740
在查询和段落之间双向流动。01:08:14.740 --> 01:08:18.985
这是他们的主要创新，在他们的模型中非常有用。01:08:18.985 --> 01:08:20.575
但除此之外，01:08:20.575 --> 01:08:23.500
你知道，这个模型有更多的东西。01:08:23.500 --> 01:08:27.324
在注意力流动层之后01:08:27.324 --> 01:08:31.675
运行多个双向LSTMs层。01:08:31.675 --> 01:08:35.770
在那之上，它们的输出层更多01:08:35.770 --> 01:08:41.230
比我之前展示的简单注意力版本复杂。01:08:41.230 --> 01:08:45.145
让我们更详细地看一下。01:08:45.145 --> 01:08:47.935
对于注意力流动层。01:08:47.935 --> 01:08:53.905
所以，这里的动机是在斯坦福的细心读者，01:08:53.905 --> 01:08:57.460
我们用注意力来映射01:08:57.460 --> 01:09:03.175
把问题用文字表达出来。01:09:03.175 --> 01:09:09.325
但是，你知道，因为问题是整个映射到文章的文字。01:09:09.325 --> 01:09:11.950
他们的想法很好，01:09:11.950 --> 01:09:18.760
假设您可以通过在单词级别上映射两个方向来做得更好。01:09:18.760 --> 01:09:23.890
所以你应该找一些段落词你可以把它们映射到疑问词上，01:09:23.890 --> 01:09:26.605
对你能映射到文章中的单词提出疑问。01:09:26.605 --> 01:09:29.965
如果你在两个方向都这样做，注意力流动，01:09:29.965 --> 01:09:34.315
然后在此基础上运行另一轮序列模型，01:09:34.315 --> 01:09:38.530
这样你就能更好地匹配它们了。01:09:38.530 --> 01:09:42.940
他们的做法是，01:09:42.940 --> 01:09:46.600
他们已经触底了01:09:46.600 --> 01:09:50.800
底层运行这两个lstm。01:09:50.800 --> 01:09:57.480
所以他们用LSTM表示每个单词，01:09:57.480 --> 01:10:00.480
单词和段落位置。01:10:00.480 --> 01:10:04.440
在这一点上，我不得不稍微道歉，因为我只是01:10:04.440 --> 01:10:08.760
偷了方程，所以用的字母变了。01:10:08.760 --> 01:10:12.845
对不起。但是，这些是，01:10:12.845 --> 01:10:18.505
对单个单词提出问题，这些是文章中的单个单词。01:10:18.505 --> 01:10:23.485
所以，他们接下来要做的就是说出每一段话，01:10:23.485 --> 01:10:28.105
每个问题单词，我想算出一个相似度评分。01:10:28.105 --> 01:10:34.570
计算相似度的方法是建立一个大的串联向量。01:10:34.570 --> 01:10:40.359
这是短文词，疑问词的LSTM表示，01:10:40.359 --> 01:10:45.070
然后他们加入了第三件事他们做了一个哈达玛产品，01:10:45.070 --> 01:10:49.855
这是疑问词和上下文词的元素乘积。01:10:49.855 --> 01:10:53.590
你知道，作为一个神经网络纯粹主义者01:10:53.590 --> 01:10:57.580
这种哈达玛产品有点欺骗的意味，因为01:10:57.580 --> 01:11:01.180
你可能会希望神经网络能学到这些01:11:01.180 --> 01:11:05.635
这篇文章和这个问题之间的关系很值得研究。01:11:05.635 --> 01:11:08.380
但是你可以找到很多模型01:11:08.380 --> 01:11:11.920
这是一种哈达玛产物01:11:11.920 --> 01:11:18.415
有一种非常简单的方法让模型知道匹配是一个好主意。01:11:18.415 --> 01:11:24.790
因为本质上这是在寻找每一个问题和段落词对。01:11:24.790 --> 01:11:28.810
这些向量在不同的维度上看起来相似吗?01:11:28.810 --> 01:11:32.965
你可以从哈达玛德产品中很好地获得。01:11:32.965 --> 01:11:35.815
取这个大向量，01:11:35.815 --> 01:11:40.765
然后用一个学习过的权重矩阵，01:11:40.765 --> 01:11:43.389
这给了你们一个相似度评分01:11:43.389 --> 01:11:47.050
在问题中的每个位置和上下文之间。01:11:47.050 --> 01:11:50.395
然后你要用它来01:11:50.395 --> 01:11:55.325
定义两个方向的注意事项。嗯- - - - - -01:11:55.325 --> 01:11:58.989
所以对于上下文，01:11:58.989 --> 01:12:02.415
注意，这个问题很简单。01:12:02.415 --> 01:12:08.550
所以，你把这些相似性得分通过一个软最大值。01:12:08.550 --> 01:12:13.515
所以对于文章中的每一个i位置，01:12:13.515 --> 01:12:17.300
有一个softmax，它给你一个概率分布，01:12:17.300 --> 01:12:20.375
然后你就会想到01:12:20.375 --> 01:12:26.750
第i个位置的一个新的表示就是注意加权，01:12:26.750 --> 01:12:31.350
版本，这些问题词的注意力加权平均值。01:12:31.350 --> 01:12:32.760
所以你有点，01:12:32.760 --> 01:12:38.775
将问题的注意力加权视图映射到文章中的每个位置。01:12:38.775 --> 01:12:43.860
然后你想做一些相反的事情。01:12:43.860 --> 01:12:49.815
但是反方向的方法稍有不同。01:12:49.815 --> 01:12:53.325
所以你又开始了01:12:53.325 --> 01:13:00.690
同样的相似度但是这次他们有点想，01:13:00.690 --> 01:13:04.875
分配好位置，01:13:04.875 --> 01:13:12.120
在这个问题的哪个位置，01:13:12.120 --> 01:13:16.980
排列最多，这样他们就能找到最大值01:13:16.980 --> 01:13:22.545
哪个是排列最整齐的，01:13:22.545 --> 01:13:24.930
对于每一个i，01:13:24.930 --> 01:13:27.885
他们在寻找最一致的问题词。01:13:27.885 --> 01:13:33.670
然后他们对m个分数做了一个软最大值然后这些是01:13:33.670 --> 01:13:39.900
用来构成段落的一种新的表示，01:13:39.900 --> 01:13:43.110
把这些注意力的重量加起来。01:13:43.110 --> 01:13:47.310
好。你建立这些东西，然后这个01:13:47.310 --> 01:13:51.330
给你一个新的表示，01:13:51.330 --> 01:13:57.090
你对段落词的原始表述。01:13:57.090 --> 01:14:00.120
你会得到一个新的表示01:14:00.120 --> 01:14:02.585
这种双向的注意力流动和你01:14:02.585 --> 01:14:05.310
看看这些哈达玛产物01:14:05.310 --> 01:14:10.110
这就得到了BiDAF层的输出和01:14:10.110 --> 01:14:12.990
BiDAF层是被作为01:14:12.990 --> 01:14:18.160
输入到这些nick- next序列的LSTM层。01:14:18.350 --> 01:14:22.240
好。是的，01:14:22.240 --> 01:14:24.335
这就是模型层。01:14:24.335 --> 01:14:29.085
还有另外两个BiLSTM层，01:14:29.085 --> 01:14:32.400
悬念的选择也有点复杂。01:14:32.400 --> 01:14:35.620
这样他们就会01:14:35.620 --> 01:14:40.020
把模型层的输出放到01:14:40.020 --> 01:14:45.915
一个密集的前馈神经网络层然后在上面进行软优化，01:14:45.915 --> 01:14:49.020
然后得到01:14:49.020 --> 01:14:53.430
开始，然后运行另一种LSTM分发结束。01:14:53.430 --> 01:14:58.020
嗯,是的。这给了你们一个更复杂模型的概念。01:14:58.020 --> 01:15:01.730
在某种意义上，01:15:01.730 --> 01:15:05.895
总结一下，如果你再往前看，01:15:05.895 --> 01:15:08.835
在过去的几年里，01:15:08.835 --> 01:15:14.220
人们一直在开发越来越复杂的体系结构01:15:14.220 --> 01:15:19.710
各种各样的关注，有效地带来了良好的收益。01:15:19.710 --> 01:15:23.010
我想我要跳过了，时间不多了01:15:23.010 --> 01:15:25.230
出来，给你看那个。01:15:25.230 --> 01:15:28.980
让我提一下FusionNet模型01:15:28.980 --> 01:15:32.500
这是微软的人做的因为这和答案有关，01:15:32.500 --> 01:15:35.145
注意力问题，对吧?01:15:35.145 --> 01:15:40.740
所以p-所以人们肯定使用了不同版本的注意力，对吧?01:15:40.740 --> 01:15:44.880
所以在我们展示的一些东西中我们倾向于强调01:15:44.880 --> 01:15:49.335
这个双线注意，你有两个向量由一个矩阵介导。01:15:49.335 --> 01:15:51.825
我想传统上在斯坦福NLP，01:15:51.825 --> 01:15:53.460
我们很喜欢这个01:15:53.460 --> 01:15:56.460
版本的注意因为它似乎很直接的学习01:15:56.460 --> 01:16:00.690
相似之处，但其他人使用了一点神经网络。01:16:00.690 --> 01:16:03.000
这是一个浅层的神经网络01:16:03.000 --> 01:16:05.340
算出注意力分数，01:16:05.340 --> 01:16:07.740
没有理由你不能说，也许如果我01:16:07.740 --> 01:16:10.710
把它做成一个深度神经网络，然后再加一层。01:16:10.710 --> 01:16:12.465
还有一些，你知道，01:16:12.465 --> 01:16:14.920
老实说，01:16:14.920 --> 01:16:18.425
有些结果是由包括谷歌在内的人得出的01:16:18.425 --> 01:16:22.520
认为NLP版本的注意力更好。01:16:22.520 --> 01:16:25.700
所以在这个方向上有一些值得探索的东西。01:16:25.700 --> 01:16:31.635
但事实上，FusionNet的人并没有朝那个方向走，因为他们说，01:16:31.635 --> 01:16:34.710
“瞧，我们想要用大量的注意力。01:16:34.710 --> 01:16:37.740
我们想要一个很漂亮的注意力计算01:16:37.740 --> 01:16:41.160
效率很高，所以如果你不得不这么做，那就是个坏消息01:16:41.160 --> 01:16:44.115
正在评估一个有点密集的神经网络01:16:44.115 --> 01:16:47.880
每一个姿势，每一次你的注意力。”01:16:47.880 --> 01:16:51.630
这个双线形式很吸引人01:16:51.630 --> 01:16:55.665
但是之后他们做了一些处理而不是用一个W矩阵01:16:55.665 --> 01:16:59.700
你可以减少排序和复杂度01:16:59.700 --> 01:17:06.135
将W矩阵除以两个低秩矩阵的乘积。01:17:06.135 --> 01:17:08.985
你可以有一个U和一个V矩阵。01:17:08.985 --> 01:17:12.689
如果你把这些矩形矩阵做得很细，01:17:12.689 --> 01:17:16.455
然后你可以做一个低秩因子分解，01:17:16.455 --> 01:17:18.420
这似乎是个好主意。01:17:18.420 --> 01:17:19.680
然后他们想，01:17:19.680 --> 01:17:23.265
也许你真的想要你的注意力分布是对称的。01:17:23.265 --> 01:17:26.460
所以我们可以把它放在中间，01:17:26.460 --> 01:17:29.100
我们可以有U和V，01:17:29.100 --> 01:17:32.160
是一样的，只是有一个对角矩阵01:17:32.160 --> 01:17:35.565
中间这可能是一个有用的方法。01:17:35.565 --> 01:17:39.555
从线性代数的角度来看，这些都是有意义的，01:17:39.555 --> 01:17:43.055
“哦，非线性在深度学习中非常有用。01:17:43.055 --> 01:17:44.640
所以我们为什么不，01:17:44.640 --> 01:17:48.790
把左半边和右半边插入一个ReLU，也许会有帮助。01:17:48.790 --> 01:17:52.380
[笑声]这在你的线性代数术语中没有多大意义，01:17:52.380 --> 01:17:56.850
但这实际上是他们最后使用的注意力形式。01:17:56.850 --> 01:18:00.150
在做期末项目的时候，你可以做很多事情。01:18:00.150 --> 01:18:02.085
嗯,是的。01:18:02.085 --> 01:18:04.740
但是，他们的论点仍然是，01:18:04.740 --> 01:18:07.920
以这种方式来吸引注意力实际上非常重要01:18:07.920 --> 01:18:11.070
更便宜，所以他们可以使用很多注意力。01:18:11.070 --> 01:18:16.640
所以他们建立了这个非常复杂的注意力模型，01:18:16.640 --> 01:18:19.155
我就不解释了01:18:19.155 --> 01:18:21.555
现在，嗯，01:18:21.555 --> 01:18:24.750
但我会给你们看这张照片。01:18:24.750 --> 01:18:28.295
他们的观点是01:18:28.295 --> 01:18:32.340
人们在不同的年代探索了不同的模型，01:18:32.340 --> 01:18:33.915
你知道，他们有点，01:18:33.915 --> 01:18:36.305
做不同种类的注意力。01:18:36.305 --> 01:18:39.180
你可以集中注意力，01:18:39.180 --> 01:18:42.240
与最初的LSTM一致，01:18:42.240 --> 01:18:46.340
你可以把两边都看一遍然后集中注意力，01:18:46.340 --> 01:18:49.740
你可以在你的图层里面做自我注意，有很多01:18:49.740 --> 01:18:53.300
不同的模型探索了不同的注意事项。01:18:53.300 --> 01:18:55.710
本质上他们想说的是，01:18:55.710 --> 01:18:59.980
让我们做所有这些，让我们做得更深入，做所有这些01:18:59.980 --> 01:19:04.210
五次，数字就会上升。某种程度上，答案是，01:19:04.210 --> 01:19:09.405
是的，他们是这样做的，模型最终得分非常好。01:19:09.405 --> 01:19:15.585
好吧，最后一件我想说但没有解释的事是，01:19:15.585 --> 01:19:18.450
我的意思是在过去的一年里01:19:18.450 --> 01:19:22.955
这是人类如何更好地完成这些任务的又一场革命。01:19:22.955 --> 01:19:29.795
因此，人们开发了一种算法，可以生成上下文相关的单词表示。01:19:29.795 --> 01:19:32.790
这意味着，不是传统的向量，01:19:32.790 --> 01:19:36.660
每个单词在特定上下文中都有一个表示。01:19:36.660 --> 01:19:41.700
这是青蛙这个词在这个特定的背景下以及人们建造的方式01:19:41.700 --> 01:19:44.490
这些表示使用了一些东西01:19:44.490 --> 01:19:47.580
就像Abby提到的语言建模任务，01:19:47.580 --> 01:19:50.730
也就是输入单词的概率01:19:50.730 --> 01:19:54.795
上下文:学习上下文特定的单词表示。01:19:54.795 --> 01:19:57.870
埃尔莫是第一个著名的模型。01:19:57.870 --> 01:20:00.410
然后谷歌的人想到了伯特，01:20:00.410 --> 01:20:01.830
这样效果更好。01:20:01.830 --> 01:20:06.490
所以伯特在某种程度上是01:20:06.490 --> 01:20:11.235
超级复杂的注意力架构，像objective这样的语言建模。01:20:11.235 --> 01:20:13.680
我们稍后会讲到这些，01:20:13.680 --> 01:20:16.580
我现在不打算谈这些01:20:16.580 --> 01:20:22.260
但是如果你看一下当前的2。0版排行榜，01:20:22.260 --> 01:20:24.090
你会很快，01:20:24.090 --> 01:20:28.485
抱歉，我放错幻灯片了，那是排行榜的底部。01:20:28.485 --> 01:20:30.270
哎呀，在最后一分钟滑倒了。01:20:30.270 --> 01:20:34.785
如果你回到我的幻灯片上面的排行榜，01:20:34.785 --> 01:20:38.805
你会注意到排行榜的顶部，01:20:38.805 --> 01:20:42.825
每一个顶级系统都使用BERT。01:20:42.825 --> 01:20:45.240
这就是你想要的01:20:45.240 --> 01:20:47.820
考虑一下，但你可能想要考虑如何才能做到01:20:47.820 --> 01:20:52.800
使用它作为子模块，您也可以添加其他东西，就像许多这些系统所做的那样。01:20:52.800 --> 01:20:56.140
Okay. Done for today.

