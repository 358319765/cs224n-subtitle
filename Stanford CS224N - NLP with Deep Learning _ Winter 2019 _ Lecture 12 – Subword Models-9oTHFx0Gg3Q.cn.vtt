WEBVTT
Kind: captions
Language: en

00:00:05.480 --> 00:00:07.890
好。你好,每个人。00:00:07.890 --> 00:00:10.365
让我们重新开始。00:00:10.365 --> 00:00:17.100
好了，首先我来讲一下作业5。00:00:17.100 --> 00:00:20.715
作业5今天就要出来了。00:00:20.715 --> 00:00:22.580
这是一个全新的任务00:00:22.580 --> 00:00:25.080
所以你们就是实验对象。00:00:25.080 --> 00:00:31.365
它将会是什么，它本质上建立在作业4的基础上。00:00:31.365 --> 00:00:34.560
如果你在作业4上做得不好也没关系，00:00:34.560 --> 00:00:36.405
但我认为实际上大多数人都是这样做的。00:00:36.405 --> 00:00:39.990
我们要做的是加上，00:00:39.990 --> 00:00:42.290
卷积神经网络和子词00:00:42.290 --> 00:00:45.590
神经机器翻译系统建模，00:00:45.590 --> 00:00:47.305
寻求它使它变得更好。00:00:47.305 --> 00:00:52.850
所以这个作业编码很重，书面问题很少。00:00:52.850 --> 00:00:56.360
我是说你的编码00:00:56.360 --> 00:01:00.410
其实做起来并不比作业4难，00:01:00.410 --> 00:01:02.585
有点像作业4。00:01:02.585 --> 00:01:08.710
但我们希望这次你能独立完成。00:01:08.710 --> 00:01:10.725
我的意思是，00:01:10.725 --> 00:01:12.480
作业4。00:01:12.480 --> 00:01:16.640
有很多脚手架告诉你应该怎么做，00:01:16.640 --> 00:01:20.270
这些都是自动评分的检查，你可以继续做下去00:01:20.270 --> 00:01:24.980
你的代码直到他们通过了所有的autograder检查，而每个人都通过了。00:01:24.980 --> 00:01:29.270
嗯，可以说，那是一种被溺爱的生活。00:01:29.270 --> 00:01:31.625
但我是说，00:01:31.625 --> 00:01:37.160
我想我们真正想要达到的是有一个更多的-对不起，问题。00:01:37.160 --> 00:01:38.390
(听不清)?00:01:38.390 --> 00:01:43.510
是的。所以我们希望这能有用。00:01:43.510 --> 00:01:48.380
虽然只是短期的痛苦，但很有用00:01:48.380 --> 00:01:51.350
一个更有效的方法00:01:51.350 --> 00:01:55.400
最后一个项目，甚至是你的余生。00:01:55.400 --> 00:01:58.220
事实上，在你的余生中，00:01:58.220 --> 00:02:01.820
如果你要做一些有深度学习的事情，00:02:01.820 --> 00:02:03.920
你得弄清楚是哪种00:02:03.920 --> 00:02:06.740
模型要建立，哪些部分要缝合在一起，00:02:06.740 --> 00:02:11.090
以及如何编写一些测试，看看它是否在做一些明智的事情。00:02:11.090 --> 00:02:13.775
如果它没有做一些明智的事情，00:02:13.775 --> 00:02:17.509
为了弄清楚你如何能改变事物，尝试不同的事物，00:02:17.509 --> 00:02:19.355
让它理智地工作。00:02:19.355 --> 00:02:21.155
这就是我们所希望的，00:02:21.155 --> 00:02:22.730
那些人00:02:22.730 --> 00:02:24.650
可以在作业5中做，00:02:24.650 --> 00:02:25.850
所以你必须，00:02:25.850 --> 00:02:28.090
把事情弄清楚。00:02:28.090 --> 00:02:31.790
应该自己写测试代码。00:02:31.790 --> 00:02:34.490
我们没有公共自动评分机00:02:34.490 --> 00:02:38.510
所以你应该――这是你自己做理智检查的一部分，00:02:38.510 --> 00:02:44.150
试着像我上周说的那样去做00:02:44.150 --> 00:02:47.420
简单的工作，确认他们的工作00:02:47.420 --> 00:02:51.620
大量的测试数据等等，做事情更理智。00:02:51.620 --> 00:02:53.999
我的意思是，00:02:55.490 --> 00:02:59.100
其中一个特别的部分，00:02:59.100 --> 00:03:02.640
我们本来打算这么做的00:03:02.640 --> 00:03:06.015
这个作业，我一直在找，00:03:06.015 --> 00:03:07.920
嗯，在下一张幻灯片上。00:03:07.920 --> 00:03:11.620
所以，对于这次作业和以后的作业，00:03:11.620 --> 00:03:16.260
我们要像CS107一样执行规则，00:03:16.260 --> 00:03:18.130
对于你们中的本科生来说，00:03:18.130 --> 00:03:22.835
这意味着助教不会帮你检查和调试代码。00:03:22.835 --> 00:03:24.860
所以，你知道，00:03:24.860 --> 00:03:29.930
当然我们还是希望助教能帮上忙，带着你的问题来找他们，00:03:29.930 --> 00:03:33.514
谈论你应该如何使用不同的东西，00:03:33.514 --> 00:03:35.420
在PyTorch图书馆00:03:35.420 --> 00:03:41.300
但你不应该把它看作助教的工作，这是一个大的Python文件。00:03:41.300 --> 00:03:43.580
你能告诉我有什么问题吗00:03:43.580 --> 00:03:45.830
帮你把它修好。00:03:45.830 --> 00:03:49.485
好。具体的政策是，00:03:49.485 --> 00:03:51.870
写在广场上。00:03:51.870 --> 00:03:58.130
好。有什么问题吗，还是直接讲?00:03:59.580 --> 00:04:05.145
好。今天的讲座，00:04:05.145 --> 00:04:10.650
从某种意义上说，今天的课很简单。00:04:10.650 --> 00:04:13.350
上次课上，00:04:13.350 --> 00:04:16.420
有很多新东西00:04:16.420 --> 00:04:19.975
神经网络上的其他东西你们以前没见过，00:04:19.975 --> 00:04:23.410
我们做了Convnets和pooling层，00:04:23.410 --> 00:04:26.260
我们做了公路和剩余连接，00:04:26.260 --> 00:04:29.970
批量标准，我不知道，不管我们做了什么。00:04:29.970 --> 00:04:32.525
嗯，我想要一个卷的大小。00:04:32.525 --> 00:04:36.110
所以有很多新东西00:04:36.110 --> 00:04:39.580
在这节课中，关于神经网络机制，00:04:39.580 --> 00:04:41.300
根本没有什么新东西。00:04:41.300 --> 00:04:42.500
这很简单。00:04:42.500 --> 00:04:48.875
这也是一门新课程，但它是有原因的。00:04:48.875 --> 00:04:53.570
原因和我说过的一句话有关00:04:53.570 --> 00:04:58.700
上节课我们讲了神经网络领域中很多东西是如何不断变化的。00:04:58.700 --> 00:05:01.850
当我们第一次设计这门课的时候00:05:01.850 --> 00:05:05.090
它的很多结构仍然是。00:05:05.090 --> 00:05:10.830
大概在2014-2015年我们设计这门课的时候，00:05:10.830 --> 00:05:13.175
这基本上是不言自明的00:05:13.175 --> 00:05:18.185
所有自然语言处理的深度学习模型都是靠单词来完成的。00:05:18.185 --> 00:05:22.400
因此，我们从单词向量开始，00:05:22.400 --> 00:05:26.170
然后我们开始研究像单词上的循环模型。00:05:26.170 --> 00:05:31.580
然而事实是在最近三年左右，00:05:31.580 --> 00:05:36.455
有很多新作品，包括一些最有影响力的新作品。00:05:36.455 --> 00:05:40.960
有些语言模型不是，不是，不是，00:05:40.960 --> 00:05:46.895
被建立在他们正在建立的单词之上，建立在单词或字符的片段之上。00:05:46.895 --> 00:05:49.640
所以这堂课的目的就是给你们00:05:49.640 --> 00:05:52.985
还有一些其他的做事方式，00:05:52.985 --> 00:05:56.900
对正在发生的事情有一定的了解。00:05:56.900 --> 00:05:59.930
但是我们正在研究的实际模型，00:05:59.930 --> 00:06:03.920
使用我们已经学过的所有构建块，比如，00:06:03.920 --> 00:06:08.030
RNNs和ConvNets之类的。00:06:08.030 --> 00:06:09.950
我们开始吧。00:06:09.950 --> 00:06:13.130
我要从一点开始00:06:13.130 --> 00:06:17.105
学习语言结构的语言学，00:06:17.105 --> 00:06:22.520
首先是语言的低级单位然后我们会看到结果，00:06:22.520 --> 00:06:25.345
比如角色等级模型。00:06:25.345 --> 00:06:27.440
在语言学上，00:06:27.440 --> 00:06:30.980
如果你从图腾柱的底部开始，00:06:30.980 --> 00:06:34.099
语言学的第一级是语音学，00:06:34.099 --> 00:06:38.420
也就是理解人类语言的声音和生理。00:06:38.420 --> 00:06:40.860
这有点像物理学或生理学，00:06:40.860 --> 00:06:44.355
或者别的什么，对的，嘴巴的某些部分会动，00:06:44.355 --> 00:06:46.529
耳朵的某些部分起着过滤的作用，00:06:46.529 --> 00:06:50.030
它们之间有声波。00:06:50.030 --> 00:06:53.270
这在某种意义上是没有争议的。00:06:53.270 --> 00:06:55.420
但是在那个水平之上，00:06:55.420 --> 00:07:00.460
人们分析人类语言的标准做法是，00:07:00.460 --> 00:07:06.230
人类语言似乎只使用了一小部分00:07:06.230 --> 00:07:12.320
独特的单位，通常被称为音素，实际上是分类的。00:07:12.320 --> 00:07:16.480
这里的想法是，00:07:16.480 --> 00:07:19.890
我们的嘴巴是连续的空间。00:07:19.890 --> 00:07:23.030
他们有不同的嘴巴，00:07:23.030 --> 00:07:24.890
舌头和咽部等等，00:07:24.890 --> 00:07:27.020
但它是一个连续的空间。00:07:27.020 --> 00:07:31.610
实际上，我们可以发出无数种声音。00:07:31.610 --> 00:07:37.830
所以，如果我张开嘴，发出声音，然后摆动舌头，我就可以发出声音了。00:07:37.830 --> 00:07:41.570
我可以发出无数种不同的声音。00:07:41.570 --> 00:07:46.990
但事实是人类语言不是这样的，00:07:46.990 --> 00:07:49.915
从那无穷无尽的声音中，00:07:49.915 --> 00:07:53.230
我们分辨出一小段声音。00:07:53.230 --> 00:07:58.570
当语言发生变化时，00:07:58.570 --> 00:08:01.535
声音的空间被视为00:08:01.535 --> 00:08:05.365
在语言变化中重要而杰出的。00:08:05.365 --> 00:08:10.570
即使是在像英语这样的一门语言中，这种情况也会发生。00:08:10.570 --> 00:08:13.255
我要举个例子。00:08:13.255 --> 00:08:20.630
认知心理学的人谈论分类知觉的现象。00:08:20.630 --> 00:08:24.535
这意味着确实存在连续的东西，00:08:24.535 --> 00:08:30.410
但人类认为它属于相当尖锐的类别。00:08:30.410 --> 00:08:31.720
你知道，00:08:31.720 --> 00:08:34.345
你可以用它来，00:08:34.345 --> 00:08:38.350
衣服的款式，是否算胖。00:08:38.350 --> 00:08:43.745
分类知觉最著名的例子是语言，00:08:43.745 --> 00:08:46.930
我们可以发出各种各样的声音，但是00:08:46.930 --> 00:08:50.435
人们把它们视为类别。00:08:50.435 --> 00:08:55.775
这就意味着当你有绝对知觉时，00:08:55.775 --> 00:09:00.650
一个类别内的差异似乎缩小了。00:09:00.650 --> 00:09:03.700
你几乎注意不到他们的不同之处00:09:03.700 --> 00:09:07.615
跨类别的扩展非常清晰。00:09:07.615 --> 00:09:09.850
这是我们研究过的一个案例00:09:09.850 --> 00:09:12.770
很多是所谓的，00:09:12.770 --> 00:09:14.770
嗯，开始发声的时间。00:09:14.770 --> 00:09:19.930
包括英语在内的很多语言都有p和b这样的成对发音，00:09:19.930 --> 00:09:25.150
pah和bah，它们的区别在于什么时候开始发声。00:09:25.150 --> 00:09:29.165
所以buh，它的发音听起来像一个带r的元音。00:09:29.165 --> 00:09:31.510
这是一个连续参数，00:09:31.510 --> 00:09:36.835
你可以在a p和b之间的光谱上做任何点但是，00:09:36.835 --> 00:09:39.710
说英语的人，00:09:39.710 --> 00:09:43.010
嗯，只感知光谱上的两点。00:09:43.010 --> 00:09:47.320
你并没有真正注意到它们之间的细微差别。00:09:47.320 --> 00:09:50.380
有些语言能区分更多的点。00:09:50.380 --> 00:09:53.980
泰语区分了三种不同的辅音，00:09:53.980 --> 00:09:56.810
根据声音开始的时间。00:09:56.810 --> 00:09:58.840
嗯，可能是，嗯，00:09:58.840 --> 00:10:01.400
更容易理解的是，00:10:01.400 --> 00:10:03.920
这是语言变化的一个例子。00:10:03.920 --> 00:10:06.375
对于像我这样的演讲者，00:10:06.375 --> 00:10:10.480
有caught和cot这是不同的元音，00:10:10.480 --> 00:10:12.815
我听到它们是不同的元音。00:10:12.815 --> 00:10:17.455
但如果你是在美国西南部长大的人，00:10:17.455 --> 00:10:21.820
嗯，那么这两个元音是完全一样的，你不能区分它们。00:10:21.820 --> 00:10:26.390
然后――你以为我说了两次同样的话，尽管我说的是两个不同的元音。00:10:26.390 --> 00:10:34.120
这就是人们在方言层面发展的地方00:10:34.120 --> 00:10:36.420
关于。的范畴知觉00:10:36.420 --> 00:10:43.400
他们对哪些区别和声音敏感或者不敏感。00:10:43.950 --> 00:10:46.535
好。和总结,00:10:46.535 --> 00:10:51.370
我的意思是为什么我要提到这个在某种意义上是这些声音的区别00:10:51.370 --> 00:10:54.460
分类声音的区别有很多00:10:54.460 --> 00:10:58.975
我们的语言写作系统，我们很快就会讲到。00:10:58.975 --> 00:11:03.500
好。在传统语言学中，00:11:03.500 --> 00:11:08.195
你有声音，但声音在语言中没有任何意义。00:11:08.195 --> 00:11:10.805
所以pah和bah没有意义，00:11:10.805 --> 00:11:12.935
a和e没有意义。00:11:12.935 --> 00:11:15.875
所以人们通常会区分为00:11:15.875 --> 00:11:20.150
下一个层次的形态学是单词的组成部分。00:11:20.150 --> 00:11:23.540
这被认为是有意义的最低层次。00:11:23.540 --> 00:11:27.620
所以我的想法是，很多单词都是复杂的，是可以造出来的00:11:27.620 --> 00:11:31.660
u-由碎片组成，但这些碎片确实有意义。00:11:31.660 --> 00:11:36.050
所以财富是有意义的，嗯，幸运，00:11:36.050 --> 00:11:38.805
在这个结尾，00:11:38.805 --> 00:11:42.510
某种程度上给了某人财富。00:11:42.510 --> 00:11:45.405
这意味着，你知道，拥有财富，00:11:45.405 --> 00:11:46.610
这是有意义的，00:11:46.610 --> 00:11:49.360
联合国的意思是要扭转这种局面。00:11:49.360 --> 00:11:54.195
所以不幸意味着你没有财富，00:11:54.195 --> 00:11:57.265
然后就有了把这些都变成副词的意思，00:11:57.265 --> 00:11:59.825
不幸的是，00:11:59.825 --> 00:12:02.290
发了财，出事了。00:12:02.290 --> 00:12:04.400
所以这些，嗯，00:12:04.400 --> 00:12:07.910
文字是有意义的最小事物。00:12:07.910 --> 00:12:10.775
在深度学习方面几乎没有研究尝试过00:12:10.775 --> 00:12:13.480
利用这种语素层次的结构。00:12:13.480 --> 00:12:17.785
事实上，我和一些学生在六年前做过00:12:17.785 --> 00:12:22.515
试着建立一个系统它建立了这些树状结构的神经网络，00:12:22.515 --> 00:12:25.710
把单词的意思拼凑在一起。00:12:25.710 --> 00:12:29.875
但这并不是一个被广泛接受的观点。00:12:29.875 --> 00:12:33.660
这就是为什么它没有被广泛采用的原因，00:12:33.660 --> 00:12:38.490
也就是计算出语义上有意义的单词，00:12:38.490 --> 00:12:44.925
在NLP中很多时候人们发现了什么00:12:44.925 --> 00:12:48.215
你能得到差不多一样的吗00:12:48.215 --> 00:12:52.090
如果您只处理字符n-gram，就会得到结果。00:12:52.090 --> 00:12:55.560
你放入卷积神经网络的单位。00:12:55.560 --> 00:12:59.105
因为如果你有一个模型，00:12:59.105 --> 00:13:05.045
字符三角，还有单词开头，un, nfo等等。00:13:05.045 --> 00:13:08.770
感谢你读完了单词的末尾，00:13:08.770 --> 00:13:10.840
这些不同的单位。00:13:10.840 --> 00:13:13.085
有不同的字符三角，00:13:13.085 --> 00:13:15.280
以一种分布式的方式00:13:15.280 --> 00:13:19.765
这个词所有重要的意思都很好，00:13:19.765 --> 00:13:21.545
这就足够了。00:13:21.545 --> 00:13:27.035
这实际上是一个非常经典的想法，现在又复活了。00:13:27.035 --> 00:13:30.220
回到第二节00:13:30.220 --> 00:13:35.045
80年代中期到90年代早期的神经网络，00:13:35.045 --> 00:13:37.255
有屈，00:13:37.255 --> 00:13:41.440
有很多有争议的工作00:13:41.440 --> 00:13:46.030
语言的结构，尤其是Dave Rumelhart和Jay McClelland。00:13:46.030 --> 00:13:48.500
杰・麦克莱伦还在心理学系，00:13:48.500 --> 00:13:50.725
如果你想在业余时间去看他的话。00:13:50.725 --> 00:13:57.460
他们提出了一个模型来模拟英语中过去时态的生成。00:13:57.460 --> 00:14:00.610
所以这是一种我们可以建立的心理实验00:14:00.610 --> 00:14:04.045
一个可以学习英语动词过去时态的系统?00:14:04.045 --> 00:14:07.780
最难的是，很多动词都是规则的，00:14:07.780 --> 00:14:09.740
加上ed结尾，00:14:09.740 --> 00:14:14.015
但是有些单词是不规则的，你必须学习不规则的模式。00:14:14.015 --> 00:14:17.015
但是他们那样做的方式。00:14:17.015 --> 00:14:23.530
我的意思是，部分原因是因为这是关于序列模型的早期研究，00:14:23.530 --> 00:14:27.094
他们是在表示的地方使用了表示吗00:14:27.094 --> 00:14:31.255
精确地用这些字符三角表示的单词。00:14:31.255 --> 00:14:35.780
这就是他们在模型中使用和传递的单词的表示。00:14:35.780 --> 00:14:40.220
这个想法引起了很多争议00:14:40.220 --> 00:14:43.535
语言学家，哲学家，和其他人的想法00:14:43.535 --> 00:14:47.075
当时有很多关于语言的争论。00:14:47.075 --> 00:14:50.500
但是作为一个纯粹的工程解决方案00:14:50.500 --> 00:14:54.070
这被证明是一种很好的做事方式。00:14:54.070 --> 00:14:59.245
所以这十年有其他的研究包括这个模型，00:14:59.245 --> 00:15:02.995
在微软开发的00:15:02.995 --> 00:15:05.860
语义模型他们使用的是00:15:05.860 --> 00:15:09.420
用n个字符表示单词的意思。00:15:09.420 --> 00:15:17.195
好的，那么，嗯，现在我们可能对建立非文字的模型感兴趣。00:15:17.195 --> 00:15:21.065
所以我们要把一个单词写成字符00:15:21.065 --> 00:15:25.355
我们将用它做一些事情，比如创建字符n-g。00:15:25.355 --> 00:15:28.775
所以一些有用的东西，00:15:28.775 --> 00:15:31.010
要知道实际上有00:15:31.010 --> 00:15:33.980
当你这样做的时候，语言之间有相当多的差异。00:15:33.980 --> 00:15:36.200
所以它们并不完全相同，对吧?00:15:36.200 --> 00:15:38.465
第一个问题是，00:15:38.465 --> 00:15:42.455
有些语言在单词之间没有空格。00:15:42.455 --> 00:15:45.125
最著名的例子是中文。00:15:45.125 --> 00:15:51.545
但是对于那些有欧洲血统的人来说，一个有趣的事实是，00:15:51.545 --> 00:15:56.630
你知道当古希腊人写古希腊文的时候，00:15:56.630 --> 00:15:59.525
他们也没有在单词之间加空格。00:15:59.525 --> 00:16:02.390
它实际上是后来的发明00:16:02.390 --> 00:16:06.170
中世纪的学者们正在翻看他们的手稿，00:16:06.170 --> 00:16:08.690
他们决定(噪音)是谁，如果我们这样做，也许会更容易读懂00:16:08.690 --> 00:16:11.720
填上空格，然后他们就开始做了。00:16:11.720 --> 00:16:20.000
嗯，[噪音]现在大多数语言在单词之间都有空格，00:16:20.000 --> 00:16:22.580
还有很多很好的案例。00:16:22.580 --> 00:16:26.930
特别地，很多语言都有一些位元00:16:26.930 --> 00:16:31.550
比如代词，介词，00:16:31.550 --> 00:16:36.380
或者各种各样的连接词，比如and and so，00:16:36.380 --> 00:16:42.050
有时他们一起写，有时分开写。00:16:42.050 --> 00:16:44.750
所以在法语中，00:16:44.750 --> 00:16:49.865
你会得到这样的介词，抱歉，00:16:49.865 --> 00:16:55.865
名字，嗯，你的记号笔，我，你，带来了。00:16:55.865 --> 00:16:58.640
嗯，你知道，这些小字眼00:16:58.640 --> 00:17:01.940
发音就像je vous ai一样，00:17:01.940 --> 00:17:05.060
可以说这几乎是一个词，00:17:05.060 --> 00:17:07.325
但它是分开写的。00:17:07.325 --> 00:17:11.480
嗯，有其他语言把东西粘在一起，00:17:11.480 --> 00:17:13.580
这两个词可以说是分开的。00:17:13.580 --> 00:17:19.550
所以在阿拉伯语中，你会听到一些代词clitics和一些像这样的连接词，00:17:19.550 --> 00:17:23.960
它们就像一个单词一样写在一起，00:17:23.960 --> 00:17:27.485
他们认为应该是四个词。00:17:27.485 --> 00:17:31.790
另一个著名的例子是复合名词。00:17:31.790 --> 00:17:34.025
在英语中，00:17:34.025 --> 00:17:37.160
我们在复合名词之间加空格，00:17:37.160 --> 00:17:38.690
所以你可以看到每个名词。00:17:38.690 --> 00:17:42.290
即使在很多方面复合名词00:17:42.290 --> 00:17:46.460
像白板这样的东西就像一个单词，或者高中。00:17:46.460 --> 00:17:48.770
而其他语言，00:17:48.770 --> 00:17:50.480
德语是最著名的例子，00:17:50.480 --> 00:17:52.580
还有其他日耳曼语言，00:17:52.580 --> 00:17:57.470
把它们都写成一个单词，就会得到很长的单词。00:17:57.470 --> 00:18:03.680
所以我们可以得到不同的单词如果我们只使用空格，不做其他的。嗯,很好。00:18:03.680 --> 00:18:07.310
好。是的。所以在处理文字的时候，00:18:07.310 --> 00:18:10.175
这些都是实际问题。00:18:10.175 --> 00:18:13.070
我们已经开始了00:18:13.070 --> 00:18:16.339
如果你想建立基于单词的模型，00:18:16.339 --> 00:18:18.785
有这么大的文字空间，00:18:18.785 --> 00:18:21.740
严格来说，单词的空间是无限的00:18:21.740 --> 00:18:24.845
因为一旦你考虑到像数字这样的东西，00:18:24.845 --> 00:18:28.985
更别说联邦快递的路由号码了00:18:28.985 --> 00:18:31.520
或者如果你只允许形态学，00:18:31.520 --> 00:18:34.175
不幸的是，当你能做出那些。00:18:34.175 --> 00:18:37.190
是的，你可以扩大单词的空间，00:18:37.190 --> 00:18:40.265
所以你有了这么大的开放词汇量。00:18:40.265 --> 00:18:43.340
英语，你知道，有点问题。00:18:43.340 --> 00:18:46.400
它比很多其他语言都有问题。00:18:46.400 --> 00:18:49.160
这是一个可爱的捷克词，00:18:49.160 --> 00:18:51.635
最糟糕的农场00:18:51.635 --> 00:18:56.420
你可以用很多其他语言创造出更复杂的单词。00:18:56.420 --> 00:18:59.150
很多美洲土著语言00:18:59.150 --> 00:19:03.170
其他欧洲语言，比如芬兰语，有一些非常复杂的单词，00:19:03.170 --> 00:19:05.570
土耳其语有非常复杂的单词。00:19:05.570 --> 00:19:07.895
这是个坏消息。00:19:07.895 --> 00:19:10.190
我们还有其他的原因00:19:10.190 --> 00:19:12.410
看单词水平以下的单词，00:19:12.410 --> 00:19:13.970
了解他们的情况。00:19:13.970 --> 00:19:16.115
所以当你翻译的时候，00:19:16.115 --> 00:19:18.110
有广阔的空间，00:19:18.110 --> 00:19:23.405
尤其是名字，翻译基本上就是音译，00:19:23.405 --> 00:19:29.015
你要把某人的名字改写成，00:19:29.015 --> 00:19:31.790
也许不是完全正确，但大致正确00:19:31.790 --> 00:19:34.730
根据不同语言的语音系统。00:19:34.730 --> 00:19:36.020
如果我们想这样做，00:19:36.020 --> 00:19:38.900
我们本质上想要在字母层面上工作，00:19:38.900 --> 00:19:40.310
不是水平这个词。00:19:40.310 --> 00:19:46.340
但另一个重要的现代原因是我们想要在文字层面下开始建模，00:19:46.340 --> 00:19:51.350
我们生活在社交媒体的时代，如果你在社交媒体领域，00:19:51.350 --> 00:19:53.810
有很多东西没有被写出来00:19:53.810 --> 00:19:56.975
使用你在字典里找到的规范词汇，00:19:56.975 --> 00:19:58.730
不知何故，我们想要开始，00:19:58.730 --> 00:20:00.005
嗯，建模。00:20:00.005 --> 00:20:02.180
所以从某种意义上说，00:20:02.180 --> 00:20:03.545
简单的情况。00:20:03.545 --> 00:20:06.260
嗯,良好氛围。00:20:06.260 --> 00:20:08.720
但是不管怎样，这个单词的拼写是1，00:20:08.720 --> 00:20:10.370
二，三，四，五，六，00:20:10.370 --> 00:20:13.010
七个O，一个，两个，00:20:13.010 --> 00:20:15.875
三，四，五，哦，还有七个S，它们匹配。00:20:15.875 --> 00:20:20.270
我不知道这是不是故意的[笑声]。嗯,好吧。00:20:20.270 --> 00:20:24.905
所以这种发霉的写作方式很常见，00:20:24.905 --> 00:20:27.830
你知道，如果我们是00:20:27.830 --> 00:20:31.100
从文字的层面来看待事物，我们正试图建立正确的模型。00:20:31.100 --> 00:20:34.730
这显然不是人类在做的，我们在观察00:20:34.730 --> 00:20:38.675
角色和意识到发生了什么。00:20:38.675 --> 00:20:45.290
从某种意义上说，这是一个简单的例子你可以想象一下预处理。00:20:45.290 --> 00:20:49.100
还有很多更难的东西。00:20:49.100 --> 00:20:53.645
嗯，我想有一种缩写的说法，好像我不在乎。00:20:53.645 --> 00:20:58.640
然后你就有了很多创造性的拼写，00:20:58.640 --> 00:21:04.565
这是一种简化的发音比如I'mma go sumn。00:21:04.565 --> 00:21:08.450
我们似乎需要别的东西00:21:08.450 --> 00:21:13.265
如果我们想要更好地处理这篇文章的话。00:21:13.265 --> 00:21:22.070
哦。好。这表明我们想要用我们的模型来做。00:21:22.070 --> 00:21:28.055
这就引起了人们对使用角色等级模型的兴趣。00:21:28.055 --> 00:21:35.300
我的意思是你可以在两种程度上做到这一点，00:21:35.300 --> 00:21:38.045
我们来看看它们。00:21:38.045 --> 00:21:40.940
一个层面是说，00:21:40.940 --> 00:21:43.820
听着，我们的系统里还有单词。00:21:43.820 --> 00:21:47.345
基本上，我们要建立一个文字系统，00:21:47.345 --> 00:21:49.670
但我们希望能够创造00:21:49.670 --> 00:21:54.950
任何字符序列的单词表示，我们想00:21:54.950 --> 00:21:58.910
以一种利用能力的方式去做00:21:58.910 --> 00:22:03.050
识别字符序列中看起来熟悉的部分，00:22:03.050 --> 00:22:07.790
这样我们就能猜出vibes是什么意思了。00:22:07.790 --> 00:22:10.670
这样就解决了问题00:22:10.670 --> 00:22:13.489
用未知的词，我们得到相似的词，00:22:13.489 --> 00:22:18.125
具有相似术语、拼写等的单词的类似嵌入。00:22:18.125 --> 00:22:20.810
但另一种说法是，00:22:20.810 --> 00:22:23.820
不，把这些话都忘了，谁需要00:22:23.820 --> 00:22:28.880
为什么我们不把所有的语言处理都放在字符序列上，00:22:28.880 --> 00:22:30.410
会好的。00:22:30.410 --> 00:22:35.090
这两种方法都被证明非常成功。00:22:35.090 --> 00:22:38.810
嗯，我只是想细想一下，00:22:38.810 --> 00:22:41.480
这有点回到我的，00:22:41.480 --> 00:22:44.630
形态学幻灯片。00:22:44.630 --> 00:22:47.840
当人们第一次提出他们要去的时候00:22:47.840 --> 00:22:51.245
建立基于角色的深度学习模型。00:22:51.245 --> 00:22:54.650
我的意思是，我的第一感觉是，哦，永远不会00:22:54.650 --> 00:22:57.965
工作是因为，00:22:57.965 --> 00:23:01.310
好的，单词有它的意义，00:23:01.310 --> 00:23:03.890
你可以做一些建造之类的事情00:23:03.890 --> 00:23:07.160
一个word2vec模型，这将真正能够00:23:07.160 --> 00:23:09.650
看看单词和它们的分布，然后学习00:23:09.650 --> 00:23:12.905
单词的意义是因为单词有意义。00:23:12.905 --> 00:23:15.680
你会说，00:23:15.680 --> 00:23:19.310
我要提出一个h的向量表示，00:23:19.310 --> 00:23:22.880
a的另一个向量表示，00:23:22.880 --> 00:23:26.404
t的另一种vec-向量表示，00:23:26.404 --> 00:23:29.330
这对于表示帽子很有用00:23:29.330 --> 00:23:32.495
意思是一旦我把它放入足够的神经网络层，00:23:32.495 --> 00:23:36.335
坦白地说，这听起来很不可信。00:23:36.335 --> 00:23:40.315
嗯，但是，嗯，我想，你知道00:23:40.315 --> 00:23:44.595
但它，它，完全有效，所以我现在相信，经验证明。00:23:44.595 --> 00:23:48.885
我认为我们需要意识到，00:23:48.885 --> 00:23:52.350
是这样吗?是的，00:23:52.350 --> 00:23:55.755
在某种程度上，我们只是拥有这些不太有意义的角色。00:23:55.755 --> 00:24:02.940
但是我们有这些非常强大的组合模型里面有很多参数，00:24:02.940 --> 00:24:04.950
比如递归神经网络00:24:04.950 --> 00:24:10.305
卷积神经网络它们分别能够，00:24:10.305 --> 00:24:15.330
存储和构建多字母组的意义表示，00:24:15.330 --> 00:24:18.720
以这种方式，他们可以建模的意义00:24:18.720 --> 00:24:22.590
语素和更大的单位，因此把单词的意思放在一起。00:24:22.590 --> 00:24:24.420
嗯,是的。所以,嗯,00:24:24.420 --> 00:24:27.930
关于使用字符还有一个细节，00:24:27.930 --> 00:24:30.195
嗯，来自写作系统。00:24:30.195 --> 00:24:33.660
所以如果你是一个语言学家，你倾向于认为声音是首要的。00:24:33.660 --> 00:24:36.900
这些就是我们之前提到的音素。00:24:36.900 --> 00:24:40.170
实际上，00:24:40.170 --> 00:24:43.250
深度学习根本没有尝试使用音素。00:24:43.250 --> 00:24:46.425
传统的语音识别器经常使用音素，00:24:46.425 --> 00:24:48.135
但在深度学习领域，00:24:48.135 --> 00:24:53.580
你想要有很多的数据你得到很多数据的方法就是，嗯，00:24:53.580 --> 00:24:55.340
写东西是因为，00:24:55.340 --> 00:25:00.920
这是很容易找到的数据，你可以得到数以百万计的单词。00:25:00.920 --> 00:25:04.220
从数据的角度来看，这是有道理的。00:25:04.220 --> 00:25:07.805
但最后有点奇怪的是，00:25:07.805 --> 00:25:11.005
当你在建立角色等级模型时，00:25:11.005 --> 00:25:13.714
你的角色等级模型是什么，00:25:13.714 --> 00:25:17.325
实际上随语言的书写系统而异。00:25:17.325 --> 00:25:18.875
所以你，有点，00:25:18.875 --> 00:25:21.990
拥有这些完全不同的书写系统。00:25:21.990 --> 00:25:27.405
所以有些书写系统完全是音位的，00:25:27.405 --> 00:25:32.130
有些字母有一个特定的声音，你说那个声音。00:25:32.130 --> 00:25:34.575
像西班牙语这样的语言很有音位。00:25:34.575 --> 00:25:36.810
有时候有点复杂。00:25:36.810 --> 00:25:38.580
你可能有一个有向图。00:25:38.580 --> 00:25:41.700
这个有向图，ngabulu，有点像，00:25:41.700 --> 00:25:46.470
英语中表示"ng"的N-G听起来就像，00:25:46.470 --> 00:25:49.080
但是，你知道，基本上这只是“jiyawu”，00:25:49.080 --> 00:25:50.939
每个字母都是一个音，00:25:50.939 --> 00:25:52.860
你可以读一下，00:25:52.860 --> 00:25:54.960
它只是，嗯，音位的。00:25:54.960 --> 00:25:58.140
这和英语中的where形成了对比00:25:58.140 --> 00:26:01.890
所有非英语母语人士都知道拼写很糟糕。00:26:01.890 --> 00:26:04.515
它有这种高度石化的东西，00:26:04.515 --> 00:26:05.700
从前，00:26:05.700 --> 00:26:08.885
十世纪左右的音位系统。00:26:08.885 --> 00:26:12.030
但是现在我们有了文字所拥有的系统00:26:12.030 --> 00:26:17.700
相当随意的拼写，实际上并不能很清楚地表示声音。00:26:17.700 --> 00:26:20.490
但它是一种音位系统。00:26:20.490 --> 00:26:23.430
但是有些语言使用更大的单位。00:26:23.430 --> 00:26:25.235
这是，00:26:25.235 --> 00:26:29.715
我把加拿大和inukinstitut写在这里因为这是一个非常漂亮的书写系统。00:26:29.715 --> 00:26:35.820
嗯，但是有很多语言是用字符来表示音节的。00:26:35.820 --> 00:26:38.880
比如在韩语中，00:26:38.880 --> 00:26:41.190
用韩语，每个，嗯，00:26:41.190 --> 00:26:46.860
字母是一种辅音元音组合的音节，就像bar。00:26:46.860 --> 00:26:52.455
如果你能在此基础上再提高一层如果我们再回到中文，00:26:52.455 --> 00:26:57.465
嗯，你可以说，这也是一个音节系统。00:26:57.465 --> 00:27:02.130
但实际上，汉字不仅仅是声音。00:27:02.130 --> 00:27:03.515
它们也有意义。00:27:03.515 --> 00:27:06.435
这是一个表意文字系统00:27:06.435 --> 00:27:09.570
有特定含义的字符。00:27:09.570 --> 00:27:10.860
所以他们有点，呃，00:27:10.860 --> 00:27:14.025
整个语素是一个字母。00:27:14.025 --> 00:27:16.964
你知道，这类语言的另一个例子，00:27:16.964 --> 00:27:19.500
是埃及象形文字，如果你看过的话。00:27:19.500 --> 00:27:23.700
它们是一种表意文字系统，你可以用字母来表达意思。00:27:23.700 --> 00:27:27.420
然后你就有了混合了这几种语言的语言系统。00:27:27.420 --> 00:27:31.875
所以日语有点混合了部分的道德感，00:27:31.875 --> 00:27:34.875
部分表意系统混合在一起。00:27:34.875 --> 00:27:36.100
所以如果你，00:27:36.100 --> 00:27:37.695
一开始就说，00:27:37.695 --> 00:27:41.805
“好吧，我要建立一个基于字符的系统。”这很好。00:27:41.805 --> 00:27:45.495
但实际上，你的角色单位喜欢00:27:45.495 --> 00:27:50.610
字母三元组在像汉语这样的语言中是非常不同的，00:27:50.610 --> 00:27:54.030
通常一个字母三元组，00:27:54.030 --> 00:27:55.140
一个半字，00:27:55.140 --> 00:27:57.915
三个有意义的语素。00:27:57.915 --> 00:27:59.820
而如果你用的是英语，00:27:59.820 --> 00:28:03.105
你的角色三元组就像T-H-O00:28:03.105 --> 00:28:06.980
这个单位仍然太小，没有任何意义。00:28:06.980 --> 00:28:08.810
继续。00:28:08.810 --> 00:28:11.550
所以这两种方法，00:28:11.550 --> 00:28:15.875
一个是做一个完整的角色级模型，另一个是，00:28:15.875 --> 00:28:18.120
某种程度上，你使用字符00:28:18.120 --> 00:28:20.640
为了建造更大的东西，00:28:20.640 --> 00:28:22.860
比如，变成一个更文字级的模型。00:28:22.860 --> 00:28:25.455
我先做这个，再做另一个。00:28:25.455 --> 00:28:27.810
所以对于纯粹的字符级模型，00:28:27.810 --> 00:28:30.785
上次我举了一个例子。你还记得吗?00:28:30.785 --> 00:28:33.820
这就是卷积网络00:28:33.820 --> 00:28:37.710
文本分类的Conneau et-al单词，00:28:37.710 --> 00:28:39.660
这是从一个很大的线开始的00:28:39.660 --> 00:28:43.620
字符，并在上面建立这些卷积层，00:28:43.620 --> 00:28:47.565
在视觉上喜欢网络和分类文件。00:28:47.565 --> 00:28:51.450
这是一种完全的角色层次的模型。00:28:51.450 --> 00:28:54.405
但是这里还有更多的工作要做。00:28:54.405 --> 00:28:59.325
所以机器翻译的人，00:28:59.325 --> 00:29:05.250
机器翻译系统，只读取和写入字符。00:29:05.250 --> 00:29:08.615
当人们第一次尝试这么做的时候，00:29:08.615 --> 00:29:11.940
嗯，有点不太管用，对吧?00:29:11.940 --> 00:29:13.740
人们认为它可能有助于建设00:29:13.740 --> 00:29:17.775
字符级模型，特别是对于像汉语这样的语言。00:29:17.775 --> 00:29:21.240
但是人们无法建立起这样的模型00:29:21.240 --> 00:29:25.365
以及基于单词的模型和前神经系统，00:29:25.365 --> 00:29:28.095
非神经或神经世界。00:29:28.095 --> 00:29:30.930
但这种情况逐渐开始改变。00:29:30.930 --> 00:29:36.480
所以人们开始有成功的字符级解码器，00:29:36.480 --> 00:29:40.065
大约在2015年和2016年，00:29:40.065 --> 00:29:41.790
人们开始说，00:29:41.790 --> 00:29:45.840
你可以做机器翻译00:29:45.840 --> 00:29:50.055
好吧，只是在一个字符级别，有几个星号。00:29:50.055 --> 00:29:53.910
这是我们做的一些工作。00:29:53.910 --> 00:29:56.400
Luong and Manning乐队，来自，00:29:56.400 --> 00:29:59.145
最后一张幻灯片是2015年。00:29:59.145 --> 00:30:03.570
这就是从英语到捷克语的翻译和捷克语的翻译00:30:03.570 --> 00:30:07.800
如果你想激励自己在角色层面上做事，00:30:07.800 --> 00:30:10.650
因为里面有很多可怕的大字00:30:10.650 --> 00:30:15.555
形态学，就像我之前给你们看的例子，稍后我会给你们看更多。00:30:15.555 --> 00:30:20.145
所以人们为捷克语建立了文字级的模型。00:30:20.145 --> 00:30:22.650
嗯，你知道，他们的效果并不好，00:30:22.650 --> 00:30:25.320
部分原因是这些词汇的问题。00:30:25.320 --> 00:30:26.880
所以，嗯，有点，00:30:26.880 --> 00:30:31.905
当时的文字水平是15。7蓝色，00:30:31.905 --> 00:30:37.680
你知道的，这比我们给你的家庭作业打满分要低得多。00:30:37.680 --> 00:30:39.555
[笑声]嗯，但是，你知道，00:30:39.555 --> 00:30:43.740
一个好的蓝线分数取决于语言对的难度。00:30:43.740 --> 00:30:46.320
所以你不是在说捷克语。00:30:46.320 --> 00:30:48.875
但是，这是，00:30:48.875 --> 00:30:51.690
我们讲过的神经MT模型。00:30:51.690 --> 00:30:53.400
这是一个Seq2Seq模型，00:30:53.400 --> 00:31:00.375
然后它有额外的东西来代替任何一种，00:31:00.375 --> 00:31:05.790
单字翻译或者从原文中复制。00:31:05.790 --> 00:31:07.200
所以基本上，00:31:07.200 --> 00:31:12.825
2015年最先进的神经MT，获得15.7蓝。00:31:12.825 --> 00:31:14.970
差别不大，00:31:14.970 --> 00:31:16.380
但是我们能够证明，00:31:16.380 --> 00:31:18.875
听着，我们可以完全建立这个，00:31:18.875 --> 00:31:22.530
字符级模型，实际上，它做得稍微好一些。00:31:22.530 --> 00:31:24.300
所以这个，00:31:24.300 --> 00:31:28.260
在翻译质量方面，00:31:28.260 --> 00:31:32.760
字符，完全基于字符的模型是完全可行的00:31:32.760 --> 00:31:37.380
捕获文本的含义以及基于单词的模型。00:31:37.380 --> 00:31:40.395
这个结果好吗?00:31:40.395 --> 00:31:42.630
在很多方面，在某些方面，00:31:42.630 --> 00:31:45.105
是的，从另一个角度来说，不是。00:31:45.105 --> 00:31:48.810
我的意思是，这个模型真的很难训练，对吧?00:31:48.810 --> 00:31:52.590
所以我们花了大约三周的时间来训练这个模型，在运行时，00:31:52.590 --> 00:31:54.855
它的工作也非常缓慢。00:31:54.855 --> 00:31:57.240
所以字符级模型的问题是，00:31:57.240 --> 00:32:00.135
如果你把它们放到类似LSTM的东西里，00:32:00.135 --> 00:32:02.580
你的序列变长了，对吧。00:32:02.580 --> 00:32:07.110
所以得到的序列长度是原来的7倍。00:32:07.110 --> 00:32:10.440
因为没有太多的信息，00:32:10.440 --> 00:32:15.840
你需要做时间的反向传播。00:32:15.840 --> 00:32:19.040
所以我们把时间往回传播00:32:19.040 --> 00:32:22.590
在我们截短它之前走了600步。00:32:22.590 --> 00:32:23.990
所以这个，有点，00:32:23.990 --> 00:32:25.210
也许这太过分了00:32:25.210 --> 00:32:27.770
但它让模型，嗯，非常慢。00:32:27.770 --> 00:32:32.520
但是我们能够证明它能够得到一些好的效果。00:32:32.520 --> 00:32:33.705
这是一个捷克人，00:32:33.705 --> 00:32:35.230
呃，翻译成捷克语，00:32:35.230 --> 00:32:36.540
她11岁的女儿，00:32:36.540 --> 00:32:39.645
沙尼・巴特说这感觉有点奇怪。00:32:39.645 --> 00:32:42.885
我不知道，也许吧。00:32:42.885 --> 00:32:45.480
有人会说捷克语吗?00:32:45.480 --> 00:32:48.615
没有说捷克语的?00:32:48.615 --> 00:32:51.195
我也不会说捷克语00:32:51.195 --> 00:32:57.390
但是我们可以看到(笑声)我们可以看到这做了一些有趣的事情。00:32:57.390 --> 00:33:00.045
第二行是人工翻译00:33:00.045 --> 00:33:03.010
进入捷克，我们可以用它来做一些指导。00:33:03.010 --> 00:33:04.590
特别是，00:33:04.590 --> 00:33:07.815
在捷克有一个词代表11岁，00:33:07.815 --> 00:33:11.295
你可以看到在第二行有一个蓝色的单词。00:33:11.295 --> 00:33:15.645
你可以看到，尽管只有11岁，00:33:15.645 --> 00:33:19.425
对于11岁的孩子来说00:33:19.425 --> 00:33:21.720
一个字母一个字母地写，00:33:21.720 --> 00:33:25.830
捷克语的“11岁”这个词用起来很漂亮。00:33:25.830 --> 00:33:29.160
相比之下，对于单词级模型，00:33:29.160 --> 00:33:33.585
11岁是一个不为人知的单词，因为它不在词汇表中。00:33:33.585 --> 00:33:37.920
然后它有两种机制来处理未知的单词。00:33:37.920 --> 00:33:39.435
它可以，呃，00:33:39.435 --> 00:33:43.485
或者它可以直接复制它们。00:33:43.485 --> 00:33:46.230
不管出于什么原因，它在这里决定了最好的策略00:33:46.230 --> 00:33:49.580
就是复制，所以完全失败了。00:33:49.580 --> 00:33:53.115
如果我们继续研究字符级模型，00:33:53.115 --> 00:33:55.890
另一件很酷的事情是，00:33:55.890 --> 00:33:57.995
是沙尼・巴特00:33:57.995 --> 00:34:02.370
它能够很好地完成我刚才提到的音译任务。00:34:02.370 --> 00:34:04.755
现在轮到沙尼・巴尔托瓦了00:34:04.755 --> 00:34:07.760
这正是人类翻译所做的。00:34:07.760 --> 00:34:11.440
所以，你知道，它实际上做了一些非常好的事情，嗯，00:34:11.440 --> 00:34:15.635
人工翻译之类的。00:34:15.635 --> 00:34:17.480
事实上，00:34:17.480 --> 00:34:20.605
我在谷歌翻译上花了一点时间，00:34:20.605 --> 00:34:23.195
它在这句话中表现得很好。00:34:23.195 --> 00:34:25.880
好了，这部分开始不一样了，00:34:25.880 --> 00:34:27.815
呃，来自人工翻译。00:34:27.815 --> 00:34:29.480
但实际上并不坏。00:34:29.480 --> 00:34:31.460
这是一种直译。00:34:31.460 --> 00:34:33.210
这个花旗，00:34:33.210 --> 00:34:37.340
翻译的感觉就像在英语文本中。00:34:37.340 --> 00:34:38.875
而人类，00:34:38.875 --> 00:34:43.230
捷克版本中并没有使用feel这个词，00:34:43.230 --> 00:34:45.550
有一点，00:34:45.550 --> 00:34:49.360
嗯，奇怪或奇怪。这就是酷。00:34:49.360 --> 00:34:54.965
好。这里还有一些结果。00:34:54.965 --> 00:34:58.535
这是第二年建立的另一个系统。00:34:58.535 --> 00:35:00.830
这些人杰森・李，00:35:00.830 --> 00:35:03.425
Kyunghyun Cho和Thomas Hoffman。00:35:03.425 --> 00:35:09.515
所以他们想做些什么，我不知道，00:35:09.515 --> 00:35:12.980
更复杂，更神经化00:35:12.980 --> 00:35:16.385
理解原文的意思。00:35:16.385 --> 00:35:20.555
所以他们更多地使用我们上次看到的技术。00:35:20.555 --> 00:35:29.960
在编码端，你从字符嵌入的字母序列开始。00:35:29.960 --> 00:35:34.850
然后用4的卷积，00:35:34.850 --> 00:35:39.710
三个和五个字符来表示。00:35:39.710 --> 00:35:44.270
然后你最大限度地以五步的速度拼车。00:35:44.270 --> 00:35:49.370
所以你得到了三个文本的最大集合表示，00:35:49.370 --> 00:35:51.680
四五圈。00:35:51.680 --> 00:35:54.590
然后你要通过多层的00:35:54.590 --> 00:35:57.815
高速公路网络，并通过它00:35:57.815 --> 00:36:05.570
一个双向门控循环单元这就是你的源表示。00:36:05.570 --> 00:36:07.519
在解码器方面，00:36:07.519 --> 00:36:09.440
它有点像我们的解码器，00:36:09.440 --> 00:36:13.680
它只是运行一个字符级序列模型。00:36:14.110 --> 00:36:21.590
总的来说，他们做的是相反的任务。00:36:21.590 --> 00:36:24.330
这是捷克语到英语的转换。00:36:25.120 --> 00:36:28.325
但是，他们开始取得更好的成绩。00:36:28.325 --> 00:36:31.610
但我的意思是，如果你看这些不同的数字，00:36:31.610 --> 00:36:34.475
我稍后会详细解释这个系统，00:36:34.475 --> 00:36:41.630
我的意思是，这似乎是他们得到很多价值的地方是使用00:36:41.630 --> 00:36:46.700
字符级译码器给了它们很大的价值00:36:46.700 --> 00:36:53.550
源端的这个非常复杂的模型几乎没有给它们任何价值。00:36:54.760 --> 00:36:58.085
最近的一篇论文，00:36:58.085 --> 00:37:03.425
这是Colin Cherry和谷歌的同事们。00:37:03.425 --> 00:37:09.560
所以他们去年又做了一次探索00:37:09.560 --> 00:37:11.570
按顺序执行LSTM00:37:11.570 --> 00:37:16.505
比较单词和基于字符的模型的样式模型。00:37:16.505 --> 00:37:19.220
这是英语到法语的转换00:37:19.220 --> 00:37:23.090
从捷克语到英语这就是我们正在做的。00:37:23.090 --> 00:37:27.440
所以在这两种情况下，当你有一个大模型时，00:37:27.440 --> 00:37:30.005
角色模型为他们赢得了胜利。00:37:30.005 --> 00:37:34.670
蓝色的模型在上面，但是和你一样有趣的是00:37:34.670 --> 00:37:36.830
看这些不同的效果00:37:36.830 --> 00:37:39.140
语言的形态复杂性。00:37:39.140 --> 00:37:41.555
所以对于像捷克语这样的语言，00:37:41.555 --> 00:37:43.850
这是个好主意，00:37:43.850 --> 00:37:45.995
如果你想建立一个好的模型，00:37:45.995 --> 00:37:49.760
使用角色等级，他们会得到一个蓝点的区别，00:37:49.760 --> 00:37:54.710
而对于不使用法语或英语的模型，则有00:37:54.710 --> 00:38:01.295
实际上，使用角色级模型只会获得很少的收益。00:38:01.295 --> 00:38:05.300
我来解释一下这些模型，00:38:05.300 --> 00:38:08.405
这些模型大小不同。00:38:08.405 --> 00:38:17.720
因此，这些模型采用了双向LSTM编码器和单向LSTM解码器。00:38:17.720 --> 00:38:20.405
最简单的模型00:38:20.405 --> 00:38:28.430
一种浅层双向LSTM编码器和一种两层LSTM解码器。00:38:28.430 --> 00:38:33.080
中间的模型有三个深度堆栈00:38:33.080 --> 00:38:39.935
双向LSTM编码器和四个深堆栈LSTM解码器。00:38:39.935 --> 00:38:43.940
最复杂的模型有六层00:38:43.940 --> 00:38:51.530
双向LSTM编码器和8个深堆栈LSTM解码器。00:38:51.530 --> 00:38:53.660
这就是在谷歌工作的好处。00:38:53.660 --> 00:38:55.310
可能是你的项目，00:38:55.310 --> 00:38:59.750
你不想超过三四个。呆在这里。00:38:59.750 --> 00:39:04.310
好的，这就是结果。00:39:04.310 --> 00:39:07.010
所以基本上你发现的是如果你在制作00:39:07.010 --> 00:39:10.100
更小的模型你更擅长文字，00:39:10.100 --> 00:39:15.680
但当你使用大型模型时特别是当你使用一种形态丰富的语言时，00:39:15.680 --> 00:39:18.425
很明显，你开始从角色中获益。00:39:18.425 --> 00:39:21.410
但仍然有一个损失，本质上是00:39:21.410 --> 00:39:27.635
和我们在2015年遭受的损失完全一样，对吧?00:39:27.635 --> 00:39:35.510
这是时间曲线图这三个模型是一样的，00:39:35.510 --> 00:39:40.610
只是轴被改变成LSTM层的总数。00:39:40.610 --> 00:39:45.050
所以本质上，如果你是在文字层面，00:39:45.050 --> 00:39:51.800
你可以运行这三种模型中的任何一种，它们都是可以快速转换的00:39:51.800 --> 00:39:59.000
不是很长时间但是对于字符级模型来说斜率要高得多。00:39:59.000 --> 00:40:04.770
所以运行深层角色级别的模型会变得非常昂贵。00:40:05.860 --> 00:40:09.035
好了，这就是这部分。00:40:09.035 --> 00:40:12.810
然后继续前进。00:40:12.910 --> 00:40:18.395
然后我想看看其他做事的方法。00:40:18.395 --> 00:40:23.930
所以这些模型在某种意义上仍然有文字，但是在哪里00:40:23.930 --> 00:40:29.675
我们要用一些片段来构建单词表示。00:40:29.675 --> 00:40:35.465
人们探索的方法有两种。00:40:35.465 --> 00:40:39.830
一种方法是，看，我们只是想用00:40:39.830 --> 00:40:44.495
与我们在word模型中使用的架构完全相同00:40:44.495 --> 00:40:48.710
只是我们的语言并不能真正做到这一点00:40:48.710 --> 00:40:53.390
至少在某些时候，它们会变成一个个单词。00:40:53.390 --> 00:40:56.750
这些通常被称为词块模型。00:40:56.750 --> 00:40:59.450
特别是，有一种最常见的方法。00:40:59.450 --> 00:41:03.740
它叫BPE，我会详细讲一下。00:41:03.740 --> 00:41:06.695
另一种说法是，00:41:06.695 --> 00:41:09.710
我们要做一种混合物。00:41:09.710 --> 00:41:14.420
所以我们的主要模型是用文字表示的，但是我们00:41:14.420 --> 00:41:19.415
将会有某种工具我们可以构造一个表示，00:41:19.415 --> 00:41:21.530
对于其他未知的单词，00:41:21.530 --> 00:41:24.380
通过在角色或较低层次上做事。00:41:24.380 --> 00:41:26.750
我也会给你们看一些。00:41:26.750 --> 00:41:31.370
这是BPE。00:41:31.370 --> 00:41:36.080
BPE实际上是一个非常简单的概念，没有任何关系00:41:36.080 --> 00:41:41.150
和深度学习有关但是BPE的使用已经成为00:41:41.150 --> 00:41:49.340
非常标准和成功地表示允许您使用的单词00:41:49.340 --> 00:41:52.685
拥有无限的词汇量00:41:52.685 --> 00:41:58.490
而一个无限有效的词汇，同时实际工作的有限的词汇。00:41:58.490 --> 00:42:01.670
字节对编码的起源00:42:01.670 --> 00:42:07.760
名称字节对与自然语言处理或神经网络无关，00:42:07.760 --> 00:42:10.115
我们只是在写一个压缩算法。00:42:10.115 --> 00:42:14.375
这有点像用gzip压缩文档。00:42:14.375 --> 00:42:18.830
什么是基本的字节对编码，00:42:18.830 --> 00:42:23.090
你有一个字节的集合，你是00:42:23.090 --> 00:42:28.820
寻找两个字节中最频繁的序列，00:42:28.820 --> 00:42:32.675
好的，我要把这两个字节的序列相加00:42:32.675 --> 00:42:37.700
我的可能值字典的新元素。00:42:37.700 --> 00:42:42.830
这意味着我可以有257个不同的字节值00:42:42.830 --> 00:42:45.110
说我能把长度缩短00:42:45.110 --> 00:42:49.370
我的顺序，我可以重复一遍，再做一遍。00:42:49.370 --> 00:42:53.900
所以本质上，这项研究表明，00:42:53.900 --> 00:42:58.700
我们可以用这种压缩算法00:42:58.700 --> 00:43:03.890
这是一种创造单词片段的方法00:43:03.890 --> 00:43:08.480
很有用，但不严格使用字节00:43:08.480 --> 00:43:13.400
而是用字符和字符n-gram来命名。00:43:13.400 --> 00:43:16.190
最常见的方法是00:43:16.190 --> 00:43:19.850
字符和字符n克，如果你了解现代社会，00:43:19.850 --> 00:43:22.790
这意味着有unicode可以表示00:43:22.790 --> 00:43:26.060
所有这些可爱的字母，比如加拿大inukinstitut的00:43:26.060 --> 00:43:28.445
音节什么的。00:43:28.445 --> 00:43:31.340
但是Unicode有一个问题，00:43:31.340 --> 00:43:34.595
实际上有很多Unicode字符。00:43:34.595 --> 00:43:36.815
理论上我忘了这个数字。00:43:36.815 --> 00:43:40.520
我想大概有200,000个Unicode字符。00:43:40.520 --> 00:43:44.900
但无论如何，如果你想处理包括东亚语言在内的一些语言，00:43:44.900 --> 00:43:49.205
也许你需要20000个字符，这是很多的。00:43:49.205 --> 00:43:54.920
所以实际上有些人回到字节上说，00:43:54.920 --> 00:43:57.650
“你知道20万，这是一个很大的词汇量。00:43:57.650 --> 00:43:59.270
我不想处理任何事情。”00:43:59.270 --> 00:44:01.700
不好意思，200,000是一个很大的词汇量。00:44:01.700 --> 00:44:03.845
我甚至不想处理这么大的问题。00:44:03.845 --> 00:44:10.010
为什么我不直接用字节来做这些算法呢?00:44:10.010 --> 00:44:14.495
这意味着在UTF-8编码中，00:44:14.495 --> 00:44:17.870
中文字符每三个字节。00:44:17.870 --> 00:44:21.890
所以你实际上必须――只有当你00:44:21.890 --> 00:44:27.100
实际上，合并在一起的几个字节是常见的排序器。00:44:27.100 --> 00:44:30.600
好。更具体地说,00:44:30.600 --> 00:44:32.040
这是怎么回事?00:44:32.040 --> 00:44:37.110
所以我们在做这种自底向上的短序列聚类。00:44:37.110 --> 00:44:40.245
我们从一元词汇表开始，00:44:40.245 --> 00:44:44.120
也就是所有Unicode字符和一些数据。00:44:44.120 --> 00:44:49.310
然后我们会问，这里最常见的ngram是什么?00:44:49.310 --> 00:44:53.985
一开始是双字母对，我们把它加到词汇表中。00:44:53.985 --> 00:44:56.610
所以如果我们开始，00:44:56.610 --> 00:44:59.670
我们可以取我们的文本，00:44:59.670 --> 00:45:00.810
我待会再谈这个。00:45:00.810 --> 00:45:05.895
假设我们有一个文本，它被分成单词，所以我们有单词标记。00:45:05.895 --> 00:45:11.670
所以我们可以用字典来表示这里有一些频率相同的单词。00:45:11.670 --> 00:45:17.985
现在我们要找一个普通的字母序列，我们说，“哦，es。”00:45:17.985 --> 00:45:20.610
这种情况发生了9次，00:45:20.610 --> 00:45:25.250
在这个数据中，因为我们在左边有单词的计数。00:45:25.250 --> 00:45:30.600
所以，嗯，我们从词汇表中所有的字母开始。00:45:30.600 --> 00:45:34.515
我们发现了一个最常见的字母序列，比如es，00:45:34.515 --> 00:45:40.245
所以我们会说，“让我们把这些集合起来，让它成为我们词汇中的一个新东西。”00:45:40.245 --> 00:45:43.185
现在我们的词汇中又多了一项。00:45:43.185 --> 00:45:47.340
那么最常见的ngram序列是什么呢?00:45:47.340 --> 00:45:51.030
实际上所有这些e后面都是t，00:45:51.030 --> 00:45:52.650
我们还有es，00:45:52.650 --> 00:45:55.065
频率为9的t，00:45:55.065 --> 00:45:58.050
所以我们可以把它加到我们的词汇表中。00:45:58.050 --> 00:45:59.670
然后我们又问，00:45:59.670 --> 00:46:03.030
另一个常见的字母序列是什么?00:46:03.030 --> 00:46:06.825
我们看一下，有7种o的情况，00:46:06.825 --> 00:46:10.170
我想有七种情况要么是o要么是o w，00:46:10.170 --> 00:46:13.380
所以我们可以把它们混在一起00:46:13.380 --> 00:46:17.795
再做一个lo w，如果我们运行这个，00:46:17.795 --> 00:46:22.410
我们开始构建这些公共字母序列的簇，00:46:22.410 --> 00:46:26.205
像est这样的普通比特，00:46:26.205 --> 00:46:28.450
还有一些常用词，00:46:28.450 --> 00:46:31.470
英语中类似的东西很快就会出现00:46:31.470 --> 00:46:35.130
聚集在一起，成为我们词汇的一个单元。00:46:35.130 --> 00:46:38.300
我们这样做了一段时间。00:46:38.300 --> 00:46:40.650
通常我们做的是决定00:46:40.650 --> 00:46:44.250
我们想要使用的词汇量。我们说,“好吧。00:46:44.250 --> 00:46:47.400
我想要的词汇量是8000个单词。”00:46:47.400 --> 00:46:49.905
这意味着我的模型会很快，00:46:49.905 --> 00:46:54.255
我们一直这样做直到我们的词汇量达到8000个。00:46:54.255 --> 00:46:56.900
这意味着我们的词汇将会包含在其中00:46:56.900 --> 00:47:00.650
都是单个字母，因为我们从它们开始00:47:00.650 --> 00:47:07.285
有公共子序列的单词，比如现在我们词汇表中的es和est，00:47:07.285 --> 00:47:11.420
但也有完整的单词当有普通单词的时候，比如，你知道，00:47:11.420 --> 00:47:13.280
the, and too, and with，00:47:13.280 --> 00:47:16.835
等等，将成为我们词汇的一部分。00:47:16.835 --> 00:47:21.000
当我们有了一段文字，我们就可以00:47:21.000 --> 00:47:25.125
一个确定的最长的单词片段分割，00:47:25.125 --> 00:47:28.310
我们会说这是我们的一组单词。00:47:28.310 --> 00:47:30.900
对于输入的文本，00:47:30.900 --> 00:47:32.700
我们变成单词块，00:47:32.700 --> 00:47:37.695
然后我们用MT系统运行它就像我们在用单词一样，00:47:37.695 --> 00:47:39.900
但实际上是一些文字，00:47:39.900 --> 00:47:42.000
然后在输出端，00:47:42.000 --> 00:47:45.845
我们只是根据需要把它们连接起来。00:47:45.845 --> 00:47:49.875
好。所以我们得到了这种基于单词的自动系统。00:47:49.875 --> 00:47:53.525
这被证明是一个非常成功的系统。00:47:53.525 --> 00:47:57.525
使用字节对编码的想法00:47:57.525 --> 00:48:02.115
2015年出现，2016年，00:48:02.115 --> 00:48:06.480
机器翻译研修班已成为每年的主要比赛项目00:48:06.480 --> 00:48:12.300
MT系统是用字节对编码来构建的。00:48:12.300 --> 00:48:14.700
如果你看看去年的比赛，00:48:14.700 --> 00:48:16.350
有更多的种类，00:48:16.350 --> 00:48:21.044
但实际上，许多顶级系统仍然使用字节对编码，00:48:21.044 --> 00:48:24.345
这是一种很好的做事方式。00:48:24.345 --> 00:48:29.130
对于谷歌的神经机器翻译，00:48:29.130 --> 00:48:33.450
它们有效地使用了字节对编码的一种变体。00:48:33.450 --> 00:48:37.160
所以他们不用完全相同的算法，00:48:37.160 --> 00:48:40.260
他们使用稍微不同的算法00:48:40.260 --> 00:48:44.010
使用语言模型，他们说，00:48:44.010 --> 00:48:47.285
不是单纯的计数，00:48:47.285 --> 00:48:52.575
他们在说，“聚集在一起的东西会最大限度地减少00:48:52.575 --> 00:48:58.410
我的语言模型的困惑，然后把这些东西集中起来，重复一遍?”00:48:58.410 --> 00:49:02.280
他们做了，他们做了这个模型的两个版本。00:49:02.280 --> 00:49:07.170
第一个版本，文字模型有点像，00:49:07.170 --> 00:49:11.585
字节对编码假设您有一个初始标记00:49:11.585 --> 00:49:16.035
然后你就有了一些单词，00:49:16.035 --> 00:49:17.880
用这个算法。00:49:17.880 --> 00:49:20.790
然后他们做了第二个版本，00:49:20.790 --> 00:49:25.050
你可以在GitHub网站上找到sentencepiece模型它说，00:49:25.050 --> 00:49:28.560
如果我们需要先用符号表示单词，那就有问题了00:49:28.560 --> 00:49:30.208
因为我们需要一个记号赋予器00:49:30.208 --> 00:49:32.550
每一种语言，都需要大量的工作。”00:49:32.550 --> 00:49:34.545
所以也许与其这样，00:49:34.545 --> 00:49:36.825
我们可以治疗，00:49:36.825 --> 00:49:39.080
从一个字符序列，00:49:39.080 --> 00:49:44.855
保留空白，并将其视为凝聚过程的一部分，00:49:44.855 --> 00:49:47.045
所以，嗯，00:49:47.045 --> 00:49:50.015
你只需要构建你的单词片段00:49:50.015 --> 00:49:54.485
通常会在一边或另一边留出空间，00:49:54.485 --> 00:49:57.710
因为单词里面的东西通常是00:49:57.710 --> 00:50:01.680
更普通的-更常见的团块，你把它们堆积起来，00:50:01.680 --> 00:50:04.755
这被证明是非常成功的。00:50:04.755 --> 00:50:10.530
特别地，有一个地方你们可能会看到这个，00:50:10.530 --> 00:50:14.375
我们还没有在课堂上描述它，00:50:14.375 --> 00:50:19.950
但是最近有一项研究我们下周会在课堂上讨论00:50:19.950 --> 00:50:22.980
特别是建立这些变压器模型，00:50:22.980 --> 00:50:26.160
谷歌发布了这个BERT模型00:50:26.160 --> 00:50:29.460
非常好的文字表达。00:50:29.460 --> 00:50:32.835
如果你下载BERT并尝试使用它，00:50:32.835 --> 00:50:36.615
你会发现它不是通过文字来运作的，00:50:36.615 --> 00:50:40.050
它对字块进行操作。00:50:40.050 --> 00:50:43.260
所以它的词汇量很大。00:50:43.260 --> 00:50:46.470
它不是8000个单词。00:50:46.470 --> 00:50:47.685
我忘了电话号码，00:50:47.685 --> 00:50:50.640
但是这些模型有很大的词汇量，00:50:50.640 --> 00:50:55.380
但它们仍然不是一个庞大的词汇量，它使用的是单词片段。00:50:55.380 --> 00:50:57.450
所以词汇表中有很多单词。00:50:57.450 --> 00:50:59.070
如果你看一下英语模型，00:50:59.070 --> 00:51:01.140
它不仅有f这样的词，00:51:01.140 --> 00:51:04.470
但它甚至有费尔法克斯和1910年的文字，00:51:04.470 --> 00:51:06.045
这并不常见。00:51:06.045 --> 00:51:10.245
但无论如何，它涵盖了所有的单词，00:51:10.245 --> 00:51:12.660
它再次使用了这个单词的概念。00:51:12.660 --> 00:51:16.075
如果我想用hypatia这个词来表示，00:51:16.075 --> 00:51:17.895
这不在词汇表中，00:51:17.895 --> 00:51:19.800
所以我把它拼凑起来。00:51:19.800 --> 00:51:21.795
有一个h表示，00:51:21.795 --> 00:51:24.110
在伯特版本中，00:51:24.110 --> 00:51:26.880
这与谷歌NMT版本不同，00:51:26.880 --> 00:51:32.625
非初始词块在开头用两个散列表示，00:51:32.625 --> 00:51:36.944
所以我可以把它和h##yp等放在一起，00:51:36.944 --> 00:51:40.200
这就是我对hypatia的描述。00:51:40.200 --> 00:51:43.245
有效地，我有字向量，00:51:43.245 --> 00:51:45.435
四个单词，00:51:45.435 --> 00:51:47.880
然后我要想办法怎么处理它们。00:51:47.880 --> 00:51:51.600
最简单也是最常见的方法就是求它们的平均值。00:51:51.600 --> 00:51:53.340
显然，你还可以做其他事情。00:51:53.340 --> 00:51:56.100
你可以使用ConvNet和maxpool，也可以运行00:51:56.100 --> 00:52:00.640
一些LSTM之类的东西来组合一个表示。00:52:00.840 --> 00:52:07.220
好。是的。这些就是模型，00:52:07.220 --> 00:52:10.530
用一些单词给你00:52:10.530 --> 00:52:14.835
无限的词汇量，并通过一个正常的系统运行它们。00:52:14.835 --> 00:52:18.335
另一种可能性是，00:52:18.335 --> 00:52:22.530
我们想和角色打交道，这样我们就能处理无穷无尽的词汇，00:52:22.530 --> 00:52:27.945
但我们将把这些整合到一个更大的系统中。”00:52:27.945 --> 00:52:30.540
我们做了很多工作00:52:30.540 --> 00:52:33.990
从某种意义上说，这是很明显的事情。00:52:33.990 --> 00:52:38.730
2014年的这项研究是早期的研究之一。00:52:38.730 --> 00:52:40.005
他们说，00:52:40.005 --> 00:52:42.165
我们可以从角色开始。00:52:42.165 --> 00:52:47.640
我们可以对字符进行卷积来生成单词嵌入，00:52:47.640 --> 00:52:53.250
然后我们可以用这些词嵌入到更高层次的模型中。”00:52:53.250 --> 00:52:58.215
这实际上是一种固定的窗口模型用来做词性标注。00:52:58.215 --> 00:53:00.240
这说得通。00:53:00.240 --> 00:53:01.935
不是卷积，00:53:01.935 --> 00:53:03.690
您可以使用LSTM。00:53:03.690 --> 00:53:06.720
这是一年后的工作，00:53:06.720 --> 00:53:07.835
他们说，00:53:07.835 --> 00:53:09.795
我们还会建立00:53:09.795 --> 00:53:12.245
字符的单词表示。00:53:12.245 --> 00:53:17.520
我们要做的是运行字符级的Bi-LSTMs，00:53:17.520 --> 00:53:20.044
把最后两种状态连接起来，00:53:20.044 --> 00:53:23.430
我们称之为外显表示，00:53:23.430 --> 00:53:27.750
然后我们要把这个词表示成a00:53:27.750 --> 00:53:34.890
语言模型是一个更高层次的LSTM，它沿着一系列单词工作。00:53:34.890 --> 00:53:37.785
我想我只是-哦，是的。00:53:37.785 --> 00:53:41.400
词汇，它们是训练，嗯，像性格00:53:41.400 --> 00:53:44.130
是的。这一点很重要。00:53:44.130 --> 00:53:50.550
是的，如果你在学习，你会学习，我的意思是这是隐藏层。00:53:50.550 --> 00:53:54.150
我想我实际上并没有显示输入层，而是显示了输入层00:53:54.150 --> 00:53:58.260
你要为每个字符学习一个向量。00:53:58.260 --> 00:54:01.800
所以实际上你在做我们看到的同样的事情00:54:01.800 --> 00:54:07.545
在此之前，您将从每个字符的随机表示开始。00:54:07.545 --> 00:54:13.590
你把它嵌入到一个单词序列LSTM中，00:54:13.590 --> 00:54:20.160
您的目标是尽量减少高层次LSTM as的复杂性，00:54:20.160 --> 00:54:26.100
作为一个语言模型，它过滤掉它的梯度。00:54:26.100 --> 00:54:30.450
它想要得到字符向量00:54:30.450 --> 00:54:35.860
产生好的词向量，产生低，嗯，困惑。00:54:35.900 --> 00:54:41.220
好问题。这是，00:54:41.220 --> 00:54:44.820
一个稍微复杂一点的版本00:54:44.820 --> 00:54:48.720
这是最近的想法是我们可以建立吗00:54:48.720 --> 00:54:53.505
一个好的语言模型从字符开始00:54:53.505 --> 00:54:59.010
想要开发一些相关的子单词和稀有单词。00:54:59.010 --> 00:55:02.760
所以他们建造了这种00:55:02.760 --> 00:55:07.815
这个更复杂的模型，我们将会讲到为什么00:55:07.815 --> 00:55:11.805
我们从一个表示为字符的单词开始。00:55:11.805 --> 00:55:14.700
我们构建了字符嵌入00:55:14.700 --> 00:55:17.910
进入卷积网络，然后向上。00:55:17.910 --> 00:55:20.354
所以如果我们一次取一块，00:55:20.354 --> 00:55:24.420
你为每个字符嵌入了一个字符。00:55:24.420 --> 00:55:28.680
然后是卷积层00:55:28.680 --> 00:55:33.540
然后是rep，上面有各种过滤器，00:55:33.540 --> 00:55:37.800
2克，3克和4克的字符序列。00:55:37.800 --> 00:55:41.340
所以你得到了部分单词的表示。00:55:41.340 --> 00:55:48.600
然后从卷积网络中，你可以做最大汇聚00:55:48.600 --> 00:55:51.510
就像选择哪一个00:55:51.510 --> 00:55:56.155
这些n克最能代表一个单词的意思。00:55:56.155 --> 00:56:00.350
在那之后他们所做的就是00:56:00.350 --> 00:56:05.030
他们有一个字符n克的输出表示，00:56:05.030 --> 00:56:12.095
然后他们把这些信息输入到高速公路网络中就像我们上次提到的那样。00:56:12.095 --> 00:56:16.985
然后它的输出在word级别，00:56:16.985 --> 00:56:20.405
进入LSTM网络，00:56:20.405 --> 00:56:24.365
这个LSTM网络现在是文字级的LSTM网络，00:56:24.365 --> 00:56:27.710
你想要的是最小化00:56:27.710 --> 00:56:32.365
就像我们之前看到的神经语言模型。00:56:32.365 --> 00:56:35.385
那么，他们用这个能说明什么呢?00:56:35.385 --> 00:56:38.910
嗯，他们可以用它证明的第一件事是它00:56:38.910 --> 00:56:43.950
实际上，尽管存在这种怀疑，但作为一种语言模型，它还是很好用00:56:43.950 --> 00:56:46.800
我没有告诉你关于…的事实00:56:46.800 --> 00:56:50.040
问题是你可以建立这种角色级别的模型00:56:50.040 --> 00:56:56.535
训练他们，他们工作到第一近似值以及单词级语言模型。00:56:56.535 --> 00:56:59.790
但是他们观察到的一个现象是你可以00:56:59.790 --> 00:57:03.840
得到了同样好的结果，但与小得多的模型。00:57:03.840 --> 00:57:05.235
上面这里是00:57:05.235 --> 00:57:10.695
字符级LSTM模型和它们构建的模型的word模型。00:57:10.695 --> 00:57:14.460
这是这个数据集上的一大堆模型。00:57:14.460 --> 00:57:19.410
随着时间的流逝，困惑越来越少，00:57:19.410 --> 00:57:23.790
得到78。4，题目说我们能00:57:23.790 --> 00:57:29.100
用78.9 perplexity创建一个非常好的角色模型00:57:29.100 --> 00:57:32.730
我们的模型要小得多，这个模型00:57:32.730 --> 00:57:35.760
5200万个参数，而我们的模型00:57:35.760 --> 00:57:39.240
一个角色等级的作品只有1900万个参数。00:57:39.240 --> 00:57:41.685
大概是大小的40%00:57:41.685 --> 00:57:45.210
这看起来很有趣。00:57:45.210 --> 00:57:51.510
但也许更有趣的是往里面看00:57:51.510 --> 00:57:55.500
发生在表示单词的时候00:57:55.500 --> 00:57:59.745
由人物塑造而成，这部分有点酷。00:57:59.745 --> 00:58:06.064
这张图显示的是，对于顶部的单词，00:58:06.064 --> 00:58:08.235
他的，你的，理查德，交易。00:58:08.235 --> 00:58:11.520
它问的是，还有哪些词是最多的00:58:11.520 --> 00:58:15.510
和计算出来的表示形式相似。00:58:15.510 --> 00:58:21.480
上面的部分是word级LSTM模型的输出这是可以的。00:58:21.480 --> 00:58:23.550
理查德和乔纳森长得很像，00:58:23.550 --> 00:58:25.515
罗伯特，尼尔和南希等等。00:58:25.515 --> 00:58:30.810
虽然虽然让虽然分钟主要可以。00:58:30.810 --> 00:58:35.730
但有趣的是他们的角色等级模型，00:58:35.730 --> 00:58:37.605
特别是，00:58:37.605 --> 00:58:41.910
有趣的是，首先你记得他们有00:58:41.910 --> 00:58:46.845
字符嵌入，通过卷积层和最大池。00:58:46.845 --> 00:58:51.150
如果在这一点上你问什么是最重要的00:58:51.150 --> 00:58:55.830
类似的是，基本上它仍然在记忆关于人物的事情。00:58:55.830 --> 00:58:58.350
所以和while最相似的单词，00:58:58.350 --> 00:59:01.740
智利，整个，同时也是白色的。00:59:01.740 --> 00:59:06.600
至少对于第一个都以LE结尾。00:59:06.600 --> 00:59:10.560
你在其他地方也能看到这种模式，00:59:10.560 --> 00:59:15.630
hard以ARD结尾，rich。00:59:15.630 --> 00:59:18.390
你只是得到了这个字符序列的相似性，00:59:18.390 --> 00:59:20.595
它根本没有什么意义。00:59:20.595 --> 00:59:25.500
但有趣的是，当他们把它穿过高速公路的时候，00:59:25.500 --> 00:59:29.775
高速公路各层都在成功地学习如何去做00:59:29.775 --> 00:59:33.285
转换这些字符序列表示00:59:33.285 --> 00:59:35.685
变成能抓住意义的东西。00:59:35.685 --> 00:59:38.850
如果你说在。的输出00:59:38.850 --> 00:59:44.295
高速公路层什么词是最相似的然后它似乎工作得很好，00:59:44.295 --> 00:59:49.245
而与此同时，理查德则与爱德华相似，杰拉德、爱德华与卡尔。00:59:49.245 --> 00:59:51.750
他们现在工作起来更像00:59:51.750 --> 00:59:55.755
用于捕获语义相似性的词级模型。00:59:55.755 --> 00:59:57.900
这看起来很酷。00:59:57.900 --> 01:00:02.070
然后他们说如果我们问01:00:02.070 --> 01:00:06.555
不在模型词汇表中的单词。01:00:06.555 --> 01:00:09.150
如果它们不在模型的词汇表中，01:00:09.150 --> 01:00:13.890
word level模型不能做任何事情这就是为什么你会得到这些破折号。01:00:13.890 --> 01:00:16.650
他们想要展示的是01:00:16.650 --> 01:00:19.470
角色级模型仍然运行得很好。01:00:19.470 --> 01:00:23.310
所以如果你把7个O放在中间01:00:23.310 --> 01:00:27.300
它正确地决定了这个样子，01:00:27.300 --> 01:00:29.370
看，看，实际上是看01:00:29.370 --> 01:00:33.150
最相似的词，实际上工作得很好。01:00:33.150 --> 01:00:36.375
其他一些类似的例子，计算机辅助，01:00:36.375 --> 01:00:38.865
被认为最类似于计算机引导，01:00:38.865 --> 01:00:44.685
计算机驱动的，计算机化的，计算机化的，你得到了非常相似的合理的结果。01:00:44.685 --> 01:00:47.010
然后上面的小图片，01:00:47.010 --> 01:00:49.755
对的是，01:00:49.755 --> 01:00:56.235
展示，嗯，一个已经学过的单位的二维可视化。01:00:56.235 --> 01:00:58.770
所以红色，01:00:58.770 --> 01:01:02.760
红色的是单词字符前缀，01:01:02.760 --> 01:01:05.550
蓝色的是字符后缀，01:01:05.550 --> 01:01:09.180
橙色的东西是连字符的01:01:09.180 --> 01:01:13.050
就像在电脑引导和灰色之间的一切。01:01:13.050 --> 01:01:14.790
所以从某种意义上说，01:01:14.790 --> 01:01:19.050
用它来挑选单词的不同重要部分。01:01:19.050 --> 01:01:26.880
好。这就是为什么，我想这也是一个很好的例子，说明你可以01:01:26.880 --> 01:01:30.870
把不同种类的积木组合在一起01:01:30.870 --> 01:01:32.670
制作更强大的模型01:01:32.670 --> 01:01:35.415
也要考虑期末项目。01:01:35.415 --> 01:01:37.450
好吧。01:01:45.200 --> 01:01:46.770
嗯,这是01:01:46.770 --> 01:01:51.135
回到另一个神经机器翻译系统的例子01:01:51.135 --> 01:01:55.995
做这个混合架构，它有文字级和字符级。01:01:55.995 --> 01:01:59.400
我之前向您展示了一个纯粹的字符级模型。01:01:59.400 --> 01:02:02.580
我的意思是，我们建造它是出于兴趣01:02:02.580 --> 01:02:05.940
它做得很好，但是我们真的想要建造它01:02:05.940 --> 01:02:09.810
混合模型，因为它看起来更实用01:02:09.810 --> 01:02:14.265
建立一个相对快速和良好的翻译。01:02:14.265 --> 01:02:16.860
所以我们的想法是主要建造01:02:16.860 --> 01:02:20.550
一个单词级的神经机器翻译系统01:02:20.550 --> 01:02:26.970
当我们有罕见或不可见的文字时，能够处理角色级别的东西。01:02:26.970 --> 01:02:30.090
结果很不错，01:02:30.090 --> 01:02:33.000
成功地提高了性能。01:02:33.000 --> 01:02:35.355
这个模型是这样的。01:02:35.355 --> 01:02:40.575
我们将运行一个相当标准的，01:02:40.575 --> 01:02:47.100
序列到序列与注意LSTM神经机器翻译系统。01:02:47.100 --> 01:02:51.900
在我的照片中，我的意思是，它实际上是一个四层深的系统，但在我的照片中，我01:02:51.900 --> 01:02:56.490
显示了少于四个层次的堆叠，使它更容易看到的东西。01:02:56.490 --> 01:03:01.920
我们将用16000个单词的合理词汇量来运行它。01:03:01.920 --> 01:03:07.995
对于普通单词，我们只需要输入单词表示01:03:07.995 --> 01:03:13.320
我们的神经机器翻译模型但是对于不在我们词汇表中的单词01:03:13.320 --> 01:03:19.530
通过使用字符级LSTM计算出它们的单词表示形式，01:03:19.530 --> 01:03:23.130
相反地，当我们开始生成单词的时候01:03:23.130 --> 01:03:28.710
另一方面，我们有一个词汇量为16,000的软最大值。01:03:28.710 --> 01:03:34.275
它可以生成像[噪音]这样的单词，但其中一个单词就是扣篮符号。01:03:34.275 --> 01:03:38.610
如果它生成了扣篮符号，我们运行a-我们取01:03:38.610 --> 01:03:42.060
这个隐藏的表示，并将其作为01:03:42.060 --> 01:03:47.370
初始输入到字符级LSTM，然后我们有字符级01:03:47.370 --> 01:03:51.390
LSTM生成一个字符序列，直到它生成为止01:03:51.390 --> 01:03:56.650
一个停止符号，我们用它来生成单词。嗯- - - - - -01:03:57.240 --> 01:04:02.080
好。所以我们最终得到这样的结果01:04:02.080 --> 01:04:08.170
由8个LSTM层混合组成的堆栈。嗯,是的。01:04:08.170 --> 01:04:12.690
(听不清)你总会得到一些扣篮符号的概率。01:04:12.690 --> 01:04:15.180
如果你想要得到合适的梯度，01:04:15.180 --> 01:04:19.230
你――你总是要逐字逐句地读，但你――你做了什么?01:04:19.230 --> 01:04:22.870
我经常说，你只在训练时跑步，01:04:22.870 --> 01:04:28.990
只有当扣篮符号接收到最大的可能性时，才运行字符级LSTM。01:04:28.990 --> 01:04:29.905
所以我们- - - - - -01:04:29.905 --> 01:04:30.685
那是什么?01:04:30.685 --> 01:04:34.335
所以在训练的时候，01:04:34.335 --> 01:04:36.510
这是技术的决定性因素，对吧?01:04:36.510 --> 01:04:39.705
你知道来源和目标，01:04:39.705 --> 01:04:43.740
所以在训练的时候，01:04:43.740 --> 01:04:46.230
我们已经决定了词汇量，对吧?01:04:46.230 --> 01:04:51.400
我们刚刚决定了15999个最常见的单词，01:04:51.400 --> 01:04:53.950
那些和扣篮是我们的词汇。01:04:53.950 --> 01:04:57.115
对于输入端和输出端，01:04:57.115 --> 01:05:00.895
我们知道哪些单词不在我们的词汇表中。01:05:00.895 --> 01:05:03.400
如果它不在我们的词汇表中，01:05:03.400 --> 01:05:04.840
我们在运行这个。01:05:04.840 --> 01:05:07.450
如果我们的词汇表中没有输出，01:05:07.450 --> 01:05:11.950
我们在运行那个，否则我们就根本不运行它。01:05:11.950 --> 01:05:18.130
这一点我没有解释，但实际上可能很重要01:05:18.130 --> 01:05:23.920
比如当我们计算损失的时候01:05:23.920 --> 01:05:26.200
传播，在这里，01:05:26.200 --> 01:05:28.120
有两种损失。01:05:28.120 --> 01:05:32.920
在这个职位上，你知道你想要在语言层面上有所损失，01:05:32.920 --> 01:05:36.085
扣篮的概率是1，01:05:36.085 --> 01:05:41.470
这个模型我们设为softmax，我们设UNK的概率是0。2。01:05:41.470 --> 01:05:44.515
所以这里有损失，其次，01:05:44.515 --> 01:05:48.400
你想产生一个特定的字符序列，你也有01:05:48.400 --> 01:05:53.410
这是一种损失，因为你已经知道了你输入字符的概率。01:05:53.410 --> 01:05:55.825
然后，01:05:55.825 --> 01:05:58.570
我想我们，我想艾比简要地提过这个。01:05:58.570 --> 01:06:01.810
通常，解码器做一些01:06:01.810 --> 01:06:06.310
波束搜索在决定之前考虑不同的可能性，01:06:06.310 --> 01:06:10.795
一个单词序列的最大概率是1。01:06:10.795 --> 01:06:13.930
这是一个稍微复杂一点的版本。01:06:13.930 --> 01:06:17.830
所以当运行它的时候有一个字级的光束搜索01:06:17.830 --> 01:06:23.335
也做一个字符级的波束搜索，以考虑不同的可能性。01:06:23.335 --> 01:06:27.235
如果你想把这两者结合起来。01:06:27.235 --> 01:06:29.785
但本质上，01:06:29.785 --> 01:06:32.980
这招很管用。01:06:32.980 --> 01:06:40.510
这是2015年WMT的获奖系统01:06:40.510 --> 01:06:43.540
使用了30倍的数据和集成在一起01:06:43.540 --> 01:06:48.115
与为该任务提供的数据相比，还有三个系统。01:06:48.115 --> 01:06:52.675
这是我之前展示的系统，题目给了18。3。01:06:52.675 --> 01:06:54.940
如果你还记得我们的角色01:06:54.940 --> 01:06:58.375
纯文字等级系统得到18.5分。01:06:58.375 --> 01:07:02.500
通过建立这个混合系统，01:07:02.500 --> 01:07:07.600
我们能够建立一个更好的系统大约是2。5个蓝点，01:07:07.600 --> 01:07:12.805
嗯，比这个单词级或字符级系统都要多。01:07:12.805 --> 01:07:15.070
这很好，01:07:15.070 --> 01:07:17.725
这是当时最先进的技术。01:07:17.725 --> 01:07:21.355
当然，如果你仔细观察，01:07:21.355 --> 01:07:24.430
这与目前的技术水平相差甚远。01:07:24.430 --> 01:07:30.370
因为当我之前给你们看谷歌系统的幻灯片时，01:07:30.370 --> 01:07:33.310
你会注意到的01:07:33.310 --> 01:07:37.600
在20多岁时，这个数字要高得多，但随着时间的推移，情况就是这样。01:07:37.600 --> 01:07:39.490
嗯,好吧。01:07:39.490 --> 01:07:41.800
这里有一个例子01:07:41.800 --> 01:07:46.300
这些不同的系统在工作，他们犯了一些错误。01:07:46.300 --> 01:07:48.775
这是一个精心挑选的例子，01:07:48.775 --> 01:07:51.940
我们的系统，混合系统，01:07:51.940 --> 01:07:54.985
效果很好，因为这就是你想看到的。01:07:54.985 --> 01:07:57.685
所以，你知道，01:07:57.685 --> 01:08:02.260
你可以看到一些可能出错的缺陷。01:08:02.260 --> 01:08:05.155
在这种情况下，01:08:05.155 --> 01:08:09.550
你知道字符级系统在这里不起作用，因为它只是排序01:08:09.550 --> 01:08:15.250
从Steph开始，它似乎是自由联想的，01:08:15.250 --> 01:08:19.690
一个完全虚构的名字，和来源没有任何关系。01:08:19.690 --> 01:08:22.570
这个不是很好。01:08:22.570 --> 01:08:27.595
“等级系统”这个词突然出现了，01:08:27.595 --> 01:08:30.340
所以你记得当它产生一个扣篮时，01:08:30.340 --> 01:08:36.400
词级系统在产生时，会用到注意力。01:08:36.400 --> 01:08:38.890
所以当它想要生成，01:08:38.890 --> 01:08:42.010
它关注的是词语和来源。01:08:42.010 --> 01:08:45.070
当它产生灌篮有两个策略。01:08:45.070 --> 01:08:50.410
它既可以对单词进行最大限度的单字母翻译01:08:50.410 --> 01:08:56.140
把注意力放在或者它可以复制它最大程度上集中注意力的单词。01:08:56.140 --> 01:08:57.790
在这种情况下，01:08:57.790 --> 01:09:02.890
它选择翻译它最大程度上关注的词，而不是它本身01:09:02.890 --> 01:09:08.080
最重要的是把注意力放在术后而不是诊断上。01:09:08.080 --> 01:09:11.290
然后你就得到了这个po po，01:09:11.290 --> 01:09:14.140
之后，我们完全失去了这个词。01:09:14.140 --> 01:09:17.965
在这个例子中，在这个例子中，01:09:17.965 --> 01:09:20.065
一个混合系统，01:09:20.065 --> 01:09:24.370
只是工作得很漂亮，给你完全正确的翻译。01:09:24.370 --> 01:09:26.740
是的。嗯,当然,01:09:26.740 --> 01:09:29.035
在现实世界中并不总是那么好。01:09:29.035 --> 01:09:31.000
这里有一个不同的例子。01:09:31.000 --> 01:09:35.455
这是我之前展示给11岁女儿的例子。01:09:35.455 --> 01:09:40.420
在这个例子中，01:09:40.420 --> 01:09:44.350
混合模型具有与字符模型相同的强度。01:09:44.350 --> 01:09:50.065
在翻译过程中，它正确地生成了一个字符级的11岁孩子，01:09:50.065 --> 01:09:52.300
但这一次，不管什么原因，01:09:52.300 --> 01:09:56.170
这是一个混合模型01:09:56.170 --> 01:10:01.165
生成这些名字，它将Shani Bart翻译成Graham Bart。01:10:01.165 --> 01:10:04.015
而角色级模型是正确的。01:10:04.015 --> 01:10:06.700
实际上，我认为这是01:10:06.700 --> 01:10:10.120
该混合模型与字符级模型进行了比较。01:10:10.120 --> 01:10:13.060
这是因为字符级生成器是01:10:13.060 --> 01:10:16.670
这是第二层。01:10:17.190 --> 01:10:20.605
对于纯字符级模型，01:10:20.605 --> 01:10:27.010
它能够非常有效地使用字符序列作为条件上下文。01:10:27.010 --> 01:10:28.870
而我们的混合模型，01:10:28.870 --> 01:10:32.410
虽然我们提供了隐藏的表示01:10:32.410 --> 01:10:34.450
as中的单词级模型01:10:34.450 --> 01:10:37.855
字符级模型的初始隐藏表示，01:10:37.855 --> 01:10:40.570
它没有任何进一步的表示01:10:40.570 --> 01:10:43.615
回到word level模型中。01:10:43.615 --> 01:10:47.695
所以它往往不能很好地表现，01:10:47.695 --> 01:10:53.030
获取允许它对名称之类的东西进行翻译的上下文。01:10:53.100 --> 01:10:58.510
好。差不多完成了01:10:58.510 --> 01:11:01.120
这是我之前想说的一件事01:11:01.120 --> 01:11:03.985
这几乎是一件实际的事情。01:11:03.985 --> 01:11:07.465
我们从嵌入词开始，01:11:07.465 --> 01:11:10.510
但是现在我们已经讨论了很多角色级别的模型。01:11:10.510 --> 01:11:15.760
当然，只是为了嵌入文字，你应该能够用它们做一些有用的事情，01:11:15.760 --> 01:11:18.880
用字符或单词。01:11:18.880 --> 01:11:21.340
这是人们开始玩的东西。01:11:21.340 --> 01:11:24.790
所以在这篇曹和瑞的论文中，他们说得很好01:11:24.790 --> 01:11:30.280
让我们用完全相同的方法训练一个Word2vec模型，01:11:30.280 --> 01:11:34.495
就像Word2vec一样，01:11:34.495 --> 01:11:37.795
与其用文字来表达，01:11:37.795 --> 01:11:42.160
让我们从字符序列开始运行01:11:42.160 --> 01:11:47.425
一个双向LSTM来计算单词表示，01:11:47.425 --> 01:11:49.720
然后我们会更有效率01:11:49.720 --> 01:11:52.780
训练我们学习的这个更复杂的模型01:11:52.780 --> 01:12:00.550
字符嵌入和LSTM参数，这将给我们的单词表示。01:12:00.550 --> 01:12:04.480
这是一个人们一直在研究的观点，01:12:04.480 --> 01:12:08.845
我特别想提一下这些FastText嵌入。01:12:08.845 --> 01:12:10.900
几年前，01:12:10.900 --> 01:12:12.820
现在Facebook上的人，01:12:12.820 --> 01:12:16.015
同样是托马斯・米科洛夫创造了最初的单词2vec，01:12:16.015 --> 01:12:18.100
带来了一组新的嵌入，01:12:18.100 --> 01:12:21.340
FastText嵌入，他们的目标是01:12:21.340 --> 01:12:24.880
拥有下一代Word2vec，01:12:24.880 --> 01:12:27.940
这是一种高效的快速，01:12:27.940 --> 01:12:30.535
单词向量学习库，01:12:30.535 --> 01:12:35.215
但对于罕见的单词和具有大量形态学特征的语言来说，它的效果更好。01:12:35.215 --> 01:12:39.010
他们做这件事的方式是他们基本上跳过了2vec这个词01:12:39.010 --> 01:12:43.585
克模型，但他们把它扩大到字符n克。01:12:43.585 --> 01:12:46.450
更准确地说，这就是他们所做的。01:12:46.450 --> 01:12:49.720
所以当你有话要说的时候01:12:49.720 --> 01:12:52.000
我的例子是，01:12:52.000 --> 01:12:57.610
对于某个n克的大小，你把它表示为n克的集合。01:12:57.610 --> 01:13:01.180
这就像我提到的音素01:13:01.180 --> 01:13:05.080
在一开始你有一个边界符号，01:13:05.080 --> 01:13:06.820
所以你知道这个单词的开头。01:13:06.820 --> 01:13:11.965
如果长度是3，你有WH, WHE, HER的开头，01:13:11.965 --> 01:13:14.500
话到此结束，01:13:14.500 --> 01:13:17.695
作为表示的一部分。01:13:17.695 --> 01:13:20.740
然后你有一个额外的单词。01:13:20.740 --> 01:13:24.355
所以在这个模型中仍然有完整的单词表示。01:13:24.355 --> 01:13:28.980
用6个东西表示01:13:28.980 --> 01:13:34.350
然后你要在计算中用到所有这六种方法。01:13:34.350 --> 01:13:37.260
如果你还记得01:13:37.260 --> 01:13:40.590
你所做的就是你正在做的01:13:40.590 --> 01:13:45.585
这些向量点积在上下文表示之间01:13:45.585 --> 01:13:48.240
你的中心词表示。01:13:48.240 --> 01:13:52.080
所以它们会做完全一样的事情，除了for01:13:52.080 --> 01:13:56.870
中心词要用到这六个向量。01:13:56.870 --> 01:14:00.400
所有的向量对应于01:14:00.400 --> 01:14:03.685
这些表示，然后求和。01:14:03.685 --> 01:14:06.910
你只需要做一个简单的求和运算，01:14:06.910 --> 01:14:10.765
这就给了你们相似性的表示。01:14:10.765 --> 01:14:13.180
准确地说，他们并没有那么做01:14:13.180 --> 01:14:15.550
因为这里有一个散列技巧，不过我就不讲了。01:14:15.550 --> 01:14:21.370
但是他们能够展示的是这个模型实际上非常成功。01:14:21.370 --> 01:14:24.340
这些是单词相似度评分，01:14:24.340 --> 01:14:28.480
跳过gram，他们都是CBOW，01:14:28.480 --> 01:14:31.554
这是一种新的模型，01:14:31.554 --> 01:14:36.925
用的是n克。01:14:36.925 --> 01:14:38.755
在这个过程中，01:14:38.755 --> 01:14:40.960
至少对于其中一个英语数据集，01:14:40.960 --> 01:14:42.235
没有比这更好的了。01:14:42.235 --> 01:14:47.830
但是他们特别注意到的是对于那些有更多，01:14:47.830 --> 01:14:52.690
morp-更多的形态，你得到一些相当明显的增益。01:14:52.690 --> 01:14:55.375
70 69到75，01:14:55.375 --> 01:14:58.765
59 60到66，在右边一列，01:14:58.765 --> 01:15:02.440
所以这些单词模型确实给了他们一个更好的模型01:15:02.440 --> 01:15:07.330
单词和快速文本，01:15:07.330 --> 01:15:12.940
图书馆现在有大约60到70种不同语言的单词嵌入，01:15:12.940 --> 01:15:16.930
对于多语言应用程序来说，这是一个很好的词嵌入源。01:15:16.930 --> 01:15:19.735
好了，我想我讲完了。01:15:19.735 --> 01:15:23.000
So thanks a lot and see you again next week.

